<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-07-11</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2303_05086v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Event-based Visual-Inertial Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05086v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05086v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05086v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras are new type vision sensors whose pixels work independently and respond asynchronously to brightness change with microsecond resolution, instead of providing standard intensity frames. Compared with traditional cameras, event-based cameras have low latency, no motion blur, and high dynamic range (HDR), which provide possibilities for robots to deal with some challenging scenes. We propose a visual-inertial odometry for stereo event-based cameras based on Error-State Kalman Filter (ESKF). The visual module updates the pose relies on the edge alignment of a semi-dense 3D map to a 2D image, and the IMU module updates pose by median integral. We evaluate our method on public datasets with general 6-DoF motion and compare the results against ground truth. We show that our proposed pipeline provides improved accuracy over the result of the state-of-the-art visual odometry for stereo event-based cameras, while running in real-time on a standard CPU (low-resolution cameras). To the best of our knowledge, this is the first published visual-inertial odometry for stereo event-based cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05086v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的相机是一种新型的视觉传感器，其像素独立工作，并以微秒的分辨率异步响应亮度变化，而不是提供标准的强度帧。与传统相机相比，基于事件的相机具有低延迟、无运动模糊和高动态范围（HDR），这为机器人处理一些具有挑战性的场景提供了可能性。我们提出了一种基于误差状态卡尔曼滤波器（ESKF）的立体事件相机视觉惯性里程计。视觉模块根据半密集3D地图到2D图像的边缘对齐来更新姿态，IMU模块通过中值积分来更新姿态。我们在具有一般6-DoF运动的公共数据集上评估了我们的方法，并将结果与地面实况进行了比较。我们表明，我们提出的管道在标准CPU（低分辨率相机）上实时运行的同时，为基于立体事件的相机提供了比最先进的视觉里程计结果更高的精度。据我们所知，这是第一个发表的基于立体事件的相机的视觉惯性里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05086v3" target="_blank">2303.05086v3</a>
                              </td>
                              <td>Stereo Event-based Visual-Inertial Odometry</td>
                              <td>Kunfeng Wang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05086v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05086v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_07308v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_07308v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_07308v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuSE，一种新的用于对象的Neural SE（3）-等变量嵌入，并说明了它如何支持对象SLAM在长期场景变化的情况下实现一致的空间理解。NeuSE是根据部分对象观测创建的一组潜在对象嵌入。它充当完整对象模型的紧凑点云代理，对完整形状信息进行编码，同时与物理世界中的对象一起等变地变换SE（3）。使用NeuSE，可以直接从推断的潜在代码中导出相对帧变换。我们提出的SLAM范式，使用NeuSE进行物体形状和姿态表征，可以独立运行或与典型的SLAM系统结合运行。它直接推断SE（3）相机姿势约束，这些约束与通用SLAM姿势图优化兼容，同时还保持了一个适应现实世界变化的轻量级以对象为中心的贴图。我们的方法是在以变化的对象为特征的合成序列和真实世界序列上进行评估的，当单独或与通用SLAM管道联合工作时，显示出改进的定位精度和变化感知映射能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.07308v2" target="_blank">2303.07308v2</a>
                              </td>
                              <td>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</td>
                              <td>Jiahui Fu</td>
                              <td>2023-03-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_07308v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.07308v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08978v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08978v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel visual-LiDAR odometry and mapping method with low-drift characteristics. The proposed method is based on two popular approaches, ORB-SLAM and A-LOAM, with monocular scale correction and visual-bootstrapped LiDAR poses initialization modifications. The scale corrector calculates the proportion between the depth of image keypoints recovered by triangulation and that provided by LiDAR, using an outlier rejection process for accuracy improvement. Concerning LiDAR poses initialization, the visual odometry approach gives the initial guesses of LiDAR motions for better performance. This methodology is not only applicable to high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate the proposed SLAM system's robustness and accuracy, we conducted experiments on the KITTI Odometry and S3E datasets. Experimental results illustrate that our method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore, regarding the accuracy of visual odometry with scale correction, our method performs similarly to the stereo-mode ORB-SLAM2.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08978v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的具有低漂移特性的视觉激光雷达里程计和测绘方法。该方法基于两种流行的方法，ORB-SLAM和A-LOAM，并对单目尺度校正和视觉自举激光雷达姿态初始化进行了修改。比例校正器使用用于提高精度的异常值排除过程来计算通过三角测量恢复的图像关键点的深度与由激光雷达提供的深度之间的比例。关于激光雷达姿态初始化，视觉里程计方法给出了激光雷达运动的初始猜测，以获得更好的性能。该方法不仅适用于高分辨率激光雷达，也适用于低分辨率激光雷达。为了评估所提出的SLAM系统的鲁棒性和准确性，我们在KITTI Odometry和S3E数据集上进行了实验。实验结果表明，我们的方法显著优于独立的ORB-SLAM2和A-LOAM。此外，关于带刻度校正的视觉里程计的准确性，我们的方法与立体模式ORB-SLAM2类似。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08978v2" target="_blank">2304.08978v2</a>
                              </td>
                              <td>Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</td>
                              <td>Hanyu Cai</td>
                              <td>2023-04-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08978v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08978v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03890v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03890v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-quality datasets can speed up breakthroughs and reveal potential developing directions in SLAM research. To support the research on corner cases of visual SLAM systems, this paper presents Ground-Challenge: a challenging dataset comprising 36 trajectories with diverse corner cases such as aggressive motion, severe occlusion, changing illumination, few textures, pure rotation, motion blur, wheel suspension, etc. The dataset was collected by a ground robot with multiple sensors including an RGB-D camera, an inertial measurement unit (IMU), a wheel odometer and a 3D LiDAR. All of these sensors were well-calibrated and synchronized, and their data were recorded simultaneously. To evaluate the performance of cutting-edge SLAM systems, we tested them on our dataset and demonstrated that these systems are prone to drift and fail on specific sequences. We will release the full dataset and relevant materials upon paper publication to benefit the research community. For more information, visit our project website at https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03890v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高质量的数据集可以加快SLAM研究的突破并揭示潜在的发展方向。为了支持对视觉SLAM系统拐角情况的研究，本文提出了Ground Challenge：一个具有挑战性的数据集，包括36条具有不同拐角情况的轨迹，如剧烈运动、严重遮挡、不断变化的照明、少量纹理、纯旋转、运动模糊、车轮悬架等。数据集由地面机器人收集，该机器人具有多个传感器，包括RGB-D相机、惯性测量单元（IMU）、车轮里程表和3D激光雷达。所有这些传感器都经过了良好的校准和同步，并且同时记录了它们的数据。为了评估尖端SLAM系统的性能，我们在数据集上对其进行了测试，并证明这些系统在特定序列上容易漂移和失败。我们将在论文发表后发布完整的数据集和相关材料，以造福研究界。欲了解更多信息，请访问我们的项目网站https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03890v1" target="_blank">2307.03890v1</a>
                              </td>
                              <td>Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</td>
                              <td>Jie Yin</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03890v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03890v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17673v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OSPC: Online Sequential Photometric Calibration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17673v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Photometric calibration is essential to many computer vision applications. One of its key benefits is enhancing the performance of Visual SLAM, especially when it depends on a direct method for tracking, such as the standard KLT algorithm. Another advantage could be in retrieving the sensor irradiance values from measured intensities, as a pre-processing step for some vision algorithms, such as shape-from-shading. Current photometric calibration systems rely on a joint optimization problem and encounter an ambiguity in the estimates, which can only be resolved using ground truth information. We propose a novel method that solves for photometric parameters using a sequential estimation approach. Our proposed method achieves high accuracy in estimating all parameters; furthermore, the formulations are linear and convex, which makes the solution fast and suitable for online applications. Experiments on a Visual Odometry system validate the proposed method and demonstrate its advantages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17673v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光度校准对许多计算机视觉应用至关重要。它的一个关键好处是提高了Visual SLAM的性能，尤其是当它依赖于直接的跟踪方法时，例如标准的KLT算法。另一个优点可以是从测量的强度中检索传感器辐照度值，作为一些视觉算法的预处理步骤，例如来自阴影的形状。当前的光度校准系统依赖于联合优化问题，并且在估计中遇到模糊性，这只能使用地面实况信息来解决。我们提出了一种新的方法，使用顺序估计方法解决光度参数。我们提出的方法在估计所有参数方面实现了高精度；此外，该公式具有线性和凸性，使得求解速度快，适合在线应用。在视觉里程计系统上的实验验证了所提出的方法，并证明了其优点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17673v2" target="_blank">2305.17673v2</a>
                              </td>
                              <td>OSPC: Online Sequential Photometric Calibration</td>
                              <td>Jawad Haidar</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17673v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17673v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01121v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01121v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01121v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几何导航是当今机器人学的一个成熟领域，研究重点正转向更高层次的场景理解，如语义映射。当机器人需要与环境互动时，它必须能够理解周围环境的上下文信息。这项工作的重点是对正在建造或已经建造的地图中的对象进行分类和定位。为了进一步探索这一方向，我们提出了一个框架，该框架可以使用多模态传感器融合方法（结合来自RGB-D相机和激光雷达的RGB和深度数据）在已知环境中自主检测和定位预定义对象。该框架由三个关键元素组成：通过RGB数据了解环境，通过多模态传感器融合估计深度，以及管理伪影（即过滤和稳定测量）。实验表明，所提出的框架可以在真实样本环境中准确检测98%的物体，而无需进行后处理，而85%和80%的物体分别使用单个RGBD相机或RGB+激光雷达装置进行了映射。与单传感器（相机或激光雷达）实验的比较表明，传感器融合使机器人能够准确检测远近障碍物，而在纯视觉或基于激光的方法中，这些障碍物可能会有噪声或不精确。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01121v1" target="_blank">2307.01121v1</a>
                              </td>
                              <td>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</td>
                              <td>Federico Rollo</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01121v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01121v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_10029v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextSLAM: Visual SLAM with Semantic Planar Text Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_10029v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel visual SLAM method that integrates text objects tightly by treating them as semantic features via fully exploring their geometric and semantic prior. The text object is modeled as a texture-rich planar patch whose semantic meaning is extracted and updated on the fly for better data association. With the full exploration of locally planar characteristics and semantic meaning of text objects, the SLAM system becomes more accurate and robust even under challenging conditions such as image blurring, large viewpoint changes, and significant illumination variations (day and night). We tested our method in various scenes with the ground truth data. The results show that integrating texture features leads to a more superior SLAM system that can match images across day and night. The reconstructed semantic 3D text map could be useful for navigation and scene understanding in robotic and mixed reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_10029v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉SLAM方法，该方法通过充分探索文本对象的几何和语义先验，将文本对象视为语义特征，从而紧密地集成文本对象。文本对象被建模为纹理丰富的平面补丁，其语义被实时提取和更新以获得更好的数据关联。随着对文本对象局部平面特征和语义的充分探索，即使在图像模糊、大的视点变化和显著的光照变化（白天和晚上）等具有挑战性的条件下，SLAM系统也变得更加准确和稳健。我们用地面实况数据在各种场景中测试了我们的方法。结果表明，集成纹理特征可以获得更优越的SLAM系统，该系统可以匹配昼夜图像。重建的语义3D文本图可用于机器人和混合现实应用中的导航和场景理解。我们的项目页面：https://github.com/SJTU-ViSYS/TextSLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.10029v2" target="_blank">2305.10029v2</a>
                              </td>
                              <td>TextSLAM: Visual SLAM with Semantic Planar Text Features</td>
                              <td>Boying Li</td>
                              <td>2023-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_10029v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.10029v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00488v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00488v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00488v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在缓慢变化的场景中同时定位和映射（SLAM）对于长期完成机器人任务非常重要。未能检测到场景变化可能会导致地图不准确，最终导致机器人丢失。经典的SLAM算法假设静态场景，最近的工作考虑了动态，但要求在连续帧中观察场景变化。半静态场景中，物体随着时间的推移出现、消失或缓慢移动，通常被忽视，但对长期操作至关重要。我们提出了一个对象感知的因子图SLAM框架，用于跟踪和重建半静态对象级别的变化。我们新的变分期望最大化策略用于优化因子图，该因子图涉及潜在变化对象的高斯均匀双峰测量似然。我们在模拟中评估了我们的方法以及最先进的SLAM解决方案，并在四个月内在仓库中捕获了我们新颖的真实世界SLAM数据集。我们的方法在半静态变化的情况下提高了定位的鲁棒性，提供了关于场景的对象级推理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00488v1" target="_blank">2307.00488v1</a>
                              </td>
                              <td>POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</td>
                              <td>Jingxing Qian</td>
                              <td>2023-07-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00488v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00488v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17529v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17529v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose filter, resulting in improved recall across a range of localization tolerances from $0.25m$ to $5m$, compared to a state-of-the-art baseline single image PnP method and its vanilla pose filtering. Our constraint detection system is active for approximately $35\%$ of the time on the Ford AV dataset and localization is particularly improved when the constraint detection is active.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17529v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数6-DoF定位和SLAM系统使用静态地标，但忽略动态对象，因为它们不能有效地合并到典型的管道中。在包含动态对象的情况下，典型的方法试图对这些对象进行相对复杂的识别和定位，限制了它们的稳健性或通用性。在这项研究中，我们提出了一种中间立场，在自动驾驶汽车的背景下进行了演示，使用动态车辆在6-DoF逐帧PnP-RANSAC定位管道中提供有限的姿态约束信息。我们用运动模型改进了初始姿态估计，并提出了一种计算未来姿态估计预测质量的方法，该方法基于自动驾驶车辆的运动是否受到环境中动态车辆的相对帧间位置的约束而触发。我们的方法检测并识别合适的动态车辆，以定义这些姿势约束，从而修改姿势过滤器，与最先进的基线单图像PnP方法及其普通姿势过滤相比，在25万美元至500万美元的定位公差范围内，提高了召回率。在Ford AV数据集上，我们的约束检测系统在大约$35\%$的时间内是活动的，并且当约束检测是活动的时，定位特别改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17529v1" target="_blank">2306.17529v1</a>
                              </td>
                              <td>Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</td>
                              <td>Stephen Hausler</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17529v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17529v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构从运动技术中假设，除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在可变形环境中的视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'住宅/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16585v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16585v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be released on project page: http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16585v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时语义的可用性大大提高了SLAM系统的核心几何功能，实现了众多机器人和AR/VR应用。我们提出了一种从RGB-D序列进行实时语义映射的新方法，该方法将2D神经网络和基于SLAM系统的3D网络与3D占用映射相结合。在分割新帧时，我们基于可微分渲染从先前帧执行潜在特征重新投影。与独立处理图像的基线相比，将来自先前帧的重新投影的特征图与当前帧特征融合在一起大大提高了图像分割质量。对于3D地图处理，我们提出了一种新的几何准平面过分割方法，该方法根据曲面法线对可能属于相同语义类的3D地图元素进行分组。我们还描述了一种用于轻量级语义图后处理的新型神经网络设计。我们的系统在基于2D-3D网络的系统中实现了最先进的语义映射质量，并在三个真实的室内数据集上匹配3D卷积网络的性能，同时实时工作。此外，与3D细胞神经网络相比，它显示出更好的跨传感器泛化能力，能够使用不同的深度传感器进行训练和推理。代码和数据将在项目页面上发布：http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16585v1" target="_blank">2306.16585v1</a>
                              </td>
                              <td>SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</td>
                              <td>Jingwen Wang</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16585v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16585v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_05916v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_05916v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based visual odometry, TartanVO. TartanVO, like other direct methods rather than feature-based, estimates camera pose through dense optical flow, which only applies to static scenes and disregards dynamic objects. Due to the color constancy assumption, optical flow is not able to differentiate between dynamic and static pixels. Therefore, to reconstruct a static map through such direct methods, our pipeline resolves dynamic/static segmentation by leveraging the optical flow output, and only fuse static points into the map. Moreover, we rerender the input frames such that the dynamic pixels are removed and iteratively pass them back into the visual odometry to refine the pose estimate.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_05916v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于学习的视觉里程计TartanVO的密集动态RGB-D SLAM管道。TartanVO和其他直接方法一样，而不是基于特征的方法，通过密集的光流来估计相机姿态，这只适用于静态场景，而忽略了动态对象。由于颜色恒定性假设，光流无法区分动态像素和静态像素。因此，为了通过这种直接的方法重建静态地图，我们的管道通过利用光流输出来解决动态/静态分割，并且只将静态点融合到地图中。此外，我们重新绘制输入帧，以便去除动态像素，并迭代地将它们传递回视觉里程计，以细化姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.05916v2" target="_blank">2205.05916v2</a>
                              </td>
                              <td>Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</td>
                              <td>Shihao Shen</td>
                              <td>2022-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_05916v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.05916v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16530v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16530v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Using Unmanned Aerial Vehicles (UAVs) to perform high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16530v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用无人机执行高空操纵任务，而不仅仅是被动视觉应用，可以减少人类工人的时间、成本和风险。先前对空中操纵的研究依赖于地面实况状态估计或GPS/全站仪以及一些同时定位和测绘（SLAM）算法，这对于许多接近GPS信号退化或无特征环境的基础设施的应用来说可能不实用。视觉伺服可以避免估计机器人姿态的需要。用于空中操纵的视觉伺服的现有工作要么仅涉及末端执行器位置控制，要么依赖于精确的速度测量和具有已知模式的预定义视觉标记。此外，以前的大多数工作都使用欠驱动无人机，导致末端执行器的机械设计和控制设计复杂。本文利用全驱动无人机开发了一种基于图像的桥梁维护视觉伺服控制策略。主要部件是（1）视觉线检测和跟踪系统，（2）混合阻抗力和运动控制系统。我们的方法既不依赖于来自外部定位系统的机器人姿态/速度估计，也不依赖于预定义的视觉标记。由于完全致动的性质，机械系统和控制器结构的复杂性也被最小化。实验表明，该系统只需对桥梁涂装进行视觉引导，就能有效地进行运动跟踪和力保持。据我们所知，这是第一批使用视觉伺服进行空中操纵的研究之一，该伺服能够在不需要外部姿态/速度信息或预定义视觉引导的情况下实现运动和力控制。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16530v1" target="_blank">2306.16530v1</a>
                              </td>
                              <td>Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</td>
                              <td>Guanqi He</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16530v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16530v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_09553v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_09553v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_09553v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Mu$^｛2｝$SLAM，这是一种多语言序列到序列模型，在100多种语言的自动语音识别（ASR）、自动语音翻译（AST）和机器翻译（MT）中，对未标记语音、未标记文本和监督数据进行联合预训练。通过利用语音的量化表示作为目标，Mu$^{2}$SLAM用解码器上类似于T5的序列到序列掩蔽去噪目标和编码器上的掩蔽语言建模（MLM）目标来训练语音文本模型，用于未标记的语音和文本，同时利用监督任务来改进模型内的跨语言和跨模态表示对齐。在CoVoST AST上，Mu$^｛2｝$SLAM为在公共数据集上训练的模型建立了一个新的最先进的技术，在xx-en翻译上比以前的最佳翻译提高了1.9 BLEU点，在en-xx翻译上提高了1.1 BLEU点。在Voxpopuli ASR上，尽管使用了相对较弱的序列到序列架构，但我们的模型与使用RNN-T解码器微调的mSLAM模型的性能相匹配。在文本理解任务上，我们的模型在XNLI上比mSLAM改进了6\%以上，更接近于XNLI和TydiQA上容量相当的mT5模型的性能，为所有语音和文本理解任务的单一模型铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.09553v2" target="_blank">2212.09553v2</a>
                              </td>
                              <td>Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</td>
                              <td>Yong Cheng</td>
                              <td>2022-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_09553v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.09553v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14812v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14812v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate detection of movable and moving objects in LiDAR is of vital importance for navigation. Most existing works focus on extracting and removing moving objects during navigation. Movable objects like pedestrians, parked vehicles, etc. although static may move in the future. This leads to erroneous navigation and accidents. In such cases, it becomes necessary to detect potentially movable objects. To this end, we present a learning-based approach that segments movable and moving objects by generating static parts of scenes that are otherwise occluded. Our model performs superior to existing baselines on static LiDAR reconstructions using 3 datasets including a challenging sparse industrial dataset. We achieve this without the assistance of any segmentation labels because such labels might not always be available for less popular yet important settings like industrial environments. The non-movable static parts of the scene generated by our model are of vital importance for downstream navigation for SLAM. The movable objects detected by our model can be fed to a downstream 3D detector for aiding navigation. Though we do not use segmentation, we evaluate our method against navigation baselines that use it to remove dynamic objects for SLAM. Through extensive experiments on several datasets, we showcase that our model surpasses these baselines on navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14812v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>激光雷达中可移动和移动物体的精确检测对导航至关重要。现有的大多数工作都集中在导航过程中提取和移除移动对象。可移动的物体，如行人、停放的车辆等，尽管静止的物体将来可能会移动。这会导致错误导航和事故。在这种情况下，有必要检测潜在的可移动物体。为此，我们提出了一种基于学习的方法，通过生成场景中被遮挡的静态部分来分割可移动和移动对象。在使用3个数据集（包括一个具有挑战性的稀疏工业数据集）进行静态激光雷达重建时，我们的模型表现优于现有基线。我们在没有任何细分标签的帮助下实现了这一点，因为这样的标签可能并不总是适用于不太流行但重要的环境，如工业环境。我们的模型生成的场景中不可移动的静态部分对于SLAM的下游导航至关重要。我们的模型检测到的可移动物体可以被馈送到下游的3D检测器，用于辅助导航。虽然我们不使用分割，但我们根据导航基线来评估我们的方法，导航基线使用它来移除SLAM的动态对象。通过在几个数据集上进行广泛的实验，我们展示了我们的模型在导航方面超过了这些基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14812v1" target="_blank">2306.14812v1</a>
                              </td>
                              <td>MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</td>
                              <td>Prashant Kumar</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14812v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14812v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14137v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14137v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid developments of mobile robotics and autonomous navigation over the years are largely empowered by public datasets for testing and upgrading, such as SLAM and localization tasks. Impressive demos and benchmark results have arisen, indicating the establishment of a mature technical framework. However, from the view point of real-world deployments, there are still critical defects of robustness in challenging environments, especially in large-scale, GNSS-denied, textural-monotonous, and unstructured scenarios. To meet the pressing validation demands in such scope, we build a novel challenging robot navigation dataset in a large botanic garden of more than 48000m2. Comprehensive sensors are employed, including high-res/rate stereo Gray&RGB cameras, rotational and forward 3D LiDARs, and low-cost and industrial-grade IMUs, all of which are well calibrated and accurately hardware-synchronized. An all-terrain wheeled robot is configured to mount the sensor suite and provide odometry data. A total of 32 long and short sequences of 2.3 million images are collected, covering scenes of thick woods, riversides, narrow paths, bridges, and grasslands that rarely appeared in previous resources. Excitedly, both highly-accurate ego-motions and 3D map ground truth are provided, along with fine-annotated vision semantics. Our goal is to contribute a high-quality dataset to advance robot navigation and sensor fusion research to a higher level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14137v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多年来，移动机器人和自主导航的快速发展在很大程度上得益于用于测试和升级的公共数据集，如SLAM和定位任务。令人印象深刻的演示和基准测试结果已经出现，表明建立了一个成熟的技术框架。然而，从现实世界部署的角度来看，在具有挑战性的环境中，尤其是在大规模、拒绝全球导航卫星系统、纹理单调和非结构化的场景中，仍然存在稳健性的关键缺陷。为了满足这一范围内紧迫的验证需求，我们在一个超过48000平方米的大型植物园中构建了一个具有挑战性的机器人导航数据集。采用了全面的传感器，包括高分辨率/速率立体声Gray和RGB相机、旋转和正向3D激光雷达，以及低成本和工业级IMU，所有这些都经过了良好的校准和精确的硬件同步。全地形轮式机器人被配置为安装传感器套件并提供里程测量数据。共收集了32个长短序列，共230万幅图像，涵盖了以前资源中很少出现的茂密的树林、河岸、狭窄的小路、桥梁和草地的场景。令人兴奋的是，提供了高度准确的自我运动和3D地图地面实况，以及精细的注释视觉语义。我们的目标是提供高质量的数据集，将机器人导航和传感器融合研究推向更高的水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14137v1" target="_blank">2306.14137v1</a>
                              </td>
                              <td>BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</td>
                              <td>Yuanzhi Liu</td>
                              <td>2023-06-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14137v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14137v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03872v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03872v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop Closure Detection (LCD) is an essential component of visual simultaneous localization and mapping (SLAM) systems. It enables the recognition of previously visited scenes to eliminate pose and map estimate drifts arising from long-term exploration. However, current appearance-based LCD methods face significant challenges, including high computational costs, viewpoint variance, and dynamic objects in scenes. This paper introduced an online appearance based LCD using local superpixel grids descriptor and dynamic node, i.e, LSGDDN-LCD, to find similarities between scenes via hand-crafted features extracted from LSGD. Unlike traditional Bag-of-Words (BoW) based LCD, which requires pre-training, we proposed an adaptive mechanism to group similar images called $\textbf{\textit{dynamic}}$ $\textbf{\textit{node}}$, which incrementally adjusted the database in an online manner, allowing for efficient and online retrieval of previously viewed images without need of the pre-training. Experimental results confirmed that the LSGDDN-LCD significantly improved LCD precision-recall and efficiency, and outperformed several state-of-the-art (SOTA) approaches on multiple typical datasets, indicating its great potential as a generic LCD framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03872v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测（LCD）是视觉同步定位和映射（SLAM）系统的重要组成部分。它能够识别以前访问过的场景，以消除长期勘探产生的姿态和地图估计漂移。然而，当前基于外观的LCD方法面临着重大挑战，包括高计算成本、视点变化和场景中的动态对象。本文介绍了一种基于在线外观的LCD，该LCD使用局部超像素网格描述符和动态节点，即LSGDDN-LCD，通过从LSGD中提取的手工特征来查找场景之间的相似性。与传统的基于单词袋（BoW）的LCD需要预训练不同，我们提出了一种自适应机制来对类似图像进行分组，称为$\textbf｛\textit｛dynamic｝｝$\textbf｛\text it｛node｝}$，该机制以在线方式增量调整数据库，允许在不需要预训练的情况下高效在线检索先前查看的图像。实验结果证实，LSGDDN-LCD显著提高了LCD的查全率和效率，并在多个典型数据集上优于几种最先进的（SOTA）方法，表明其作为通用LCD框架的巨大潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03872v2" target="_blank">2304.03872v2</a>
                              </td>
                              <td>LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</td>
                              <td>Baosheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03872v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03872v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_08207v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_08207v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most feature-based stereo visual odometry (SVO) approaches estimate the motion of mobile robots by matching and tracking point features along a sequence of stereo images. However, in dynamic scenes mainly comprising moving pedestrians, vehicles, etc., there are insufficient robust static point features to enable accurate motion estimation, causing failures when reconstructing robotic motion. In this paper, we proposed DynPL-SVO, a complete dynamic SVO method that integrated united cost functions containing information between matched point features and re-projection errors perpendicular and parallel to the direction of the line features. Additionally, we introduced a \textit{dynamic} \textit{grid} algorithm to enhance its performance in dynamic scenes. The stereo camera motion was estimated through Levenberg-Marquard minimization of the re-projection errors of both point and line features. Comprehensive experimental results on KITTI and EuRoC MAV datasets showed that accuracy of the DynPL-SVO was improved by over 20\% on average compared to other state-of-the-art SVO systems, especially in dynamic scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_08207v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于特征的立体视觉里程计（SVO）方法通过沿着立体图像序列匹配和跟踪点特征来估计移动机器人的运动。然而，在主要包括移动的行人、车辆等的动态场景中，没有足够的鲁棒静态点特征来实现精确的运动估计，导致在重建机器人运动时失败。在本文中，我们提出了DynPL SVO，这是一种完整的动态SVO方法，它集成了包含匹配点特征之间信息的联合代价函数和垂直和平行于线特征方向的重投影误差。此外，我们引入了\textit｛dynamic｝\textit{grid｝算法，以增强其在动态场景中的性能。立体相机的运动是通过Levenberg-Marquard最小化点和线特征的重投影误差来估计的。在KITTI和EuRoC MAV数据集上的综合实验结果表明，与其他最先进的SVO系统相比，DynPL SVO的精度平均提高了20%以上，尤其是在动态场景中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.08207v3" target="_blank">2205.08207v3</a>
                              </td>
                              <td>DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</td>
                              <td>Baosheng Zhang</td>
                              <td>2022-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_08207v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.08207v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2109_12910v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2109_12910v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is one of the essential techniques and functionalities used by robots to perform autonomous navigation tasks. Inspired by the rodent hippocampus, this paper presents a biologically inspired SLAM system based on a LiDAR sensor using a hippocampal model to build a cognitive map and estimate the robot pose in indoor environments. Based on the biologically inspired models mimicking boundary cells, place cells, and head direction cells, the SLAM system using LiDAR point cloud data is capable of leveraging the self-motion cues from the LiDAR odometry and the boundary cues from the LiDAR boundary cells to build a cognitive map and estimate the robot pose. Experiment results show that with the LiDAR boundary cells the proposed SLAM system greatly outperforms the camera-based brain-inspired method in both simulation and indoor environments, and is competitive with the conventional LiDAR-based SLAM methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2109_12910v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人执行自主导航任务的基本技术和功能之一。受啮齿动物海马体的启发，本文提出了一种基于激光雷达传感器的生物启发SLAM系统，该系统使用海马模型构建认知地图并估计机器人在室内环境中的姿势。基于模仿边界细胞、位置细胞和头部方向细胞的生物启发模型，使用激光雷达点云数据的SLAM系统能够利用来自激光雷达里程计的自运动线索和来自激光雷达边界细胞的边界线索来构建认知图并估计机器人姿态。实验结果表明，使用激光雷达边界单元，所提出的SLAM系统在模拟和室内环境中都大大优于基于相机的大脑启发方法，并且与传统的基于激光雷达的SLAM方法具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2109.12910v2" target="_blank">2109.12910v2</a>
                              </td>
                              <td>A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</td>
                              <td>Genghang Zhuang</td>
                              <td>2021-09-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2109_12910v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2109.12910v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_11823v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_11823v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a novel visual simultaneous localization and mapping (SLAM) system called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM), that employs a panoramic camera and a tilted multi-beam LiDAR scanner to generate accurate and metrically-scaled trajectories. RGB-D SLAM was the design basis for HDPV-SLAM, which added depth information to visual features. It aims to solve the two major issues hindering the performance of similar SLAM systems. The first obstacle is the sparseness of LiDAR depth, which makes it difficult to correlate it with the extracted visual features of the RGB image. A deep learning-based depth estimation module for iteratively densifying sparse LiDAR depth was suggested to address this issue. The second issue pertains to the difficulties in depth association caused by a lack of horizontal overlap between the panoramic camera and the tilted LiDAR sensor. To surmount this difficulty, we present a hybrid depth association module that optimally combines depth information estimated by two independent procedures, feature-based triangulation and depth estimation. During a phase of feature tracking, this hybrid depth association module aims to maximize the use of more accurate depth information between the triangulated depth with visual features tracked and the deep learning-based corrected depth. We evaluated the efficacy of HDPV-SLAM using the 18.95 km-long York University and Teledyne Optech (YUTO) MMS dataset. The experimental results demonstrate that the two proposed modules contribute substantially to the performance of HDPV-SLAM, which surpasses that of the state-of-the-art (SOTA) SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_11823v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的视觉同时定位和映射（SLAM）系统，称为混合深度增强全景视觉SLAM（HDPV-SLAM），该系统使用全景相机和倾斜的多光束激光雷达扫描仪来生成精确且按度量比例缩放的轨迹。RGB-D SLAM是HDPV-SLAM的设计基础，它为视觉特征添加了深度信息。它旨在解决阻碍类似SLAM系统性能的两个主要问题。第一个障碍是激光雷达深度的稀疏性，这使得它很难与RGB图像的提取视觉特征相关联。为了解决这个问题，提出了一种基于深度学习的深度估计模块，用于迭代加密稀疏激光雷达深度。第二个问题涉及由于全景相机和倾斜的激光雷达传感器之间缺乏水平重叠而导致的深度关联的困难。为了克服这一困难，我们提出了一种混合深度关联模块，该模块将通过两个独立过程（基于特征的三角测量和深度估计）估计的深度信息进行最佳组合。在特征跟踪阶段，该混合深度关联模块旨在最大限度地利用具有跟踪的视觉特征的三角测量深度和基于深度学习的校正深度之间更准确的深度信息。我们使用长18.95公里的约克大学和Teledyne Optech（YUTO）MMS数据集评估了HDPV-SLAM的疗效。实验结果表明，所提出的两个模块对HDPV-SLAM的性能有很大贡献，超过了最先进的（SOTA）SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.11823v3" target="_blank">2301.11823v3</a>
                              </td>
                              <td>HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</td>
                              <td>Mostafa Ahmadi</td>
                              <td>2023-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_11823v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.11823v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12901v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Map Point Selection for Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12901v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localisation and mapping (SLAM) play a vital role in autonomous robotics. Robotic platforms are often resource-constrained, and this limitation motivates resource-efficient SLAM implementations. While sparse visual SLAM algorithms offer good accuracy for modest hardware requirements, even these more scalable sparse approaches face limitations when applied to large-scale and long-term scenarios. A contributing factor is that the point clouds resulting from SLAM are inefficient to use and contain significant redundancy.   This paper proposes the use of subset selection algorithms to reduce the map produced by sparse visual SLAM algorithms. Information-theoretic techniques have been applied to simpler related problems before, but they do not scale if applied to the full visual SLAM problem. This paper proposes a number of novel information\hyp{}theoretic utility functions for map point selection and optimises these functions using greedy algorithms. The reduced maps are evaluated using practical data alongside an existing visual SLAM implementation (ORB-SLAM 2). Approximate selection techniques proposed in this paper achieve trajectory accuracy comparable to an offline baseline while being suitable for online use. These techniques enable the practical reduction of maps for visual SLAM with competitive trajectory accuracy.   Results also demonstrate that SLAM front-end performance can significantly impact the performance of map point selection. This shows the importance of testing map point selection with a front-end implementation. To exploit this, this paper proposes an approach that includes a model of the front-end in the utility function when additional information is available. This approach outperforms alternatives on applicable datasets and highlights future research directions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12901v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）在自主机器人中发挥着至关重要的作用。机器人平台通常受到资源限制，这种限制促使实现资源高效的SLAM。虽然稀疏视觉SLAM算法为适度的硬件需求提供了良好的准确性，但即使是这些更具可扩展性的稀疏方法在应用于大规模和长期场景时也面临限制。一个促成因素是SLAM产生的点云使用效率低，并且包含显著的冗余。本文提出使用子集选择算法来减少稀疏视觉SLAM算法产生的映射。信息论技术以前曾被应用于更简单的相关问题，但如果应用于全视觉SLAM问题，它们就无法扩展。本文提出了许多新的用于地图点选择的信息论效用函数，并使用贪婪算法对这些函数进行了优化。使用实际数据以及现有的可视化SLAM实现（ORB-SLAM 2）来评估缩减后的映射。本文提出的近似选择技术实现了与离线基线相当的轨迹精度，同时适合在线使用。这些技术能够以具有竞争力的轨迹精度对视觉SLAM的地图进行实际缩减。结果还表明，SLAM前端性能会显著影响地图点选择的性能。这表明了使用前端实现测试地图点选择的重要性。为了利用这一点，本文提出了一种方法，当有额外信息可用时，在效用函数中包括前端的模型。这种方法在适用的数据集上优于其他方法，并突出了未来的研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12901v1" target="_blank">2306.12901v1</a>
                              </td>
                              <td>Map Point Selection for Visual SLAM</td>
                              <td>Christiaan J. Müller</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12901v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12901v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11048v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11048v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11048v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11048v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an uncertainty learning framework for dense neural simultaneous localization and mapping (SLAM). Estimating pixel-wise uncertainties for the depth input of dense SLAM methods allows to re-weigh the tracking and mapping losses towards image regions that contain more suitable information that is more reliable for SLAM. To this end, we propose an online framework for sensor uncertainty estimation that can be trained in a self-supervised manner from only 2D input data. We further discuss the advantages of the uncertainty learning for the case of multi-sensor input. Extensive analysis, experimentation, and ablations show that our proposed modeling paradigm improves both mapping and tracking accuracy and often performs better than alternatives that require ground truth depth or 3D. Our experiments show that we achieve a 38% and 27% lower absolute trajectory tracking error (ATE) on the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset on two types of depth sensors we report an 11% F1-score improvement on RGBD SLAM compared to the recent state-of-the-art neural implicit approaches. Our source code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11048v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一个用于密集神经同时定位和映射（SLAM）的不确定性学习框架。估计密集SLAM方法的深度输入的逐像素不确定性允许对包含更适合SLAM的信息的图像区域的跟踪和映射损失进行重新加权。为此，我们提出了一种用于传感器不确定性估计的在线框架，该框架可以仅从2D输入数据以自监督的方式进行训练。我们进一步讨论了不确定性学习在多传感器输入情况下的优势。广泛的分析、实验和消融表明，我们提出的建模范式提高了测绘和跟踪精度，并且通常比需要地面实况深度或3D的替代方案表现更好。我们的实验表明，在7场景和TUM-RGBD数据集上，我们分别实现了38%和27%的绝对轨迹跟踪误差（ATE）降低。在两种类型的深度传感器的流行副本数据集上，我们报告称，与最近最先进的神经隐式方法相比，RGBD SLAM的F1分数提高了11%。我们的源代码将提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11048v1" target="_blank">2306.11048v1</a>
                              </td>
                              <td>UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM</td>
                              <td>Erik Sandström</td>
                              <td>2023-06-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11048v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10561v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-Based Place Recognition For Autonomous Driving: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10561v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10561v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10561v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based place recognition (LPR) plays a pivotal role in autonomous driving, which assists Simultaneous Localization and Mapping (SLAM) systems in reducing accumulated errors and achieving reliable localization. However, existing reviews predominantly concentrate on visual place recognition (VPR) methods. Despite notable advancements in LPR in recent years, there is yet a systematic review dedicated to this field to the best of our knowledge. This paper bridges the gap by providing a comprehensive review of place recognition methods employing LiDAR sensors, thus facilitating and encouraging further research. We commence by delving into the problem formulation of place recognition and exploring existing challenges, describing relations to previous surveys. Subsequently, we conduct an in-depth review of related research, which offers detailed classifications, strengths and weaknesses, and architectures. Finally, we summarize existing datasets, commonly used evaluation metrics, and comprehensive evaluation results from various methods on public datasets. This paper can serve as a valuable tutorial for newcomers entering the realm of place recognition and researchers interested in long-term robot localization. We pledge to maintain an up-to-date project on our website https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10561v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的位置识别（LPR）在自动驾驶中发挥着关键作用，它有助于同时定位和映射（SLAM）系统减少累积误差，实现可靠的定位。然而，现有的综述主要集中在视觉位置识别（VPR）方法上。尽管近年来LPR取得了显著进步，但据我们所知，仍有专门针对该领域的系统综述。本文通过对使用激光雷达传感器的位置识别方法进行全面综述来弥补这一差距，从而促进和鼓励进一步的研究。我们首先深入研究地点识别的问题公式，探索现有的挑战，描述与以往调查的关系。随后，我们对相关研究进行了深入回顾，提供了详细的分类、优势和劣势以及架构。最后，我们总结了现有的数据集、常用的评估指标，以及在公共数据集上各种方法的综合评估结果。本文可以为进入位置识别领域的新手和对长期机器人定位感兴趣的研究人员提供宝贵的指导。我们保证在我们的网站上保持最新的项目https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10561v1" target="_blank">2306.10561v1</a>
                              </td>
                              <td>LiDAR-Based Place Recognition For Autonomous Driving: A Survey</td>
                              <td>Pengcheng Shi</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10561v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10561v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10463v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10463v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10463v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10463v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous exploration to build a map of an unknown environment is a fundamental robotics problem. However, the quality of the map directly influences the quality of subsequent robot operation. Instability in a simultaneous localization and mapping (SLAM) system can lead to poorquality maps and subsequent navigation failures during or after exploration. This becomes particularly noticeable in consumer robotics, where compute budget and limited field-of-view are very common. In this work, we propose (i) the concept of lighthouses: panoramic views with high visual information content that can be used to maintain the stability of the map locally in their neighborhoods and (ii) the final stabilization strategy for global pose graph stabilization. We call our novel exploration strategy SLAM-aware exploration (SAE) and evaluate its performance on real-world home environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10463v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自主探索构建未知环境的地图是机器人学的一个基本问题。然而，地图的质量直接影响后续机器人操作的质量。同时定位和测绘（SLAM）系统的不稳定性可能导致勘探期间或之后的低质量地图和随后的导航故障。这在消费机器人中变得尤为明显，因为计算预算和有限的视野非常常见。在这项工作中，我们提出了（i）灯塔的概念：具有高视觉信息含量的全景视图，可用于在其邻域内局部保持地图的稳定性；以及（ii）全局姿态图稳定的最终稳定策略。我们将我们的新探索策略称为SLAM感知探索（SAE），并评估其在真实家庭环境中的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10463v1" target="_blank">2306.10463v1</a>
                              </td>
                              <td>Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots</td>
                              <td>Mohit Deshpande</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10463v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10463v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01173v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01173v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01173v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01173v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford and Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01173v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了BAMF-SLAM，这是一种新颖的多鱼眼视觉惯性SLAM系统，它利用束平差（BA）和递归场变换（RFT）在具有挑战性的场景中实现精确和鲁棒的状态估计。首先，我们的系统直接对原始鱼眼图像进行操作，使我们能够充分利用鱼眼相机的宽视场（FoV）。其次，为了克服低纹理的挑战，我们通过统一的因子图探索了多相机输入和互补惯性测量的紧密耦合集成，并联合优化姿态和密集深度图。第三，为了全局一致性，鱼眼相机的宽FoV允许系统找到更多潜在的环路闭合，并且在RFT的宽收敛池的支持下，我们的系统可以执行非常宽的基线环路闭合，几乎没有重叠。此外，我们引入了一种半姿态图BA方法来避免昂贵的全全局BA。通过将相对姿态因子与闭环因子相结合，可以在保持高精度的同时，以适度的内存占用有效地调整全局状态。在TUM-VI、Hilti Oxford和Newer College数据集上的评估表明，与先前的工作相比，所提出的系统具有优越的性能。在2022年希尔蒂SLAM挑战赛中，我们的VIO版本获得了第二名。在随后的提交中，我们的完整系统，包括全球BA后端，优于获胜方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01173v2" target="_blank">2306.01173v2</a>
                              </td>
                              <td>BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</td>
                              <td>Wei Zhang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01173v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01173v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08738v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigation of the Challenges of Underwater-Visual-Monocular-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08738v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08738v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08738v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comprehensive investigation of the challenges of Monocular Visual Simultaneous Localization and Mapping (vSLAM) methods for underwater robots. While significant progress has been made in state estimation methods that utilize visual data in the past decade, most evaluations have been limited to controlled indoor and urban environments, where impressive performance was demonstrated. However, these techniques have not been extensively tested in extremely challenging conditions, such as underwater scenarios where factors such as water and light conditions, robot path, and depth can greatly impact algorithm performance. Hence, our evaluation is conducted in real-world AUV scenarios as well as laboratory settings which provide precise external reference. A focus is laid on understanding the impact of environmental conditions, such as optical properties of the water and illumination scenarios, on the performance of monocular vSLAM methods. To this end, we first show that all methods perform very well in in-air settings and subsequently show the degradation of their performance in challenging underwater environments. The final goal of this study is to identify techniques that can improve accuracy and robustness of SLAM methods in such conditions. To achieve this goal, we investigate the potential of image enhancement techniques to improve the quality of input images used by the SLAM methods, specifically in low visibility and extreme lighting scenarios in scattering media. We present a first evaluation on calibration maneuvers and simple image restoration techniques to determine their ability to enable or enhance the performance of monocular SLAM methods in underwater environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08738v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对水下机器人的单目视觉同时定位和映射（vSLAM）方法的挑战进行了全面的研究。尽管在过去十年中，利用视觉数据的状态估计方法取得了重大进展，但大多数评估仅限于受控的室内和城市环境，在这些环境中表现出了令人印象深刻的性能。然而，这些技术尚未在极具挑战性的条件下进行广泛测试，例如水下场景，其中水和光线条件、机器人路径和深度等因素会极大地影响算法性能。因此，我们的评估是在真实世界的AUV场景以及提供精确外部参考的实验室环境中进行的。重点是了解环境条件对单目vSLAM方法性能的影响，如水和照明场景的光学特性。为此，我们首先证明了所有方法在空中环境中都表现得很好，随后证明了它们在具有挑战性的水下环境中的性能下降。本研究的最终目标是确定在这种情况下可以提高SLAM方法的准确性和稳健性的技术。为了实现这一目标，我们研究了图像增强技术提高SLAM方法使用的输入图像质量的潜力，特别是在散射介质中的低能见度和极端照明场景中。我们首次评估了校准操作和简单的图像恢复技术，以确定它们在水下环境中实现或增强单眼SLAM方法性能的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08738v1" target="_blank">2306.08738v1</a>
                              </td>
                              <td>Investigation of the Challenges of Underwater-Visual-Monocular-SLAM</td>
                              <td>Michele Grimaldi</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08738v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08738v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08531v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FROG: A new people detection dataset for knee-high 2D range finders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08531v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08531v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08531v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Mobile robots require knowledge of the environment, especially of humans located in its vicinity. While the most common approaches for detecting humans involve computer vision, an often overlooked hardware feature of robots for people detection are their 2D range finders. These were originally intended for obstacle avoidance and mapping/SLAM tasks. In most robots, they are conveniently located at a height approximately between the ankle and the knee, so they can be used for detecting people too, and with a larger field of view and depth resolution compared to cameras.   In this paper, we present a new dataset for people detection using knee-high 2D range finders called FROG. This dataset has greater laser resolution, scanning frequency, and more complete annotation data compared to existing datasets such as DROW. Particularly, the FROG dataset contains annotations for 100% of its laser scans (unlike DROW which only annotates 5%), 17x more annotated scans, 100x more people annotations, and over twice the distance traveled by the robot. We propose a benchmark based on the FROG dataset, and analyze a collection of state-of-the-art people detectors based on 2D range finder data.   We also propose and evaluate a new end-to-end deep learning approach for people detection. Our solution works with the raw sensor data directly (not needing hand-crafted input data features), thus avoiding CPU preprocessing and releasing the developer of understanding specific domain heuristics. Experimental results show how the proposed people detector attains results comparable to the state of the art, while an optimized implementation for ROS can operate at more than 500 Hz.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08531v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人需要了解环境，尤其是附近的人类。虽然最常见的检测人类的方法涉及计算机视觉，但用于检测人类的机器人的一个经常被忽视的硬件功能是它们的2D测距仪。这些最初用于避障和测绘/SLAM任务。在大多数机器人中，它们方便地位于脚踝和膝盖之间的高度，因此它们也可以用于检测人，并且与相机相比具有更大的视场和深度分辨率。在本文中，我们提出了一个新的数据集，用于使用膝盖高的二维测距仪进行人员检测，称为FROG。与DROW等现有数据集相比，该数据集具有更高的激光分辨率、扫描频率和更完整的注释数据。特别是，FROG数据集包含100%激光扫描的注释（与仅注释5%的DROW不同）、17倍多的注释扫描、100倍多的人注释，以及超过机器人行进距离两倍的距离。我们提出了一个基于FROG数据集的基准，并基于2D测距仪数据分析了一组最先进的人体探测器。我们还提出并评估了一种新的用于人员检测的端到端深度学习方法。我们的解决方案直接处理原始传感器数据（不需要手工制作的输入数据功能），从而避免了CPU预处理，并释放了理解特定领域启发式的开发人员。实验结果表明，所提出的人员检测器如何获得与现有技术相当的结果，而ROS的优化实现可以在超过500Hz的频率下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08531v1" target="_blank">2306.08531v1</a>
                              </td>
                              <td>FROG: A new people detection dataset for knee-high 2D range finders</td>
                              <td>Fernando Amodeo</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08531v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08531v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08522v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Challenges of Indoor SLAM: A multi-modal multi-floor dataset for SLAM evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08522v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08522v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08522v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robustness in Simultaneous Localization and Mapping (SLAM) remains one of the key challenges for the real-world deployment of autonomous systems. SLAM research has seen significant progress in the last two and a half decades, yet many state-of-the-art (SOTA) algorithms still struggle to perform reliably in real-world environments. There is a general consensus in the research community that we need challenging real-world scenarios which bring out different failure modes in sensing modalities. In this paper, we present a novel multi-modal indoor SLAM dataset covering challenging common scenarios that a robot will encounter and should be robust to. Our data was collected with a mobile robotics platform across multiple floors at Northeastern University's ISEC building. Such a multi-floor sequence is typical of commercial office spaces characterized by symmetry across floors and, thus, is prone to perceptual aliasing due to similar floor layouts. The sensor suite comprises seven global shutter cameras, a high-grade MEMS inertial measurement unit (IMU), a ZED stereo camera, and a 128-channel high-resolution lidar. Along with the dataset, we benchmark several SLAM algorithms and highlight the problems faced during the runs, such as perceptual aliasing, visual degradation, and trajectory drift. The benchmarking results indicate that parts of the dataset work well with some algorithms, while other data sections are challenging for even the best SOTA algorithms. The dataset is available at https://github.com/neufieldrobotics/NUFR-M3F.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08522v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）的鲁棒性仍然是自主系统在现实世界中部署的关键挑战之一。在过去的二十五年里，SLAM研究取得了重大进展，但许多最先进的（SOTA）算法仍难以在现实世界环境中可靠运行。研究界普遍认为，我们需要具有挑战性的现实世界场景，在传感模式中产生不同的故障模式。在本文中，我们提出了一个新的多模态室内SLAM数据集，涵盖了机器人将遇到的具有挑战性的常见场景，并且应该具有鲁棒性。我们的数据是通过东北大学ISEC大楼多个楼层的移动机器人平台收集的。这种多层序列是典型的商业办公空间，其特征是楼层之间的对称性，因此，由于类似的楼层布局，容易出现感知混叠。传感器套件包括七个全球快门相机、一个高级MEMS惯性测量单元（IMU）、一个ZED立体相机和一个128通道高分辨率激光雷达。与数据集一起，我们对几种SLAM算法进行了基准测试，并强调了运行过程中面临的问题，如感知混叠、视觉退化和轨迹漂移。基准测试结果表明，部分数据集与某些算法配合良好，而其他数据部分即使是最好的SOTA算法也具有挑战性。数据集位于https://github.com/neufieldrobotics/NUFR-M3F.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08522v1" target="_blank">2306.08522v1</a>
                              </td>
                              <td>Challenges of Indoor SLAM: A multi-modal multi-floor dataset for SLAM evaluation</td>
                              <td>Pushyami Kaveti</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08522v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08522v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07894v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iSLAM: Imperative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07894v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07894v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07894v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) stands as one of the critical challenges in robot navigation. Recent advancements suggest that methods based on supervised learning deliver impressive performance in front-end odometry, while traditional optimization-based methods still play a vital role in the back-end for minimizing estimation drift. In this paper, we found that such decoupled paradigm can lead to only sub-optimal performance, consequently curtailing system capabilities and generalization potential. To solve this problem, we proposed a novel self-supervised learning framework, imperative SLAM (iSLAM), which fosters reciprocal correction between the front-end and back-end, thus enhancing performance without necessitating any external supervision. Specifically, we formulate a SLAM system as a bi-level optimization problem so that the two components are bidirectionally connected. As a result, the front-end model is able to learn global geometric knowledge obtained through pose graph optimization by back-propagating the residuals from the back-end. This significantly improves the generalization ability of the entire system and thus achieves the accuracy improvement up to 45%. To the best of our knowledge, iSLAM is the first SLAM system showing that the front-end and back-end can learn jointly and mutually contribute to each other in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07894v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人导航中的关键挑战之一。最近的进展表明，基于监督学习的方法在前端里程计中提供了令人印象深刻的性能，而传统的基于优化的方法在后端仍然发挥着至关重要的作用，以最大限度地减少估计漂移。在本文中，我们发现这种解耦范式只能导致次优性能，从而削弱系统能力和泛化潜力。为了解决这个问题，我们提出了一种新的自我监督学习框架，即命令式SLAM（iSLAM），它促进了前端和后端之间的相互校正，从而在不需要任何外部监督的情况下提高了性能。具体来说，我们将SLAM系统公式化为双层优化问题，使两个组件双向连接。因此，前端模型能够通过从后端反向传播残差来学习通过位姿图优化获得的全局几何知识。这显著提高了整个系统的泛化能力，从而实现了高达45%的精度提高。据我们所知，iSLAM是第一个SLAM系统，表明前端和后端可以以自我监督的方式共同学习并相互贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07894v2" target="_blank">2306.07894v2</a>
                              </td>
                              <td>iSLAM: Imperative SLAM</td>
                              <td>Taimeng Fu</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07894v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07894v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07363v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">H-SLAM: Hybrid Direct-Indirect Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07363v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07363v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07363v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent success of hybrid methods in monocular odometry has led to many attempts to generalize the performance gains to hybrid monocular SLAM. However, most attempts fall short in several respects, with the most prominent issue being the need for two different map representations (local and global maps), with each requiring different, computationally expensive, and often redundant processes to maintain. Moreover, these maps tend to drift with respect to each other, resulting in contradicting pose and scene estimates, and leading to catastrophic failure. In this paper, we propose a novel approach that makes use of descriptor sharing to generate a single inverse depth scene representation. This representation can be used locally, queried globally to perform loop closure, and has the ability to re-activate previously observed map points after redundant points are marginalized from the local map, eliminating the need for separate and redundant map maintenance processes. The maps generated by our method exhibit no drift between each other, and can be computed at a fraction of the computational cost and memory footprint required by other monocular SLAM systems. Despite the reduced resource requirements, the proposed approach maintains its robustness and accuracy, delivering performance comparable to state-of-the-art SLAM methods (e.g., LDSO, ORB-SLAM3) on the majority of sequences from well-known datasets like EuRoC, KITTI, and TUM VI. The source code is available at: https://github.com/AUBVRL/fslam_ros_docker.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07363v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近混合方法在单目里程计中的成功导致了许多将性能增益推广到混合单目SLAM的尝试。然而，大多数尝试在几个方面都达不到要求，最突出的问题是需要两种不同的地图表示（局部地图和全局地图），每种地图表示都需要不同的、计算成本高昂且经常冗余的过程来维护。此外，这些地图往往会相互漂移，导致姿势和场景估计相互矛盾，并导致灾难性的失败。在本文中，我们提出了一种新的方法，该方法利用描述符共享来生成单个反向深度场景表示。这种表示可以在本地使用，全局查询以执行循环闭合，并且在冗余点从本地地图边缘化后，能够重新激活先前观察到的地图点，从而消除了对单独和冗余地图维护过程的需要。由我们的方法生成的映射彼此之间没有漂移，并且可以以其他单目SLAM系统所需的计算成本和内存占用的一小部分来计算。尽管减少了资源需求，但所提出的方法保持了其稳健性和准确性，在EuRoC、KITTI和TUM VI等知名数据集的大多数序列上提供了与最先进的SLAM方法（例如LDSO、ORB-SLAM3）相当的性能。源代码位于：https://github.com/AUBVRL/fslam_ros_docker.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07363v1" target="_blank">2306.07363v1</a>
                              </td>
                              <td>H-SLAM: Hybrid Direct-Indirect Visual SLAM</td>
                              <td>Georges Younes</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07363v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07363v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06850v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Volume-DROID: A Real-Time Implementation of Volumetric Mapping with DROID-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06850v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06850v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06850v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Volume-DROID, a novel approach for Simultaneous Localization and Mapping (SLAM) that integrates Volumetric Mapping and Differentiable Recurrent Optimization-Inspired Design (DROID). Volume-DROID takes camera images (monocular or stereo) or frames from a video as input and combines DROID-SLAM, point cloud registration, an off-the-shelf semantic segmentation network, and Convolutional Bayesian Kernel Inference (ConvBKI) to generate a 3D semantic map of the environment and provide accurate localization for the robot. The key innovation of our method is the real-time fusion of DROID-SLAM and Convolutional Bayesian Kernel Inference (ConvBKI), achieved through the introduction of point cloud generation from RGB-Depth frames and optimized camera poses. This integration, engineered to enable efficient and timely processing, minimizes lag and ensures effective performance of the system. Our approach facilitates functional real-time online semantic mapping with just camera images or stereo video input. Our paper offers an open-source Python implementation of the algorithm, available at https://github.com/peterstratton/Volume-DROID.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06850v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了体积DROID，这是一种同时定位和映射（SLAM）的新方法，它集成了体积映射和可微递归优化启发设计（DROID）。Volume DROID从视频中获取相机图像（单眼或立体）或帧作为输入，并结合DROID-SLAM、点云配准、现成的语义分割网络和卷积贝叶斯核推断（ConvBKI）来生成环境的3D语义图，并为机器人提供准确的定位。我们方法的关键创新是DROID-SLAM和卷积贝叶斯核推断（ConvBKI）的实时融合，通过引入RGB深度帧的点云生成和优化的相机姿态来实现。这种集成旨在实现高效和及时的处理，最大限度地减少了延迟，并确保了系统的有效性能。我们的方法只需相机图像或立体视频输入，即可实现功能实时在线语义映射。我们的论文提供了该算法的开源Python实现，可在https://github.com/peterstratton/Volume-DROID.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06850v1" target="_blank">2306.06850v1</a>
                              </td>
                              <td>Volume-DROID: A Real-Time Implementation of Volumetric Mapping with DROID-SLAM</td>
                              <td>Peter Stratton</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06850v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06850v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02395v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NICE-SLAM with Adaptive Feature Grids</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02395v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02395v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02395v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM is a dense visual SLAM system that combines the advantages of neural implicit representations and hierarchical grid-based scene representation. However, the hierarchical grid features are densely stored, leading to memory explosion problems when adapting the framework to large scenes. In our project, we present sparse NICE-SLAM, a sparse SLAM system incorporating the idea of Voxel Hashing into NICE-SLAM framework. Instead of initializing feature grids in the whole space, voxel features near the surface are adaptively added and optimized. Experiments demonstrated that compared to NICE-SLAM algorithm, our approach takes much less memory and achieves comparable reconstruction quality on the same datasets. Our implementation is available at https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02395v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM是一个密集的视觉SLAM系统，它结合了神经隐式表示和基于层次网格的场景表示的优点。然而，分层网格特征存储密集，导致在将框架适应大型场景时出现内存爆炸问题。在我们的项目中，我们提出了稀疏NICE-SLAM，这是一个稀疏SLAM系统，将体素哈希的思想结合到NICE-SLM框架中。自适应地添加和优化曲面附近的体素特征，而不是初始化整个空间中的特征网格。实验表明，与NICE-SLAM算法相比，我们的方法占用的内存要少得多，并且在相同的数据集上实现了相当的重建质量。我们的实施可在https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02395v2" target="_blank">2306.02395v2</a>
                              </td>
                              <td>NICE-SLAM with Adaptive Feature Grids</td>
                              <td>Ganlin Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02395v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02395v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_03806v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLaM: Student-Label Mixing for Distillation with Unlabeled Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_03806v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_03806v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_03806v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with margin under random classification noise, and provide the first convergence analysis for so-called ``forward loss-adjustment" methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_03806v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在标记数据量有限但可以访问大量未标记数据的应用中，使用未标记示例进行知识提取是生成紧凑和轻量级学生模型的强大训练范式。在这种设置中，大型教师模型为未标记的数据集生成“软”伪标签，然后用于训练学生模型。尽管这种方法在各种应用中都取得了成功，但它的一个缺点是，教师的伪标签往往很嘈杂，导致学生的表现受损。在本文中，我们提出了一种带有未标记示例的知识提取的原则性方法，我们称之为学生标签混合（SLaM），并通过在几个标准基准上对其进行评估，表明它比以前的方法持续改进。最后，我们证明了SLaM是有理论保证的；在此基础上，我们给出了一种改进随机分类噪声下带裕度半空间学习的已知样本复杂度的算法，并首次对所谓的前向损失调整方法进行了收敛性分析。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.03806v2" target="_blank">2302.03806v2</a>
                              </td>
                              <td>SLaM: Student-Label Mixing for Distillation with Unlabeled Examples</td>
                              <td>Vasilis Kontonis</td>
                              <td>2023-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_03806v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.03806v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04570v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04570v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In many robotics problems, there is a significant gain in collaborative information sharing between multiple robots, for exploration, search and rescue, tracking multiple targets, or mapping large environments. One of the key implicit assumptions when solving cooperative multi-robot problems is that all robots use the same (homogeneous) underlying algorithm. However, in practice, we want to allow collaboration between robots possessing different capabilities and that therefore must rely on heterogeneous algorithms. We present a system architecture and the supporting theory, to enable collaboration in a decentralized network of robots, where each robot relies on different estimation algorithms. To develop our approach, we focus on multi-robot simultaneous localization and mapping (SLAM) with multi-target tracking. Our theoretical framework builds on our idea of exploiting the conditional independence structure inherent to many robotics applications to separate between each robot's local inference (estimation) tasks and fuse only relevant parts of their non-equal, but overlapping probability density function (pdfs). We present a new decentralized graph-based approach to the multi-robot SLAM and tracking problem. We leverage factor graphs to split between different parts of the problem for efficient data sharing between robots in the network while enabling robots to use different local sparse landmark/dense/metric-semantic SLAM algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04570v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在许多机器人问题中，多个机器人之间的协作信息共享有很大的好处，用于探索、搜索和救援、跟踪多个目标或绘制大型环境地图。解决多机器人协作问题时的一个关键隐含假设是，所有机器人都使用相同（同质）的底层算法。然而，在实践中，我们希望允许具有不同能力的机器人之间进行协作，因此必须依赖于异构算法。我们提出了一种系统架构和支持理论，以实现分散机器人网络中的协作，其中每个机器人依赖于不同的估计算法。为了开发我们的方法，我们专注于具有多目标跟踪的多机器人同时定位和映射（SLAM）。我们的理论框架建立在我们的想法之上，即利用许多机器人应用程序固有的条件独立性结构，在每个机器人的局部推理（估计）任务之间进行分离，并仅融合其不相等但重叠的概率密度函数（pdfs）的相关部分。我们提出了一种新的基于分散图的方法来解决多机器人SLAM和跟踪问题。我们利用因子图在问题的不同部分之间进行划分，以实现网络中机器人之间的有效数据共享，同时使机器人能够使用不同的局部稀疏地标/密集/度量语义SLAM算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04570v1" target="_blank">2306.04570v1</a>
                              </td>
                              <td>Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</td>
                              <td>Ofer Dagan</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04570v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04570v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03953v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03953v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is the task of building a map representation of an unknown environment while it at the same time is used for positioning. A probabilistic interpretation of the SLAM task allows for incorporating prior knowledge and for operation under uncertainty. Contrary to the common practice of computing point estimates of the system states, we capture the full posterior density through approximate Bayesian inference. This dynamic learning task falls under state estimation, where the state-of-the-art is in sequential Monte Carlo methods that tackle the forward filtering problem. In this paper, we introduce a framework for probabilistic SLAM using particle smoothing that does not only incorporate observed data in current state estimates, but it also back-tracks the updated knowledge to correct for past drift and ambiguities in both the map and in the states. Our solution can efficiently handle both dense and sparse map representations by Rao-Blackwellization of conditionally linear and conditionally linearized models. We show through simulations and real-world experiments how the principles apply to radio (BLE/Wi-Fi), magnetic field, and visual SLAM. The proposed solution is general, efficient, and works well under confounding noise.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03953v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是在用于定位的同时构建未知环境的地图表示的任务。SLAM任务的概率解释允许结合先验知识并在不确定性下进行操作。与计算系统状态的点估计的常见做法相反，我们通过近似贝叶斯推理来获取完整的后验密度。这种动态学习任务属于状态估计，其中最先进的是解决前向滤波问题的顺序蒙特卡罗方法。在本文中，我们介绍了一种使用粒子平滑的概率SLAM框架，该框架不仅将观测数据纳入当前状态估计，而且还对更新后的知识进行回溯，以纠正地图和状态中过去的漂移和模糊性。我们的解决方案可以通过条件线性化和条件线性化模型的Rao-Blackwell化有效地处理密集和稀疏映射表示。我们通过模拟和真实世界的实验展示了这些原理如何应用于无线电（BLE/Wi-Fi）、磁场和视觉SLAM。所提出的解决方案是通用的、有效的，并且在混杂噪声下工作良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03953v1" target="_blank">2306.03953v1</a>
                              </td>
                              <td>Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</td>
                              <td>Manon Kok</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03953v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03953v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2102_07448v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2102_07448v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Surround View fisheye cameras are commonly deployed in automated driving for 360\deg{} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2102_07448v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环绕视图鱼眼摄像头通常部署在自动驾驶中，用于车辆周围的360度近场传感。这项工作在未修正的鱼眼图像上提出了一个多任务视觉感知网络，使车辆能够感知周围环境。它由自动驾驶系统所需的六项主要任务组成：深度估计、视觉里程计、语义分割、运动分割、物体检测和镜头污染检测。我们证明，联合训练的模型比各自的单任务版本表现得更好。我们的多任务模型具有一个共享编码器，提供了显著的计算优势，并具有任务相互支持的协同解码器。我们提出了一种新的基于相机几何的自适应机制来对鱼眼失真模型进行训练和推理编码。这对于在WoodScape数据集上进行训练至关重要，该数据集由安装在三辆不同汽车上的12台不同相机收集的来自世界不同地区的数据组成，具有不同的本质和视角。考虑到边界框不是失真鱼眼图像的良好表示，我们还将对象检测扩展到使用具有非均匀采样顶点的多边形。此外，我们还在标准汽车数据集上评估了我们的模型，即KITTI和Cityscapes。我们在深度估计和姿态估计任务的KITTI上获得了最先进的结果，并在其他任务上获得了有竞争力的性能。我们对各种架构选择和任务加权方法进行了广泛的消融研究。上的短视频https://youtu.be/xbSjZ5OfPes提供了定性结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2102.07448v3" target="_blank">2102.07448v3</a>
                              </td>
                              <td>OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2021-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2102_07448v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2102.07448v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2007_06676v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2007_06676v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190$^\circ$ horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet. Qualitative results on a distorted test scene video sequence indicate excellent performance https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2007_06676v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在经典的计算机视觉中，校正是多视图深度估计的一个组成部分。它通常包括对极校正和透镜畸变校正。该过程大大简化了深度估计，因此已被CNN方法所采用。然而，校正有几个副作用，包括视场（FOV）减小、重采样失真和对校准误差的敏感性。在严重失真的情况下，效果尤其明显（例如，广角鱼眼相机）。在本文中，我们提出了一种通用的尺度感知自监督管道，用于从未修正的单目视频中估计深度、欧氏距离和视觉里程。我们在未校正的KITTI数据集上证明了类似的精度水平，桶形失真与校正的KITT数据集相当。直觉是，校正步骤可以隐含地吸收在CNN模型中，CNN模型在不增加复杂性的情况下学习失真模型。我们的方法不会减少视野，并避免了推理时校正的计算成本。为了进一步说明所提出的框架的普遍适用性，我们将其应用于水平视场为190$^\circ$的广角鱼眼相机。训练框架UnRectDepthNet以相机失真模型为自变量，并相应地调整投影和非投影函数。在KITTI校正的数据集上进一步评估了所提出的算法，我们获得了最先进的结果，改进了我们之前的工作FisheyeDistanceNet。失真的测试场景视频序列的定性结果表明性能优异https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2007.06676v4" target="_blank">2007.06676v4</a>
                              </td>
                              <td>UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2020-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2007_06676v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2007.06676v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03660v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PQM: A Point Quality Evaluation Metric for Dense Maps</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03660v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based mapping/reconstruction are important for various applications, but evaluating the quality of the dense maps they produce is challenging. The current methods have limitations, including the inability to capture completeness, structural information, and local variations in error. In this paper, we propose a novel point quality evaluation metric (PQM) that consists of four sub-metrics to provide a more comprehensive evaluation of point cloud quality. The completeness sub-metric evaluates the proportion of missing data, the artifact score sub-metric recognizes and characterizes artifacts, the accuracy sub-metric measures registration accuracy, and the resolution sub-metric quantifies point cloud density. Through an ablation study using a prototype dataset, we demonstrate the effectiveness of each of the sub-metrics and compare them to popular point cloud distance measures. Using three LiDAR SLAM systems to generate maps, we evaluate their output map quality and demonstrate the metrics robustness to noise and artifacts. Our implementation of PQM, datasets and detailed documentation on how to integrate with your custom dense mapping pipeline can be found at github.com/droneslab/pqm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03660v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的地图绘制/重建对于各种应用都很重要，但评估它们生成的密集地图的质量是一项挑战。目前的方法有局限性，包括无法捕获完整性、结构信息和错误的局部变化。在本文中，我们提出了一种新的点质量评估度量（PQM），该度量由四个子度量组成，以提供对点云质量的更全面的评估。完整性子度量评估缺失数据的比例，伪影得分子度量识别和表征伪影，准确性子度量测量配准准确性，分辨率子度量量化点云密度。通过使用原型数据集进行消融研究，我们证明了每个子指标的有效性，并将其与流行的点云距离测量进行了比较。使用三个激光雷达SLAM系统生成地图，我们评估了它们的输出地图质量，并证明了它们对噪声和伪影的鲁棒性。我们的PQM实施、数据集以及关于如何与您的自定义密集映射管道集成的详细文档，请访问github.com/droneslab/PQM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03660v1" target="_blank">2306.03660v1</a>
                              </td>
                              <td>PQM: A Point Quality Evaluation Metric for Dense Maps</td>
                              <td>Yash Turkar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03660v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03660v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01891v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01891v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a robust approach for a visual parallel tracking and mapping (PTAM) system that excels in challenging environments. Our proposed method combines the strengths of heterogeneous multi-modal visual sensors, including stereo event-based and frame-based sensors, in a unified reference frame through a novel spatio-temporal synchronization of stereo visual frames and stereo event streams. We employ deep learning-based feature extraction and description for estimation to enhance robustness further. We also introduce an end-to-end parallel tracking and mapping optimization layer complemented by a simple loop-closure algorithm for efficient SLAM behavior. Through comprehensive experiments on both small-scale and large-scale real-world sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM) demonstrates superior performance compared to state-of-the-art methods in terms of robustness and accuracy in adverse conditions. Our implementation's research-based Python API is publicly available on GitHub for further research and development: https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01891v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于视觉并行跟踪和映射（PTAM）系统的鲁棒方法，该系统在具有挑战性的环境中表现出色。我们提出的方法通过立体视觉帧和立体事件流的新颖时空同步，在统一的参考系中结合了异构多模态视觉传感器的优势，包括基于立体事件和基于帧的传感器。我们采用基于深度学习的特征提取和描述进行估计，以进一步增强鲁棒性。我们还引入了一个端到端并行跟踪和映射优化层，并辅以一个简单的闭环算法，以实现高效的SLAM行为。通过对VECtor和TUM-VIE基准的小规模和大规模真实世界序列的综合实验，我们提出的方法（DH-PTAM）在不利条件下的稳健性和准确性方面优于最先进的方法。我们的实现基于研究的Python API在GitHub上公开提供，用于进一步的研究和开发：https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01891v1" target="_blank">2306.01891v1</a>
                              </td>
                              <td>DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</td>
                              <td>Abanob Soliman</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01891v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01891v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01188v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Visual Odometry with Full Temporal Resolution via Continuous-time Gaussian Process Regression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01188v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01188v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01188v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time.   Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but sacrifices the inherent temporal resolution of event-based cameras.   This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01188v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的摄影机异步捕捉场景中的各个视觉变化。这使得它们比传统的基于帧的相机对高动态运动和较差的照明更具鲁棒性。这也意味着场景中的每一次测量都可以在唯一的时间发生。处理这些不同的测量时间是使用基于事件的相机的一个主要挑战。它通常在视觉里程计（VO）管道中通过将时间上的近距离测量近似为在一个公共时间发生来解决。这种分组简化了估计问题，但牺牲了基于事件的摄像机固有的时间分辨率。相反，本文提出了一个完整的立体VO管道，该管道直接估计单个事件的测量时间，而不需要任何分组或近似。它使用连续时间轨迹估计，通过具有物理动机先验的高斯过程回归来保持基于事件的相机的时间保真度和异步性质。它的性能在MVSEC数据集上进行了评估，在两个独立序列上实现了7.9e-3和5.9e-3 RMS相对误差，分别比现有的公开的基于事件的立体声VO流水线高出两倍和四倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01188v1" target="_blank">2306.01188v1</a>
                              </td>
                              <td>Event-based Visual Odometry with Full Temporal Resolution via Continuous-time Gaussian Process Regression</td>
                              <td>Jianeng Wang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01188v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01188v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00579v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00579v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00579v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00579v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce FMapping, an efficient neural field mapping framework that facilitates the continuous estimation of a colorized point cloud map in real-time dense RGB SLAM. To achieve this challenging goal without depth, a hurdle is how to improve efficiency and reduce the mapping uncertainty of the RGB SLAM system. To this end, we first build up a theoretical analysis by decomposing the SLAM system into tracking and mapping parts, and the mapping uncertainty is explicitly defined within the frame of neural representations. Based on the analysis, we then propose an effective factorization scheme for scene representation and introduce a sliding window strategy to reduce the uncertainty for scene reconstruction. Specifically, we leverage the factorized neural field to decompose uncertainty into a lower-dimensional space, which enhances robustness to noise and improves training efficiency. We then propose the sliding window sampler to reduce uncertainty by incorporating coherent geometric cues from observed frames during map initialization to enhance convergence. Our factorized neural mapping approach enjoys some advantages, such as low memory consumption, more efficient computation, and fast convergence during map initialization. Experiments on two benchmark datasets show that our method can update the map of high-fidelity colorized point clouds around 2 seconds in real time while requiring no customized CUDA kernels. Additionally, it utilizes x20 fewer parameters than the most concise neural implicit mapping of prior methods for SLAM, e.g., iMAP [ 31] and around x1000 fewer parameters than the state-of-the-art approach, e.g., NICE-SLAM [ 42]. For more details, please refer to our project homepage: https://vlis2022.github.io/fmap/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00579v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了FMapping，这是一种高效的神经场映射框架，有助于在实时密集RGB SLAM中连续估计彩色点云图。为了在没有深度的情况下实现这一具有挑战性的目标，一个障碍是如何提高效率并减少RGB SLAM系统的映射不确定性。为此，我们首先通过将SLAM系统分解为跟踪和映射部分来建立理论分析，并在神经表示的框架内明确定义映射的不确定性。在此基础上，我们提出了一种有效的场景表示因子分解方案，并引入了一种滑动窗口策略来减少场景重建的不确定性。具体来说，我们利用因子化的神经场将不确定性分解到低维空间中，这增强了对噪声的鲁棒性，并提高了训练效率。然后，我们提出了滑动窗口采样器，通过在地图初始化期间结合来自观测帧的相干几何线索来提高收敛性，从而降低不确定性。我们的因子分解神经映射方法具有内存消耗低、计算效率高、映射初始化收敛快等优点。在两个基准数据集上的实验表明，我们的方法可以实时更新高保真彩色点云的地图约2秒，而不需要定制CUDA内核。此外，它使用的参数比SLAM的现有方法的最简洁的神经隐式映射（例如iMAP[31]）少x20个，比现有技术的方法（例如NICE-SLAM[42]）少大约x1000个。有关更多详细信息，请参阅我们的项目主页：https://vlis2022.github.io/fmap/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00579v1" target="_blank">2306.00579v1</a>
                              </td>
                              <td>FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM</td>
                              <td>Tongyan Hua</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00579v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00579v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_12185v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implementation of a Blind navigation method in outdoors/indoors areas</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_12185v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_12185v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_12185v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>According to WHO statistics, the number of visually impaired people is increasing annually. One of the most critical necessities for visually impaired people is the ability to navigate safely. This paper proposes a navigation system based on the visual slam and Yolo algorithm using monocular cameras. The proposed system consists of three steps: obstacle distance estimation, path deviation detection, and next-step prediction. Using the ORB-SLAM algorithm, the proposed method creates a map from a predefined route and guides the users to stay on the route while notifying them if they deviate from it. Additionally, the system utilizes the YOLO algorithm to detect obstacles along the route and alert the user. The experimental results, obtained by using a laptop camera, show that the proposed system can run in 30 frame per second while guiding the user within predefined routes of 11 meters in indoors and outdoors. The accuracy of the positioning system is 8cm, and the system notifies the users if they deviate from the predefined route by more than 60 cm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_12185v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据世界卫生组织的统计，视障人数每年都在增加。视障人士最重要的必需品之一是安全导航的能力。本文提出了一种基于视觉slam和Yolo算法的单目相机导航系统。所提出的系统包括三个步骤：障碍物距离估计、路径偏差检测和下一步预测。该方法利用ORB-SLAM算法，根据预先定义的路线创建地图，引导用户停留在路线上，同时在偏离路线时通知他们。此外，该系统还利用YOLO算法检测路线上的障碍物并提醒用户。使用笔记本电脑摄像头获得的实验结果表明，所提出的系统可以以每秒30帧的速度运行，同时在室内外11米的预定路线内引导用户。定位系统的精度为8cm，如果用户偏离预定路线超过60cm，系统会通知用户。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.12185v2" target="_blank">2212.12185v2</a>
                              </td>
                              <td>Implementation of a Blind navigation method in outdoors/indoors areas</td>
                              <td>Mohammad Javadian Farzaneh</td>
                              <td>2022-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_12185v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.12185v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11654v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM: A Review On Last Decade</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11654v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11654v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11654v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article presents a novel review of Active SLAM (A-SLAM) research conducted in the last decade. We discuss the formulation, application, and methodology applied in A-SLAM for trajectory generation and control action selection using information theory based approaches. Our extensive qualitative and quantitative analysis highlights the approaches, scenarios, configurations, types of robots, sensor types, dataset usage, and path planning approaches of A-SLAM research. We conclude by presenting the limitations and proposing future research possibilities. We believe that this survey will be helpful to researchers in understanding the various methods and techniques applied to A-SLAM formulation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11654v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对近十年来进行的主动SLAM（a-SLAM）研究进行了新的综述。我们使用基于信息论的方法讨论了A-SLAM中用于轨迹生成和控制动作选择的公式、应用和方法。我们广泛的定性和定量分析强调了A-SLAM研究的方法、场景、配置、机器人类型、传感器类型、数据集使用和路径规划方法。最后，我们介绍了研究的局限性，并提出了未来研究的可能性。我们相信，这项调查将有助于研究人员了解应用于A-SLAM配方的各种方法和技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11654v2" target="_blank">2212.11654v2</a>
                              </td>
                              <td>Active SLAM: A Review On Last Decade</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11654v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11654v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_03323v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_03323v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_03323v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_03323v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous vehicles must often contend with conflicting planning requirements, e.g., safety and comfort could be at odds with each other if avoiding a collision calls for slamming the brakes. To resolve such conflicts, assigning importance ranking to rules (i.e., imposing a rule hierarchy) has been proposed, which, in turn, induces rankings on trajectories based on the importance of the rules they satisfy. On one hand, imposing rule hierarchies can enhance interpretability, but introduce combinatorial complexity to planning; while on the other hand, differentiable reward structures can be leveraged by modern gradient-based optimization tools, but are less interpretable and unintuitive to tune. In this paper, we present an approach to equivalently express rule hierarchies as differentiable reward structures amenable to modern gradient-based optimizers, thereby, achieving the best of both worlds. We achieve this by formulating rank-preserving reward functions that are monotonic in the rank of the trajectories induced by the rule hierarchy; i.e., higher ranked trajectories receive higher reward. Equipped with a rule hierarchy and its corresponding rank-preserving reward function, we develop a two-stage planner that can efficiently resolve conflicting planning requirements. We demonstrate that our approach can generate motion plans in ~7-10 Hz for various challenging road navigation and intersection negotiation scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_03323v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动驾驶汽车必须经常应对相互冲突的规划要求，例如，如果避免碰撞需要急刹车，安全性和舒适性可能会相互矛盾。为了解决这种冲突，已经提出了将重要性排序分配给规则（即，强加规则层次结构），这反过来又根据它们所满足的规则的重要性对轨迹进行排序。一方面，强加规则层次结构可以提高可解释性，但会给规划带来组合复杂性；而另一方面，现代基于梯度的优化工具可以利用可微的奖励结构，但其可解释性较差，难以调整。在本文中，我们提出了一种方法，将规则层次等效地表示为适用于现代基于梯度的优化器的可微奖励结构，从而实现两全其美。我们通过制定秩保持奖励函数来实现这一点，该函数在由规则层次引起的轨迹的秩中是单调的；即排名更高的轨迹获得更高的奖励。配备了规则层次结构及其相应的保秩奖励函数，我们开发了一个两阶段规划器，可以有效地解决冲突的规划需求。我们证明，我们的方法可以为各种具有挑战性的道路导航和交叉口协商场景生成7-10Hz的运动计划。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.03323v2" target="_blank">2212.03323v2</a>
                              </td>
                              <td>Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</td>
                              <td>Sushant Veer</td>
                              <td>2022-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_03323v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.03323v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_04286v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Simulation of Dynamic Environments for SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_04286v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_04286v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_04286v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rrSimulation of Dynamic Environments for SLAM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_04286v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>仿真引擎在机器人技术中被广泛采用。然而，它们要么缺乏完全的模拟控制，ROS集成，逼真的物理，要么缺乏真实感。近年来，合成数据生成和真实感绘制具有诸如目标跟踪和人体姿态估计等高级任务。然而，当专注于视觉应用时，通常缺乏传感器测量或时间连续性等信息。另一方面，大多数机器人任务的模拟都是在（半）静态环境中进行的，具有特定的传感器和低视觉保真度。为了解决这个问题，我们在之前的工作中引入了一个完全可定制的框架，用于生成逼真的动画动态环境（GRADE）[1]。我们使用GRADE生成室内动态环境数据集，然后比较不同序列上的多种SLAM算法。通过这样做，我们展示了当前的研究是如何过度依赖于已知的基准，而不能概括。我们用改进的YOLO和Mask R-CNN模型进行的测试提供了进一步的证据，表明有必要对动态SLAM进行额外的研究。代码、结果和生成的数据以开源形式提供，网址为https://eliabntt.github.io/grade-rrSimulationSLAM的动态环境</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.04286v2" target="_blank">2305.04286v2</a>
                              </td>
                              <td>Simulation of Dynamic Environments for SLAM</td>
                              <td>Elia Bonetto</td>
                              <td>2023-05-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_04286v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.04286v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14959v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UAV Trajectory Optimization and Tracking for User Localization in Wireless Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14959v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14959v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14959v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we investigate the problem of UAV-aided user localization in wireless networks. Unlike the existing works, we do not assume perfect knowledge of the UAV location, hence we not only need to localize the users but also to track the UAV location. To do so, we utilize the time-of-arrival along with received signal strength radio measurements collected from users using a UAV. A simultaneous localization and mapping (SLAM) framework building on the Expectation-Maximization-based least-squares method is proposed to classify measurements into line-of-sight or non-line-of-sight categories and learn the radio channel, and at the same, localize the users and track the UAV. This framework also allows us to exploit other types of measurements such as the rough estimate of the UAV location available from GPS, and the UAV velocity measured by an inertial measurement unit (IMU) on-board, to achieve better localization accuracy. Moreover, the trajectory of the UAV is optimized which brings considerable improvement to the localization performance. The simulations show the out-performance of the developed algorithm when compared to other approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14959v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了无线网络中无人机辅助用户定位的问题。与现有的工作不同，我们没有假设对无人机位置有完美的了解，因此我们不仅需要定位用户，还需要跟踪无人机位置。为此，我们利用无人机从用户那里收集的到达时间以及接收信号强度无线电测量值。在基于期望最大化的最小二乘法的基础上，提出了一种同时定位和映射（SLAM）框架，将测量分为视线或非视线类别，并学习无线电信道，同时定位用户并跟踪无人机。该框架还允许我们利用其他类型的测量，如GPS对无人机位置的粗略估计，以及机载惯性测量单元（IMU）测量的无人机速度，以实现更好的定位精度。此外，对无人机的轨迹进行了优化，大大提高了定位性能。仿真结果表明，与其他方法相比，所开发的算法具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14959v1" target="_blank">2305.14959v1</a>
                              </td>
                              <td>UAV Trajectory Optimization and Tracking for User Localization in Wireless Networks</td>
                              <td>Omid Esrafilian</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14959v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14959v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14885v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards View-invariant and Accurate Loop Detection Based on Scene Graph</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14885v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14885v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14885v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop detection plays a key role in visual Simultaneous Localization and Mapping (SLAM) by correcting the accumulated pose drift. In indoor scenarios, the richly distributed semantic landmarks are view-point invariant and hold strong descriptive power in loop detection. The current semantic-aided loop detection embeds the topology between semantic instances to search a loop. However, current semantic-aided loop detection methods face challenges in dealing with ambiguous semantic instances and drastic viewpoint differences, which are not fully addressed in the literature. This paper introduces a novel loop detection method based on an incrementally created scene graph, targeting the visual SLAM at indoor scenes. It jointly considers the macro-view topology, micro-view topology, and occupancy of semantic instances to find correct correspondences. Experiments using handheld RGB-D sequence show our method is able to accurately detect loops in drastically changed viewpoints. It maintains a high precision in observing objects with similar topology and appearance. Our method also demonstrates that it is robust in changed indoor scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14885v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环路检测通过校正累积的姿态漂移，在视觉同步定位和映射（SLAM）中发挥着关键作用。在室内场景中，丰富分布的语义地标是视点不变的，并且在循环检测中具有强大的描述能力。当前的语义辅助循环检测将拓扑嵌入语义实例之间以搜索循环。然而，当前的语义辅助循环检测方法在处理模糊的语义实例和激烈的观点差异方面面临挑战，而这些问题在文献中没有得到充分解决。本文介绍了一种新的基于增量创建场景图的循环检测方法，针对室内场景中的视觉SLAM。它联合考虑宏视图拓扑、微观视图拓扑和语义实例的占用，以找到正确的对应关系。使用手持RGB-D序列的实验表明，我们的方法能够在急剧变化的视角下准确地检测环路。它在观察拓扑结构和外观相似的物体时保持了高精度。我们的方法还证明了它在变化的室内场景中是稳健的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14885v1" target="_blank">2305.14885v1</a>
                              </td>
                              <td>Towards View-invariant and Accurate Loop Detection Based on Scene Graph</td>
                              <td>Chuhao Liu</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14885v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14885v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑到无人机图像和局部特征的冗余性，在线训练单个码本，避免了训练码本与其他数据集的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRFs）成功的基础上，近年来在新视图合成领域取得了重大进展。这些模型捕捉了场景的体积辐射场，通过使用简单、可微分的渲染方程创建了令人信服的密集真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本技术报告中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。聚焦于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v1" target="_blank">2307.03404v1</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一种新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM在预测精度上提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构从运动技术中假设，除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在可变形环境中的视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'住宅/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v2" target="_blank">2306.15667v2</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15669v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector-Free Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15669v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new structure-from-motion framework to recover accurate camera poses and point clouds from unordered images. Traditional SfM systems typically rely on the successful detection of repeatable keypoints across multiple views as the first step, which is difficult for texture-poor scenes, and poor keypoint detection may break down the whole SfM system. We propose a new detector-free SfM framework to draw benefits from the recent success of detector-free matchers to avoid the early determination of keypoints, while solving the multi-view inconsistency issue of detector-free matchers. Specifically, our framework first reconstructs a coarse SfM model from quantized detector-free matches. Then, it refines the model by a novel iterative refinement pipeline, which iterates between an attention-based multi-view matching module to refine feature tracks and a geometry refinement module to improve the reconstruction accuracy. Experiments demonstrate that the proposed framework outperforms existing detector-based SfM systems on common benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate the capability of our framework to reconstruct texture-poor scenes. Based on this framework, we take $\textit{first place}$ in Image Matching Challenge 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15669v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从运动框架中提出了一种新的结构，以从无序图像中恢复准确的相机姿态和点云。传统的SfM系统通常依赖于跨多个视图的可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是困难的，并且较差的关键点检测可能会破坏整个SfM体系。我们提出了一种新的无检测器SfM框架，以从无检测器匹配器最近的成功中获益，避免早期确定关键点，同时解决无检测器匹配的多视图不一致问题。具体而言，我们的框架首先从量化的无检测器匹配中重建粗略的SfM模型。然后，它通过一种新的迭代精化流水线对模型进行精化，该流水线在基于注意力的多视图匹配模块和几何精化模块之间迭代以精化特征轨迹，从而提高重建精度。实验表明，所提出的框架在通用基准数据集上优于现有的基于检测器的SfM系统。我们还收集了一个纹理较差的SfM数据集，以证明我们的框架重建纹理较差场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得$\textit｛first place｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15669v1" target="_blank">2306.15669v1</a>
                              </td>
                              <td>Detector-Free Structure from Motion</td>
                              <td>Xingyi He</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15669v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12770v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Reconstruction of Spherical Images based on Incremental Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12770v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12770v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维重建在现代摄影测量系统中发挥着越来越重要的作用。传统的卫星或航空遥感平台可以为大规模地形和城市的三维重建提供必要的数据源。即使使用低空无人机，由于相机帧之间频繁的跟踪故障和高昂的数据收集成本，在城市峡谷和室内场景等复杂情况下的3D重建也具有挑战性。最近，球形图像由于能够通过一台相机曝光记录周围环境而被广泛利用。然而，经典的3D重建管道不能用于球面图像。此外，用于球面图像的三维重建的软件包很少。基于球面相机的成像几何，本研究研究了使用球面对应关系的相对方位、使用场景和球面点之间的3D对应关系的绝对方位以及BA（束调整）优化的成本函数的算法。此外，已经使用上述算法针对球面图像提出了增量SfM（运动结构）工作流程。通过使用消费者级和专业球形相机拍摄的三个球形数据集，最终验证了所提出的解决方案。结果表明，所提出的SfM工作流可以成功地实现复杂场景的三维重建，并为开源软件包的实现提供了有用的线索。设计的SfM工作流程的源代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12770v2" target="_blank">2306.12770v2</a>
                              </td>
                              <td>3D Reconstruction of Spherical Images based on Incremental Structure from Motion</td>
                              <td>San Jiang</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12770v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12770v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了使从偶然图像捕获中进行3D重建的系统研究取得进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v1" target="_blank">2306.09109v1</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻居搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎普遍认为，全局嵌入对于视觉定位中的图像检索至关重要，尽管必须为每个查询图像计算两种特征类型有很大的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k-最近邻的联合解决方案。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v1" target="_blank">2306.09012v1</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06360v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D reconstruction using Structure for Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06360v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06360v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们正在使用安装在室内移动地板机器人上的一对立体视觉配置的HDR相机对室内空间进行3D重建，该机器人将各种纹理和空间特征捕捉为2D图像，这些数据同时被用作我们算法的反馈，这将使我们能够可视化深度图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06360v1" target="_blank">2306.06360v1</a>
                              </td>
                              <td>3D reconstruction using Structure for Motion</td>
                              <td>Kshitij Karnawat</td>
                              <td>2023-06-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06360v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06360v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05410v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05410v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05410v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>阻碍NeRF模型在野外广泛部署的一个关键障碍是它们对精确相机姿势的依赖。因此，人们对扩展NeRF模型以联合优化相机姿态和场景表示越来越感兴趣，这为具有众所周知的故障模式的现成SfM管道提供了一种替代方案。现有的无基线NeRF方法在有限的假设下运行，例如先前的姿态分布或粗略的姿态初始化，这使得它们在一般情况下效果较差。在这项工作中，我们提出了一种新的方法，即LU-NeRF，该方法通过对姿势配置的宽松假设来联合估计相机姿势和神经辐射场。我们的方法以局部到全局的方式运行，首先对数据的局部子集进行优化，称为迷你场景。LU NeRF估计了这项具有挑战性的少镜头任务的局部姿态和几何结构。通过稳健的姿态同步步骤，将迷你场景姿态带入全局参考系，其中可以执行姿态和场景的最终全局优化。我们展示了我们的LU-NeRF流水线在没有对姿势先验进行限制性假设的情况下，在未建模的NeRF上优于先前的尝试。这使我们能够在一般的SE（3）姿势设置中进行操作，与基线不同。我们的结果还表明，我们的模型可以与基于特征的SfM管道互补，因为它在低纹理和低分辨率图像上优于COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05410v1" target="_blank">2306.05410v1</a>
                              </td>
                              <td>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</td>
                              <td>Zezhou Cheng</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05410v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05410v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用钻爆法的隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，而没有考虑动态障碍物。为了最大限度地提高自主性，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法采用了分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v2" target="_blank">2301.08422v2</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01938v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01938v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01938v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测和匹配是许多计算机视觉问题中的一项基本任务，从形状重建到结构从运动到AR/VR应用和机器人。这是一个研究得很好的问题，取得了显著的成功，如SIFT和最近的深度学习方法。虽然这些技术在噪声、照明变化和刚性运动变换方面表现出了很大的鲁棒性，但对图像失真灵敏度的关注较少。在这项工作中，我们重点关注由用于图像采集的相机的几何形状引起的情况，并考虑鱼眼和投影图像的混合场景之间的关键点检测和匹配问题。我们以最先进的方法为基础，推导出了一种自监督程序，该程序能够训练兴趣点检测器和描述符网络。我们还收集了两个新的数据集，用于在这个未探索的场景中进行额外的训练和测试，我们证明了当前的方法是次优的，因为它们被设计为在传统的投影条件下工作，而所提出的方法被证明是最有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01938v1" target="_blank">2306.01938v1</a>
                              </td>
                              <td>Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</td>
                              <td>Marcela Mera-Trujillo</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01938v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00180v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00180v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00180v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从姿态图像重建三维神经场已成为自监督表示学习的一种很有前途的方法。阻止将这些3D场景学习器部署在大规模视频数据上的关键挑战是，它们依赖于从结构到运动的精确相机姿态，这在规模上运行成本高得令人望而却步。我们提出了一种在线和单次前向联合重建相机姿态和3D神经场景表示的方法。我们通过可微分渲染将帧到帧的光流提升到3D场景流来估计姿态，保持图像处理主干的局部性和平移等变性。然后通过对场景流场的加权最小二乘拟合来执行SE（3）相机姿态估计。该公式使我们能够通过重新渲染输入视频来联合监督姿态估计和可推广的神经场景表示，从而在真实世界的视频数据集上进行端到端和完全自监督的训练。我们证明了我们的方法在不同的真实世界视频上表现稳健，尤其是在传统上对基于优化的姿态估计技术具有挑战性的序列上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00180v1" target="_blank">2306.00180v1</a>
                              </td>
                              <td>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</td>
                              <td>Cameron Smith</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00180v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00180v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16342v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16342v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16342v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征和全局特征对于自动语音识别（ASR）都是必不可少的。最近的许多方法已经证明，简单地结合局部和全局特征可以进一步提高ASR性能。然而，这些方法很少关注局部和全局特征的相互作用，并且它们的串联架构是刚性的，无法反映局部和全局关系。为了解决这些问题，本文提出了用于交互式局部和全局特征融合的InterFormer，以学习ASR的更好表示。具体地说，我们在并行设计中将卷积块与变换器块相结合。此外，我们提出了一个双向特征交互模块（BFIM）和一个选择性融合模块（SFM），分别实现局部和全局特征的交互和融合。在公共ASR数据集上进行的大量实验证明了我们提出的InterFormer的有效性及其优于其他Transformer和Conformer模型的优越性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16342v2" target="_blank">2305.16342v2</a>
                              </td>
                              <td>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</td>
                              <td>Zhi-Hao Lai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16342v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16342v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12036v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIDAR: Synthetic Image Dataset for Alignment & Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12036v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, generating perspective distortions more consistent with real-world scenarios, with homographies closely resembling those of camera projections rather than randomized homographies. For each scene, we provide a sequence of distorted images with corresponding occlusion masks, homographies, and ground-truth labels. The resulting dataset can serve as a training and evaluation set for a multitude of tasks involving image alignment and artifact removal, such as deep homography estimation, dense image matching, 2D bundle adjustment, inpainting, shadow removal, denoising, content retrieval, and background subtraction. Our data generation pipeline is customizable and can be applied to any existing dataset, serving as a data augmentation to further improve the feature learning of any existing method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12036v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像对齐和图像恢复是经典的计算机视觉任务。然而，仍然缺乏提供足够数据来训练和评估端到端深度学习模型的数据集。获得用于图像对准的地面实况数据需要来自运动方法或光流系统的复杂结构，这些运动方法或光学流系统通常不能提供足够的数据方差，即，通常提供大量的图像对应，而在底层图像序列内仅引入很少的风景变化。替代方法利用现有图像数据上的随机透视失真。然而，这只提供了微不足道的扭曲，缺乏现实世界场景的复杂性和多样性。相反，我们提出的数据增强通过使用3D渲染有助于克服数据稀缺的问题：将图像作为纹理添加到平面上，然后将不同的照明条件、阴影和遮挡添加到场景中。场景是从多个视点渲染的，生成的透视扭曲更符合真实世界场景，单应性与相机投影的单应性非常相似，而不是随机单应性。对于每个场景，我们提供一系列扭曲的图像，其中包含相应的遮挡遮罩、单形图和基本事实标签。所得数据集可以作为涉及图像对齐和伪影去除的大量任务的训练和评估集，例如深度单应性估计、密集图像匹配、2D束调整、修复、阴影去除、去噪、内容检索和背景减法。我们的数据生成管道是可定制的，可以应用于任何现有的数据集，作为数据扩充，以进一步改进任何现有方法的特征学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12036v1" target="_blank">2305.12036v1</a>
                              </td>
                              <td>SIDAR: Synthetic Image Dataset for Alignment & Restoration</td>
                              <td>Monika Kwiatkowski</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12036v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08810v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRecon: Automated 3D Object Discovery and Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08810v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08810v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>完全自动化的对象重建管道对于数字内容创建至关重要。虽然3D重建领域已经取得了深刻的发展，但去除背景以获得干净的对象模型仍然依赖于不同形式的手工劳动，如边界框标记、遮罩注释和网格操作。在本文中，我们提出了一个名为AutoRecon的新框架，用于从多视图图像中自动发现和重建对象。我们证明，通过利用自监督2D视觉变换器特征，可以从SfM点云中稳健地定位和分割前景对象。然后，我们在分解的点云提供的密集监督下重建分解的神经场景表示，从而实现精确的对象重建和分割。在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08810v1" target="_blank">2305.08810v1</a>
                              </td>
                              <td>AutoRecon: Automated 3D Object Discovery and Reconstruction</td>
                              <td>Yuang Wang</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08810v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08810v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体而言，我们首先提出了一种空间对齐模块（SAM），用于将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v1" target="_blank">2305.06794v1</a>
                              </td>
                              <td>Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05301v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05301v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of five years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05301v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位在机器人系统在先前访问的环境中的定位和导航中起着重要作用。当访问发生在长时间内时，与季节或昼夜周期相关的环境变化是一个重大挑战。在水下，变异的来源是由于其他因素，如水条件或海洋生物的生长。然而，它仍然是一个主要障碍，也是一个研究较少的障碍，部分原因是缺乏数据。本文提出了一个新的深海数据集，用于对水下长期视觉定位进行基准测试。该数据集由五年内四次访问同一热液喷口建筑物的图像组成。使用导航数据和“运动结构”来估计摄影机姿态和场景的常见几何体。这可作为评估视觉定位技术时的参考。对数据的分析提供了多年来观察到的主要变化的见解。此外，在数据集上评估了几种公认的视觉定位方法，表明水下长期视觉定位仍有改进空间。数据可在https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05301v1" target="_blank">2305.05301v1</a>
                              </td>
                              <td>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</td>
                              <td>Clémentin Boittiaux</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05301v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05301v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05268v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rotation Synchronization via Deep Matrix Factorization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05268v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05268v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了旋转同步问题，其中目标是从成对旋转开始恢复绝对旋转，其中未知数和测度分别表示为图的节点和边。这个问题是结构从运动到同时定位和映射的一个重要任务。我们专注于通过神经网络进行同步的公式化，这是最近才开始在文献中进行探索的。受深度矩阵完备的启发，我们用深度神经网络的矩阵分解来表达旋转同步。我们的公式具有隐式正则化性质，更重要的是，它是无监督的，而以前的深度方法是有监督的。我们的实验表明，在大多数场景中，我们实现了与最接近的竞争对手相当的准确性，同时在较弱的假设下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05268v1" target="_blank">2305.05268v1</a>
                              </td>
                              <td>Rotation Synchronization via Deep Matrix Factorization</td>
                              <td>Gk Tejus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05268v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05268v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，其中机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与准确性之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v3" target="_blank">2210.05020v3</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10664v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10664v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) are trained using a set of camera poses and associated images as input to estimate density and color values for each position. The position-dependent density learning is of particular interest for photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF coordinate system based on the object density. While traditional methods like Structure from Motion are commonly used for camera pose calculation in pre-processing for NeRFs, the HoloLens offers an interesting interface for extracting the required input data directly. We present a workflow for high-resolution 3D reconstructions almost directly from HoloLens data using NeRFs. Thereby, different investigations are considered: Internal camera poses from the HoloLens trajectory via a server application, and external camera poses from Structure from Motion, both with an enhanced variant applied through pose refinement. Results show that the internal camera poses lead to NeRF convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and enable a 3D reconstruction. Pose refinement enables comparable quality compared to external camera poses, resulting in improved training process with a PSNR of 27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform the conventional photogrammetric dense reconstruction using Multi-View Stereo in terms of completeness and level of detail.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10664v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用一组相机姿势和相关图像作为输入来训练神经辐射场（NeRF），以估计每个位置的密度和颜色值。位置相关密度学习对摄影测量特别感兴趣，它通过基于物体密度查询和过滤NeRF坐标系来实现3D重建。虽然在NeRF的预处理中，像“运动结构”这样的传统方法通常用于相机姿态计算，但HoloLens为直接提取所需的输入数据提供了一个有趣的界面。我们提出了一种使用NeRFs几乎直接从HoloLens数据进行高分辨率3D重建的工作流程。因此，考虑了不同的研究：通过服务器应用程序从HoloLens轨迹中获得的内部相机姿势，以及从运动中获得的结构中获得的外部相机姿势，两者都通过姿势细化应用了增强的变体。结果表明，内部相机姿态导致NeRF收敛，PSNR为25dB，绕x轴简单旋转，并实现3D重建。与外部相机姿势相比，姿势细化能够实现相当的质量，从而改进训练过程，PSNR为27\，dB，并实现更好的3D重建。总体而言，NeRF重建在完整性和细节水平方面优于使用多视图立体的传统摄影测量密集重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10664v1" target="_blank">2304.10664v1</a>
                              </td>
                              <td>A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</td>
                              <td>Miriam Jäger</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10664v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10664v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13875v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Geometric Models by Clustering in the Consensus Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13875v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems - at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13875v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的算法来寻找未知数量的几何模型，例如单应性。该问题被形式化为逐步找到主导模型实例，而不形成清晰的点到模型分配。主要实例是通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的合并过程来发现的。通过在一致性空间中进行聚类来发现新的一致性。这种新的公式产生了一种简单的迭代算法，具有最先进的精度，同时实时处理许多视觉问题——在双视图运动估计方面比竞争对手快至少两个数量级。此外，我们提出了一种确定性采样器，反映了真实世界的数据往往形成空间相干结构的事实。采样器返回逐渐加密的邻域图中的连接分量。我们介绍了许多应用，其中使用多个几何模型可以提高精度。这些包括从多个广义单应性的姿态估计；快速移动物体的轨迹估计；并且我们还提出了一种在全局SfM算法中使用多个单应性的方法。源代码：https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13875v2" target="_blank">2103.13875v2</a>
                              </td>
                              <td>Finding Geometric Models by Clustering in the Consensus Space</td>
                              <td>Daniel Barath</td>
                              <td>2021-03-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13875v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13875v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位是机器人、虚拟和增强现实以及仓库货物运输等各种应用中的一项关键任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复图案和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v1" target="_blank">2304.07250v1</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05947v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Localization using Imperfect 3D Models from the Internet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05947v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05947v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是包括增强现实（AR）在内的许多应用中的核心组件。定位算法计算查询图像的相机姿态，该图像通常是根据图像构建的场景表示。这通常需要捕获和存储大量数据，然后运行运动结构（SfM）算法。用于建筑场景表示的一个有趣且未充分探索的数据源是在互联网上容易获得的3D模型，例如手绘CAD模型、从建筑足迹或从航空图像生成的3D模型。这些模型允许立即执行视觉定位，而无需耗时的场景捕捉和模型构建步骤。然而，它也带来了挑战，因为可用的3D模型往往是对现实的不完美反映。例如，模型可能只具有通用纹理或根本没有纹理，可能只提供场景几何体的简单近似，或者可能被拉伸。本文研究了这些模型的缺陷如何影响定位精度。我们为这项任务创建了一个新的基准，并基于每个场景的多个3D模型提供了详细的实验评估。我们表明，来自互联网的3D模型有望成为一种易于获得的场景表示。同时，视觉定位管道还有很大的改进空间。为了促进对这项有趣而富有挑战性的任务的研究，我们在v-pnk.github.io/cadloc上发布了我们的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05947v1" target="_blank">2304.05947v1</a>
                              </td>
                              <td>Visual Localization using Imperfect 3D Models from the Internet</td>
                              <td>Vojtech Panek</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05947v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05947v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像技术已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索从红外图像中通过SfM进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v1" target="_blank">2304.03930v1</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03560v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03560v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.   Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03560v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督多帧深度估计通过计算相邻帧之间像素对应的匹配成本，将几何信息注入网络，实现了高精度。这些像素对应候选是基于帧之间的相对姿态估计来计算的。精确的姿态预测对于精确的匹配成本计算至关重要，因为它们会影响核极几何。此外，改进的深度估计反过来可以用于对准姿态估计。受传统运动结构（SfM）原理的启发，我们提出了DualRefine模型，该模型通过反馈回路将深度和姿态估计紧密耦合。我们新颖的更新管道使用深度平衡模型框架，通过基于核极几何计算局部匹配成本，迭代细化深度估计和特征图的隐藏状态。重要的是，我们使用精细的深度估计和特征图来计算每一步的姿势更新。姿态估计的这种更新在细化过程中缓慢地改变了极线几何结构。KITTI数据集上的实验结果表明，竞争性深度预测和里程计预测性能超过了已公布的自监督基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03560v1" target="_blank">2304.03560v1</a>
                              </td>
                              <td>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</td>
                              <td>Antyanta Bangunharcana</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03560v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03560v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$\widetilde{O}（n^2）$oracle复杂度。然而，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$，这是由于昂贵的子程序，如Lenstra-Lenstra-Lov\asz（LLL）算法[Lenstra，Lenstra，Lov\asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]提出的LLL算法的更快版本、[Vaidya，FOCS 1989]提出的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了这个问题的一个强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\log n）$额外的算术运算。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v1" target="_blank">2304.03426v1</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Validation in Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Structure from Motion (SfM) challenge in computer vision is the process of recovering the 3D structure of a scene from a series of projective measurements that are calculated from a collection of 2D images, taken from different perspectives. SfM consists of three main steps; feature detection and matching, camera motion estimation, and recovery of 3D structure from estimated intrinsic and extrinsic parameters and features.   A problem encountered in SfM is that scenes lacking texture or with repetitive features can cause erroneous feature matching between frames. Semantic segmentation offers a route to validate and correct SfM models by labelling pixels in the input images with the use of a deep convolutional neural network. The semantic and geometric properties associated with classes in the scene can be taken advantage of to apply prior constraints to each class of object. The SfM pipeline COLMAP and semantic segmentation pipeline DeepLab were used. This, along with planar reconstruction of the dense model, were used to determine erroneous points that may be occluded from the calculated camera position, given the semantic label, and thus prior constraint of the reconstructed plane. Herein, semantic segmentation is integrated into SfM to apply priors on the 3D point cloud, given the object detection in the 2D input images. Additionally, the semantic labels of matched keypoints are compared and inconsistent semantically labelled points discarded. Furthermore, semantic labels on input images are used for the removal of objects associated with motion in the output SfM models. The proposed approach is evaluated on a data-set of 1102 images of a repetitive architecture scene. This project offers a novel method for improved validation of 3D SfM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉中的运动结构（SfM）挑战是从一系列投影测量中恢复场景的3D结构的过程，这些投影测量是从不同视角拍摄的2D图像集合中计算出来的。SfM由三个主要步骤组成；特征检测和匹配，相机运动估计，以及从估计的内在和外在参数和特征中恢复3D结构。在SfM中遇到的问题是，缺乏纹理或具有重复特征的场景可能导致帧之间的错误特征匹配。语义分割通过使用深度卷积神经网络标记输入图像中的像素，提供了一种验证和校正SfM模型的途径。可以利用与场景中的类相关联的语义和几何特性来将先验约束应用于每类对象。使用了SfM流水线COLMAP和语义分割流水线DeepLab。这与密集模型的平面重建一起，被用于确定可能被计算的相机位置遮挡的错误点，给定语义标签，从而确定重建平面的先验约束。在此，在给定2D输入图像中的对象检测的情况下，语义分割被集成到SfM中，以在3D点云上应用先验。此外，对匹配的关键点的语义标签进行比较，并丢弃语义上不一致的标记点。此外，输入图像上的语义标签用于去除与输出SfM模型中的运动相关联的对象。所提出的方法是在重复建筑场景的1102幅图像的数据集上进行评估的。该项目为改进三维SfM模型的验证提供了一种新方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02420v1" target="_blank">2304.02420v1</a>
                              </td>
                              <td>Semantic Validation in Structure from Motion</td>
                              <td>Joseph Rowell</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13551v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13551v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13551v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个视图估计密集深度图在几何上是不适定的，最先进的方法依赖于使用深度神经网络学习深度与视觉外观的关系。另一方面，运动结构（SfM）利用多视图约束来生成非常精确但稀疏的地图，因为图像之间的匹配通常受到局部判别纹理的限制。在这项工作中，我们结合了这两种方法的优势，提出了一种新的测试时间细化（TTR）方法，称为SfM-TTR，该方法使用SfM多视图线索在测试时提高了单视图深度网络的性能。具体而言，与现有技术不同的是，我们使用稀疏的SfM点云作为测试时间自监督信号，对网络编码器进行微调，以学习测试场景的更好表示。我们的结果表明，将SfM-TTR添加到几个最先进的自监督和监督网络中，显著提高了它们的性能，优于以前主要基于光度多视图一致性的TTR基线。代码位于https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13551v2" target="_blank">2211.13551v2</a>
                              </td>
                              <td>SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</td>
                              <td>Sergio Izquierdo</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13551v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Line Mapping Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与稀疏关键点相比，少数线段可以简明地对高级场景布局进行编码，因为它们通常描绘主要的结构元素。除了提供强烈的几何线索外，它们还在城市景观和室内场景中无处不在。尽管有明显的优势，但目前基于线的重建方法远远落后于基于点的重建方法。在本文中，我们旨在通过引入LIMAP来缩小这一差距，LIMAP是一个用于3D线图绘制的库，可以从多视图图像中稳健有效地创建3D线图。这是通过重新审视线三角测量的退化问题、精心制作的评分和轨迹构建，以及利用线重合、平行和正交等结构先验来实现的。我们的代码与现有的基于点的运动结构方法无缝集成，可以利用它们的3D点来进一步改进线重建。此外，作为副产品，该方法能够恢复线和点/消失点（VP）之间的3D关联图。在深入的实验中，我们表明LIMAP在3D线映射方面显著优于现有的方法。我们强大的3D折线图也开辟了新的研究方向。我们展示了两个示例应用程序：视觉定位和束调整，其中将线与点一起积分会产生最佳结果。代码位于https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17504v1" target="_blank">2303.17504v1</a>
                              </td>
                              <td>3D Line Mapping Revisited</td>
                              <td>Shaohui Liu</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15069v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15069v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15069v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一种轻量级网络来改进同一图像中关键点的描述符。该网络以原始描述符和关键点的几何特性为输入，并使用基于MLP的自提升级和基于Transformer的交叉提升级来增强描述符。增强的描述符可以是实数描述符，也可以是二进制描述符。我们使用所提出的网络来增强手工制作的（ORB，SIFT）和最先进的基于学习的描述符（SuperPoint，ALIKE），并在图像匹配、视觉定位和运动任务的结构方面对它们进行评估。结果表明，我们的方法显著提高了每个任务的性能，特别是在具有挑战性的情况下，如大的照明变化或重复模式。我们的方法只需要在桌面GPU上3.2ms，在嵌入式GPU上27ms就可以处理2000个特征，这足够快，可以应用于实际系统。代码和训练过的重量可在github.com/SJTU-ViSYS/FeatureBooster上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15069v3" target="_blank">2211.15069v3</a>
                              </td>
                              <td>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</td>
                              <td>Xinjiang Wang</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15069v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15069v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15060v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15060v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15060v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的管道，可以通过一部智能手机在野外获取纹理网格，该智能手机可以访问图像、深度图和有效姿势。我们的方法首先引入了一种基于运动的RGBD辅助结构，该结构可以生成过滤后的深度图，并根据相应的深度细化相机姿态。然后，我们采用了神经隐式曲面重建方法，该方法可以获得高质量的网格，并开发了一种新的训练过程，用于应用经典多视图立体方法提供的正则化。此外，我们应用可微分渲染来微调不完整的纹理贴图，并生成在感知上更接近原始场景的纹理。我们的管道可以应用于现实世界中的任何常见对象，而无需实验室环境或精确的掩模图像。我们展示了具有复杂形状的捕捉对象的结果，并将我们的方法与现有的3D重建和纹理映射方法进行了数值验证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15060v1" target="_blank">2303.15060v1</a>
                              </td>
                              <td>TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</td>
                              <td>Jaehoon Choi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15060v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15060v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12018v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12018v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S$^2$fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12018v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于运动的神经增量结构（SfM）方法，即S$^2$fM级，该方法通过从建立的关键点对应关系中学习隐式表面的坐标MLP和辐射场，从一组未校准的图像中估计相机姿态和场景几何。由于增量SfM管道中不可避免的两视图和少视图配置，我们的新公式提出了一些新的挑战，这使用于具有未知相机姿态的体积神经渲染的坐标MLP的优化变得复杂。然而，我们证明了在2D对应关系中传递的强归纳基有望通过利用射线采样方案之间的关系来解决这些挑战。基于此，我们重新审视了增量SfM的管道，并更新了关键组件，包括两视图几何初始化、相机姿态配准、3D点三角测量和束平差，以基于神经隐式曲面的全新视角。通过通过坐标MLP统一小型MLP网络中的场景几何结构，我们的Level-S$^2$fM将隐式曲面的零级集视为自上而下的信息正则化，以管理重建的3D点，通过查询SDF拒绝对应关系中的异常值，并通过NBA（Neural BA）细化估计的几何结构。我们的S$^2$fM级不仅在相机姿态估计和场景几何重建方面取得了有希望的结果，而且它还为神经隐式渲染提供了一种很有前途的方法，而无需事先了解相机的外在情况。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12018v2" target="_blank">2211.12018v2</a>
                              </td>
                              <td>Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</td>
                              <td>Yuxi Xiao</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12018v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12018v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_14840v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_14840v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{dataset available at https://github.com/Junggy/HAMMER-dataset} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_14840v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>解决密集3D视觉问题的基于学习的方法通常基于3D传感器数据进行训练。分别使用的测量距离的原理提供了优点和缺点。由于缺乏多模态数据集，文献中通常不会对这些数据进行比较或讨论。无纹理区域对运动和立体的结构来说是有问题的，反射材料对主动传感来说是个问题，而半透明物体的距离用现有硬件测量起来很复杂。对不准确或损坏的数据进行培训会导致模型偏差，阻碍泛化能力。如果在评估过程中将传感器测量视为基本事实，则这些影响不会被注意到。本文研究了传感器误差对深度估计和重建的密集三维视觉任务的影响。我们严格展示了传感器特性对学习预测的重大影响，并注意到日常家庭环境中各种技术产生的泛化问题。为了进行评估，我们引入了一个精心设计的数据集\脚注｛数据集，可在https://github.com/Junggy/HAMMER-dataset}包括来自商品传感器的测量，即D-ToF、I-ToF、无源/有源立体声和单目RGB+P。我们的研究量化了相当大的传感器噪声影响，并为改进密集视觉估计和有针对性的数据融合铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.14840v1" target="_blank">2303.14840v1</a>
                              </td>
                              <td>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</td>
                              <td>HyunJun Jung</td>
                              <td>2023-03-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_14840v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.14840v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13543v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13543v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reasoning the 3D structure of a non-rigid dynamic scene from a single moving camera is an under-constrained problem. Inspired by the remarkable progress of neural radiance fields (NeRFs) in photo-realistic novel view synthesis of static scenes, extensions have been proposed for dynamic settings. These methods heavily rely on neural priors in order to regularize the problem. In this work, we take a step back and reinvestigate how current implementations may entail deleterious effects, including limited expressiveness, entanglement of light and density fields, and sub-optimal motion localization. As a remedy, we advocate for a bridge between classic non-rigid-structure-from-motion (\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain the latter. To this end, we propose a framework that factorizes time and space by formulating a scene as a composition of bandlimited, high-dimensional signals. We demonstrate compelling results across complex dynamic scenes that involve changes in lighting, texture and long-range dynamics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13543v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个运动摄像机推断非刚性动态场景的三维结构是一个欠约束问题。受神经辐射场（NeRFs）在静态场景的逼真新颖视图合成中取得的显著进展的启发，提出了动态设置的扩展。这些方法在很大程度上依赖于神经先验来正则化问题。在这项工作中，我们后退一步，重新研究当前的实现可能带来的有害影响，包括有限的表现力、光场和密度场的纠缠以及次优运动定位。作为补救措施，我们主张在经典的非刚性运动结构（\nrsfm）和NeRF之间建立一座桥梁，使前者经过充分研究的先验能够约束后者。为此，我们提出了一个框架，通过将场景公式化为带限高维信号的合成，来分解时间和空间。我们在复杂的动态场景中展示了令人信服的结果，这些场景涉及照明、纹理和长程动力学的变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13543v3" target="_blank">2302.13543v3</a>
                              </td>
                              <td>BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</td>
                              <td>Sameera Ramasinghe</td>
                              <td>2023-02-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13543v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13543v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13805v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13805v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and hybrid rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13805v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们定义了一个新的问题，即恢复被限制在透明外壳中的物体的三维几何结构。我们还提出了一种新的方法来解决这个具有挑战性的问题。透明外壳在不同传播介质（例如空气或玻璃）之间的界面处带来了多重光反射和折射的挑战。这些多重反射和折射会导致严重的图像失真，从而使单一视点假设无效。因此，使用现有的方法，例如传统的运动结构或现代神经重建方法，不能可靠地重建这些物体的3D几何结构。我们通过将场景明确建模为透明外壳内外两个不同的子空间来解决这个问题。我们使用现有的神经重建方法（NeuS），该方法隐式地表示内子空间的几何形状和外观。为了解决复杂的灯光交互，我们开发了一种混合渲染策略，将体积渲染与光线跟踪相结合。然后，我们通过最小化真实渲染图像和混合渲染图像之间的差异来恢复模型的基本几何结构和外观。我们根据合成数据和真实数据对我们的方法进行了评估。实验结果表明，我们的方法优于最先进的（SOTA）方法。代码和数据将在https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13805v1" target="_blank">2303.13805v1</a>
                              </td>
                              <td>Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</td>
                              <td>Jinguang Tong</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13805v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13805v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13791v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Progressively Optimized Local Radiance Fields for Robust View Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13791v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13791v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从单个随意捕捉的视频中重建大规模场景辐射场的算法。这项任务提出了两个核心挑战。首先，大多数现有的辐射场重建方法都依赖于运动结构算法中精确的预估计相机姿态，而这些算法在野外视频中经常失败。其次，在无界场景中，使用具有有限表示能力的单个全局辐射场不会缩放到更长的轨迹。为了处理未知姿态，我们以渐进的方式联合估计具有辐射场的相机姿态。我们表明，渐进优化显著提高了重建的鲁棒性。为了处理大型无界场景，我们动态分配用时间窗口内的帧训练的新的局部辐射场。这进一步提高了鲁棒性（例如，即使在中等姿态漂移的情况下也表现良好），并允许我们缩放到大场景。我们对Tanks and Temples数据集和收集的户外数据集Static Hikes的广泛评估表明，我们的方法与最先进的方法相比是有利的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13791v1" target="_blank">2303.13791v1</a>
                              </td>
                              <td>Progressively Optimized Local Radiance Fields for Robust View Synthesis</td>
                              <td>Andreas Meuleman</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13791v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13791v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_02239v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Dynamic Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_02239v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_02239v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态辐射场重建方法旨在对动态场景的时变结构和外观进行建模。然而，现有的方法假设通过运动结构（SfM）算法可以可靠地估计精确的相机姿态。因此，这些方法是不可靠的，因为SfM算法在具有高度动态对象、纹理较差的表面和旋转相机运动的具有挑战性的视频中经常失败或产生错误的姿势。我们通过联合估计静态和动态辐射场以及相机参数（姿态和焦距）来解决这个鲁棒性问题。我们通过大量的定量和定性实验证明了我们方法的稳健性。与最先进的动态视图合成方法相比，我们的结果显示出良好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.02239v2" target="_blank">2301.02239v2</a>
                              </td>
                              <td>Robust Dynamic Radiance Fields</td>
                              <td>Yu-Lun Liu</td>
                              <td>2023-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_02239v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.02239v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_01160v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">High-Res Facial Appearance Capture from Polarized Smartphone Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_01160v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach. Our method optimizes for high-resolution normal textures, diffuse albedo, and specular albedo using a coarse-to-fine optimization scheme. We show that the optimized textures can be used in a standard rendering pipeline to synthesize high-quality photo-realistic 3D digital humans in novel environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_01160v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从RGB图像中重建高质量面部纹理的新方法，该方法基于我们配备了廉价偏振箔的单个智能手机，使用了一种新的捕获程序。具体来说，我们把手电筒变成一个偏振光源，并在相机顶部添加一个偏振滤光片。利用这种设置，我们用交叉偏振光和平行偏振光捕捉被摄对象的面部。对于每个受试者，我们使用改进的智能手机在不同光偏振的闪光灯照射下，在黑暗环境中记录两个短序列。基于这些观察结果，我们使用运动结构重建了人脸的显式表面网格。然后，我们利用相机和光线在可微分渲染器中的协同定位，使用综合分析方法优化面部纹理。我们的方法使用从粗到细的优化方案优化高分辨率法线纹理、漫射反照率和镜面反照率。我们表明，优化的纹理可以在标准渲染管道中使用，以在新的环境中合成高质量的照片逼真的3D数字人。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.01160v2" target="_blank">2212.01160v2</a>
                              </td>
                              <td>High-Res Facial Appearance Capture from Polarized Smartphone Images</td>
                              <td>Dejan Azinović</td>
                              <td>2022-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_01160v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.01160v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_08695v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_08695v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis (NVS) is a challenging task in computer vision that involves synthesizing new views of a scene from a limited set of input images. Neural Radiance Fields (NeRF) have emerged as a powerful approach to address this problem, but they require accurate knowledge of camera \textit{intrinsic} and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM) and multi-view stereo (MVS) approaches have been used to extract camera parameters, but these methods can be unreliable and may fail in certain cases. In this paper, we propose a novel technique that leverages unposed images from dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera parameters directly from data. Our approach is highly extensible and can be integrated into existing NeRF architectures with minimal modifications. We demonstrate the effectiveness of our method on a variety of static and dynamic scenes and show that it outperforms traditional SfM and MVS approaches. The code for our method is publicly available at \href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}. Our approach offers a promising new direction for improving the accuracy and robustness of NVS using NeRF, and we anticipate that it will be a valuable tool for a wide range of applications in computer vision and graphics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_08695v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>新视图合成（NVS）是计算机视觉中一项具有挑战性的任务，涉及从有限的一组输入图像合成场景的新视图。神经辐射场（NeRF）已成为解决这一问题的一种强大方法，但它们需要准确了解相机的固有参数和外在参数。传统上，运动结构（SfM）和多视图立体（MVS）方法已被用于提取相机参数，但这些方法可能不可靠，并且在某些情况下可能失败。在本文中，我们提出了一种新技术，该技术利用来自动态数据集（如NVIDIA动态场景数据集）的未渲染图像，直接从数据中学习相机参数。我们的方法具有高度的可扩展性，可以集成到现有的NeRF架构中，只需进行最小的修改。我们在各种静态和动态场景中证明了我们的方法的有效性，并表明它优于传统的SfM和MVS方法。我们方法的代码可在\href上公开获取{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}。我们的方法为使用NeRF提高NVS的准确性和稳健性提供了一个有前景的新方向，我们预计它将成为计算机视觉和图形领域广泛应用的宝贵工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.08695v1" target="_blank">2303.08695v1</a>
                              </td>
                              <td>RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</td>
                              <td>Shuja Khalid</td>
                              <td>2023-03-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_08695v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.08695v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05195v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Rotation Averaging: Uncertainties and Robust Losses</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05195v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05195v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05195v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we revisit the rotation averaging problem applied in global Structure-from-Motion pipelines. We argue that the main problem of current methods is the minimized cost function that is only weakly connected with the input data via the estimated epipolar geometries.We propose to better model the underlying noise distributions by directly propagating the uncertainty from the point correspondences into the rotation averaging. Such uncertainties are obtained for free by considering the Jacobians of two-view refinements. Moreover, we explore integrating a variant of the MAGSAC loss into the rotation averaging problem, instead of using classical robust losses employed in current frameworks. The proposed method leads to results superior to baselines, in terms of accuracy, on large-scale public benchmarks. The code is public. https://github.com/zhangganlin/GlobalSfMpy</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05195v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们从运动管道重新审视了应用于全局结构的旋转平均问题。我们认为，当前方法的主要问题是最小化成本函数，该函数仅通过估计的核极几何结构与输入数据弱连接。我们建议通过将点对应的不确定性直接传播到旋转平均中来更好地对潜在的噪声分布建模。这样的不确定性是通过考虑两个视图精化的雅可比派而免费获得的。此外，我们探索将MAGSAC损失的变体集成到旋转平均问题中，而不是使用当前框架中使用的经典鲁棒损失。在大型公共基准上，所提出的方法在精度方面优于基线。该代码是公开的。https://github.com/zhangganlin/GlobalSfMpy</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05195v1" target="_blank">2303.05195v1</a>
                              </td>
                              <td>Revisiting Rotation Averaging: Uncertainties and Robust Losses</td>
                              <td>Ganlin Zhang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05195v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05195v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13201v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DRO: Deep Recurrent Optimizer for Video to Depth</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13201v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13201v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13201v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There are increasing interests of studying the video-to-depth (V2D) problem with machine learning techniques. While earlier methods directly learn a mapping from images to depth maps and camera poses, more recent works enforce multi-view geometry constraints through optimization embedded in the learning framework. This paper presents a novel optimization method based on recurrent neural networks to further exploit the potential of neural networks in V2D. Specifically, our neural optimizer alternately updates the depth and camera poses through iterations to minimize a feature-metric cost, and two gated recurrent units iteratively improve the results by tracing historical information. Extensive experimental results demonstrate that our method outperforms previous methods and is more efficient in computation and memory consumption than cost-volume-based methods. In particular, our self-supervised method outperforms previous supervised methods on the KITTI and ScanNet datasets. Our source code is available at https://github.com/aliyun/dro-sfm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13201v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们对使用机器学习技术研究视频到深度（V2D）问题越来越感兴趣。虽然早期的方法直接学习从图像到深度图和相机姿态的映射，但最近的工作通过嵌入学习框架中的优化来加强多视图几何约束。本文提出了一种新的基于递归神经网络的优化方法，以进一步挖掘神经网络在V2D中的潜力。具体而言，我们的神经优化器通过迭代交替更新深度和相机姿态，以最小化特征度量成本，并且两个门控递归单元通过跟踪历史信息迭代改进结果。大量的实验结果表明，我们的方法优于以前的方法，并且在计算和内存消耗方面比基于成本体积的方法更高效。特别是，我们的自监督方法在KITTI和ScanNet数据集上优于以前的监督方法。我们的源代码可在https://github.com/aliyun/dro-sfm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13201v4" target="_blank">2103.13201v4</a>
                              </td>
                              <td>DRO: Deep Recurrent Optimizer for Video to Depth</td>
                              <td>Xiaodong Gu</td>
                              <td>2021-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13201v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13201v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_04738v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04738v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04738v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04738v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04738v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的多机器人协作方法，该方法利用预先训练的大型语言模型（LLM）的力量进行高级通信和低级路径规划。机器人配备了LLM来讨论和集体推理任务策略。然后，它们生成子任务计划和任务空间航路点路径，多臂运动规划器使用这些路径来加速轨迹规划。我们还提供来自环境的反馈，例如碰撞检查，并提示LLM代理在上下文中改进其计划和路点。为了进行评估，我们引入了RoCoBench，这是一个6任务基准测试，涵盖了广泛的多机器人协作场景，并附带了用于代理表示和推理的纯文本数据集。我们通过实验证明了我们的方法的有效性——它在RoCoBench中的所有任务中都实现了高成功率，并适应了任务语义的变化。我们的对话框设置提供了很高的可解释性和灵活性——在现实世界的实验中，我们展示了RoCo可以轻松地将人融入到循环中，用户可以与机器人代理进行通信和协作，共同完成任务。参见项目网站https://project-roco.github.io用于视频和代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04738v1" target="_blank">2307.04738v1</a>
                              </td>
                              <td>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</td>
                              <td>Zhao Mandi</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04738v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04738v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04721v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models as General Pattern Machines</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04721v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04721v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04721v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04721v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们观察到，预训练的大型语言模型（LLM）能够自回归地完成复杂的令牌序列——从概率上下文无关语法（PCFG）程序生成的任意令牌序列，到抽象推理语料库（ARC）中发现的更丰富的空间模式，这是一种以ASCII艺术风格提示的通用人工智能基准。令人惊讶的是，即使使用从词汇表中随机采样的令牌来表达序列，也可以部分保持模式完成熟练度。这些结果表明，在没有任何额外训练的情况下，LLM可以在上下文学习的驱动下充当通用序列建模器。在这项工作中，我们研究了如何将这些零样本功能应用于机器人中的问题——从外推表示状态的数字序列到完成简单运动，再到至少最大程度地提示可以发现和表示闭环策略的有酬条件的轨迹（例如CartPole的稳定控制器）。尽管由于延迟、上下文大小限制和计算成本，目前很难在实际系统中进行部署，但使用LLM来驱动低级别控制的方法可能会为如何将单词之间的模式转换为动作提供一个令人兴奋的一瞥。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04721v1" target="_blank">2307.04721v1</a>
                              </td>
                              <td>Large Language Models as General Pattern Machines</td>
                              <td>Suvir Mirchandani</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04721v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04721v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04693v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COMEX: A Tool for Generating Customized Source Code Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04693v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04693v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04693v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE.   Tool: https://pypi.org/project/comex - GitHub: https://github.com/IBM/tree-sitter-codeviews - Demo: https://youtu.be/GER6U87FVbU</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04693v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习源代码的有效表示对于任何用于软件工程的机器学习（ML4SE）系统来说都是至关重要的。受自然语言处理的启发，像Codex和CodeGen这样的大型语言模型（LLM）将代码视为通用的文本序列，并在庞大的代码数据语料库上进行训练，从而在几个软件工程（SE）任务上实现了最先进的性能。然而，与自然语言不同，有效的源代码遵循由编程语言的底层语法控制的严格结构和模式。当前的LLM没有利用源代码的这一特性，因为它们将代码视为令牌序列，并忽略了可以从代码视图中提取的代码的关键结构和语义特性，如控制流图（CFG）、数据流图（DFG）、抽象语法树（AST）等。不幸的是，为每种编程语言生成和集成代码视图的过程既繁琐又耗时。为了克服这一障碍，我们提出了我们的工具COMEX，这是一个允许研究人员和开发人员创建和组合多个代码视图的框架，机器学习（ML）模型可以将其用于各种SE任务。我们的工具的一些显著特征是：（i）它直接处理源代码（不需要可编译），（ii）它目前支持Java和C#，（iii）它可以通过使用过程内和过程间分析来分析方法级代码段和程序级代码段，以及（iv）它很容易扩展到其他语言，因为它建立在tree-sitter之上，tree-sitter是一种广泛使用的增量解析器，支持40多种语言。我们相信，这个易于使用的代码视图生成和定制工具将推动源代码表示学习方法和ML4SE的研究。工具：https://pypi.org/project/comex-GitHub：https://github.com/IBM/tree-sitter-codeviews-演示：https://youtu.be/GER6U87FVbU</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04693v1" target="_blank">2307.04693v1</a>
                              </td>
                              <td>COMEX: A Tool for Generating Customized Source Code Representations</td>
                              <td>Debeshee Das</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04693v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04693v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15666v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Testing of Detection Tools for AI-Generated Text</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15666v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15666v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15666v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AI-generated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text. Furthermore, content obfuscation techniques significantly worsen the performance of tools. The study makes several significant contributions. First, it summarises up-to-date similar scientific and non-scientific efforts in the field. Second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. Third, it discusses the implications and drawbacks of using detection tools for AI-generated text in academic settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15666v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成性预训练转换器大型语言模型的最新进展强调了在学术环境中不公平使用人工智能生成内容的潜在风险，并加大了寻找检测此类内容的解决方案的力度。本文研究了人工智能生成文本检测工具的一般功能，并根据准确性和错误类型分析对其进行了评估。具体而言，该研究试图回答现有检测工具是否能够可靠地区分人类书写的文本和ChatGPT生成的文本，以及机器翻译和内容模糊技术是否会影响人工智能生成文本的检测等研究问题。这项研究涵盖了12个公开可用的工具和两个在学术环境中广泛使用的商业系统（Turnitin和剽窃检查）。研究人员得出结论，现有的检测工具既不准确也不可靠，主要倾向于将输出分类为人工书写，而不是检测人工智能生成的文本。此外，内容混淆技术显著降低了工具的性能。这项研究作出了若干重大贡献。首先，它总结了该领域最新的类似科学和非科学努力。其次，它介绍了迄今为止进行的最全面的测试之一的结果，该测试基于严格的研究方法、原始文件集和广泛的工具。第三，讨论了在学术环境中使用人工智能生成文本检测工具的含义和缺点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15666v2" target="_blank">2306.15666v2</a>
                              </td>
                              <td>Testing of Detection Tools for AI-Generated Text</td>
                              <td>Debora Weber-Wulff</td>
                              <td>2023-06-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15666v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15666v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04657v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04657v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04657v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04657v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04657v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了BeaverTails数据集，旨在促进对大型语言模型（LLM）中的安全对齐的研究。该数据集独特地分离了问答对的有用性和无害性注释，从而为这些关键属性提供了不同的视角。总的来说，我们为30207对问答（QA）对编制了安全元标签，并收集了30144对有用性和无害性指标的专家比较数据。我们进一步展示了BeaverTails在内容调节和人类反馈强化学习（RLHF）中的应用，强调了其在LLM中实用安全措施的潜力。我们相信，该数据集为社区提供了重要资源，有助于LLM的安全开发和部署。我们的项目页面位于以下URL：https://sites.google.com/view/pku-beavertails.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04657v1" target="_blank">2307.04657v1</a>
                              </td>
                              <td>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</td>
                              <td>Jiaming Ji</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04657v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04657v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18703v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models, Natural Language Processing, Domain Specialization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18703v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18703v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18703v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18703v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）极大地推动了自然语言处理（NLP）领域的发展，为广泛的应用提供了非常有用的、与任务无关的基础。然而，直接应用LLM来解决特定领域中的复杂问题会遇到许多障碍，这些障碍是由领域数据的异质性、领域知识的复杂性、领域目标的独特性和约束的多样性（例如，领域应用中的各种社会规范、文化一致性、宗教信仰和道德标准）造成的。领域规范技术是使大型语言模型在许多应用程序中具有破坏性的关键。具体而言，为了解决这些障碍，近年来对LLM领域专业化的研究和实践显著增加。这一新兴的研究领域具有巨大的影响潜力，需要进行全面和系统的审查，以更好地总结和指导这一领域正在进行的工作。在本文中，我们对大型语言模型的领域规范技术进行了全面的综述，这是大型语言模型应用程序的一个新兴方向。首先，我们提出了一个系统的分类法，根据LLM的可访问性对LLM领域专业化技术进行分类，并总结了所有子类别的框架以及它们之间的关系和差异。其次，我们对可以从专门的LLM中显著受益的关键应用程序领域进行了广泛的分类，讨论了它们的实际意义和公开挑战。最后，我们对该领域的研究现状和未来趋势提出了见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18703v3" target="_blank">2305.18703v3</a>
                              </td>
                              <td>Large Language Models, Natural Language Processing, Domain Specialization</td>
                              <td>Chen Ling</td>
                              <td>2023-05-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18703v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18703v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04601v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04601v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04601v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04601v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (IR) tasks. The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instructions and a few examples on a prompt. InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as synthetic data generators for IR tasks. This makes them an attractive solution for IR tasks that suffer from a lack of annotated data. However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which are not widely accessible -- and because the code for Promptagator was not released and its proprietary LLM is not publicly accessible. To fully realize the potential of these methods and make their impact more widespread in the research community, the resources need to be accessible and easy to reproduce by researchers and practitioners. Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation. Additionally, we provide an interface to IR libraries widely used by the community and support for GPU. Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-play functionality allowing the use of different LLMs, exploring filtering methods and finetuning various reranker models on the generated data. We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models finetuned on the synthetic data. Code and data are available at https://github.com/zetaalphavector/InPars</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04601v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作探索了大型语言模型（LLM），以克服信息检索（IR）任务缺乏训练数据的问题。这些模型的泛化能力使得能够通过在提示下提供指令和一些示例来创建合成的域内数据。InPars和Promptagator开创了这种方法，这两种方法都证明了使用LLM作为IR任务的合成数据生成器的潜力。这使得它们成为缺乏注释数据的IR任务的一个有吸引力的解决方案。然而，这些方法的可重复性有限，因为InPars的训练脚本是基于TPU的——这些TPU不可广泛访问——而且Promptagator的代码没有发布，其专有LLM也不可公开访问。为了充分实现这些方法的潜力，并使其在研究界的影响更加广泛，研究人员和从业者需要能够获得这些资源，并易于复制。我们的主要贡献是为端到端可复制合成数据生成研究提供统一的工具包，包括生成、过滤、培训和评估。此外，我们还为社区广泛使用的IR库提供了接口，并支持GPU。我们的工具包不仅再现了InPars方法并部分再现了Promptagator，而且还提供了即插即用功能，允许使用不同的LLM，探索过滤方法并对生成的数据微调各种重新排序模型。我们还提供了BEIR基准中18个不同数据集的所有合成数据，这些数据需要2000多个GPU小时才能生成，以及对合成数据进行微调的重新排序模型。代码和数据可在https://github.com/zetaalphavector/InPars</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04601v1" target="_blank">2307.04601v1</a>
                              </td>
                              <td>InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval</td>
                              <td>Hugo Abonizio</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04601v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04601v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18185v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18185v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18185v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18185v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms -- i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks. Code is available at https://github.com/lindiatjuatja/lm_sem</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18185v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的最新进展促使研究人员检查他们在各种语言任务中的能力，但很少有人研究模型如何处理单词和更大句法形式之间的意义交互，即句法和语义交叉处的现象。我们提出了代理性的语义概念，作为探索这种互动的案例研究。我们利用选择性及物动词子集的独特语言特性创建了一个新的评估数据集。该数据集用于提示不同大小的三个模型类，看看它们是否对词汇层面的能动性敏感，以及在给定特定句法上下文的情况下，它们是否可以适当地使用这些单词层面的先验。总的来说，GPT-3 text-davinci-003在所有实验中都表现得非常好，优于迄今为止测试的所有其他模型。事实上，与句法和语义语料库统计相比，这些结果与人类判断的相关性甚至更好。这表明，与为某些任务选择语料库相比，LMs可能是语言注释、理论测试和发现的更有用的工具。代码位于https://github.com/lindiatjuatja/lm_sem</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18185v2" target="_blank">2305.18185v2</a>
                              </td>
                              <td>Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity</td>
                              <td>Lindia Tjuatja</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18185v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18185v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04492v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Calculating Originality of LLM Assisted Source Code</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04492v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04492v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04492v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ease of using a Large Language Model (LLM) to answer a wide variety of queries and their high availability has resulted in LLMs getting integrated into various applications. LLM-based recommenders are now routinely used by students as well as professional software programmers for code generation and testing. Though LLM-based technology has proven useful, its unethical and unattributed use by students and professionals is a growing cause of concern. As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes. Our tool is motivated by minimum description length measures like Kolmogorov complexity. Our initial experiments with moderate sized (up to 500 lines of code) have shown promising results that we report in this paper.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04492v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大型语言模型（LLM）回答各种查询的方便性及其高可用性导致LLM集成到各种应用程序中。基于LLM的推荐器现在被学生和专业软件程序员用于代码生成和测试。尽管基于LLM的技术已被证明是有用的，但学生和专业人士对其不道德和不负责任的使用越来越令人担忧。因此，需要有助于教师和其他评估人员识别源代码的任何部分是否是LLM生成的工具和技术。在本文中，我们提出了一种基于神经网络的工具，教师可以使用该工具来确定学生在编写源代码时的原始努力（以及LLM的贡献）。我们的工具是由最小描述长度度量（如Kolmogorov复杂性）驱动的。我们用中等大小（最多500行代码）进行的初步实验显示了我们在本文中报告的有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04492v1" target="_blank">2307.04492v1</a>
                              </td>
                              <td>Calculating Originality of LLM Assisted Source Code</td>
                              <td>Shipra Sharma</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04492v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04492v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09170v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can ChatGPT pass the Vietnamese National High School Graduation Examination?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09170v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09170v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09170v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This research article highlights the potential of AI-powered chatbots in education and presents the results of using ChatGPT, a large language model, to complete the Vietnamese National High School Graduation Examination (VNHSGE). The study dataset included 30 essays in the literature test case and 1,700 multiple-choice questions designed for other subjects. The results showed that ChatGPT was able to pass the examination with an average score of 6-7, demonstrating the technology's potential to revolutionize the educational landscape. The analysis of ChatGPT performance revealed its proficiency in a range of subjects, including mathematics, English, physics, chemistry, biology, history, geography, civic education, and literature, which suggests its potential to provide effective support for learners. However, further research is needed to assess ChatGPT performance on more complex exam questions and its potential to support learners in different contexts. As technology continues to evolve and improve, we can expect to see the use of AI tools like ChatGPT become increasingly common in educational settings, ultimately enhancing the educational experience for both students and educators.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09170v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这篇研究文章强调了人工智能聊天机器人在教育中的潜力，并介绍了使用大型语言模型ChatGPT完成越南国家高中毕业考试（VNHSGE）的结果。该研究数据集包括文献测试案例中的30篇文章和1700道为其他科目设计的多项选择题。结果显示，ChatGPT能够以6-7分的平均成绩通过考试，这表明了该技术对教育格局的革命性影响。对ChatGPT表现的分析显示，它精通一系列科目，包括数学、英语、物理、化学、生物、历史、地理、公民教育和文学，这表明它有潜力为学习者提供有效支持。然而，还需要进一步的研究来评估ChatGPT在更复杂的考试问题上的表现及其在不同背景下支持学习者的潜力。随着技术的不断发展和改进，我们可以看到像ChatGPT这样的人工智能工具在教育环境中的使用越来越普遍，最终增强了学生和教育工作者的教育体验。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09170v3" target="_blank">2306.09170v3</a>
                              </td>
                              <td>Can ChatGPT pass the Vietnamese National High School Graduation Examination?</td>
                              <td>Xuan-Quy Dao</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09170v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09170v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17181v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17181v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17181v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17181v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17181v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成对抗性网络（GAN）是一种数据合成模型，它通过生成器和鉴别器的竞争来创建可信的数据。尽管GAN在图像合成中的应用得到了广泛的研究，但它对自然语言生成有着固有的局限性。由于自然语言是由离散的标记组成的，生成器很难通过反向传播来更新其梯度；因此，大多数文本GAN研究都是从基于奖励系统的随机令牌开始生成句子的。因此，先前研究的生成器在对抗性训练之前以自回归的方式进行预训练，导致合成句子再现训练数据的数据记忆。在本文中，我们使用类似于原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗性网络（TESGAN），它生成连续的文本嵌入空间而不是离散的令牌来解决梯度反向传播问题。此外，TESGAN进行无监督学习，不直接参考训练数据的文本，以克服数据记忆问题。通过采用这种新颖的方法，TESGAN可以合成新的句子，显示出无监督学习在文本合成中的潜力。我们希望看到将大型语言模型与将文本视为连续空间的新视角相结合的扩展研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17181v2" target="_blank">2306.17181v2</a>
                              </td>
                              <td>Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis</td>
                              <td>Jun-Min Lee</td>
                              <td>2023-06-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17181v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17181v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04412v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04412v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04412v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04412v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04412v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生物医学摘要需要大型数据集来训练文本生成。我们表明，虽然迁移学习为解决这一挑战提供了一个可行的选择，但在BioASQ摘要任务中，域内预训练并不总是具有优势。我们确定了一个合适的模型架构，并使用它来展示在BioASQ摘要任务的背景下进行一般领域预训练和特定任务微调的好处，从而产生了一种新的三步微调方法，该方法仅适用于1000个领域内示例。我们的结果表明，没有特定领域预训练的大型语言模型在某些特定领域的生物医学文本生成任务中可能具有显著优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04412v1" target="_blank">2307.04412v1</a>
                              </td>
                              <td>Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training</td>
                              <td>Dima Galat</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04412v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04412v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04408v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TIM: Teaching Large Language Models to Translate with Comparison</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04408v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04408v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04408v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04408v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开源大型语言模型（LLM）在教学调整的各种任务中表现出了显著的功效。然而，这些模型有时会在需要更专业知识的任务（如翻译）中遇到困难。这种不足的一个可能原因是，教学调整旨在生成流畅连贯的文本，该文本从给定的教学中延续，而不受任何特定任务要求的约束。此外，用较低质量的训练数据调整较小的LLM可能更具挑战性。为了解决这个问题，我们提出了一个新颖的框架，通过比较的例子来教LLM学习翻译。我们的方法包括向模型提供正确和不正确翻译的例子，并使用偏好损失来指导模型的学习。我们在WMT2022测试集上评估了我们的方法，并表明它优于现有方法。我们的研究结果为翻译任务的LLM微调提供了一个新的视角，并为生成高质量的翻译提供了一种有前景的解决方案。有关更多详细信息，请参阅Github：https://github.com/lemon0830/TIM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04408v1" target="_blank">2307.04408v1</a>
                              </td>
                              <td>TIM: Teaching Large Language Models to Translate with Comparison</td>
                              <td>Jiali Zeng</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04408v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04408v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03393v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03393v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03393v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03393v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03393v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图形学习由于其在现实世界中的广泛应用而引起了人们的极大关注。在具有文本节点属性的图上学习最流行的管道主要依赖于图神经网络（GNN），并利用浅文本嵌入作为初始节点表示，这在一般知识和深刻的语义理解方面具有局限性。近年来，大型语言模型（LLM）已被证明具有广泛的公共知识和强大的语义理解能力，这彻底改变了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLM在图机器学习中的潜力，特别是在节点分类任务中，并研究两种可能的管道：LLM作为增强器和LLM作为预测器。前者利用LLM利用其海量知识增强节点的文本属性，然后通过GNN生成预测。后者试图直接使用LLM作为独立的预测因子。我们在不同的环境下对这两条管道进行了全面、系统的研究。从全面的实证结果中，我们进行了原始的观察，发现了新的见解，这些见解开辟了新的可能性，并提出了利用LLM在图上学习的有希望的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03393v2" target="_blank">2307.03393v2</a>
                              </td>
                              <td>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</td>
                              <td>Zhikai Chen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03393v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03393v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04349v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RLTF: Reinforcement Learning from Unit Test Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04349v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04349v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04349v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces. Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code can be found at: https://github.com/Zyq-scut/RLTF.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04349v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>程序合成或代码生成的目标是根据给定的描述生成可执行代码。最近，越来越多的研究采用强化学习（RL）来提高代码的大型语言模型（LLM）的性能。然而，这些RL方法只使用了离线框架，限制了它们对新样本空间的探索。此外，当前使用单元测试信号的方法相当简单，不考虑代码中的特定错误位置。为了解决这些问题，我们提出了RLTF，即从单元测试反馈中强化学习，这是一种新的在线RL框架，具有多粒度的单元测试反馈，用于细化代码LLM。我们的方法在训练期间实时生成数据，同时利用细粒度反馈信号来引导模型生成更高质量的代码。大量实验表明，RLTF在APPS和MBPP基准测试上实现了最先进的性能。我们的代码可以在以下位置找到：https://github.com/Zyq-scut/RLTF.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04349v1" target="_blank">2307.04349v1</a>
                              </td>
                              <td>RLTF: Reinforcement Learning from Unit Test Feedback</td>
                              <td>Jiate Liu</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04349v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04349v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04346v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Large Language Models Write Good Property-Based Tests?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04346v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04346v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04346v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests. We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT. We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests. PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in $\texttt{numpy}$, $\texttt{networkx}$, and $\texttt{datetime}$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04346v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于属性的测试（PBT）虽然是软件测试研究界的一项既定技术，但在现实世界的软件中仍然相对未得到充分利用。编写基于属性的测试的难点包括实现不同的随机输入生成器和思考要测试的有意义的属性。然而，开发人员更愿意编写文档；有大量库API文档可用，可以用作基于属性的测试的自然语言规范。由于大型语言模型（LLM）最近在各种编码任务中显示出了前景，我们探索了使用LLM来合成基于属性的测试的潜力。我们将我们的方法称为PBT-GPT，并提出了三种不同的策略来促进PBT的LLM。我们描述了PBT-GPT的各种失效模式，并详细介绍了基于性能的自动合成测试的评估方法。PBT-GPT在我们对$\texttt｛numpy｝$、$\textett｛networkx｝$和$\textt｛datetime｝$中的示例Python库API的初步研究中取得了有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04346v1" target="_blank">2307.04346v1</a>
                              </td>
                              <td>Can Large Language Models Write Good Property-Based Tests?</td>
                              <td>Vasudev Vikram</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04346v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04346v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02088v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02088v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02088v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02088v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Software Bill of Materials (SBOM) serves as a critical pillar in ensuring software supply chain security by providing a detailed inventory of the components and dependencies integral to software development. However, challenges abound in the sharing of SBOMs, including potential data tampering, hesitation among software vendors to disclose comprehensive information, and bespoke requirements from software procurers or users. These obstacles have stifled widespread adoption and utilization of SBOMs, underscoring the need for a more secure and flexible mechanism for SBOM sharing. This study proposes a novel solution to these challenges by introducing a blockchain-empowered approach for SBOM sharing, leveraging verifiable credentials to allow for selective disclosure. This strategy not only heightens security but also offers flexibility. Furthermore, this paper broadens the remit of SBOM to encompass AI systems, thereby coining the term AI Bill of Materials (AIBOM). This extension is motivated by the rapid progression in AI technology and the escalating necessity to track the lineage and composition of AI software and systems. Particularly in the era of foundational models like large language models (LLMs), understanding their composition and dependencies becomes crucial. These models often serve as a base for further development, creating complex dependencies and paving the way for innovative AI applications. The evaluation of our solution indicates the feasibility and flexibility of the proposed SBOM sharing mechanism, positing a new solution for securing (AI) software supply chains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02088v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>软件材料清单（SBOM）是确保软件供应链安全的关键支柱，它提供了软件开发不可或缺的组件和依赖项的详细清单。然而，SBOM的共享面临诸多挑战，包括潜在的数据篡改、软件供应商在披露全面信息方面的犹豫，以及软件采购商或用户的定制要求。这些障碍阻碍了SBOM的广泛采用和利用，突显出需要一种更安全、更灵活的SBOM共享机制。这项研究通过引入区块链授权的SBOM共享方法，利用可验证的凭证进行选择性披露，为这些挑战提出了一种新的解决方案。这种策略不仅提高了安全性，而且提供了灵活性。此外，本文将SBOM的范围扩大到包括人工智能系统，从而产生了人工智能材料清单（AIBOM）一词。这一扩展的动机是人工智能技术的快速发展，以及追踪人工智能软件和系统的谱系和组成的必要性不断增加。特别是在像大型语言模型（LLM）这样的基础模型时代，了解它们的组成和依赖关系变得至关重要。这些模型通常是进一步开发的基础，产生复杂的依赖关系，并为创新的人工智能应用铺平道路。对我们的解决方案的评估表明了所提出的SBOM共享机制的可行性和灵活性，为保护（AI）软件供应链提供了一种新的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02088v2" target="_blank">2307.02088v2</a>
                              </td>
                              <td>Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future</td>
                              <td>Boming Xia</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02088v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02088v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04317v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04317v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04317v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04317v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04317v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代图像分类是基于通过大型判别网络直接预测模型类别，这使得很难评估可能构成分类决策的直观视觉“特征”。同时，最近在联合视觉语言模型（如CLIP）中的工作提供了指定图像类的自然语言描述的方法，但通常侧重于为每个类提供单个描述。在这项工作中，我们证明了一种替代方法，可以说更类似于我们对每类多个“视觉特征”的理解，也可以在稳健的少镜头学习环境中提供令人信服的性能。特别是，我们通过大型语言模型（LLM）自动枚举每个类别的多个视觉描述，然后使用视觉图像模型将这些描述翻译为每个图像的一组多个视觉特征；最后，我们使用稀疏逻辑回归来选择这些特征的相关子集来对每个图像进行分类。这既为每个类提供了一组“直观”的相关特征，又在少镜头学习环境中，优于线性探测等标准方法。当与微调相结合时，我们还表明，该方法能够在分布内和分布外性能上优于现有的最先进的微调方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04317v1" target="_blank">2307.04317v1</a>
                              </td>
                              <td>Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</td>
                              <td>Zhili Feng</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04317v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04317v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04280v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Shaping the Emerging Norms of Using Large Language Models in Social Computing Research</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04280v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04280v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04280v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research. On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena. On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are involved. This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and collectively shaping the emerging norms of using LLMs in social computing research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04280v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的出现给社会计算研究带来了兴奋和关注。一方面，LLM在分析大量文本数据和生成类似人类的反应方面提供了前所未有的能力，使研究人员能够深入研究复杂的社会现象。另一方面，当LLM参与时，人们对研究的有效性、隐私和伦理也产生了担忧。该SIG旨在为有兴趣了解LLM影响的社会计算研究人员提供一个开放的空间，以讨论他们在日常工作中与LLM接触时的当前实践、观点和挑战，并共同形成在社会计算研究中使用LLM的新兴规范。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04280v1" target="_blank">2307.04280v1</a>
                              </td>
                              <td>Shaping the Emerging Norms of Using Large Language Models in Social Computing Research</td>
                              <td>Hong Shen</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04280v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04280v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04274v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Assessing the efficacy of large language models in generating accurate teacher responses</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04274v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04274v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04274v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04274v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>（Tack et al.，2023）组织了第18届创新利用NLP构建教育应用研讨会主办的关于在教育对话中生成教师语言的共同任务。根据共享任务的结构，在本研究中，我们试图评估大型语言模型在向学生提供信息和有用见解方面的生成能力，从而模拟知识渊博的教师的角色。为此，我们对几个基准生成模型进行了广泛的评估，包括GPT-4（少镜头，上下文学习）、微调的GPT-2和微调的DialoGPT。此外，为了优化教学质量，我们使用强化学习对Flan-T5模型进行了微调。我们在师生聊天室语料库子集上的实验结果表明，GPT-4优于其他微调模型，使用BERTScore和DialogRPT测量。我们假设，包括采样、代表性和对话框完整性在内的几个数据集特征对微调提出了重大挑战，从而导致微调模型的可推广性较差。最后，我们注意到，需要用一种指标来评估这些生成模型，该指标不仅依赖于对话连贯性和匹配的语言建模分布，还依赖于模型展示教学技能的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04274v1" target="_blank">2307.04274v1</a>
                              </td>
                              <td>Assessing the efficacy of large language models in generating accurate teacher responses</td>
                              <td>Yann Hicke</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04274v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04274v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04251v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04251v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04251v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04251v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04251v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ChatGPT是OpenAI创建的一个大型语言模型（LLM），经过大量数据的精心训练。它彻底改变了自然语言处理（NLP）领域，并突破了LLM功能的界限。ChatGPT在实现与生成人工智能（GAI）的大规模广泛公众互动方面发挥了关键作用。它还激发了开发类似技术并调查其应用和影响的研究兴趣。在本文中，我们的主要目标是对ChatGPT及其演变的当前研究方向进行简要的调查。我们考虑了ChatGPT的玻璃盒和黑盒视图，包括该技术的组件和基本元素，以及其应用、影响和含义。玻璃盒方法侧重于理解技术的内部工作原理，而黑盒方法将其视为一个复杂的系统，从而检查其输入、输出和效果。这为该技术的全面探索铺平了道路，并为进一步的研究和实验提供了路线图。我们还列出了关于LLM和GAI的基本基础文献，以及它们与ChatGPT的联系。这一概述揭示了LLM新兴领域中现有和缺失的研究路线，使公众用户和开发人员都受益。此外，本文深入探讨了教育、研究、医疗保健、金融等领域的广泛应用和重大问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04251v1" target="_blank">2307.04251v1</a>
                              </td>
                              <td>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</td>
                              <td>Salman Mohamadi</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04251v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04251v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_10264v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_10264v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_10264v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_10264v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_10264v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一种新型的测试，称为图灵实验（TE），用于评估给定的语言模型（如GPT模型）在多大程度上可以模拟人类行为的不同方面。TE还可以揭示语言模型对特定人类行为的模拟中的一致扭曲。与图灵测试不同，图灵测试涉及模拟单个任意个体，TE需要模拟人类主题研究参与者的代表性样本。我们进行了TE，试图复制先前研究的既定发现。我们设计了一种模拟TE的方法，并说明了它的使用，以比较不同的语言模型在多大程度上能够再现经典的经济、心理语言学和社会心理学实验：终极游戏、花园小径句子、米尔格拉姆休克实验和群众智慧。在前三个TE中，使用最近的模型复制了现有的发现，而最后一个TE揭示了一些语言模型（包括ChatGPT和GPT-4）中存在的“超准确性失真”，这可能会影响教育和艺术的下游应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.10264v5" target="_blank">2208.10264v5</a>
                              </td>
                              <td>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</td>
                              <td>Gati Aher</td>
                              <td>2022-08-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_10264v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.10264v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_00008v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Creativity of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_00008v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_00008v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_00008v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion around the dimensions of value, novelty and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press and person. We discuss a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered by them, the challenges arising by them and the potential associated risks, from both legal and ethical points of view.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_00008v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）正在改变人工智能的几个领域。最显著的应用之一是创造性写作，例如诗歌或讲故事：产生的产出往往具有惊人的质量。然而，一个自然的问题出现了：LLM真的可以被认为是创造性的吗？在这篇文章中，我们首先从创造力理论的角度分析了LLM的发展，探讨了关键的开放问题和挑战。特别是，我们围绕玛格丽特·博登在其作品中提出的价值、新颖性和惊喜的维度进行了讨论。然后，我们考虑不同的经典视角，即产品、过程、媒体和人。我们讨论了机器创造力中的一组“容易”和“难”问题，并将它们与LLM联系起来。最后，我们研究了这些技术的社会影响，特别关注创意产业，从法律和伦理的角度分析了这些技术提供的机会、带来的挑战以及潜在的相关风险。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.00008v3" target="_blank">2304.00008v3</a>
                              </td>
                              <td>On the Creativity of Large Language Models</td>
                              <td>Giorgio Franceschelli</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_00008v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.00008v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01370v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multilingual Language Models are not Multicultural: A Case Study in Emotion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01370v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01370v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01370v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01370v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>世界各地的情绪体验和表达方式各不相同。为了将大型语言模型用于需要情感敏感性的多语言任务，大型语言模型必须反映情感中的这种文化差异。在这项研究中，我们调查了2023年广泛使用的多语言LMs是否反映了不同文化和语言的情感表达差异。我们发现，从LMs（例如XLM-RoBERTa）获得的嵌入是以英语为中心的，而生成LMs（如ChatGPT）反映了西方规范，即使在响应其他语言的提示时也是如此。我们的研究结果表明，多语言LMs不能成功地学习到与文化相适应的情感细微差别，我们强调了纠正这一点的可能研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01370v2" target="_blank">2307.01370v2</a>
                              </td>
                              <td>Multilingual Language Models are not Multicultural: A Case Study in Emotion</td>
                              <td>Shreya Havaldar</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01370v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01370v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04172v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Generative Large Language Models Perform ASR Error Correction?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04172v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04172v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04172v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04172v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ASR纠错一直是语音识别系统后处理的重要组成部分。传统上，使用底层ASR系统的解码结果和参考文本，通过监督训练来训练这些模型。这种方法计算密集，并且在切换底层ASR模型时需要重新训练模型。近年来，大型语言模型得到了发展，它们能够以零样本的方式执行自然语言处理任务。本文以ChatGPT为例，研究了它在零样本或单快照设置下执行ASR错误纠正的能力。我们使用ASR N最佳列表作为模型输入，并提出了无约束误差校正和N最佳约束误差校正方法。Conformer Transducer模型和预训练的Whisper模型的结果表明，使用强大的ChatGPT模型，我们可以通过纠错大大提高ASR系统的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04172v1" target="_blank">2307.04172v1</a>
                              </td>
                              <td>Can Generative Large Language Models Perform ASR Error Correction?</td>
                              <td>Rao Ma</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04172v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04172v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04057v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bidirectional Attention as a Mixture of Continuous Word Experts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04057v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04057v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04057v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous data. It also suggests an immediate extension to categorical tabular data, if we view each word location in a sentence as a tabular feature. Across empirical studies, we find that this extension outperforms existing tabular extensions of transformers in out-of-distribution (OOD) generalization. Finally, this statistical perspective of bidirectional attention enables us to theoretically characterize when linear word analogies are present in its word embeddings. These analyses show that bidirectional attention can require much stronger assumptions to exhibit linear word analogies than its non-attention predecessors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04057v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>双向注意$\unicode{x2013}$由具有位置编码的自注意和掩蔽语言模型（MLM）目标$\unicode{x2013}$组成，已成为现代大型语言模型（LLM）的关键组成部分。尽管它在实证上取得了成功，但很少有研究考察它的统计基础：双向注意力隐含地适合什么统计模型？是什么使它与那些不受关注的前辈不同？我们在本文中探讨了这些问题。关键的观察结果是，在重新参数化后，拟合单层单头双向注意力相当于拟合具有混合专家权重的连续单词袋（CBOW）模型。此外，具有多个头和多层的双向注意力分别等同于堆叠的MoE和MoE的混合物。这种统计观点揭示了MoE在双向注意力中的独特用途，这与其在处理异构数据方面的实际有效性相一致。如果我们将句子中的每个单词位置视为表格特征，它还建议立即扩展分类表格数据。通过实证研究，我们发现这种扩展在分布外（OOD）泛化方面优于现有的变压器表格扩展。最后，这种双向注意力的统计视角使我们能够从理论上表征线性单词类比何时出现在其单词嵌入中。这些分析表明，与非注意的前辈相比，双向注意可能需要更强的假设才能表现出线性单词类比。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04057v1" target="_blank">2307.04057v1</a>
                              </td>
                              <td>Bidirectional Attention as a Mixture of Continuous Word Experts</td>
                              <td>Kevin Christian Wibisono</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04057v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04057v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06569v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How to Index Item IDs for Recommendation Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06569v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06569v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06569v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our reproducibility study of P5 highlights the significant influence of item indexing methods on the model performance, and our results on real-world datasets validate the effectiveness of our proposed solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06569v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>推荐基础模型通过将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它实现了生成推荐，直接生成要推荐的项目，而不是计算传统推荐模型中每个候选项目的排名分数，从而简化了从多级过滤到单级过滤的推荐流程。为了避免在决定推荐哪些项目时生成过长的文本，创建LLM兼容的项目ID对于推荐基础模型至关重要。在本研究中，我们系统地研究了推荐基础模型的项目索引问题，使用P5作为代表性的主干模型，并用各种索引方法复制了其结果。为了强调项目索引的重要性，我们首先讨论了几种琐碎的项目索引方法的问题，如独立索引、标题索引和随机索引。然后，我们提出了四种简单而有效的解决方案，包括顺序索引、协作索引、语义（基于内容）索引和混合索引。我们对P5的再现性研究强调了项目索引方法对模型性能的显著影响，我们在真实世界数据集上的结果验证了我们提出的解决方案的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06569v4" target="_blank">2305.06569v4</a>
                              </td>
                              <td>How to Index Item IDs for Recommendation Foundation Models</td>
                              <td>Wenyue Hua</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06569v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06569v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03987v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03987v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03987v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03987v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of 88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3 model from 47.5% to 14.5% on average. In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03987v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近开发的大型语言模型在生成流畅连贯的文本方面取得了显著的成功。然而，这些模型往往会产生“幻觉”，这严重阻碍了它们的可靠性。在这项工作中，我们解决了这一关键问题，并提出了一种在生成过程中积极检测和缓解幻觉的方法。具体而言，我们首先利用模型的logit输出值识别潜在幻觉的候选者，通过验证程序检查其正确性，缓解检测到的幻觉，然后继续生成过程。通过“文章生成任务”的广泛实验，我们首先证明了我们的检测和缓解技术的个体功效。具体而言，检测技术实现了88%的回忆，缓解技术成功缓解了57.6%的正确检测到的幻觉。重要的是，即使在错误检测到幻觉（即假阳性）的情况下，我们的缓解技术也不会引入新的幻觉。然后，我们表明，所提出的主动检测和缓解方法成功地将GPT-3模型的幻觉平均从47.5%减少到14.5%。总之，我们的工作有助于提高大型语言模型的可靠性和可信度，这是实现其在现实世界应用中广泛采用的关键一步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03987v1" target="_blank">2307.03987v1</a>
                              </td>
                              <td>A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation</td>
                              <td>Neeraj Varshney</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03987v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03987v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03972v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03972v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03972v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03972v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chinese GEC task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03972v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模语言模型（LLM）在各种自然语言处理（NLP）任务中表现出了非凡的能力，近年来引起了人们的广泛关注。然而，一些研究表明，在英语语法纠错（GEC）任务中，除了最先进的模型之外，大型语言模型并没有取得有希望的结果。在本报告中，我们旨在探索大型语言模型在汉语语法纠错任务中的表现，并为未来的工作提供指导。我们在4个中国GEC数据集上用3种不同模型规模的LLM进行了实验。我们的实验结果表明，由于过度校正的问题，LLM在自动评估度量上的性能不如以前的sota模型。此外，当对不同的数据分布进行评估时，我们还发现LLM的性能存在显著差异。我们的研究结果表明，LLM在中国GEC任务中的应用需要进一步的调查。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03972v1" target="_blank">2307.03972v1</a>
                              </td>
                              <td>Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task</td>
                              <td>Fanyi Qu</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03972v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03972v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03941v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03941v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03941v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03941v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unlearning, model editing, and prompting engineering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03941v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>被遗忘权（RTBF）最初是由Google Spain SL、Google Inc.v AEPD、Mario Costeja Gonz\'alez的裁决确立的，后来被纳入欧盟一般数据保护条例（GDPR）下的擦除权，以允许个人有权请求组织删除个人数据。特别是对于搜索引擎，个人可以向组织发送请求，将其信息从查询结果中排除。随着大型语言模型（LLM）的最新发展及其在聊天机器人中的使用，支持LLM的软件系统变得流行起来。但他们并没有被排除在RTBF之外。与搜索引擎使用的索引方法相比，LLM以完全不同的方式存储和处理信息。这对遵守RTBF提出了新的挑战。在本文中，我们探讨了这些挑战，并就如何实现RTBF的技术解决方案提供了我们的见解，包括使用机器遗忘、模型编辑和提示工程。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03941v1" target="_blank">2307.03941v1</a>
                              </td>
                              <td>Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions</td>
                              <td>Dawen Zhang</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03941v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03941v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07230v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07230v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07230v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07230v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have significantly advanced the field of natural language processing, with GPT models at the forefront. While their remarkable performance spans a range of tasks, adapting LLMs for real-world business scenarios still poses challenges warranting further investigation. This paper presents an empirical analysis aimed at bridging the gap in adapting LLMs to practical use cases. To do that, we select the question answering (QA) task of insurance as a case study due to its challenge of reasoning. Based on the task we design a new model relied on LLMs which are empowered by additional knowledge extracted from insurance policy rulebooks and DBpedia. The additional knowledge helps LLMs to understand new concepts of insurance for domain adaptation. Preliminary results on two QA datasets show that knowledge enhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and 57.83% in terms of accuracy). The analysis also indicates that existing public knowledge bases, e.g., DBPedia is beneficial for knowledge enhancement. Our findings reveal that the inherent complexity of business scenarios often necessitates the incorporation of domain-specific knowledge and external resources for effective problem-solving.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07230v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在自然语言处理领域取得了重大进展，GPT模型处于领先地位。尽管LLM的卓越性能涵盖了一系列任务，但将其适应现实世界的业务场景仍然带来了挑战，需要进一步调查。本文提出了一个实证分析，旨在弥合LLM适应实际用例的差距。为了做到这一点，我们选择了保险的问答（QA）任务作为案例研究，因为它具有推理的挑战性。基于这项任务，我们设计了一个基于LLM的新模型，该模型通过从保险单规则手册和DBpedia中提取的额外知识来增强。这些额外的知识有助于LLM理解领域适应保险的新概念。在两个QA数据集上的初步结果表明，知识增强显著提高了GPT-3.5的推理能力（准确率分别为55.80%和57.83%）。分析还表明，现有的公共知识库，如DBPedia，有利于知识的增强。我们的研究结果表明，业务场景的固有复杂性往往需要结合特定领域的知识和外部资源来有效解决问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07230v2" target="_blank">2305.07230v2</a>
                              </td>
                              <td>When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust</td>
                              <td>Minh-Tien Nguyen</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07230v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07230v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01388v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From Large Language Models to Databases and Back: A discussion on research and education</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01388v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01388v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01388v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This discussion was conducted at a recent panel at the 28th International Conference on Database Systems for Advanced Applications (DASFAA 2023), held April 17-20, 2023 in Tianjin, China. The title of the panel was "What does LLM (ChatGPT) Bring to Data Science Research and Education? Pros and Cons". It was moderated by Lei Chen and Xiaochun Yang. The discussion raised several questions on how large language models (LLMs) and database research and education can help each other and the potential risks of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01388v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这一讨论是在2023年4月17日至20日于中国天津举行的第28届高级应用数据库系统国际会议（DASFA2023）上的一个小组讨论会上进行的。该小组的标题是“LLM（ChatGPT）给数据科学研究和教育带来了什么？优点和缺点”。由陈磊和杨晓春主持。讨论提出了几个问题，即大型语言模型（LLM）与数据库研究和教育如何相互帮助，以及LLM的潜在风险。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01388v2" target="_blank">2306.01388v2</a>
                              </td>
                              <td>From Large Language Models to Databases and Back: A discussion on research and education</td>
                              <td>Sihem Amer-Yahia</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01388v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01388v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18404v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Conformal Prediction with Large Language Models for Multi-Choice Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18404v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18404v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18404v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18404v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型的不断广泛开发，稳健的不确定性量化技术将成为其在高风险场景中安全部署的关键。在这项工作中，我们探索了如何使用保形预测在语言模型中为多项选择题回答的特定任务提供不确定性量化。我们发现保角预测的不确定性估计与预测精度密切相关。这一观察结果可用于下游应用，如选择性分类和过滤低质量预测。我们还研究了对主题外问题进行保角预测所需的可交换性假设，这可能是许多实际应用中更现实的场景。我们的工作有助于在安全关键的情况下更可信和可靠地使用大型语言模型，在这种情况下，需要可靠的错误率保证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18404v3" target="_blank">2305.18404v3</a>
                              </td>
                              <td>Conformal Prediction with Large Language Models for Multi-Choice Question Answering</td>
                              <td>Bhawesh Kumar</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18404v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18404v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03875v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models for Supply Chain Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03875v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03875v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03875v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead of supplier A for a given demand?). Importantly, our design does not require sending proprietary data over to LLMs, which can be a privacy concern in some circumstances. We demonstrate the effectiveness of our framework on a real server placement scenario within Microsoft's cloud supply chain. Along the way, we develop a general evaluation benchmark, which can be used to evaluate the accuracy of the LLM output in other scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03875v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>供应链运营传统上涉及各种复杂的决策问题。在过去的几十年里，供应链极大地受益于计算的进步，这使得从手工处理过渡到自动化和成本效益高的优化。尽管如此，业务运营商仍然需要花费大量精力向利益相关者解释和解释优化结果。受大型语言模型（LLM）最新进展的启发，我们研究了这种颠覆性技术如何帮助弥合供应链自动化与人类理解和信任之间的差距。我们设计了\name｛｝——一个框架，它接受纯文本的输入查询，并输出有关底层优化结果的见解。我们的框架并没有放弃最先进的组合优化技术，而是利用它来定量回答假设情况（例如，如果我们使用供应商B而不是供应商A来满足给定的需求，成本会如何变化？）。重要的是，我们的设计不需要将专有数据发送给LLM，这在某些情况下可能会引起隐私问题。我们在微软云供应链中的真实服务器部署场景中展示了我们的框架的有效性。在此过程中，我们开发了一个通用的评估基准，可用于评估LLM输出在其他场景中的准确性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03875v1" target="_blank">2307.03875v1</a>
                              </td>
                              <td>Large Language Models for Supply Chain Optimization</td>
                              <td>Beibin Li</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03875v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03875v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03838v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RADAR: Robust AI-Text Detection via Adversarial Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03838v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03838v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03838v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03838v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展和类似ChatGPT的应用程序的日益流行模糊了人类和机器之间高质量文本生成的边界。然而，除了我们的技术和社会预期的革命性变化之外，区分LLM生成的文本（AI文本）和人类生成的文本的困难也带来了滥用和公平的新挑战，例如虚假内容生成、剽窃和对无辜作者的诬告。虽然现有工作表明，当前的人工智能文本检测器对基于LLM的转述不具有鲁棒性，但本文旨在通过提出一个名为RADAR的新框架来弥合这一差距，该框架通过对抗性学习联合训练鲁棒的人工智能文本检测器。雷达是基于一个转述者和一个检测器的对抗性训练。转述者的目标是生成逼真的内容来躲避人工智能文本检测。雷达使用来自探测器的反馈来更新转述器，反之亦然。在4个数据集上用8种不同的LLM（Pythia、Dolly 2.0、Palmyra、Camel、GPT-J、Dolly 1.0、LLaMA和Vicuna）进行评估，实验结果表明，RADAR显著优于现有的人工智能文本检测方法，尤其是在转述到位的情况下。我们还确定了RADAR从指令调谐LLM到其他LLM的强大可转移性，并通过GPT-3.5评估了RADAR的改进能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03838v1" target="_blank">2307.03838v1</a>
                              </td>
                              <td>RADAR: Robust AI-Text Detection via Adversarial Learning</td>
                              <td>Xiaomeng Hu</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03838v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03838v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03817v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring and Characterizing Large Language Models For Embedded System Development and Debugging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03817v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03817v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03817v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have shown remarkable abilities to generate code, however their ability to develop software for embedded systems, which requires cross-domain knowledge of hardware and software has not been studied. In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to assess their performance for embedded system development, study how human programmers interact with these tools, and develop an AI-based software engineering workflow for building embedded systems.   We develop an an end-to-end hardware-in-the-loop evaluation platform for verifying LLM generated programs using sensor actuator pairs. We compare all three models with N=450 experiments and find surprisingly that GPT-4 especially shows an exceptional level of cross-domain understanding and reasoning, in some cases generating fully correct programs from a single prompt. In N=50 trials, GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces register-level drivers, code for LoRa communication, and context-specific power optimizations for an nRF52 program resulting in over 740x current reduction to 12.2 uA. We also characterize the models' limitations to develop a generalizable workflow for using LLMs in embedded system development. We evaluate the workflow with 15 users including novice and expert programmers. We find that our workflow improves productivity for all users and increases the success rate for building a LoRa environmental sensor from 25% to 100%, including for users with zero hardware or C/C++ experience.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03817v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经显示出非凡的生成代码的能力，但它们为嵌入式系统开发软件的能力尚未得到研究，这需要跨领域的硬件和软件知识。在本文中，我们系统地评估了领先的LLM（GPT-3.5、GPT-4、PaLM 2），以评估其在嵌入式系统开发中的性能，研究人类程序员如何与这些工具交互，并开发用于构建嵌入式系统的基于人工智能的软件工程工作流。我们开发了一个端到端的硬件在环评估平台，用于使用传感器-执行器对验证LLM生成的程序。我们将所有三个模型与N=450个实验进行了比较，令人惊讶的是，GPT-4特别表现出了非凡的跨领域理解和推理水平，在某些情况下，可以从一个提示生成完全正确的程序。在N=50次试验中，GPT-4 66%的时间产生功能性I2C接口。GPT-4还为nRF52程序生成寄存器级驱动程序、LoRa通信代码和特定于上下文的功率优化，从而将电流减少了740倍以上，达到12.2 uA。我们还描述了模型的局限性，以开发在嵌入式系统开发中使用LLM的通用工作流程。我们与包括新手和专家程序员在内的15名用户一起评估工作流程。我们发现，我们的工作流程提高了所有用户的生产力，并将构建LoRa环境传感器的成功率从25%提高到100%，包括零硬件或C/C++经验的用户。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03817v1" target="_blank">2307.03817v1</a>
                              </td>
                              <td>Exploring and Characterizing Large Language Models For Embedded System Development and Debugging</td>
                              <td>Zachary Englhardt</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03817v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02792v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Should Data Science Education Do with Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02792v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02792v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02792v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and learning tools, contributing to personalized education. This paper discusses the opportunities, resources and open challenges for each of these directions. As with any transformative technology, integrating LLMs into education calls for careful consideration. While LLMs can perform repetitive tasks efficiently, it's crucial to remember that their role is to supplement human intelligence and creativity, not to replace it. Therefore, the new era of data science education should balance the benefits of LLMs while fostering complementary human expertise and innovations. In conclusion, the rise of LLMs heralds a transformative period for data science and its education. This paper seeks to shed light on the emerging trends, potential opportunities, and challenges accompanying this paradigm shift, hoping to spark further discourse and investigation into this exciting, uncharted territory.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02792v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的快速发展，如ChatGPT，正在彻底改变数据科学和统计学。这些最先进的工具可以简化复杂的流程。因此，它重塑了数据科学家的角色。我们认为，LLM正在转变数据科学家的职责，将他们的重点从动手编码、数据争论和进行标准分析转移到评估和管理这些自动化AI执行的分析。这种角色的演变让人想起了从软件工程师到产品经理的转变。我们在本文中使用LLM通过具体的数据科学案例研究来说明这种转变。这些发展要求数据科学教育进行有意义的演变。教育学现在必须更加重视培养学生的多样化技能，如LLM知情创造力、批判性思维、人工智能指导编程。LLM还可以作为互动教学工具在课堂上发挥重要作用，为个性化教育做出贡献。本文讨论了每一个方向的机遇、资源和公开挑战。与任何变革性技术一样，将LLM融入教育需要仔细考虑。虽然LLM可以有效地执行重复任务，但重要的是要记住，它们的作用是补充人类的智力和创造力，而不是取代它。因此，数据科学教育的新时代应该平衡LLM的好处，同时培养互补的人类专业知识和创新。总之，LLM的兴起预示着数据科学及其教育的变革时期。本文试图阐明伴随这一范式转变而来的新趋势、潜在机遇和挑战，希望能引发对这一令人兴奋的未知领域的进一步讨论和调查。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02792v2" target="_blank">2307.02792v2</a>
                              </td>
                              <td>What Should Data Science Education Do with Large Language Models?</td>
                              <td>Xinming Tu</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02792v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02792v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03744v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03744v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03744v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03744v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in the development of large language models are rapidly changing how online applications function. LLM-based search tools, for instance, offer a natural language interface that can accommodate complex queries and provide detailed, direct responses. At the same time, there have been concerns about the veracity of the information provided by LLM-based tools due to potential mistakes or fabrications that can arise in algorithmically generated text. In a set of online experiments we investigate how LLM-based search changes people's behavior relative to traditional search, and what can be done to mitigate overreliance on LLM-based output. Participants in our experiments were asked to solve a series of decision tasks that involved researching and comparing different products, and were randomly assigned to do so with either an LLM-based search tool or a traditional search engine. In our first experiment, we find that participants using the LLM-based tool were able to complete their tasks more quickly, using fewer but more complex queries than those who used traditional search. Moreover, these participants reported a more satisfying experience with the LLM-based search tool. When the information presented by the LLM was reliable, participants using the tool made decisions with a comparable level of accuracy to those using traditional search, however we observed overreliance on incorrect information when the LLM erred. Our second experiment further investigated this issue by randomly assigning some users to see a simple color-coded highlighting scheme to alert them to potentially incorrect or misleading information in the LLM responses. Overall we find that this confidence-based highlighting substantially increases the rate at which users spot incorrect information, improving the accuracy of their overall decisions while leaving most other measures unaffected.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03744v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型开发的最新进展正在迅速改变在线应用程序的功能。例如，基于LLM的搜索工具提供了一个自然语言界面，可以容纳复杂的查询并提供详细、直接的响应。与此同时，由于算法生成的文本中可能出现的潜在错误或捏造，人们一直担心基于LLM的工具提供的信息的准确性。在一组在线实验中，我们研究了基于LLM的搜索相对于传统搜索如何改变人们的行为，以及可以做些什么来减轻对基于LLM输出的过度依赖。我们实验中的参与者被要求解决一系列涉及研究和比较不同产品的决策任务，并被随机分配使用基于LLM的搜索工具或传统搜索引擎来解决这些任务。在我们的第一个实验中，我们发现使用基于LLM的工具的参与者能够比使用传统搜索的参与者更快地完成任务，使用更少但更复杂的查询。此外，这些参与者报告了使用基于LLM的搜索工具的更令人满意的体验。当LLM提供的信息可靠时，使用该工具的参与者做出的决策的准确性与使用传统搜索的参与者相当，然而，当LLM出错时，我们观察到过度依赖不正确的信息。我们的第二个实验进一步调查了这个问题，随机分配一些用户观看一个简单的颜色编码的突出显示方案，以提醒他们LLM响应中可能存在的不正确或误导性信息。总的来说，我们发现这种基于信心的突出显示大大提高了用户发现错误信息的速度，提高了他们整体决策的准确性，同时使大多数其他措施不受影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03744v1" target="_blank">2307.03744v1</a>
                              </td>
                              <td>Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment</td>
                              <td>Sofia Eleni Spatharioti</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03744v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03744v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03738v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03738v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03738v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03738v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03738v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种新的自动代码生成方法，用于支持LLM上的量化生成推理，如现成CPU上的LLaMA或OPT。我们的方法由目标体系结构和性能模型提供信息，包括硬件特性和方法特定的精度约束。LLaMA模型基于CPU的推理结果表明，与现有最好的开源解决方案相比，我们的方法可以带来高性能和高精度。初步实施可在https://github.com/IST-DASLab/QIGen.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03738v1" target="_blank">2307.03738v1</a>
                              </td>
                              <td>QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models</td>
                              <td>Tommaso Pegolotti</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03738v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03738v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_08286v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ALERT: Adapting Language Models to Reasoning Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_08286v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_08286v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_08286v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_08286v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前的大型语言模型可以在复杂的任务中表现得相当好，这些任务需要逐步推理，很少需要镜头学习。这些模型是应用了他们在预训练中学到的推理技能，并在训练上下文之外推理，还是只是以更精细的粒度记忆训练语料库，并学会了更好地理解上下文？为了区分这些可能性，我们引入了ALERT，这是一个用于评估语言模型推理能力的基准和分析套件，比较了在需要推理技能才能解决的复杂任务上预先训练和微调的模型。ALERT提供了一个测试平台来评估任何关于细粒度推理技能的语言模型，该测试平台横跨20多个数据集，涵盖10种不同的推理技能。我们利用ALERT进一步调查微调的作用。通过广泛的实证分析，我们发现与预训练状态相比，语言模型在微调阶段学习了更多的推理技能，如文本蕴涵、溯因推理和类比推理。我们还发现，当对语言模型进行微调时，它们往往会过度拟合提示模板，这会损害模型的稳健性，从而导致泛化问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.08286v2" target="_blank">2212.08286v2</a>
                              </td>
                              <td>ALERT: Adapting Language Models to Reasoning Tasks</td>
                              <td>Ping Yu</td>
                              <td>2022-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_08286v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.08286v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03712v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03712v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03712v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03712v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim will enable researchers to flexibly simulate models at various precisions to support further research in quantization of LLMs and vision transformers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03712v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，大型语言模型（LLM）的兴起导致越来越多的人致力于以更低的精度运行LLM。以较低的精度运行LLM支持资源限制，并进一步促进其民主化，使用户能够在个人设备上运行十亿个参数的LLM。为了补充这项正在进行的工作，我们提出了INT FP QSim：一种开源模拟器，能够以各种数值精度和格式灵活评估LLM和视觉变换器。INT FP QSim利用现有的开源存储库，如TensorRT、QPytarch和AIMET，作为一个支持各种浮点和整数格式的组合模拟器。在我们的模拟器的帮助下，我们调查了不同的数字格式对LLM和视觉转换器在4位权重和4位或8位激活下的性能的影响。我们还比较了最近提出的自适应块浮点、SmoothQuant、GPTQ和RPTQ等方法的模型性能。我们希望INT-FP-QSim将使研究人员能够灵活地模拟各种精度的模型，以支持LLM和视觉变换器量化的进一步研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03712v1" target="_blank">2307.03712v1</a>
                              </td>
                              <td>INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers</td>
                              <td>Lakshmi Nair</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03712v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03712v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17103v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17103v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17103v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17103v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17103v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入LyricWhiz，这是一种健壮、多语言和零样本的自动歌词转录方法，在各种歌词转录数据集上实现了最先进的性能，甚至在摇滚和金属等具有挑战性的流派中也是如此。我们新颖的无训练方法利用了弱监督鲁棒语音识别模型Whisper和当今性能最高的基于聊天的大型语言模型GPT-4。在所提出的方法中，Whisper通过转录音频充当“耳朵”，而GPT-4充当“大脑”，充当注释器，在上下文输出选择和校正方面具有强大的性能。我们的实验表明，与现有的英语方法相比，LyricWhiz显著降低了单词错误率，并且可以有效地跨多种语言转录歌词。此外，我们使用LyricWhiz创建了第一个基于MTG Jamendo的具有CC-BY-NC-SA版权许可的公开、大规模、多语言歌词转录数据集，并提供了一个用于噪声水平估计和评估的人工注释子集。我们预计，我们提出的方法和数据集将推动多语言歌词转录的发展，这是一项具有挑战性的新兴任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17103v2" target="_blank">2306.17103v2</a>
                              </td>
                              <td>LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</td>
                              <td>Le Zhuo</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17103v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17103v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17760v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17760v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17760v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17760v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17760v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言模型是如何“思考”的？本文建立了一个称为有界语用说话人的概率认知模型，该模型可以表征语言模型的不同变体的操作。具体而言，我们证明，通过从人类反馈中强化学习进行微调的大型语言模型（Ouyang et al.，2022）体现了一种在概念上类似于快速和慢速模型的思维模型（Kahneman，2011），心理学家将其归因于人类。我们讨论了从人类反馈中强化学习作为一种快速和慢速思维模式的局限性，并提出了扩展这一框架的途径。本质上，我们的研究强调了采用认知概率建模方法来深入理解、评估和推进语言模型的价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17760v4" target="_blank">2305.17760v4</a>
                              </td>
                              <td>Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective</td>
                              <td>Khanh Nguyen</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17760v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17760v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15964v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15964v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15964v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15964v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of Computer-Assisted Diagnosis (CAD) with Large Language Models (LLMs) holds great potential in clinical applications, specifically in the roles of virtual family doctors and clinic assistants. However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness. To tackle these challenges, we introduce ChatCAD+, which is designed to be universal and reliable. It is capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice. Additionally, it incorporates a template retrieval system that improves report generation performance via exemplar reports. This approach ensures greater consistency with the expertise of human professionals. The source code is available at https://github.com/zhaozh10/ChatCAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15964v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机辅助诊断（CAD）与大型语言模型（LLM）的集成在临床应用中具有巨大的潜力，特别是在虚拟家庭医生和临床助理的角色中。然而，目前该领域的工作受到限制，特别是适用图像领域的范围有限和提供不可靠的医疗建议。这限制了它们的整体处理能力。此外，LLM和放射科医生在写作风格上的不匹配破坏了他们的实用性。为了应对这些挑战，我们引入了ChatCAD+，它被设计为通用和可靠的。它能够处理来自不同领域的医学图像，并利用信誉良好的医学网站的最新信息提供可靠的医疗建议。此外，它还集成了一个模板检索系统，通过示例报告提高了报告生成性能。这种方法确保了与人类专业人员的专业知识更加一致。源代码位于https://github.com/zhaozh10/ChatCAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15964v4" target="_blank">2305.15964v4</a>
                              </td>
                              <td>ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs</td>
                              <td>Zihao Zhao</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15964v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15964v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03699v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03699v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03699v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03699v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Social media platforms such as Instagram and Twitter have emerged as critical channels for drug marketing and illegal sale. Detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. To overcome this limitation, we conduct the first systematic study on leveraging large language models (LLMs), such as ChatGPT, to detect illicit drug trafficking activities on social media. We propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform the detection task. Additionally, we design a Monte Carlo dropout based prompt optimization method to further to improve performance and interpretability. Our experimental findings demonstrate that the proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, showing a remarkable improvement of nearly 12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can effectively identify and label drug trafficking activities on social networks, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection. The implications of our research extend to social networks, emphasizing the importance of incorporating prior knowledge and scenario-based prompts into analytical tools to improve online security and public safety.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03699v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instagram和Twitter等社交媒体平台已成为毒品营销和非法销售的关键渠道。检测和标记网上非法贩毒活动对于解决这一问题变得非常重要。然而，传统的监督学习方法在侦查贩毒方面的有效性在很大程度上取决于能否获得大量的标记数据，而数据注释则耗时且资源密集。此外，当毒贩使用欺骗性语言和委婉语来逃避侦查时，这些模型在准确识别贩运活动方面往往面临挑战。为了克服这一限制，我们首次对利用大型语言模型（LLM）（如ChatGPT）来检测社交媒体上的非法贩毒活动进行了系统研究。我们提出了一个分析框架来组成\emph{知识知情提示}，作为人类可以与LLM交互并使用LLM执行检测任务的界面。此外，我们设计了一种基于蒙特卡罗丢弃的即时优化方法，以进一步提高性能和可解释性。我们的实验结果表明，所提出的框架在贩毒检测准确性方面优于其他基线语言模型，显示出近12%的显著改进。通过整合先验知识和建议的提示，ChatGPT可以有效地识别和标记社交网络上的贩毒活动，即使存在毒贩用来逃避侦查的欺骗性语言和委婉语。我们的研究影响延伸到了社交网络，强调了将先验知识和基于场景的提示纳入分析工具以提高网络安全和公共安全的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03699v1" target="_blank">2307.03699v1</a>
                              </td>
                              <td>Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media</td>
                              <td>Chuanbo Hu</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03699v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03699v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03637v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Discovering Variable Binding Circuitry with Desiderata</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03637v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03637v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03637v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03637v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，语言模型中的计算可能是人类可以理解的，并成功地定位和干预了单个单元特征和输入输出电路。在这里，我们介绍了一种方法，该方法扩展了因果中介实验，通过仅指定执行特定子任务的模型组件的一组\textit｛desiderata｝或因果属性，自动识别负责执行该子任务的模块组件。作为概念验证，我们应用我们的方法自动发现LLaMA-13B中的共享\textit｛变量绑定电路｝，该电路为多个算术任务检索变量值。我们的方法成功地将变量绑定定位到最终令牌的残差流中的9个注意力头（1.6k）和一个MLP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03637v1" target="_blank">2307.03637v1</a>
                              </td>
                              <td>Discovering Variable Binding Circuitry with Desiderata</td>
                              <td>Xander Davies</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03637v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03637v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03762v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03762v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03762v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03762v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, knowledge acquisition isn't solely reliant on passive input but requires repeated trials and errors. We conclude by outlining promising future research directions in the field of artificial general intelligence.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03762v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这篇前瞻性的论文中，我们首先使用标准化测试和面向能力的基准来全面回顾现有的大型语言模型（LLM）评估。我们指出了当前评估方法中的几个问题，这些问题往往夸大了LLM的能力。然后，我们阐明了在LLM的能力之外，人工通用智能应该包含什么。我们提出了一般智能代理的四个特征：1）它们可以执行无限的任务；2） 它们可以在上下文中生成新的任务；3） 它们基于支撑任务生成的价值体系运作；以及4）他们有一个反映现实的世界模型，它塑造了他们与世界的互动。基于这一观点，我们强调了人工通用智能中缺失的部分，即知与行的统一。我们认为，与现实世界中的物体的积极接触为形成概念表示提供了更有力的信号。此外，知识获取不仅仅依赖于被动输入，还需要反复的尝试和错误。最后，我们概述了人工通用智能领域未来有希望的研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03762v1" target="_blank">2307.03762v1</a>
                              </td>
                              <td>Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models</td>
                              <td>Yuxi Ma</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03762v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03762v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03601v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03601v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03601v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03601v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction tuning large language model (LLM) on image-text pairs has achieved unprecedented vision-language multimodal abilities. However, their vision-language alignments are only built on image-level, the lack of region-level alignment limits their advancements to fine-grained multimodal understanding. In this paper, we propose instruction tuning on region-of-interest. The key design is to reformulate the bounding box as the format of spatial instruction. The interleaved sequences of visual features extracted by the spatial instruction and the language embedding are input to LLM, and trained on the transformed region-text data in instruction tuning format. Our region-level vision-language model, termed as GPT4RoI, brings brand new conversational and interactive experience beyond image-level understanding. (1) Controllability: Users can interact with our model by both language and spatial instructions to flexibly adjust the detail level of the question. (2) Capacities: Our model supports not only single-region spatial instruction but also multi-region. This unlocks more region-level multimodal capacities such as detailed region caption and complex region reasoning. (3) Composition: Any off-the-shelf object detector can be a spatial instruction provider so as to mine informative object attributes from our model, like color, shape, material, action, relation to other objects, etc. The code, data, and demo can be found at https://github.com/jshilong/GPT4RoI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03601v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于图像-文本对的指令调整大语言模型（LLM）实现了前所未有的视觉-语言多模态能力。然而，他们的视觉语言比对仅建立在图像级别上，缺乏区域级别的比对限制了他们对细粒度多模式理解的进步。在本文中，我们建议对感兴趣的区域进行指令调整。设计的关键是将边界框重新表述为空间指令的格式。通过空间指令和语言嵌入提取的视觉特征的交织序列被输入到LLM，并以指令调整格式在变换后的区域文本数据上进行训练。我们的区域级视觉语言模型被称为GPT4RoI，它带来了超越图像级理解的全新对话和互动体验。（1） 可控性：用户可以通过语言和空间指令与我们的模型进行交互，以灵活调整问题的细节级别。（2） 容量：我们的模型不仅支持单区域空间教学，还支持多区域教学。这释放了更多的区域级多模式能力，如详细的区域说明和复杂的区域推理。（3） 组成：任何现成的对象检测器都可以是空间指令提供者，以便从我们的模型中挖掘信息对象属性，如颜色、形状、材料、动作、与其他对象的关系等。代码、数据和演示可以在https://github.com/jshilong/GPT4RoI.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03601v1" target="_blank">2307.03601v1</a>
                              </td>
                              <td>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest</td>
                              <td>Shilong Zhang</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03601v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03601v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_04767v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic-SAM: Segment and Recognize Anything at Any Granularity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04767v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04767v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04767v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04767v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了语义SAM，这是一种通用的图像分割模型，可以以任何所需的粒度分割和识别任何东西。我们的模型提供了两个关键优势：语义感知和粒度丰富。为了实现语义感知，我们整合了三个粒度的多个数据集，并引入了对象和部分的解耦分类。这使我们的模型能够捕获丰富的语义信息。对于多粒度能力，我们在训练期间提出了一种多选学习方案，使每次点击都能生成多个级别的掩码，这些掩码对应于多个地面实况掩码。值得注意的是，这项工作首次尝试在SA-1B、通用和零件分割数据集上联合训练模型。实验结果和可视化结果表明，我们的模型成功地实现了语义感知和粒度丰富。此外，将SA-1B训练与其他分割任务（如全景分割和零件分割）相结合，可以提高性能。我们将提供代码和演示以供进一步探索和评估。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04767v1" target="_blank">2307.04767v1</a>
                              </td>
                              <td>Semantic-SAM: Segment and Recognize Anything at Any Granularity</td>
                              <td>Feng Li</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04767v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04767v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02913v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02913v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02913v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02913v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02913v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分散随机梯度下降（D-SGD）允许在没有中央服务器控制的情况下同时在大规模设备上进行协作学习。然而，现有的理论声称，权力下放必然会破坏泛化。在本文中，我们挑战了传统的信念，并为理解去中心化学习提供了一个全新的视角。我们证明了在一般非凸非$\beta$-光滑设置下，D-SGD隐式地最小化了平均方向清晰度感知最小化（SAM）算法的损失函数。这种令人惊讶的渐近等价揭示了一种内在的正则化优化权衡和去中心化的三个优点：（1）D-SGD中存在一种自由的不确定性评估机制来改进后验估计；（2） D-SGD表现出梯度平滑效应；和（3）D-SGD的锐度正则化效应不会随着总批量的增加而降低，这证明了在大批量场景中，D-SGD相对于集中式SGD（C-SGD）的潜在泛化优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02913v3" target="_blank">2306.02913v3</a>
                              </td>
                              <td>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</td>
                              <td>Tongtian Zhu</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02913v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02913v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04455v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04455v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04455v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04455v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04455v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像质量评估（IQA）是一项具有挑战性的任务，需要在大量数据集上进行训练以实现准确的预测。然而，由于缺乏IQA数据，基于深度学习的IQA方法通常依赖于在海量数据集上训练的预训练网络作为特征提取器来增强其泛化能力，例如在ImageNet上训练的ResNet网络。在本文中，我们利用Segment Anything的编码器进行高级语义特征提取，Segment Anything是最近提出的一种在海量数据集上训练的分割模型。大多数IQA方法仅限于提取空间域特征，而频域特征已被证明可以更好地表示噪声和模糊。因此，我们通过分别对提取的特征应用傅立叶和标准卷积来利用空间域和频域特征。进行了大量的实验来证明所有提出的组件的有效性，结果表明，我们的方法在四个具有代表性的数据集中，无论是定性还是定量，都优于最先进的（SOTA）。我们的实验证实了Segment Anything强大的特征提取能力，并强调了在IQA任务中结合空间域和频域特征的价值。代码：https://github.com/Hedlen/SAM-IQA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04455v1" target="_blank">2307.04455v1</a>
                              </td>
                              <td>SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</td>
                              <td>Xinpeng Li</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04455v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04455v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02508v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02508v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02508v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02508v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object tracking and segmentation. In this study, we convert the bounding boxes to masks in reference frames with the help of the Segment Anything Model (SAM) and Alpha-Refine, and then propagate the masks to the current frame, transforming the task from Video Object Tracking (VOT) to video object segmentation (VOS). Furthermore, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. MSDeAOT efficiently propagates object masks from previous frames to the current frame using two feature scales of 16 and 8. As a testament to the effectiveness of our design, we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02508v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将对象与变换器相关联（AOT）框架在视频对象跟踪和分割的各种复杂场景中表现出了卓越的性能。在这项研究中，我们在Segment Anything Model（SAM）和Alpha Refine的帮助下，将边界框转换为参考帧中的掩码，然后将掩码传播到当前帧，将任务从视频对象跟踪（VOT）转换为视频对象分割（VOS）。此外，我们还介绍了MSDeAOT，这是AOT系列的一个变体，包含了多个功能级别的转换器。MSDeAOT使用16和8这两个特征尺度将对象遮罩从先前帧有效地传播到当前帧。为了证明我们设计的有效性，我们在EPIC-KITCHENS TREK-150对象跟踪挑战赛中获得了第一名。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02508v2" target="_blank">2307.02508v2</a>
                              </td>
                              <td>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</td>
                              <td>Yuanyou Xu</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02508v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02508v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04308v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04308v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04308v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04308v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04308v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于过去几十年数据库系统的稳定开发和大规模部署，表格数据（也称为结构化数据）是现存最常见的数据形式之一。然而，目前，尽管ChatGPT或SAM等其他领域的大型预训练模型带来了冲击，但我们如何在表格中提取公共知识，最终可能导致表格数据的可推广表示，仍然是一片空白。事实上，围绕这个主题已经有了一些作品。它们中的大多数（如果不是全部的话）被限制在单个表或固定形式的模式的范围内。在这项工作中，我们首先确定了表格数据预训练背后的关键研究挑战，特别是在跨表格场景中。我们将这项工作的贡献分为两个方面：（i）-我们收集和整理了近2k个高质量的表格数据集，每个数据集都保证具有清晰的语义、干净的标签和其他必要的元信息。（ii）-我们提出了一种新的框架，该框架允许称为CT-BERT的跨表预训练。值得注意的是，鉴于开创了规模交叉表训练，CT-BERT与监督和自监督方案都是完全兼容的，其中CT-BERT的具体实例化在很大程度上取决于下游任务。我们进一步在CT-BERT中提出并实现了一个基于对比学习和掩蔽表建模（MTM）的目标，该目标的灵感来自计算机视觉和自然语言处理社区，但针对表进行了巧妙的定制。在15个数据集上的大量实证结果证明了CT-BERT最先进的性能，其监督和自监督设置都显著优于先前的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04308v1" target="_blank">2307.04308v1</a>
                              </td>
                              <td>CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</td>
                              <td>Chao Ye</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04308v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04308v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04008v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Toward Interactive Dictation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04008v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04008v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04008v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04008v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音听写是一种越来越重要的文本输入方式。允许听写和语音编辑的现有系统将其命令语言限制为触发词调用的平面模板。在这项工作中，我们研究了允许用户用开放式自然语言中的口语编辑命令中断听写的可行性。我们引入了一个新的任务和数据集，TERTiUS，来对这样的系统进行实验。为了实时支持这种灵活性，系统必须增量地将语音的跨度分段和分类为听写或命令，并将这些跨度解释为命令。我们实验使用大型预先训练的语言模型来预测编辑后的文本，或者预测小型文本编辑程序。实验表明，模型准确性和延迟之间存在自然的权衡：较小的模型在1.3秒的延迟下实现了30%的最终状态准确性，而较大的模型在7秒的延迟内实现了55%的最终状态准确率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04008v1" target="_blank">2307.04008v1</a>
                              </td>
                              <td>Toward Interactive Dictation</td>
                              <td>Belinda Z. Li</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04008v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04008v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03535v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03535v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03535v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03535v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Radiotherapists require accurate registration of MR/CT images to effectively use information from both modalities. In a typical registration pipeline, rigid or affine transformations are applied to roughly align the fixed and moving images before proceeding with the deformation step. While recent learning-based methods have shown promising results in the rigid/affine step, these methods often require images with similar field-of-view (FOV) for successful alignment. As a result, aligning images with different FOVs remains a challenging task. Self-supervised landmark detection methods like self-supervised Anatomical eMbedding (SAM) have emerged as a useful tool for mapping and cropping images to similar FOVs. However, these methods are currently limited to intra-modality use only. To address this limitation and enable cross-modality matching, we propose a new approach called Cross-SAM. Our approach utilizes a novel iterative process that alternates between embedding learning and CT-MRI registration. We start by applying aggressive contrast augmentation on both CT and MRI images to train a SAM model. We then use this SAM to identify corresponding regions on paired images using robust grid-points matching, followed by a point-set based affine/rigid registration, and a deformable fine-tuning step to produce registered paired images. We use these registered pairs to enhance the matching ability of SAM, which is then processed iteratively. We use the final model for cross-modality matching tasks. We evaluated our approach on two CT-MRI affine registration datasets and found that Cross-SAM achieved robust affine registration on both datasets, significantly outperforming other methods and achieving state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03535v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>放射治疗师需要对MR/CT图像进行准确配准，以有效地使用来自两种模式的信息。在典型的配准流水线中，在进行变形步骤之前，应用刚性或仿射变换来大致对准固定图像和运动图像。虽然最近基于学习的方法在刚性/仿射步骤中显示出了有希望的结果，但这些方法通常需要具有相似视场（FOV）的图像才能成功对准。因此，将图像与不同的FOV对准仍然是一项具有挑战性的任务。自监督地标检测方法，如自监督解剖eMbedding（SAM），已成为将图像映射和裁剪到类似FOV的有用工具。然而，这些方法目前仅限于模态内使用。为了解决这一限制并实现跨模态匹配，我们提出了一种称为cross-SAM的新方法。我们的方法利用了一种新的迭代过程，该过程在嵌入学习和CT-MRI注册之间交替。我们首先在CT和MRI图像上应用积极的对比度增强来训练SAM模型。然后，我们使用这种SAM来识别配对图像上的对应区域，使用鲁棒的网格点匹配，然后是基于点集的仿射/刚性配准，以及可变形的微调步骤来产生配准的配对图像。我们使用这些注册对来增强SAM的匹配能力，然后对其进行迭代处理。我们将最终模型用于跨模态匹配任务。我们在两个CT-MRI仿射配准数据集上评估了我们的方法，发现Cross-SAM在两个数据集上都实现了稳健的仿射配准，显著优于其他方法，并实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03535v1" target="_blank">2307.03535v1</a>
                              </td>
                              <td>Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images</td>
                              <td>Xiaoyu Bai</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03535v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03535v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03492v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large AI Model-Based Semantic Communications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03492v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03492v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03492v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed-reality, and the Internet of everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model provides new solutions to overcome above issues. Here, we propose a large AI model-based SC framework (LAM-SC) specifically designed for image data, where we first design the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic-aware image. Additionally, we propose an adaptive semantic compression (ASC) encoding to remove redundant information in semantic features, thereby reducing communication overhead. Finally, through simulations, we demonstrate the effectiveness of the LAM-SC framework and the significance of the large AI model-based KB development in future SC paradigms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03492v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义通信（SC）是一种新兴的智能范式，为元宇宙、混合现实和万物互联等各种未来应用提供解决方案。然而，在当前的供应链系统中，知识库的构建面临着几个问题，包括有限的知识表示、频繁的知识更新和不安全的知识共享。幸运的是，大型人工智能模型的开发为克服上述问题提供了新的解决方案。在这里，我们提出了一个专门为图像数据设计的基于AI模型的大型SC框架（LAM-SC），其中我们首先设计了基于分段任意模型（SAM）的KB（SKB），该框架可以通过通用语义知识将原始图像分割成不同的语义分段。然后，我们提出了一种基于注意力的语义集成（ASI）来衡量SKB在没有人类参与的情况下生成的语义片段，并将其集成为语义感知图像。此外，我们提出了一种自适应语义压缩（ASC）编码来去除语义特征中的冗余信息，从而减少通信开销。最后，通过仿真，我们展示了LAM-SC框架的有效性，以及基于人工智能的大型知识库开发在未来SC范式中的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03492v1" target="_blank">2307.03492v1</a>
                              </td>
                              <td>Large AI Model-Based Semantic Communications</td>
                              <td>Feibo Jiang</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03492v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03492v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_02677v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Caption Anything: Interactive Image Description with Diverse Multimodal Controls</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_02677v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_02677v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_02677v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, $\textit{e.g.}$, looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality. Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/Caption-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_02677v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可控图像字幕是一个新兴的多模式主题，旨在用自然语言描述图像，遵循人类的目的，$\textit｛例如｝$，观察指定的区域或以特定的文本风格讲述。现有技术的方法是在带注释的输入控件和输出字幕对上进行训练的。然而，这种注释良好的多模式数据的稀缺性在很大程度上限制了它们在交互式人工智能系统中的可用性和可扩展性。利用遵循基础模型的单一模式教学是一种很有前途的替代方案，它受益于更广泛的数据来源。在本文中，我们提出了Caption AnyThing（CAT），这是一个基础模型增强图像字幕框架，支持广泛的多模型控件：1）视觉控件，包括点、框和轨迹；2） 语言控制，如情感、长度、语言和真实性。在Segment Anything Model（SAM）和ChatGPT的支持下，我们将视觉和语言提示统一到一个模块化的框架中，实现了不同控件之间的灵活组合。大量的案例研究证明了我们框架的用户意图对齐能力，为视觉语言应用程序中的有效用户交互建模提供了线索。我们的代码可在https://github.com/ttengwang/Caption-Anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.02677v3" target="_blank">2305.02677v3</a>
                              </td>
                              <td>Caption Anything: Interactive Image Description with Diverse Multimodal Controls</td>
                              <td>Teng Wang</td>
                              <td>2023-05-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_02677v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.02677v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03263v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient automatic design of robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03263v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03263v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03263v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots are notoriously difficult to design because of complex interdependencies between their physical structure, sensory and motor layouts, and behavior. Despite this, almost every detail of every robot built to date has been manually determined by a human designer after several months or years of iterative ideation, prototyping, and testing. Inspired by evolutionary design in nature, the automated design of robots using evolutionary algorithms has been attempted for two decades, but it too remains inefficient: days of supercomputing are required to design robots in simulation that, when manufactured, exhibit desired behavior. Here we show for the first time de-novo optimization of a robot's structure to exhibit a desired behavior, within seconds on a single consumer-grade computer, and the manufactured robot's retention of that behavior. Unlike other gradient-based robot design methods, this algorithm does not presuppose any particular anatomical form; starting instead from a randomly-generated apodous body plan, it consistently discovers legged locomotion, the most efficient known form of terrestrial movement. If combined with automated fabrication and scaled up to more challenging tasks, this advance promises near instantaneous design, manufacture, and deployment of unique and useful machines for medical, environmental, vehicular, and space-based tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03263v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>众所周知，机器人很难设计，因为它们的物理结构、感官和运动布局以及行为之间存在复杂的相互依赖关系。尽管如此，迄今为止制造的每一个机器人的几乎每一个细节都是由人类设计师在经过几个月或几年的迭代构思、原型设计和测试后手动确定的。受自然界进化设计的启发，使用进化算法的机器人自动化设计已经尝试了20年，但效率仍然很低：设计出在制造时表现出所需行为的模拟机器人需要数天的超级计算。在这里，我们首次展示了机器人结构的从头优化，以在几秒钟内在单一消费级计算机上表现出所需的行为，以及制造的机器人对该行为的保留。与其他基于梯度的机器人设计方法不同，该算法不预设任何特定的解剖形式；相反，从随机生成的apodous身体计划开始，它不断发现腿的运动，这是已知的最有效的陆地运动形式。如果与自动化制造相结合，并扩大规模以完成更具挑战性的任务，这一进步有望为医疗、环境、车辆和天基任务提供近乎即时的设计、制造和部署独特而有用的机器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03263v2" target="_blank">2306.03263v2</a>
                              </td>
                              <td>Efficient automatic design of robots</td>
                              <td>David Matthews</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03263v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03263v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2112_08645v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Interpretable Models Through Multi-Objective Neural Architecture Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2112_08645v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2112_08645v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2112_08645v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Monumental advances in deep learning have led to unprecedented achievements across various domains. While the performance of deep neural networks is indubitable, the architectural design and interpretability of such models are nontrivial. Research has been introduced to automate the design of neural network architectures through neural architecture search (NAS). Recent progress has made these methods more pragmatic by exploiting distributed computation and novel optimization algorithms. However, there is little work in optimizing architectures for interpretability. To this end, we propose a multi-objective distributed NAS framework that optimizes for both task performance and "introspectability," a surrogate metric for aspects of interpretability. We leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable AI (XAI) techniques to reward architectures that can be better comprehended by domain experts. The framework is evaluated on several image classification datasets. We demonstrate that jointly optimizing for task error and introspectability leads to more disentangled and debuggable architectures that perform within tolerable error.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2112_08645v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习的重大进步在各个领域取得了前所未有的成就。虽然深度神经网络的性能是毋庸置疑的，但这种模型的架构设计和可解释性是不平凡的。已经引入了通过神经架构搜索（NAS）来自动化神经网络架构设计的研究。最近的进展通过利用分布式计算和新的优化算法使这些方法更加实用。然而，在为可解释性优化体系结构方面几乎没有什么工作。为此，我们提出了一个多目标分布式NAS框架，该框架针对任务性能和“内省性”（可解释性方面的代理指标）进行了优化。我们利用非支配排序遗传算法（NSGA-II）和可解释人工智能（XAI）技术来奖励领域专家能够更好地理解的架构。该框架在几个图像分类数据集上进行了评估。我们证明，针对任务错误和内省性的联合优化会导致在可容忍的错误范围内执行的架构更加混乱和可调试。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2112.08645v4" target="_blank">2112.08645v4</a>
                              </td>
                              <td>Learning Interpretable Models Through Multi-Objective Neural Architecture Search</td>
                              <td>Zachariah Carmichael</td>
                              <td>2021-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2112_08645v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2112.08645v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_03108v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_03108v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_03108v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_03108v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results show that GAM improves the generalization of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Furthermore, we show that GAM can help SAM find flatter minima and achieve better generalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_03108v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，平面极小值被证明可以有效地提高泛化能力，并且清晰度感知最小化（SAM）实现了最先进的性能。然而，SAM及其后续研究中讨论的平坦度的当前定义仅限于零阶平坦度（即扰动半径内的最坏情况损失）。我们证明，当在给定的扰动半径内存在单个极小值或多个极小值时，零阶平坦度不足以区分具有低泛化误差的极小值和具有高泛化误差的最小值。因此，我们提出了一阶平坦性，这是一种更强的平坦性度量，关注于扰动半径内的最大梯度范数，它限制了Hessian在局部极小值的最大特征值和SAM的正则化函数。我们还提出了一种新的训练程序，称为梯度范数感知最小化（GAM），以寻求在所有方向上具有均匀小曲率的最小值。实验结果表明，GAM提高了使用当前优化器（如SGD和AdamW）在各种数据集和网络上训练的模型的泛化能力。此外，我们还证明了GAM可以帮助SAM找到更平坦的极小值，并实现更好的泛化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.03108v3" target="_blank">2303.03108v3</a>
                              </td>
                              <td>Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization</td>
                              <td>Xingxuan Zhang</td>
                              <td>2023-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_03108v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.03108v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11019v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Annotation-free Audio-Visual Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11019v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11019v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11019v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The objective of Audio-Visual Segmentation (AVS) is to localise the sounding objects within visual scenes by accurately predicting pixel-wise segmentation masks. To tackle the task, it involves a comprehensive consideration of both the data and model aspects. In this paper, first, we initiate a novel pipeline for generating artificial data for the AVS task without human annotating. We leverage existing image segmentation and audio datasets to match the image-mask pairs with its corresponding audio samples with the linkage of category labels, that allows us to effortlessly compose (image, audio, mask) triplets for training AVS models. The pipeline is annotation-free and scalable to cover a large number of categories. Additionally, we introduce a lightweight approach SAMA-AVS to adapt the pre-trained segment anything model~(SAM) to the AVS task. By introducing only a small number of trainable parameters with adapters, the proposed model can effectively achieve adequate audio-visual fusion and interaction in the encoding stage with vast majority of parameters fixed. We conduct extensive experiments, and the results show our proposed model remarkably surpasses other competing methods. Moreover, by using the proposed model pretrained with our synthetic data, the performance on real AVSBench data is further improved, achieving 83.17 mIoU on S4 subset and 66.95 mIoU on MS3 set.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11019v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视听分割（AVS）的目标是通过准确预测逐像素分割掩模来定位视觉场景中的声音对象。为了解决这项任务，它涉及到对数据和模型两个方面的全面考虑。在本文中，首先，我们启动了一种新的管道，用于为AVS任务生成人工数据，而无需人工注释。我们利用现有的图像分割和音频数据集，通过类别标签的链接，将图像掩码对与其对应的音频样本进行匹配，这使我们能够毫不费力地合成（图像、音频、掩码）三元组，用于训练AVS模型。该管道是无注释的，并且可扩展以覆盖大量类别。此外，我们引入了一种轻量级方法SAMA-AVS，以使预先训练的分段任意模型~（SAM）适应AVS任务。通过适配器只引入少量可训练参数，该模型可以在绝大多数参数固定的编码阶段有效地实现充分的视听融合和交互。我们进行了大量的实验，结果表明我们提出的模型显著优于其他竞争方法。此外，通过使用用我们的合成数据预训练的所提出的模型，进一步提高了对真实AVSBench数据的性能，在S4子集上达到83.17mIoU，在MS3集合上达到66.95mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11019v3" target="_blank">2305.11019v3</a>
                              </td>
                              <td>Annotation-free Audio-Visual Segmentation</td>
                              <td>Jinxiang Liu</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11019v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11019v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01197v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Meets Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01197v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01197v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01197v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, employing interactive prompts such as points to generate masks. This paper presents SAM-PT, a method extending SAM's capability to tracking and segmenting anything in dynamic videos. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information that is agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. To further enhance our approach, we utilize K-Medoids clustering for point initialization and track both positive and negative points to clearly distinguish the target object. We also employ multiple mask decoding passes for mask refinement and devise a point re-initialization strategy to improve tracking accuracy. Our code integrates different point trackers and video segmentation benchmarks and will be released at https://github.com/SysCV/sam-pt.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01197v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）已经建立了一个强大的零样本图像分割模型，它使用诸如点之类的交互式提示来生成遮罩。本文介绍了SAM-PT，这是一种将SAM的能力扩展到跟踪和分割动态视频中的任何内容的方法。SAM-PT利用健壮且稀疏的点选择和传播技术生成掩模，证明基于SAM的分割跟踪器可以在流行的视频对象分割基准（包括DAVIS、YouTube-VOS和MOSE）中产生强大的零样本性能。与传统的以对象为中心的掩码传播策略相比，我们独特地使用点传播来利用对对象语义不可知的局部结构信息。通过对零样本开放世界未识别视频对象（UVO）基准的直接评估，我们强调了基于点的跟踪的优点。为了进一步增强我们的方法，我们利用K-Medoids聚类进行点初始化，并跟踪正点和负点，以清楚地区分目标对象。我们还采用多个掩码解码通道进行掩码细化，并设计了一种点重新初始化策略来提高跟踪精度。我们的代码集成了不同的点跟踪器和视频分割基准，将于https://github.com/SysCV/sam-pt.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01197v1" target="_blank">2307.01197v1</a>
                              </td>
                              <td>Segment Anything Meets Point Tracking</td>
                              <td>Frano Rajič</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01197v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01197v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01187v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMAug: Point Prompt Augmentation for Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01187v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01187v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01187v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information to SAM. From the initial point prompt, SAM produces the initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We evaluate four point augmentation techniques: random selection, maximum difference entropy, maximum distance, and a saliency model. Experiments on the COCO, Fundus, and Chest X-ray datasets demonstrate that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency model methods. SAMAug underscores the potential of visual prompt engineering to advance interactive computer vision models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01187v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种用于分段任意模型（SAM）的新的视点增强方法SAMAug，该方法提高了交互式图像分割的性能。SAMAug生成增强点提示以向SAM提供更多信息。从初始点提示开始，SAM生成初始掩码，然后将其输入到我们提出的SAMAug中以生成增强点提醒。通过结合这些额外的点，SAM可以基于增强的点提示和初始提示生成增强的分割掩码，从而提高分割性能。我们评估了四种点增强技术：随机选择、最大差分熵、最大距离和显著性模型。在COCO、眼底和胸部X射线数据集上的实验表明，SAMAug可以提高SAM的分割结果，特别是使用最大距离和显著性模型方法。SAMAug强调了视觉提示工程在推进交互式计算机视觉模型方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01187v1" target="_blank">2307.01187v1</a>
                              </td>
                              <td>SAMAug: Point Prompt Augmentation for Segment Anything Model</td>
                              <td>Haixing Dai</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01187v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01187v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01024v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01024v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01024v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01024v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample. This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking. Recently, the notable Segment Anything Model (SAM) has achieved remarkable zero-shot generalization ability to discover abundant potential objects due to its huge data-driven training approach. To solve the aforementioned issue, this work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA. Specifically, an innovative SAM-powered target domain training sample swelling is designed to determine enormous high-quality target domain training samples from every single raw nighttime image. This novel one-to-many method significantly expands the high-quality target domain training sample for DA. Comprehensive experiments on extensive nighttime UAV videos prove the robustness and domain adaptability of SAM-DA for nighttime UAV tracking. Especially, compared to the SOTA DA, SAM-DA can achieve better performance with fewer raw nighttime images, i.e., the fewer-better training. This economized training approach facilitates the quick validation and deployment of algorithms for UAVs. The code is available at https://github.com/vision4robotics/SAM-DA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01024v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>领域自适应（DA）在夜间无人机（UAV）实时跟踪方面表现出了巨大的前景。然而，最先进的（SOTA）DA仍然缺乏具有精确像素级位置和边界的潜在对象来生成高质量的目标域训练样本。这一关键问题限制了实时日间SOTA跟踪器的迁移学习，以用于具有挑战性的夜间无人机跟踪。最近，著名的分段任意模型（SAM）由于其巨大的数据驱动训练方法，在发现大量潜在对象方面取得了显著的零样本泛化能力。为了解决上述问题，本工作提出了一种用于夜间无人机实时跟踪的新型SAM驱动DA框架，即SAM-DA。具体而言，一种创新的SAM驱动的目标域训练样本膨胀设计用于从每一张原始夜间图像中确定大量高质量的目标域培训样本。这种新的一对多方法显著扩展了DA的高质量目标域训练样本。在大量夜间无人机视频上的综合实验证明了SAM-DA对夜间无人机跟踪的鲁棒性和域适应性。特别是，与SOTA DA相比，SAM-DA可以用更少的夜间原始图像获得更好的性能，即更少的更好的训练。这种节约的训练方法有助于无人机算法的快速验证和部署。代码位于https://github.com/vision4robotics/SAM-DA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01024v1" target="_blank">2307.01024v1</a>
                              </td>
                              <td>SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation</td>
                              <td>Liangliang Yao</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01024v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01024v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00997v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00997v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00997v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00997v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which for the first time explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Subsequently, a parameter-efficient tuning strategy is employed to effectively align and fuse the language and vision features. Through comprehensive ablation studies, we demonstrate the practical and effective design choices of our strategy. Extensive experiments conducted on Ref-Youtu-VOS and Ref-DAVIS17 datasets validate the superiority and effectiveness of our RefSAM model over existing methods. The code and models will be made publicly at \href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00997v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）因其在图像分割方面令人印象深刻的性能而备受关注。然而，由于需要精确的用户交互提示以及对不同模态（如语言和视觉）的理解有限，它在参考视频对象分割（RVOS）方面缺乏熟练度。本文提出了RefSAM模型，该模型首次通过合并来自不同模态和不同时间戳的连续帧的多视图信息来探索SAM对RVOS的潜力。我们提出的方法通过使用轻量级跨模态MLP来调整原始SAM模型，以增强跨模态学习，该MLP将引用表达式的文本嵌入投影到稀疏和密集嵌入中，用作用户交互提示。随后，采用参数有效调整策略来有效地对齐和融合语言和视觉特征。通过全面的消融研究，我们展示了我们策略的实用和有效的设计选择。在Ref Youtu VOS和Ref-DAVIS17数据集上进行的大量实验验证了我们的RefSAM模型相对于现有方法的优越性和有效性。代码和模型将在\href上公开{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00997v1" target="_blank">2307.00997v1</a>
                              </td>
                              <td>RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation</td>
                              <td>Yonglin Li</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00997v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00997v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06211v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06211v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06211v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Meta AI Research开发的分段任意模型（SAM）最近引起了人们的极大关注。在超过10亿个掩模的大型分割数据集上训练，SAM能够分割特定图像上的任何对象。在最初的SAM工作中，作者转向零短转移任务（如边缘检测）来评估SAM的性能。最近，许多工作试图研究SAM在各种场景中的性能，以识别和分割对象。此外，通过将SAM与其他模型（如Grounding DINO、Stable Diffusion、ChatGPT等）相结合，已经出现了许多项目来展示SAM作为基础模型的多功能性。随着相关论文和项目呈指数级增长，读者很难跟上SAM的发展。为此，本工作首次对SAM进行了全面的调查。这是一个正在进行的项目，我们打算定期更新手稿。因此，如果读者完成了与SAM相关的新作品，欢迎与我们联系，以便我们将其纳入下一版本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06211v3" target="_blank">2306.06211v3</a>
                              </td>
                              <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06211v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06211v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13093v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Restore Anything Pipeline: Segment Anything Meets Image Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13093v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13093v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13093v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent image restoration methods have produced significant advancements using deep learning. However, existing methods tend to treat the whole image as a single entity, failing to account for the distinct objects in the image that exhibit individual texture properties. Existing methods also typically generate a single result, which may not suit the preferences of different users. In this paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive and per-object level image restoration approach that incorporates a controllable model to generate different results that users may choose from. RAP incorporates image segmentation through the recent Segment Anything Model (SAM) into a controllable image restoration model to create a user-friendly pipeline for several image restoration tasks. We demonstrate the versatility of RAP by applying it to three common image restoration tasks: image deblurring, image denoising, and JPEG artifact removal. Our experiments show that RAP produces superior visual results compared to state-of-the-art methods. RAP represents a promising direction for image restoration, providing users with greater control, and enabling image restoration at an object level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13093v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的图像恢复方法使用深度学习取得了重大进展。然而，现有的方法倾向于将整个图像视为单个实体，未能考虑到图像中表现出单独纹理特性的不同对象。现有方法通常也会生成单个结果，这可能不适合不同用户的偏好。在本文中，我们介绍了Restore Anything Pipeline（RAP），这是一种新颖的交互式和每对象级别的图像恢复方法，它结合了一个可控的模型来生成用户可以选择的不同结果。RAP通过最近的Segment Anything Model（SAM）将图像分割合并到可控的图像恢复模型中，为几个图像恢复任务创建用户友好的管道。我们通过将RAP应用于三种常见的图像恢复任务来证明其多功能性：图像去模糊、图像去噪和JPEG伪影去除。我们的实验表明，与最先进的方法相比，RAP产生了优越的视觉效果。RAP代表了图像恢复的一个很有前途的方向，为用户提供了更大的控制，并实现了对象级别的图像恢复。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13093v2" target="_blank">2305.13093v2</a>
                              </td>
                              <td>Restore Anything Pipeline: Segment Anything Meets Image Restoration</td>
                              <td>Jiaxi Jiang</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13093v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00362v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00362v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00362v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00362v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For a given graph $G$, a depth-first search (DFS) tree $T$ of $G$ is an $r$-rooted spanning tree such that every edge of $G$ is either an edge of $T$ or is between a \textit{descendant} and an \textit{ancestor} in $T$. A graph $G$ together with a DFS tree is called a \textit{lineal topology} $\mathcal{T} = (G, r, T)$. Sam et al. (2023) initiated study of the parameterized complexity of the \textsc{Min-LLT} and \textsc{Max-LLT} problems which ask, given a graph $G$ and an integer $k\geq 0$, whether $G$ has a DFS tree with at most $k$ and at least $k$ leaves, respectively. Particularly, they showed that for the dual parameterization, where the tasks are to find DFS trees with at least $n-k$ and at most $n-k$ leaves, respectively, these problems are fixed-parameter tractable when parameterized by $k$. However, the proofs were based on Courcelle's theorem, thereby making the running times a tower of exponentials. We prove that both problems admit polynomial kernels with $\Oh(k^3)$ vertices. In particular, this implies FPT algorithms running in $k^{\Oh(k)}\cdot n^{O(1)}$ time. We achieve these results by making use of a $\Oh(k)$-sized vertex cover structure associated with each problem. This also allows us to demonstrate polynomial kernels for \textsc{Min-LLT} and \textsc{Max-LLT} for the structural parameterization by the vertex cover number.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00362v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于给定的图$G$，$G$的深度优先搜索（DFS）树$T$是一个$r$根的生成树，使得$G$中的每条边要么是$T$的边，要么在$T$中的\textit｛subscendance｝和\textit{祖先｝之间。图$G$与DFS树一起称为\textit｛linear topology｝$\mathcal｛T｝=（G，r，T）$。Sam等人（2023）开始研究\textsc｛Min LLT｝和\textsc{Max LLT}问题的参数化复杂性，这些问题询问，给定图$G$和整数$k\geq 0$，$G$是否具有分别具有最多$k$和至少$k$叶的DFS树。特别地，他们表明，对于对偶参数化，其中任务是分别找到叶数至少为$n-k$和最多为$n-k$的DFS树，当用$k$参数化时，这些问题是固定参数可处理的。然而，这些证明是基于库塞尔定理，从而使运行时间成为指数塔。我们证明了这两个问题都允许具有$\Oh（k^3）$顶点的多项式核。特别地，这意味着FPT算法在$k^｛\Oh（k）｝\cdot n^｛O（1）｝$时间内运行。我们通过使用与每个问题相关的$\Oh（k）$大小的顶点覆盖结构来实现这些结果。这也使我们能够通过顶点覆盖数证明\textsc｛Min LLT｝和\textsc{Max LLT}的结构参数化的多项式核。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00362v1" target="_blank">2307.00362v1</a>
                              </td>
                              <td>Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</td>
                              <td>Emmanuel Sam</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00362v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00362v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00290v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00290v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00290v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00290v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pipeline surpasses the state-of-the-art (SOTA) methods in a nuclei segmentation task on the public Monuseg dataset, and 2) the utilization of weak and few annotations for SAM finetuning achieves competitive performance compared to using strong pixel-wise annotated data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00290v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）是最近在通用零样本分割方法中提出的基于提示的分割模型。凭借零样本分割能力，SAM在各种分割任务上实现了令人印象深刻的灵活性和准确性。然而，目前的流水线在推理阶段需要手动提示，这对于生物医学图像分割来说仍然是资源密集型的。在本文中，我们引入了一种在整个人工智能开发工作流程（从注释生成到模型微调）中使用SAM的管道，而不是在推理阶段使用提示，称为all-In-SAM，而不需要在推理阶段手动提示。具体而言，SAM首先用于从弱提示（例如，点、边界框）生成像素级注释。然后，使用像素级注释来微调SAM分割模型，而不是从头开始训练。我们的实验结果揭示了两个关键发现：1）在公共Monuseg数据集上的核分割任务中，所提出的流水线超过了最先进的（SOTA）方法，以及2）与使用强像素注释数据相比，利用弱和少量注释进行SAM微调实现了有竞争力的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00290v1" target="_blank">2307.00290v1</a>
                              </td>
                              <td>All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning</td>
                              <td>Can Cui</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00290v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00290v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14289v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Faster Segment Anything: Towards Lightweight SAM for Mobile Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14289v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14289v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14289v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot transfer performance and high versatility for numerous vision applications (like image editing with fine-grained control). Many of such applications need to be run on resource-constraint edge devices, like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance, especially when limited training sources are available. We find that this is mainly caused by the coupled optimization of the image encoder and mask decoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM. The training can be completed on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM which is more than 60 times smaller yet performs on par with the original SAM. For inference speed, With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover, we show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided at \href{https://github.com/ChaoningZhang/MobileSAM}{\textcolor{red}{MobileSAM}}), with a demo showing that MobileSAM can run relatively smoothly on CPU.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14289v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）因其令人印象深刻的零样本传输性能和对许多视觉应用程序的高度通用性（如具有细粒度控制的图像编辑）而备受关注。许多这样的应用程序需要在资源受限的边缘设备上运行，比如手机。在这项工作中，我们的目标是通过用轻量级图像编码器取代重量级图像编码器，使SAM对移动友好。原始SAM文件中训练这种新SAM的天真方式会导致性能不令人满意，尤其是当可用的训练来源有限时。我们发现，这主要是由图像编码器和掩模解码器的耦合优化引起的，因此我们提出了解耦蒸馏。具体来说，我们将知识从重图像编码器（原始SAM中的ViT-H）提取到轻量级图像编码器，该编码器可以自动与原始SAM的掩码解码器兼容。训练可以在不到一天的时间内在单个GPU上完成，由此产生的轻量级SAM被称为MobileSAM，它比原始SAM小60多倍，但性能与原始SAM相当。就推理速度而言，使用单个GPU，MobileSAM每幅图像运行约10ms：图像编码器运行8ms，掩码解码器运行4ms。凭借卓越的性能，我们的MobileSAM比并发FastSAM快约5倍，小7倍，更适合移动应用程序。此外，我们还证明了MobileSAM在CPU上可以相对平稳地运行。我们项目的代码提供于\ href{https://github.com/ChaoningZhang/MobileSAM}｛\textcolor｛red｝｛MobileSAM｝｝），演示显示MobileSAM可以在CPU上相对平稳地运行。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14289v2" target="_blank">2306.14289v2</a>
                              </td>
                              <td>Faster Segment Anything: Towards Lightweight SAM for Mobile Applications</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-06-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14289v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14289v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05392v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sharpness-Aware Minimization Alone can Improve Adversarial Robustness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05392v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05392v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05392v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-Aware Minimization (SAM) is an effective method for improving generalization ability by regularizing loss sharpness. In this paper, we explore SAM in the context of adversarial robustness. We find that using only SAM can achieve superior adversarial robustness without sacrificing clean accuracy compared to standard training, which is an unexpected benefit. We also discuss the relation between SAM and adversarial training (AT), a popular method for improving the adversarial robustness of DNNs. In particular, we show that SAM and AT differ in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model. Finally, while AT suffers from decreased clean accuracy and computational overhead, we suggest that SAM can be regarded as a lightweight substitute for AT under certain requirements. Code is available at https://github.com/weizeming/SAM_AT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05392v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>清晰度感知最小化（SAM）是一种通过正则化损失清晰度来提高泛化能力的有效方法。在本文中，我们在对抗性稳健性的背景下探索SAM。我们发现，与标准训练相比，仅使用SAM可以在不牺牲干净准确性的情况下实现卓越的对抗鲁棒性，这是一个意想不到的好处。我们还讨论了SAM和对抗性训练（AT）之间的关系，这是一种提高DNN对抗性鲁棒性的流行方法。特别是，我们表明SAM和AT在扰动强度方面不同，导致不同的精度和鲁棒性权衡。我们在一个简化的模型中为这些主张提供了理论证据。最后，虽然AT的清洁精度和计算开销降低，但我们建议，在某些要求下，SAM可以被视为AT的轻量级替代品。代码位于https://github.com/weizeming/SAM_AT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05392v2" target="_blank">2305.05392v2</a>
                              </td>
                              <td>Sharpness-Aware Minimization Alone can Improve Adversarial Robustness</td>
                              <td>Zeming Wei</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05392v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05392v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00038v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Training-free Object Counting with Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00038v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00038v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00038v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper tackles the problem of object counting in images. Existing approaches rely on extensive training data with point annotations for each object, making data collection labor-intensive and time-consuming. To overcome this, we propose a training-free object counter that treats the counting task as a segmentation problem. Our approach leverages the Segment Anything Model (SAM), known for its high-quality masks and zero-shot segmentation capability. However, the vanilla mask generation method of SAM lacks class-specific information in the masks, resulting in inferior counting accuracy. To overcome this limitation, we introduce a prior-guided mask generation method that incorporates three types of priors into the segmentation process, enhancing efficiency and accuracy. Additionally, we tackle the issue of counting objects specified through free-form text by proposing a two-stage approach that combines reference object selection and prior-guided mask generation. Extensive experiments on standard datasets demonstrate the competitive performance of our training-free counter compared to learning-based approaches. This paper presents a promising solution for counting objects in various scenarios without the need for extensive data collection and model training. Code is available at https://github.com/shizenglin/training-free-object-counter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00038v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文解决了图像中物体计数的问题。现有的方法依赖于为每个对象添加点注释的大量训练数据，这使得数据收集耗费大量人力和时间。为了克服这一点，我们提出了一种无训练的对象计数器，将计数任务视为分割问题。我们的方法利用了Segment Anything Model（SAM），SAM以其高质量的掩码和零样本分割功能而闻名。然而，SAM的香草掩模生成方法在掩模中缺乏类特定信息，导致计数精度较差。为了克服这一限制，我们引入了一种先验引导掩模生成方法，该方法将三种类型的先验结合到分割过程中，提高了效率和准确性。此外，我们通过提出一种两阶段方法来解决通过自由格式文本指定的对象计数问题，该方法结合了参考对象选择和先前引导的掩码生成。在标准数据集上进行的大量实验表明，与基于学习的方法相比，我们的无训练计数器具有竞争力。本文提出了一种很有前途的解决方案，用于在各种场景中对对象进行计数，而无需大量的数据收集和模型训练。代码位于https://github.com/shizenglin/training-free-object-counter.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00038v1" target="_blank">2307.00038v1</a>
                              </td>
                              <td>Training-free Object Counting with Prompts</td>
                              <td>Zenglin Shi</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00038v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00038v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17504v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17504v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17504v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17504v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate as SAM, i.e., $O(\log T/\sqrt{T})$. Sparse SAM has the potential to accelerate training and smooth the loss landscape effectively. Extensive experimental results on CIFAR and ImageNet-1K confirm that our method is superior to SAM in terms of efficiency, and the performance is preserved or even improved with a perturbation of merely 50\% sparsity. Code is available at https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17504v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于复杂和非凸的损失景观，深度神经网络往往具有较差的泛化能力。Sharpness Aware Minimization（SAM）是一种流行的解决方案，当向权重添加扰动时，通过最小化训练损失的最大变化来平滑损失。然而，SAM对所有参数的不加区分的扰动是次优的，并导致过度计算，是随机梯度下降（SGD）等常见优化器的两倍开销。在本文中，我们提出了稀疏SAM（SSAM），这是一种通过二进制掩码实现稀疏扰动的高效训练方案。为了获得稀疏掩模，我们分别提供了基于Fisher信息和动态稀疏训练的两种解决方案。我们研究了不同掩码的影响，包括非结构化、结构化和$N$:$M$结构化模式，以及实现稀疏扰动的显式和隐式形式。我们从理论上证明了SSAM可以以与SAM相同的速率收敛，即$O（\log T/\sqrt｛T｝）$。稀疏SAM有可能加速训练并有效地消除损失。在CIFAR和ImageNet-1K上的大量实验结果证实，我们的方法在效率方面优于SAM，并且在只有50%稀疏性的扰动下，性能得以保留甚至提高。代码位于https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17504v1" target="_blank">2306.17504v1</a>
                              </td>
                              <td>Systematic Investigation of Sparse Perturbed Sharpness-Aware Minimization Optimizer</td>
                              <td>Peng Mi</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17504v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14752v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MedLSAM: Localize and Segment Anything Model for 3D Medical Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14752v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14752v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14752v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has recently emerged as a groundbreaking model in the field of image segmentation. Nevertheless, both the original SAM and its medical adaptations necessitate slice-by-slice annotations, which directly increase the annotation workload with the size of the dataset. We propose MedLSAM to address this issue, ensuring a constant annotation workload irrespective of dataset size and thereby simplifying the annotation process. Our model introduces a few-shot localization framework capable of localizing any target anatomical part within the body. To achieve this, we develop a Localize Anything Model for 3D Medical Images (MedLAM), utilizing two self-supervision tasks: relative distance regression (RDR) and multi-scale similarity (MSS) across a comprehensive dataset of 14,012 CT scans. We then establish a methodology for accurate segmentation by integrating MedLAM with SAM. By annotating only six extreme points across three directions on a few templates, our model can autonomously identify the target anatomical region on all data scheduled for annotation. This allows our framework to generate a 2D bounding box for every slice of the image, which are then leveraged by SAM to carry out segmentations. We conducted experiments on two 3D datasets covering 38 organs and found that MedLSAM matches the performance of SAM and its medical adaptations while requiring only minimal extreme point annotations for the entire dataset. Furthermore, MedLAM has the potential to be seamlessly integrated with future 3D SAM models, paving the way for enhanced performance. Our code is public at https://github.com/openmedlab/MedLSAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14752v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）最近作为图像分割领域的一个突破性模型出现。然而，原始SAM及其医学适应都需要逐片注释，这会随着数据集的大小直接增加注释工作量。我们提出了MedLSAM来解决这个问题，无论数据集大小如何，都能确保恒定的注释工作负载，从而简化注释过程。我们的模型引入了一个能够定位身体内任何目标解剖部位的多镜头定位框架。为了实现这一点，我们开发了一个3D医学图像的局部任意模型（MedLAM），利用两个自我监督任务：14012次CT扫描的综合数据集的相对距离回归（RDR）和多尺度相似性（MSS）。然后，我们通过将MedLAM与SAM集成，建立了一种精确分割的方法。通过在几个模板上只注释三个方向上的六个极值点，我们的模型可以在所有计划注释的数据上自主识别目标解剖区域。这允许我们的框架为图像的每个切片生成2D边界框，然后SAM利用该边界框进行分割。我们在覆盖38个器官的两个3D数据集上进行了实验，发现MedLSAM与SAM的性能及其医学适应相匹配，而整个数据集只需要最小的极值点注释。此外，MedLAM有可能与未来的3D SAM模型无缝集成，为增强性能铺平道路。我们的代码公开于https://github.com/openmedlab/MedLSAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14752v2" target="_blank">2306.14752v2</a>
                              </td>
                              <td>MedLSAM: Localize and Segment Anything Model for 3D Medical Images</td>
                              <td>Wenhui Lei</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14752v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14752v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17400v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17400v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17400v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17400v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Emerging foundation models in machine learning are models trained on vast amounts of data that have been shown to generalize well to new tasks. Often these models can be prompted with multi-modal inputs that range from natural language descriptions over images to point clouds. In this paper, we propose topological data analysis (TDA) guided prompt optimization for the Segment Anything Model (SAM) and show preliminary results in the biological image segmentation domain. Our approach replaces the standard grid search approach that is used in the original implementation and finds point locations based on their topological significance. Our results show that the TDA optimized point cloud is much better suited for finding small objects and massively reduces computational complexity despite the extra step in scenarios which require many segmentations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17400v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习中新兴的基础模型是在大量数据上训练的模型，这些数据已被证明能够很好地推广到新任务中。通常，这些模型可以通过多模态输入进行提示，这些输入从图像上的自然语言描述到点云。在本文中，我们提出了拓扑数据分析（TDA）引导的分段任意模型（SAM）的即时优化，并在生物图像分割领域显示了初步结果。我们的方法取代了原始实现中使用的标准网格搜索方法，并根据点的拓扑意义来查找点位置。我们的结果表明，尽管在需要许多分割的场景中需要额外的步骤，但TDA优化的点云更适合于寻找小对象，并大大降低了计算复杂性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17400v1" target="_blank">2306.17400v1</a>
                              </td>
                              <td>Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging</td>
                              <td>Ruben Glatt</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17400v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17400v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17075v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17075v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17075v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17075v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid advancements in computer vision have stimulated remarkable progress in face forgery techniques, capturing the dedicated attention of researchers committed to detecting forgeries and precisely localizing manipulated areas. Nonetheless, with limited fine-grained pixel-wise supervision labels, deepfake detection models perform unsatisfactorily on precise forgery detection and localization. To address this challenge, we introduce the well-trained vision segmentation foundation model, i.e., Segment Anything Model (SAM) in face forgery detection and localization. Based on SAM, we propose the Detect Any Deepfakes (DADF) framework with the Multiscale Adapter, which can capture short- and long-range forgery contexts for efficient fine-tuning. Moreover, to better identify forged traces and augment the model's sensitivity towards forgery regions, Reconstruction Guided Attention (RGA) module is proposed. The proposed framework seamlessly integrates end-to-end forgery localization and detection optimization. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach for both forgery detection and localization. The codes will be released soon at https://github.com/laiyingxin2/DADF.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17075v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉的快速发展激发了人脸伪造技术的显著进步，吸引了致力于检测伪造品和精确定位被操纵区域的研究人员的注意力。尽管如此，在有限的细粒度像素监督标签的情况下，深度伪造检测模型在精确的伪造检测和定位方面表现不佳。为了应对这一挑战，我们在人脸伪造检测和定位中引入了训练有素的视觉分割基础模型，即分段任意模型（SAM）。基于SAM，我们提出了带有多尺度适配器的Detect Any Deepfakes（DADF）框架，该框架可以捕获短期和长期伪造上下文，以进行有效的微调。此外，为了更好地识别伪造痕迹并提高模型对伪造区域的敏感性，提出了重建引导注意（RGA）模块。所提出的框架无缝集成了端到端伪造定位和检测优化。在三个基准数据集上的大量实验证明了我们的方法在伪造检测和定位方面的优越性。代码将很快在发布https://github.com/laiyingxin2/DADF.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17075v1" target="_blank">2306.17075v1</a>
                              </td>
                              <td>Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization</td>
                              <td>Yingxin Lai</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17075v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17075v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16623v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16623v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16623v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16623v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segmentation is an essential step for remote sensing image processing. This study aims to advance the application of the Segment Anything Model (SAM), an innovative image segmentation model by Meta AI, in the field of remote sensing image analysis. SAM is known for its exceptional generalization capabilities and zero-shot learning, making it a promising approach to processing aerial and orbital images from diverse geographical contexts. Our exploration involved testing SAM across multi-scale datasets using various input prompts, such as bounding boxes, individual points, and text descriptors. To enhance the model's performance, we implemented a novel automated technique that combines a text-prompt-derived general example with one-shot training. This adjustment resulted in an improvement in accuracy, underscoring SAM's potential for deployment in remote sensing imagery and reducing the need for manual annotation. Despite the limitations encountered with lower spatial resolution images, SAM exhibits promising adaptability to remote sensing data analysis. We recommend future research to enhance the model's proficiency through integration with supplementary fine-tuning techniques and other networks. Furthermore, we provide the open-source code of our modifications on online repositories, encouraging further and broader adaptations of SAM to the remote sensing domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16623v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分割是遥感图像处理的一个重要步骤。本研究旨在推进元人工智能创新的图像分割模型Segment Anything Model（SAM）在遥感图像分析领域的应用。SAM以其卓越的泛化能力和零样本学习而闻名，使其成为处理来自不同地理环境的航空和轨道图像的一种很有前途的方法。我们的探索涉及使用各种输入提示（如边界框、单个点和文本描述符）在多尺度数据集上测试SAM。为了提高模型的性能，我们实现了一种新的自动化技术，该技术将文本提示派生的一般示例与一次性训练相结合。这一调整提高了精度，突出了SAM在遥感图像中部署的潜力，并减少了手动注释的需要。尽管空间分辨率较低的图像存在局限性，SAM在遥感数据分析方面表现出了良好的适应性。我们建议未来的研究通过与补充微调技术和其他网络的集成来提高模型的熟练度。此外，我们在在线存储库上提供了我们修改的开源代码，鼓励SAM进一步更广泛地适应遥感领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16623v1" target="_blank">2306.16623v1</a>
                              </td>
                              <td>The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot</td>
                              <td>Lucas Prado Osco</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16623v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16623v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16388v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Measuring the Representation of Subjective Global Opinions in Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16388v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16388v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16388v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16388v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）可能无法公平地代表对社会问题的不同全球视角。在本文中，我们开发了一个定量框架来评估谁的意见模型生成的回答更相似。我们首先构建了一个数据集GlobalOpinionQA，由跨国调查的问题和答案组成，旨在捕捉不同国家对全球问题的不同意见。接下来，我们定义了一个指标，以国家为条件，量化LLM生成的调查响应和人类响应之间的相似性。在我们的框架下，我们对一位LLM进行了三项实验，该LLM被训练成对宪法人工智能有益、诚实和无害。默认情况下，LLM的反应往往更类似于某些人群的意见，例如来自美国、一些欧洲和南美国家的人的意见，这突出了偏见的可能性。当我们促使该模型考虑特定国家的观点时，反应会变得更类似于被提示人群的意见，但可能反映有害的文化刻板印象。当我们将GlobalOpinionQA问题翻译成目标语言时，模型的回答并不一定与这些语言的使用者的意见最相似。我们发布数据集供其他人使用和构建。我们的数据位于https://huggingface.co/datasets/Anthropic/llm_global_opinions.我们还在https://llmglobalvalues.anthropic.com.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16388v1" target="_blank">2306.16388v1</a>
                              </td>
                              <td>Towards Measuring the Representation of Subjective Global Opinions in Language Models</td>
                              <td>Esin Durmus</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16388v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16388v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_04749v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04749v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04749v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04749v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04749v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着潜在扩散模型的出现，文本条件图像生成领域取得了前所未有的进展。尽管值得注意的是，随着给定文本输入的复杂性增加，最先进的扩散模型可能仍然无法生成准确传达给定提示语义的图像。此外，已经观察到，预训练的多模态模型（如CLIP）往往无法检测到这种错位。为了解决这些问题，在本文中，我们探索了一种简单而有效的分解方法来评估和改进文本到图像的对齐。特别是，我们首先引入了一个分解对齐分数，该分数在给定复杂提示的情况下将其分解为一组不相交的断言。然后使用VQA模型来测量每个断言与生成的图像的对准。最后，将不同断言的对齐分数组合起来，给出最终的文本到图像的对齐分数。实验分析表明，与传统的CLIP、BLIP评分相比，所提出的比对指标与人类评分的相关性显著更高。此外，我们还发现断言级别对齐分数提供了有用的反馈，然后可以在简单的迭代过程中使用该反馈来逐渐增加不同断言在最终图像输出中的表达。人类用户研究表明，所提出的方法在整体文本到图像对齐精度方面超过了以前的技术水平8.7%。我们论文的项目页面可在https://1jsingh.github.io/divide-evaluate-and-refine</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04749v1" target="_blank">2307.04749v1</a>
                              </td>
                              <td>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</td>
                              <td>Jaskirat Singh</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04749v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04749v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04725v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04725v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04725v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04725v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs. Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04725v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着文本到图像模型（例如，稳定扩散）和相应的个性化技术（如DreamBooth和LoRA）的进步，每个人都可以以负担得起的成本将自己的想象力表现为高质量的图像。随后，对图像动画技术有很大的需求，以进一步将生成的静态图像与运动动力学相结合。在本报告中，我们提出了一个实用的框架来一劳永逸地为大多数现有的个性化文本到图像模型设置动画，从而节省了特定于模型的调整工作。该框架的核心是在冻结的文本到图像模型中插入一个新初始化的运动建模模块，并在视频剪辑上对其进行训练，以提取合理的运动先验。一旦经过训练，通过简单地注入这个运动建模模块，从相同的基础T2I派生的所有个性化版本都很容易成为文本驱动的模型，从而产生多样化和个性化的动画图像。我们对动画图片和逼真照片中的几个具有公众代表性的个性化文本到图像模型进行了评估，并证明我们提出的框架有助于这些模型生成时间平滑的动画片段，同时保留其输出的领域和多样性。代码和预先训练的重量将在https://animatediff.github.io/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04725v1" target="_blank">2307.04725v1</a>
                              </td>
                              <td>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</td>
                              <td>Yuwei Guo</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04725v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04725v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04317v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04317v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04317v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04317v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04317v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代图像分类是基于通过大型判别网络直接预测模型类别，这使得很难评估可能构成分类决策的直观视觉“特征”。同时，最近在联合视觉语言模型（如CLIP）中的工作提供了指定图像类的自然语言描述的方法，但通常侧重于为每个类提供单个描述。在这项工作中，我们证明了一种替代方法，可以说更类似于我们对每类多个“视觉特征”的理解，也可以在稳健的少镜头学习环境中提供令人信服的性能。特别是，我们通过大型语言模型（LLM）自动枚举每个类别的多个视觉描述，然后使用视觉图像模型将这些描述翻译为每个图像的一组多个视觉特征；最后，我们使用稀疏逻辑回归来选择这些特征的相关子集来对每个图像进行分类。这既为每个类提供了一组“直观”的相关特征，又在少镜头学习环境中，优于线性探测等标准方法。当与微调相结合时，我们还表明，该方法能够在分布内和分布外性能上优于现有的最先进的微调方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04317v1" target="_blank">2307.04317v1</a>
                              </td>
                              <td>Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</td>
                              <td>Zhili Feng</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04317v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04317v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04192v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04192v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04192v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04192v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most domain frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIS actively searches key frames customized for each video--question pair with the assistance of auxiliary models. The experimental results on three public datasets from three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image--text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04192v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频问答是视频理解领域的一项基本任务。尽管目前配备了视频转换器的视觉语言模型（VLM）已经实现了时间建模并产生了优越的结果，但它们是以巨大的计算能力为代价的，因此在实时应用场景中部署成本太高。一种经济的解决方法只对一小部分帧进行采样，以表示视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常对一组帧或剪辑进行随机采样，而不考虑其视觉内容之间的内部相关性，也不考虑其与问题的相关性。我们认为，这种无目标的采样可能会忽略可以推断出正确答案的关键帧，并且当采样稀疏性增加时，情况会变得更糟，而这种情况总是随着视频长度的增加而发生。为了缓解这个问题，我们提出了两种帧采样策略，即最域帧（MDF）和最隐含帧（MIF），以最大限度地保留那些对给定问题最重要的帧。MDF以引导的方式被动地将关键帧遗漏的风险降至最低，而MIS则在辅助模型的帮助下主动搜索为每个视频定制的关键帧——问题对。在三个高级VLM（CLIP、GIT和All-in-one）的三个公共数据集上的实验结果表明，我们提出的策略可以提高图像-文本预训练模型的性能。与本文中提出的方法有关的源代码可在https://github.com/declare-lab/sas-vqa.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04192v1" target="_blank">2307.04192v1</a>
                              </td>
                              <td>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</td>
                              <td>Wei Han</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04192v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04192v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13485v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13485v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13485v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13485v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the computational burden and communication burden caused by large models. Extensive experiments are conducted on three datasets with distribution shifts. Qualitative and quantitative results demonstrate that FedCLIP significantly outperforms other baselines (9% overall improvements on PACS) and effectively reduces computational and communication costs (283x faster than FedAVG). Our code will be available at: https://github.com/microsoft/PersonalizedFL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13485v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，联合学习（FL）作为一种新的隐私保护计算范式出现了。不幸的是，FL面临着阻碍其实际性能的两个关键挑战：数据分布的异质性和大型基础模型带来的高资源成本。具体而言，不同客户端中的非IID数据使现有FL算法难以收敛，而高昂的资源成本，包括计算和通信成本，增加了现实场景中的部署难度。在本文中，我们提出了一种有效而简单的方法，称为FedCLIP，以实现联邦学习中CLIP的快速泛化和个性化。具体来说，我们为大型模型CLIP设计了一个基于注意力的适配器，其余操作仅依赖于适配器。轻量级适配器可以最大限度地利用预训练的模型信息，并确保模型在特定任务中适用于客户端。同时，小规模操作可以减轻大型模型造成的计算负担和通信负担。在具有分布变化的三个数据集上进行了广泛的实验。定性和定量结果表明，FedCLIP显著优于其他基线（PACS的总体改进率为9%），并有效降低了计算和通信成本（比FedAVG快283倍）。我们的代码将在以下位置提供：https://github.com/microsoft/PersonalizedFL.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13485v2" target="_blank">2302.13485v2</a>
                              </td>
                              <td>FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning</td>
                              <td>Wang Lu</td>
                              <td>2023-02-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13485v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13485v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04132v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04132v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04132v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04132v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we release two new datasets of object-behaviour-facts extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04132v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，根据描述场景序列的副词最好通过对对象行为的高级概念进行推理来识别的直觉，我们提出了一个新的框架的设计，该框架对从原始视频剪辑中提取的对象行为进行推理，以识别剪辑中相应的副词类型。重要的是，虽然以前的一般场景副词识别工作假设知道动作类型背后的片段，但我们的方法直接适用于视频片段的动作类型未知的更一般的问题设置。具体而言，我们提出了一种新的管道，从原始视频剪辑中提取人类可解释的对象行为事实，并提出了新的基于符号和变换器的推理方法，对这些提取的事实进行操作，以识别副词类型。实验结果表明，我们提出的方法与以前的先进方法相比表现良好。此外，为了支持符号视频处理，我们发布了两个新的从原始视频剪辑中提取的对象行为事实数据集——MSR-VTT-ASP和ActivityNetASP数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04132v1" target="_blank">2307.04132v1</a>
                              </td>
                              <td>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</td>
                              <td>Amrit Diggavi Seshadri</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04132v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04132v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03073v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03073v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03073v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03073v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel framework for few-shot learning by leveraging large-scale vision-language models such as CLIP. Motivated by the unimodal prototypical networks for few-shot learning, we introduce PROTO-CLIP that utilizes image prototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIP adapts the image encoder and text encoder in CLIP in a joint fashion using few-shot examples. The two encoders are used to compute prototypes of image classes for classification. During adaptation, we propose aligning the image and text prototypes of corresponding classes. Such a proposed alignment is beneficial for few-shot classification due to the contributions from both types of prototypes. We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as well as in the real world for robot perception.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03073v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们通过利用大规模视觉语言模型（如CLIP），为少镜头学习提出了一个新的框架。受用于少镜头学习的单峰原型网络的启发，我们介绍了利用图像原型和文本原型进行多镜头学习的PROTO-CLIP。具体而言，PROTO-CLIP使用少量镜头示例以联合方式调整CLIP中的图像编码器和文本编码器。这两个编码器用于计算用于分类的图像类的原型。在改编过程中，我们建议对齐相应类的图像和文本原型。由于这两种类型的原型的贡献，这种拟议的对准对于少镜头分类是有益的。我们通过在少镜头学习的基准数据集上以及在现实世界中进行机器人感知的实验，证明了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03073v2" target="_blank">2307.03073v2</a>
                              </td>
                              <td>Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning</td>
                              <td>Jishnu Jaykumar P</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03073v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03073v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04028v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring the Success of Diffusion Models at Imitating Human Artists</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04028v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04028v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04028v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP can be used to reclassify the artist (or the artist's work) from the imitation. If these tests match the imitation back to the original artist, this suggests the model can imitate that artist's expression. Our approach is simple and quantitative. Furthermore, it uses standard techniques and does not require additional training. We demonstrate our approach with an audit of Stable Diffusion's capacity to imitate 70 professional digital artists with copyrighted work online. When Stable Diffusion is prompted to imitate an artist from this set, we find that the artist can be identified from the imitation with an average accuracy of 81.0%. Finally, we also show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. Overall, these results suggest that Stable Diffusion is broadly successful at imitating individual human artists.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04028v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代扩散模型为人工智能图像生成奠定了最先进的基础。他们的成功部分归功于对互联网规模数据的培训，这些数据通常包括版权作品。这引发了关于这些模型在多大程度上学习、模仿或复制人类艺术家的作品的问题。这项工作表明，考虑到生成模型的不断发展的生态系统，将版权责任与模型的能力联系起来可能是有用的。具体而言，对版权和生成系统的大部分法律分析都集中在使用受保护的数据进行培训上。因此，数据、训练和系统之间的联系往往被掩盖。在我们的方法中，我们考虑了简单的图像分类技术来衡量模型模仿特定艺术家的能力。具体来说，我们使用对比语言图像预处理（CLIP）编码器以零样本方式对图像进行分类。我们的过程首先提示模型模仿特定的艺术家。然后，我们测试CLIP是否可以用于从模仿中重新分类艺术家（或艺术家的作品）。如果这些测试将模仿与原始艺术家相匹配，这表明模型可以模仿该艺术家的表情。我们的方法简单而定量。此外，它使用标准技术，不需要额外的培训。我们通过对Stable Diffusion在网上模仿70位拥有版权作品的专业数字艺术家的能力的审计来展示我们的方法。当Stable Diffusion被提示模仿这组艺术家时，我们发现可以从模仿中识别出艺术家，平均准确率为81.0%。最后，我们还表明，艺术家作品的样本可以与这些模仿图像匹配，具有高度的统计可靠性。总的来说，这些结果表明稳定扩散在模仿人类艺术家个体方面取得了广泛的成功。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04028v1" target="_blank">2307.04028v1</a>
                              </td>
                              <td>Measuring the Success of Diffusion Models at Imitating Human Artists</td>
                              <td>Stephen Casper</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04028v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04028v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00097v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00097v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00097v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00097v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, CLIP-based approaches have exhibited remarkable performance on generalization and few-shot learning tasks, fueled by the power of contrastive language-vision pre-training. In particular, prompt tuning has emerged as an effective strategy to adapt the pre-trained language-vision models to downstream tasks by employing task-related textual tokens. Motivated by this progress, in this work we question whether other fundamental problems, such as weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning. Our findings reveal two interesting observations that shed light on the impact of prompt tuning on WSSS. First, modifying only the class token of the text prompt results in a greater impact on the Class Activation Map (CAM), compared to arguably more complex strategies that optimize the context. And second, the class token associated with the image ground truth does not necessarily correspond to the category that yields the best CAM. Motivated by these observations, we introduce a novel approach based on a PrOmpt cLass lEarning (POLE) strategy. Through extensive experiments we demonstrate that our simple, yet efficient approach achieves SOTA performance in a well-known WSSS benchmark. These results highlight not only the benefits of language-vision models in WSSS but also the potential of prompt learning for this problem. The code is available at https://github.com/rB080/WSS_POLE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00097v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，在对比语言视觉预训练的力量的推动下，基于CLIP的方法在泛化和少镜头学习任务上表现出了显著的性能。特别是，即时调整已经成为一种有效的策略，通过使用与任务相关的文本标记，使预先训练的语言视觉模型适应下游任务。受这一进展的推动，在这项工作中，我们质疑其他基本问题，如弱监督语义分割（WSSS），是否可以从及时调整中受益。我们的发现揭示了两个有趣的观察结果，揭示了快速调谐对WSSS的影响。首先，与优化上下文的更复杂的策略相比，仅修改文本提示的类标记会对类激活映射（CAM）产生更大的影响。其次，与图像基本事实相关联的类标记不一定对应于产生最佳CAM的类别。受这些观察结果的启发，我们介绍了一种基于PrOmpt-cLass-lEarning（POLE）策略的新方法。通过大量的实验，我们证明了我们简单而有效的方法在著名的WSSS基准中实现了SOTA性能。这些结果不仅突出了WSSS中语言视觉模型的优势，还突出了快速学习解决这一问题的潜力。代码位于https://github.com/rB080/WSS_POLE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00097v2" target="_blank">2307.00097v2</a>
                              </td>
                              <td>Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation</td>
                              <td>Balamurali Murugesan</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00097v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00097v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09532v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hierarchical Planning and Control for Box Loco-Manipulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09532v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09532v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09532v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Humans perform everyday tasks using a combination of locomotion and manipulation skills. Building a system that can handle both skills is essential to creating virtual humans. We present a physically-simulated human capable of solving box rearrangement tasks, which requires a combination of both skills. We propose a hierarchical control architecture, where each level solves the task at a different level of abstraction, and the result is a physics-based simulated virtual human capable of rearranging boxes in a cluttered environment. The control architecture integrates a planner, diffusion models, and physics-based motion imitation of sparse motion clips using deep reinforcement learning. Boxes can vary in size, weight, shape, and placement height. Code and trained control policies are provided.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09532v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类利用运动和操纵技能来完成日常任务。建立一个能够同时处理这两种技能的系统对于创建虚拟人至关重要。我们展示了一个能够解决盒子重排任务的物理模拟人，这需要两种技能的结合。我们提出了一种分层控制架构，其中每个级别都在不同的抽象级别上解决任务，结果是一个基于物理的模拟虚拟人能够在杂乱的环境中重新排列盒子。控制架构集成了规划器、扩散模型和使用深度强化学习的稀疏运动片段的基于物理的运动模仿。盒子的大小、重量、形状和放置高度各不相同。提供了代码和经过培训的控制策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09532v2" target="_blank">2306.09532v2</a>
                              </td>
                              <td>Hierarchical Planning and Control for Box Loco-Manipulation</td>
                              <td>Zhaoming Xie</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09532v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09532v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_16897v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_16897v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_16897v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_16897v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are directly estimated from noisy real-world impact sound examples without sophisticated setup and learned residual parameters that interpret the sound environment via neural networks. We further implement a novel diffusion model with specific training and inference strategies to combine physics priors and visual information for impact sound synthesis. Experimental results show that our model outperforms several existing systems in generating realistic impact sounds. More importantly, the physics-based representations are fully interpretable and transparent, thus enabling us to perform sound editing flexibly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_16897v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对物理对象交互发出的声音进行建模对于真实和虚拟世界中的沉浸式感知体验至关重要。传统的冲击声合成方法是通过物理模拟来获得一组能够表示和合成声音的物理参数。然而，它们需要物体几何形状和撞击位置的精细细节，而这些细节在现实世界中很少可用，也不能用于合成常见视频中的撞击声。另一方面，现有的基于视频驱动的深度学习方法由于缺乏物理知识，只能捕捉视觉内容和冲击声之间的微弱对应。在这项工作中，我们提出了一种物理驱动的扩散模型，该模型可以合成无声视频剪辑的高保真撞击声。除了视频内容，我们建议使用额外的物理先验来指导冲击声合成过程。物理先验既包括在没有复杂设置的情况下从嘈杂的真实世界撞击声示例直接估计的物理参数，也包括通过神经网络解释声音环境的学习残差参数。我们进一步实现了一种新的扩散模型，该模型具有特定的训练和推理策略，将物理先验和视觉信息相结合，用于冲击声合成。实验结果表明，我们的模型在生成逼真的撞击声方面优于现有的几种系统。更重要的是，基于物理的表示是完全可解释和透明的，从而使我们能够灵活地执行声音编辑。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.16897v3" target="_blank">2303.16897v3</a>
                              </td>
                              <td>Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos</td>
                              <td>Kun Su</td>
                              <td>2023-03-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_16897v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.16897v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03798v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03798v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03798v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03798v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. From the perspective of vulnerability to off-manifold attacks, we therefore argue for the mitigation of modality gaps in CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints are available at https://github.com/matfrei/CLIPMasterPrints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03798v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉和文本数据的模型，如对比语言图像预训练（CLIP），越来越重要。在这项工作中，我们表明，尽管这些模型具有多功能性，但它们很容易受到我们所说的愚弄大师形象的影响。愚弄的主图像能够最大化CLIP模型对大量广泛变化的提示的置信度得分，同时对人类来说是不可识别的。我们展示了如何通过进化策略或随机梯度下降搜索生成模型的潜在空间来挖掘欺骗主图像。我们研究了挖掘的欺骗主图像的性质，发现在少量图像字幕上训练的图像可能会推广到大量语义相关的字幕。此外，我们评估了两种可能的缓解策略，发现在对比预训练的多模态网络中，容易被愚弄的主示例与模态缺口密切相关。因此，从易受非流形攻击的角度来看，我们主张缓解CLIP和相关多模态方法中的模态缺口。源代码和挖掘的CLIPMasterPrints可在https://github.com/matfrei/CLIPMasterPrints.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03798v1" target="_blank">2307.03798v1</a>
                              </td>
                              <td>CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution</td>
                              <td>Matthias Freiberger</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03798v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03798v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_06573v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_06573v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_06573v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_06573v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The dissemination of hateful memes online has adverse effects on social media platforms and the real world. Detecting hateful memes is challenging, one of the reasons being the evolutionary nature of memes; new hateful memes can emerge by fusing hateful connotations with other cultural ideas or symbols. In this paper, we propose a framework that leverages multimodal contrastive learning models, in particular OpenAI's CLIP, to identify targets of hateful content and systematically investigate the evolution of hateful memes. We find that semantic regularities exist in CLIP-generated embeddings that describe semantic relationships within the same modality (images) or across modalities (images and text). Leveraging this property, we study how hateful memes are created by combining visual elements from multiple images or fusing textual information with a hateful image. We demonstrate the capabilities of our framework for analyzing the evolution of hateful memes by focusing on antisemitic memes, particularly the Happy Merchant meme. Using our framework on a dataset extracted from 4chan, we find 3.3K variants of the Happy Merchant meme, with some linked to specific countries, persons, or organizations. We envision that our framework can be used to aid human moderators by flagging new variants of hateful memes so that moderators can manually verify them and mitigate the problem of hateful content online.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_06573v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>仇恨模因在网上的传播对社交媒体平台和现实世界产生了不利影响。检测仇恨的模因具有挑战性，原因之一是模因的进化性质；新的仇恨模因可以通过将仇恨的内涵与其他文化思想或符号融合而出现。在本文中，我们提出了一个框架，该框架利用多模态对比学习模型，特别是OpenAI的CLIP，来识别仇恨内容的目标，并系统地研究仇恨模因的演变。我们发现，在CLIP生成的嵌入中存在语义规律，这些嵌入描述了同一模态（图像）内或模态（图像和文本）之间的语义关系。利用这一特性，我们研究了仇恨模因是如何通过结合多个图像的视觉元素或将文本信息与仇恨图像融合而产生的。我们通过关注反犹太主义模因，特别是快乐商人模因，展示了我们的框架分析仇恨模因演变的能力。在4chan中提取的数据集上使用我们的框架，我们发现了“快乐商人”模因的3.3K个变体，其中一些与特定的国家、个人或组织有关。我们设想，我们的框架可以用来帮助人类版主，标记仇恨模因的新变体，以便版主可以手动验证它们，并缓解在线仇恨内容的问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.06573v2" target="_blank">2212.06573v2</a>
                              </td>
                              <td>On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning</td>
                              <td>Yiting Qu</td>
                              <td>2022-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_06573v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.06573v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03465v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03465v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03465v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03465v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The AllInOne training paradigm squeezes a wide range of tasks into a unified model in a multi-task learning manner. However, optimization in multi-task learning is more challenge than single-task learning, as the gradient norm from different tasks may vary greatly, making the backbone overly biased towards one specific task. To address this issue, we propose the task-level backbone-oriented gradient clip paradigm, compared with the vanilla gradient clip method, it has two points of emphasis:1) gradient clip is performed independently for each task. 2) backbone gradients generated from each task are rescaled to the same norm scale. Based on the experimental results, we argue that the task-level backbone-oriented gradient clip paradigm can relieve the gradient bias problem to some extent. We also propose a novel multi-branch data augmentation strategy where conflict augmentations are placed in different branches. Our approach has been shown to be effective and finally achieve 1st place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023 Foundation Model Challenge. It's worth noting that instead of evaluating all three tasks(detection, segmentation and fine-grained classification) in Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in which our team has a huge advantage.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03465v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>AllInOne训练范式以多任务学习的方式将广泛的任务压缩到一个统一的模型中。然而，多任务学习中的优化比单任务学习更具挑战性，因为不同任务的梯度范数可能差异很大，使主干过于偏向于一个特定任务。为了解决这个问题，我们提出了面向任务级骨干的梯度剪辑范式，与香草梯度剪辑方法相比，它有两个重点：1）梯度剪辑对每个任务独立执行。2） 从每个任务生成的主干梯度被重新缩放到相同的范数尺度。基于实验结果，我们认为面向任务级主干的梯度剪辑范式可以在一定程度上缓解梯度偏差问题。我们还提出了一种新的多分支数据增强策略，其中冲突增强被放置在不同的分支中。我们的方法已被证明是有效的，并最终在CVPR2023基金会模型挑战赛的排行榜A中获得第一名，在排行榜B中获得第二名。值得注意的是，与在排行榜A中评估所有三项任务（检测、分割和细粒度分类）不同，在排行榜B中没有评估分割任务，我们的团队在这方面具有巨大优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03465v1" target="_blank">2307.03465v1</a>
                              </td>
                              <td>TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning</td>
                              <td>Zelun Zhang</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03465v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03465v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_08340v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_08340v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_08340v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_08340v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the emergence of large pre-trained vison-language model like CLIP, transferable representations can be adapted to a wide range of downstream tasks via prompt tuning. Prompt tuning tries to probe the beneficial information for downstream tasks from the general knowledge stored in the pre-trained model. A recently proposed method named Context Optimization (CoOp) introduces a set of learnable vectors as text prompt from the language side. However, tuning the text prompt alone can only adjust the synthesized "classifier", while the computed visual features of the image encoder can not be affected , thus leading to sub-optimal solutions. In this paper, we propose a novel Dual-modality Prompt Tuning (DPT) paradigm through learning text and visual prompts simultaneously. To make the final image feature concentrate more on the target visual concept, a Class-Aware Visual Prompt Tuning (CAVPT) scheme is further proposed in our DPT, where the class-aware visual prompt is generated dynamically by performing the cross attention between text prompts features and image patch token embeddings to encode both the downstream task-related information and visual instance information. Extensive experimental results on 11 datasets demonstrate the effectiveness and generalization ability of the proposed method. Our code is available in https://github.com/fanrena/DPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_08340v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着像CLIP这样的大型预训练vison语言模型的出现，可转移表示可以通过即时调整适应广泛的下游任务。及时调整试图从预先训练的模型中存储的一般知识中探索下游任务的有益信息。最近提出的一种名为上下文优化（CoOp）的方法从语言方面引入了一组可学习向量作为文本提示。然而，单独调整文本提示只能调整合成的“分类器”，而图像编码器的计算视觉特征不会受到影响，从而导致次优解。在本文中，我们通过同时学习文本和视觉提示，提出了一种新的双模态提示调节（DPT）范式。为了使最终的图像特征更加集中于目标视觉概念，我们在DPT中进一步提出了一种类感知视觉提示调整（CAVPT）方案，其中通过执行文本提示特征和图像补丁令牌嵌入之间的交叉关注来对下游任务相关信息和视觉实例信息进行编码，来动态生成类感知视觉提示。在11个数据集上的大量实验结果证明了该方法的有效性和泛化能力。我们的代码位于https://github.com/fanrena/DPT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.08340v4" target="_blank">2208.08340v4</a>
                              </td>
                              <td>Dual Modality Prompt Tuning for Vision-Language Pre-Trained Model</td>
                              <td>Yinghui Xing</td>
                              <td>2022-08-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_08340v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.08340v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10278v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adaptive Strategies in Non-convex Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10278v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10278v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10278v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>An algorithm is said to be adaptive to a certain parameter (of the problem) if it does not need a priori knowledge of such a parameter but performs competitively to those that know it. This dissertation presents our work on adaptive algorithms in following scenarios: 1. In the stochastic optimization setting, we only receive stochastic gradients and the level of noise in evaluating them greatly affects the convergence rate. Tuning is typically required when without prior knowledge of the noise scale in order to achieve the optimal rate. Considering this, we designed and analyzed noise-adaptive algorithms that can automatically ensure (near)-optimal rates under different noise scales without knowing it. 2. In training deep neural networks, the scales of gradient magnitudes in each coordinate can scatter across a very wide range unless normalization techniques, like BatchNorm, are employed. In such situations, algorithms not addressing this problem of gradient scales can behave very poorly. To mitigate this, we formally established the advantage of scale-free algorithms that adapt to the gradient scales and presented its real benefits in empirical experiments. 3. Traditional analyses in non-convex optimization typically rely on the smoothness assumption. Yet, this condition does not capture the properties of some deep learning objective functions, including the ones involving Long Short-Term Memory networks and Transformers. Instead, they satisfy a much more relaxed condition, with potentially unbounded smoothness. Under this condition, we show that a generalized SignSGD algorithm can theoretically match the best-known convergence rates obtained by SGD with gradient clipping but does not need explicit clipping at all, and it can empirically match the performance of Adam and beat others. Moreover, it can also be made to automatically adapt to the unknown relaxed smoothness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10278v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如果一个算法不需要某个参数的先验知识，但其性能与已知参数具有竞争力，则称其对（问题的）某个参数具有自适应性。在随机优化设置中，我们只接收随机梯度，并且评估它们时的噪声水平极大地影响收敛速度。当没有噪声标度的先验知识时，通常需要调谐以实现最佳速率。考虑到这一点，我们设计并分析了噪声自适应算法，该算法可以在不知道的情况下自动确保不同噪声尺度下的（接近）最优速率。在训练深度神经网络时，除非使用BatchNorm等归一化技术，否则每个坐标中的梯度幅度尺度可能会分散在非常宽的范围内。在这种情况下，不解决梯度尺度问题的算法可能表现得很差。为了缓解这种情况，我们正式确立了适用于梯度尺度的无标度算法的优势，并在经验实验中展示了其真正的优势。3.非凸优化中的传统分析通常依赖于光滑性假设。然而，这种条件并没有捕捉到一些深度学习目标函数的性质，包括涉及长短期记忆网络和变形金刚的目标函数。相反，它们满足一个更宽松的条件，具有潜在的无限光滑性。在这种情况下，我们证明了广义SignSGD算法理论上可以将SGD获得的最著名的收敛速度与梯度裁剪相匹配，但根本不需要显式裁剪，并且它可以在经验上匹配Adam的性能并击败其他算法。此外，还可以使其自动适应未知的松弛平滑度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10278v2" target="_blank">2306.10278v2</a>
                              </td>
                              <td>Adaptive Strategies in Non-convex Optimization</td>
                              <td>Zhenxun Zhuang</td>
                              <td>2023-06-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10278v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10278v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03132v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03132v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03132v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03132v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those pairs where the text dominates the remaining visual features -- by first masking out the text and then filtering out those with a low CLIP similarity score of the masked image. Experimentally, T-MARS outperforms the top-ranked method on the "medium scale" of DataComp (a data filtering benchmark) by a margin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic evaluation on various data pool sizes from 2M to 64M shows that the accuracy gains enjoyed by T-MARS linearly increase as data and compute are scaled exponentially. Code is available at https://github.com/locuslab/T-MARS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03132v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型网络多模式数据集为学习通用视觉表示提供了一系列新方法，推动了计算机视觉的发展，并彻底改变了零镜头和少镜头识别。从业者面临的一个关键决定是，如果有的话，如何策划这些越来越大的数据集。例如，LAION-5B数据集的创建者选择只保留CLIP相似性得分超过指定阈值的图像字幕对。在本文中，我们提出了一种新的最先进的数据过滤方法，其动机是我们观察到近40%的LAION图像包含与标题显著重叠的文本。直观地说，这些数据可能是浪费的，因为它激励模型进行光学字符识别，而不是学习视觉特征。然而，天真地删除所有这些数据也可能是浪费，因为它会丢弃包含视觉特征的图像（除了重叠的文本）。我们的简单且可扩展的方法，T-MARS（文本屏蔽和重新评分），只过滤掉文本主导其余视觉特征的配对——首先屏蔽掉文本，然后过滤掉屏蔽图像的CLIP相似性评分较低的配对。在实验上，T-MARS在ImageNet上和VTAB上分别以6.5%和4.7%的优势优于DataComp（数据过滤基准）的“中等规模”排名最高的方法。此外，我们对2M到64M的各种数据池大小的系统评估表明，随着数据和计算的指数缩放，T-MARS所获得的精度增益线性增加。代码位于https://github.com/locuslab/T-MARS.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03132v1" target="_blank">2307.03132v1</a>
                              </td>
                              <td>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</td>
                              <td>Pratyush Maini</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03132v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03132v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02858v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02858v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02858v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02858v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Face presentation attacks, also known as spoofing attacks, pose a significant threat to biometric systems that rely on facial recognition systems, such as access control systems, mobile payments, and identity verification systems. To prevent spoofing, several video-based methods have been presented in the literature that analyze facial motion in successive video frames. However, estimating the motion between adjacent frames is a challenging task and requires high computational cost. In this paper, we reformulate the face anti-spoofing task as a motion prediction problem and introduce a deep ensemble learning model with a frame skipping mechanism. The proposed frame skipping is based on a uniform sampling approach where the original video is divided into fixed size video clips. In this way, every nth frame of the clip is selected to ensure that the temporal patterns can easily be perceived during the training of three different recurrent neural networks (RNNs). Motivated by the performance of each RNNs, a meta-model is developed to improve the overall recognition performance by combining the predictions of the individual RNNs. Extensive experiments were conducted on four datasets, and state-of-the-art performance is reported for MSU-MFSD (3.12\%), Replay-Attack (11.19\%), and OULU-NPU (12.23\%) using half total error rate (HTER) in the most challenging cross-dataset test scenario.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02858v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人脸呈现攻击，也称为欺骗攻击，对依赖人脸识别系统的生物识别系统构成重大威胁，如访问控制系统、移动支付和身份验证系统。为了防止欺骗，文献中已经提出了几种基于视频的方法来分析连续视频帧中的面部运动。然而，估计相邻帧之间的运动是一项具有挑战性的任务，并且需要高计算成本。在本文中，我们将人脸反欺骗任务重新表述为运动预测问题，并引入了一种具有跳帧机制的深度集成学习模型。所提出的跳帧是基于均匀采样方法的，其中原始视频被划分为固定大小的视频片段。以这种方式，选择剪辑的每n帧，以确保在训练三个不同的递归神经网络（RNN）期间可以容易地感知时间模式。受每个RNN性能的激励，开发了一个元模型，通过结合单个RNN的预测来提高整体识别性能。在四个数据集上进行了广泛的实验，并报告了在最具挑战性的跨数据集测试场景中，使用半总错误率（HTER）的MSU-MFSD（3.12\%）、重放攻击（11.19\%）和OULU-NPU（12.23\%）的最先进性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02858v1" target="_blank">2307.02858v1</a>
                              </td>
                              <td>Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing</td>
                              <td>Usman Muhammad</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02858v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02858v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02730v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02730v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02730v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02730v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The fine-grained action analysis of the existing action datasets is challenged by insufficient action categories, low fine granularities, limited modalities, and tasks. In this paper, we propose a Multi-modality and Multi-task dataset of Figure Skating (MMFS) which was collected from the World Figure Skating Championships. MMFS, which possesses action recognition and action quality assessment, captures RGB, skeleton, and is collected the score of actions from 11671 clips with 256 categories including spatial and temporal labels. The key contributions of our dataset fall into three aspects as follows. (1) Independently spatial and temporal categories are first proposed to further explore fine-grained action recognition and quality assessment. (2) MMFS first introduces the skeleton modality for complex fine-grained action quality assessment. (3) Our multi-modality and multi-task dataset encourage more action analysis models. To benchmark our dataset, we adopt RGB-based and skeleton-based baseline methods for action recognition and action quality assessment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02730v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有动作数据集的细粒度动作分析受到动作类别不足、细粒度低、模式和任务有限的挑战。本文提出了一个收集自世界花样滑冰锦标赛的花样滑冰多模式多任务数据集（MMFS）。MMFS具有动作识别和动作质量评估功能，捕获RGB、骨架，并从11671个片段中收集动作得分，包括256个类别，包括空间和时间标签。我们的数据集的主要贡献分为以下三个方面。（1） 首先提出了独立的空间和时间类别，以进一步探索细粒度的动作识别和质量评估。（2） MMFS首先介绍了用于复杂细粒度动作质量评估的骨架模式。（3） 我们的多模态和多任务数据集鼓励建立更多的行动分析模型。为了对我们的数据集进行基准测试，我们采用了基于RGB和基于骨架的基线方法进行动作识别和动作质量评估。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02730v1" target="_blank">2307.02730v1</a>
                              </td>
                              <td>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</td>
                              <td>Sheng-Lan Liu</td>
                              <td>2023-07-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02730v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02730v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02682v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02682v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02682v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02682v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks capture multiple distinct events within the video. Our method effectively discovers diverse significant events within the video, with the resulting captions appropriately describing these events. The empirical results demonstrate that ZeroTA surpasses zero-shot baselines and even outperforms the state-of-the-art few-shot method on the widely-used benchmark ActivityNet Captions. Moreover, our method shows greater robustness compared to supervised methods when evaluated in out-of-domain scenarios. This research provides insight into the potential of aligning widely-used models, such as language generation models and vision-language models, to unlock a new capability: understanding temporal aspects of videos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02682v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>密集视频字幕是一项定位有意义的时刻并为视频生成相关字幕的任务，通常需要一个庞大而昂贵的带注释的视频片段与文本的语料库。为了尽量减少注释成本，我们提出了ZeroTA，一种以零样本方式进行密集视频字幕的新方法。我们的方法不需要任何视频或注释进行训练；相反，它在测试时通过仅对输入进行优化来定位和描述每个输入视频中的事件。这是通过引入表示视频中的时间段的软时刻掩码并使用语言模型的前缀参数对其进行联合优化来实现的。这种联合优化通过最大化生成的文本和视频中的时刻之间的匹配分数，将冻结的语言生成模型（即GPT-2）与冻结的视觉语言对比模型（即CLIP）对齐。我们还引入了成对的时间IoU损失，让一组软时刻掩码捕捉视频中的多个不同事件。我们的方法有效地发现了视频中的各种重要事件，并提供了适当描述这些事件的字幕。实证结果表明，在广泛使用的基准ActivityNet字幕上，ZeroTA超越了零样本基线，甚至优于最先进的少快照方法。此外，当在域外场景中进行评估时，与监督方法相比，我们的方法显示出更大的鲁棒性。这项研究深入了解了调整广泛使用的模型（如语言生成模型和视觉语言模型）的潜力，以释放一种新的能力：理解视频的时间方面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02682v1" target="_blank">2307.02682v1</a>
                              </td>
                              <td>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</td>
                              <td>Yongrae Jo</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02682v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02682v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01630v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01630v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01630v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01630v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Gaze behaviors such as eye-contact or shared attention are important markers for diagnosing developmental disorders in children. While previous studies have looked at some of these elements, the analysis is usually performed on private datasets and is restricted to lab settings. Furthermore, all publicly available gaze target prediction benchmarks mostly contain instances of adults, which makes models trained on them less applicable to scenarios with young children. In this paper, we propose the first study for predicting the gaze target of children and interacting adults. To this end, we introduce the ChildPlay dataset: a curated collection of short video clips featuring children playing and interacting with adults in uncontrolled environments (e.g. kindergarten, therapy centers, preschools etc.), which we annotate with rich gaze information. We further propose a new model for gaze target prediction that is geometrically grounded by explicitly identifying the scene parts in the 3D field of view (3DFoV) of the person, leveraging recent geometry preserving depth inference methods. Our model achieves state of the art results on benchmark datasets and ChildPlay. Furthermore, results show that looking at faces prediction performance on children is much worse than on adults, and can be significantly improved by fine-tuning models using child gaze annotations. Our dataset and models will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01630v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>眼神交流或共同关注等凝视行为是诊断儿童发育障碍的重要标志。虽然之前的研究着眼于其中一些元素，但分析通常在私人数据集上进行，仅限于实验室环境。此外，所有公开可用的凝视目标预测基准大多包含成年人的实例，这使得基于这些实例训练的模型不太适用于有幼儿的场景。在本文中，我们提出了第一个预测儿童和互动成年人凝视目标的研究。为此，我们引入了ChildPlay数据集：这是一个精心策划的短视频剪辑集，以儿童在不受控制的环境中（如幼儿园、治疗中心、幼儿园等）与成年人玩耍和互动为特色，我们用丰富的凝视信息对其进行了注释。我们进一步提出了一种新的凝视目标预测模型，该模型通过利用最近的几何保持深度推理方法，明确识别人的3D视场（3DFoV）中的场景部分，在几何上建立基础。我们的模型在基准数据集和ChildPlay上实现了最先进的结果。此外，研究结果表明，儿童的注视面部预测性能比成年人差得多，并且可以通过使用儿童凝视注释对模型进行微调来显著提高。我们的数据集和模型将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01630v1" target="_blank">2307.01630v1</a>
                              </td>
                              <td>ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour</td>
                              <td>Samy Tafasca</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01630v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01630v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01467v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Technical Report for Ego4D Long Term Action Anticipation Challenge 2023</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01467v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01467v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01467v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this report, we describe the technical details of our approach for the Ego4D Long-Term Action Anticipation Challenge 2023. The aim of this task is to predict a sequence of future actions that will take place at an arbitrary time or later, given an input video. To accomplish this task, we introduce three improvements to the baseline model, which consists of an encoder that generates clip-level features from the video, an aggregator that integrates multiple clip-level features, and a decoder that outputs Z future actions. 1) Model ensemble of SlowFast and SlowFast-CLIP; 2) Label smoothing to relax order constraints for future actions; 3) Constraining the prediction of the action class (verb, noun) based on word co-occurrence. Our method outperformed the baseline performance and recorded as second place solution on the public leaderboard.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01467v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本报告中，我们描述了我们应对2023年Ego4D长期行动预期挑战的方法的技术细节。这项任务的目的是在给定输入视频的情况下，预测在任意时间或以后发生的一系列未来动作。为了完成这项任务，我们对基线模型进行了三项改进，该模型包括一个从视频中生成剪辑级特征的编码器、一个集成多个剪辑级功能的聚合器和一个输出Z个未来动作的解码器。1） 慢速和慢速CLIP模型集成；2） 标签平滑，以放松对未来操作的顺序约束；3） 基于单词共现限制动作类（动词、名词）的预测。我们的方法优于基线性能，并在公共排行榜上排名第二。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01467v1" target="_blank">2307.01467v1</a>
                              </td>
                              <td>Technical Report for Ego4D Long Term Action Anticipation Challenge 2023</td>
                              <td>Tatsuya Ishibashi</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01467v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01467v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01430v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Continual Learning in Open-vocabulary Classification with Complementary Memory Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01430v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01430v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01430v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a method for flexible continual learning in open-vocabulary image classification, drawing inspiration from the complementary learning systems observed in human cognition. We propose a "tree probe" method, an adaption of lazy learning principles, which enables fast learning from new examples with competitive accuracy to batch-trained linear models. Further, we propose a method to combine predictions from a CLIP zero-shot model and the exemplar-based model, using the zero-shot estimated probability that a sample's class is within any of the exemplar classes. We test in data incremental, class incremental, and task incremental settings, as well as ability to perform flexible inference on varying subsets of zero-shot and learned categories. Our proposed method achieves a good balance of learning speed, target task effectiveness, and zero-shot effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01430v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从人类认知中观察到的互补学习系统中汲取灵感，在开放式词汇图像分类中引入了一种灵活的连续学习方法。我们提出了一种“树探针”方法，这是对懒惰学习原理的适应，它能够从具有竞争准确性的新示例快速学习到批量训练的线性模型。此外，我们提出了一种结合CLIP零样本模型和基于样本的模型的预测的方法，使用样本类在任何样本类中的零样本估计概率。我们测试了数据增量、类增量和任务增量设置，以及对零样本和学习类别的不同子集执行灵活推断的能力。我们提出的方法在学习速度、目标任务效率和零样本效率之间取得了很好的平衡。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01430v1" target="_blank">2307.01430v1</a>
                              </td>
                              <td>Continual Learning in Open-vocabulary Classification with Complementary Memory Systems</td>
                              <td>Zhen Zhu</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01430v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01430v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05136v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05136v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05136v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05136v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) -- commonly formulated as a multiple-instance learning problem in a weakly-supervised manner due to its labor-intensive nature -- is a challenging problem in video surveillance where the frames of anomaly need to be localized in an untrimmed video. In this paper, we first propose to utilize the ViT-encoded visual features from CLIP, in contrast with the conventional C3D or I3D features in the domain, to efficiently extract discriminative representations in the novel technique. We then model temporal dependencies and nominate the snippets of interest by leveraging our proposed Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA outperforms the existing state-of-the-art (SOTA) methods by a large margin on three commonly-used benchmark datasets in the VAD problem (UCF-Crime, ShanghaiTech Campus, and XD-Violence). Our source code is available at https://github.com/joos2010kj/CLIP-TSA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05136v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）——由于其劳动密集的性质，通常被表述为一个弱监督的多实例学习问题——是视频监控中的一个具有挑战性的问题，其中异常帧需要定位在未修剪的视频中。在本文中，我们首先提出利用来自CLIP的ViT编码的视觉特征，与该领域中的传统C3D或I3D特征相比，在新技术中有效地提取判别表示。然后，我们对时间依赖性进行建模，并通过利用我们提出的时间自注意（TSA）来指定感兴趣的片段。消融研究证实了TSA和ViT特征的有效性。广泛的实验表明，在VAD问题中，我们提出的CLIP-TSA在三个常用的基准数据集（UCF犯罪、上海科技园区和XD暴力）上以很大的优势优于现有的最先进的（SOTA）方法。我们的源代码可在https://github.com/joos2010kj/CLIP-TSA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05136v3" target="_blank">2212.05136v3</a>
                              </td>
                              <td>CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection</td>
                              <td>Hyekang Kevin Joo</td>
                              <td>2022-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05136v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05136v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00862v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00862v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00862v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00862v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model's reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since Contrastive Language-Image Pre-training (CLIP) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong zero-shot ability by converting vision-language tasks into an image-text matching problem, and they mainly consider global-level matching (e.g., the whole image or sentence). However, we find visual and textual fine-grained information, e.g., keywords in the sentence and objects in the image, can be fairly informative for semantics understanding. Inspired by this, we propose a unified framework to take advantage of the fine-grained information for zero-shot vision-language learning, covering multiple tasks such as VQA, SNLI-VE, and VCR. Our experiments show that our framework outperforms former zero-shot methods on VQA and achieves substantial improvement on SNLI-VE and VCR. Furthermore, our ablation studies confirm the effectiveness and generalizability of our proposed method. Code will be available at https://github.com/ThreeSR/UniFine</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00862v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言任务，如VQA、SNLI-VE和VCR，具有挑战性，因为它们需要模型的推理能力来理解视觉世界和自然语言的语义。视觉语言任务的监督方法已经得到了很好的研究。然而，在零样本环境下解决这些任务的探索较少。由于对比语言图像预处理（CLIP）在图像文本匹配方面表现出了显著的零样本性能，以前的工作利用其强大的零样本能力，将视觉语言任务转换为图像文本匹配问题，并且主要考虑全局级匹配（例如，整个图像或句子）。然而，我们发现视觉和文本的细粒度信息，例如句子中的关键词和图像中的对象，对于语义理解来说是相当有信息量的。受此启发，我们提出了一个统一的框架，以利用细粒度信息进行零样本视觉语言学习，涵盖VQA、SNLI-VE和VCR等多个任务。我们的实验表明，我们的框架在VQA方面优于以前的零样本方法，并且在SNLI-VE和VCR方面取得了实质性的改进。此外，我们的消融研究证实了我们提出的方法的有效性和可推广性。代码将在https://github.com/ThreeSR/UniFine</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00862v1" target="_blank">2307.00862v1</a>
                              </td>
                              <td>UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding</td>
                              <td>Rui Sun</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00862v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00862v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16555v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CVB: A Video Dataset of Cattle Visual Behaviors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16555v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16555v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16555v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing image/video datasets for cattle behavior recognition are mostly small, lack well-defined labels, or are collected in unrealistic controlled environments. This limits the utility of machine learning (ML) models learned from them. Therefore, we introduce a new dataset, called Cattle Visual Behaviors (CVB), that consists of 502 video clips, each fifteen seconds long, captured in natural lighting conditions, and annotated with eleven visually perceptible behaviors of grazing cattle. We use the Computer Vision Annotation Tool (CVAT) to collect our annotations. To make the procedure more efficient, we perform an initial detection and tracking of cattle in the videos using appropriate pre-trained models. The results are corrected by domain experts along with cattle behavior labeling in CVAT. The pre-hoc detection and tracking step significantly reduces the manual annotation time and effort. Moreover, we convert CVB to the atomic visual action (AVA) format and train and evaluate the popular SlowFast action recognition model on it. The associated preliminary results confirm that we can localize the cattle and recognize their frequently occurring behaviors with confidence. By creating and sharing CVB, our aim is to develop improved models capable of recognizing all important behaviors accurately and to assist other researchers and practitioners in developing and evaluating new ML models for cattle behavior classification using video data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16555v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于牛行为识别的现有图像/视频数据集大多很小，缺乏明确定义的标签，或者是在不切实际的受控环境中收集的。这限制了从中学习的机器学习（ML）模型的效用。因此，我们引入了一个新的数据集，称为牛视觉行为（CVB），该数据集由502个视频片段组成，每个视频片段长15秒，在自然光照条件下拍摄，并用11种放牧牛的视觉可感知行为进行注释。我们使用计算机视觉注释工具（CVAT）来收集我们的注释。为了提高程序的效率，我们使用适当的预训练模型对视频中的牛进行了初步检测和跟踪。该结果与CVAT中的牛行为标记一起由领域专家进行了更正。预组织检测和跟踪步骤显著减少了手动注释的时间和精力。此外，我们将CVB转换为原子视觉动作（AVA）格式，并在其上训练和评估流行的SlowFast动作识别模型。相关的初步结果证实，我们可以对牛进行定位，并自信地识别它们的频繁行为。通过创建和共享CVB，我们的目标是开发能够准确识别所有重要行为的改进模型，并帮助其他研究人员和从业者开发和评估使用视频数据进行牛行为分类的新ML模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16555v2" target="_blank">2305.16555v2</a>
                              </td>
                              <td>CVB: A Video Dataset of Cattle Visual Behaviors</td>
                              <td>Ali Zia</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16555v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16555v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15957v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15957v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15957v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15957v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large pre-trained models have had a significant impact on computer vision by enabling multi-modal learning, where the CLIP model has achieved impressive results in image classification, object detection, and semantic segmentation. However, the model's performance on 3D point cloud processing tasks is limited due to the domain gap between depth maps from 3D projection and training images of CLIP. This paper proposes DiffCLIP, a new pre-training framework that incorporates stable diffusion with ControlNet to minimize the domain gap in the visual branch. Additionally, a style-prompt generation module is introduced for few-shot tasks in the textual branch. Extensive experiments on the ModelNet10, ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities for 3D understanding. By using stable diffusion and style-prompt generation, DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ\_BG of ScanObjectNN, which is state-of-the-art performance, and an accuracy of 80.6\% for zero-shot classification on ModelNet10, which is comparable to state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15957v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型预训练模型通过实现多模态学习对计算机视觉产生了重大影响，其中CLIP模型在图像分类、对象检测和语义分割方面取得了令人印象深刻的结果。然而，由于来自3D投影的深度图和CLIP的训练图像之间的域间隙，该模型在3D点云处理任务上的性能受到限制。本文提出了DiffCLIP，这是一种新的预训练框架，它将稳定扩散与ControlNet相结合，以最小化视觉分支中的域间隙。此外，还为文本分支中的少数镜头任务引入了样式提示生成模块。在ModelNet10、ModelNet40和ScanObjectNN数据集上进行的大量实验表明，DiffCLIP具有较强的三维理解能力。DiffCLIP通过使用稳定的扩散和样式提示生成，在ScanObjectNN的OBJ_BG上实现了43.2%的零样本分类精度，这是最先进的性能，在ModelNet10上实现了80.6%的零样本分类精度，与最新的性能相当。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15957v2" target="_blank">2305.15957v2</a>
                              </td>
                              <td>DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification</td>
                              <td>Sitian Shen</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15957v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15957v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00586v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00586v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00586v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00586v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Situation Recognition is the task of generating a structured summary of what is happening in an image using an activity verb and the semantic roles played by actors and objects. In this task, the same activity verb can describe a diverse set of situations as well as the same actor or object category can play a diverse set of semantic roles depending on the situation depicted in the image. Hence model needs to understand the context of the image and the visual-linguistic meaning of semantic roles. Therefore, we leverage the CLIP foundational model that has learned the context of images via language descriptions. We show that deeper-and-wider multi-layer perceptron (MLP) blocks obtain noteworthy results for the situation recognition task by using CLIP image and text embedding features and it even outperforms the state-of-the-art CoFormer, a Transformer-based model, thanks to the external implicit visual-linguistic knowledge encapsulated by CLIP and the expressive power of modern MLP block designs. Motivated by this, we design a cross-attention-based Transformer using CLIP visual tokens that model the relation between textual roles and visual entities. Our cross-attention-based Transformer known as ClipSitu XTF outperforms existing state-of-the-art by a large margin of 14.1% on semantic role labelling (value) for top-1 accuracy using imSitu dataset. We will make the code publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00586v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>情境识别的任务是使用活动动词以及参与者和对象所扮演的语义角色，生成图像中发生的事情的结构化摘要。在这项任务中，相同的活动动词可以描述一组不同的情况，并且根据图像中描述的情况，相同的行动者或对象类别可以扮演一系列不同的语义角色。因此，模型需要理解图像的上下文和语义角色的视觉语言意义。因此，我们利用了CLIP基础模型，该模型通过语言描述学习了图像的上下文。我们表明，通过使用CLIP图像和文本嵌入特征，更深和更宽的多层感知器（MLP）块在情况识别任务中获得了值得注意的结果，并且由于CLIP封装的外部隐含视觉语言知识和现代MLP块设计的表现力，它甚至优于最先进的基于Transformer的模型CoFormer。受此启发，我们使用CLIP视觉标记设计了一个基于交叉注意力的Transformer，该标记为文本角色和视觉实体之间的关系建模。我们的基于交叉注意力的Transformer ClipStum XTF在使用imstum数据集的前1准确性的语义角色标记（值）方面以14.1%的大幅度优于现有技术。我们将公开代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00586v1" target="_blank">2307.00586v1</a>
                              </td>
                              <td>ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition</td>
                              <td>Debaditya Roy</td>
                              <td>2023-07-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00586v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00586v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08832v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08832v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08832v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08832v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current Vision and Language Models (VLMs) demonstrate strong performance across various vision-language tasks, yet they struggle with fine-grained understanding. This issue stems from weak image-caption alignment in pretraining datasets and a simplified contrastive objective that fails to distinguish nuanced grounding elements such as relations, actions, and attributes. As a result, the models tend to learn bag-of-words representations. To mitigate these challenges, we introduce an intra-modal contrastive loss and a unique cross-modal rank loss with an adaptive threshold that serves as curriculum learning, utilizing our automatically generated hard negatives to augment the model's capacity. Our strategy, which does not necessitate additional annotations or parameters, can be incorporated into any VLM trained with an image-text contrastive loss. Upon application to CLIP, our method leads to significant improvements on four fine-grained benchmarks, and it also enhances the performance of X-VLM, which is the state-of-art moodel on fine-grained reasoning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08832v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的视觉和语言模型（VLM）在各种视觉语言任务中表现出强大的性能，但它们在细粒度理解方面存在困难。这个问题源于预训练数据集中的图像字幕对齐较弱，以及未能区分关系、动作和属性等细微基础元素的简化对比目标。因此，模型倾向于学习单词袋表示。为了缓解这些挑战，我们引入了一种模态内对比损失和一种具有自适应阈值的独特跨模态秩损失，作为课程学习，利用我们自动生成的硬否定来增强模型的能力。我们的策略不需要额外的注释或参数，可以结合到任何经过图像-文本对比损失训练的VLM中。在应用于CLIP后，我们的方法在四个细粒度基准测试上取得了显著的改进，并提高了X-VLM的性能，这是细粒度推理的最新进展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08832v2" target="_blank">2306.08832v2</a>
                              </td>
                              <td>Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding</td>
                              <td>Le Zhang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08832v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08832v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00398v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00398v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00398v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00398v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00398v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的大规模视觉语言模型（VLM）成功地找到了图像和文本之间的对应关系。通过标准的确定性映射过程，图像或文本样本被映射到嵌入空间中的单个向量。这是有问题的：由于多个样本（图像或文本）可以在物理世界中抽象出相同的概念，确定性嵌入并不能反映嵌入空间中固有的模糊性。我们提出了ProbVLM，这是一种概率适配器，它以事后的方式通过模态间/模态内对齐来估计预训练的VLM嵌入的概率分布，而不需要大规模的数据集或计算。在四个具有挑战性的数据集上，即COCO、Flickr、CUB和Oxford flowers，我们估计了两个VLM（即CLIP和BLIP）的多模态嵌入不确定性，量化了检索任务中嵌入不确定性的校准，并表明ProbVLM优于其他方法。此外，我们提出主动学习和模型选择作为VLM的两个现实世界下游任务，并表明估计的不确定性有助于这两个任务。最后，我们提出了一种使用大规模预训练的潜在扩散模型来可视化嵌入分布的新技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00398v1" target="_blank">2307.00398v1</a>
                              </td>
                              <td>ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models</td>
                              <td>Uddeshya Upadhyay</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00398v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00356v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00356v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00356v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00356v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Federated learning (FL) enables multiple clients to collaboratively train deep learning models while considering sensitive local datasets' privacy. However, adversaries can manipulate datasets and upload models by injecting triggers for federated backdoor attacks (FBA). Existing defense strategies against FBA consider specific and limited attacker models, and a sufficient amount of noise to be injected only mitigates rather than eliminates FBA. To address these deficiencies, we introduce a Flexible Federated Backdoor Defense Framework (Fedward) to ensure the elimination of adversarial backdoors. We decompose FBA into various attacks, and design amplified magnitude sparsification (AmGrad) and adaptive OPTICS clustering (AutoOPTICS) to address each attack. Meanwhile, Fedward uses the adaptive clipping method by regarding the number of samples in the benign group as constraints on the boundary. This ensures that Fedward can maintain the performance for the Non-IID scenario. We conduct experimental evaluations over three benchmark datasets and thoroughly compare them to state-of-the-art studies. The results demonstrate the promising defense performance from Fedward, moderately improved by 33% $\sim$ 75 in clustering defense methods, and 96.98%, 90.74%, and 89.8% for Non-IID to the utmost extent for the average FBA success rate over MNIST, FMNIST, and CIFAR10, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00356v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>联合学习（FL）使多个客户端能够协同训练深度学习模型，同时考虑敏感的本地数据集的隐私。然而，对手可以通过注入联邦后门攻击（FBA）的触发器来操纵数据集和上传模型。针对FBA的现有防御策略考虑了特定和有限的攻击者模型，并且注入足够数量的噪声只能缓解而不是消除FBA。为了解决这些不足，我们引入了一个灵活的联邦后门防御框架（Fedward），以确保消除对抗性后门。我们将FBA分解为各种攻击，并设计放大幅度稀疏化（AmGrad）和自适应光学聚类（AutoOPTICS）来应对每种攻击。同时，Fedward使用自适应剪裁方法，将良性组中的样本数量视为边界上的约束。这确保了Fedward能够保持非IID场景的性能。我们对三个基准数据集进行了实验评估，并将其与最先进的研究进行了彻底比较。结果表明，与MNIST、FMNIST和CIFAR10相比，Fedward的防御性能很有希望，聚类防御方法适度提高了33%$\sim$75，非IID的防御性能分别提高了96.98%、90.74%和89.8%，达到了FBA平均成功率的最大程度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00356v1" target="_blank">2307.00356v1</a>
                              </td>
                              <td>Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data</td>
                              <td>Zekai Chen</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00356v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00356v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05538v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05538v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05538v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05538v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way for image classifiers to reach high accuracy is to first zoom to the most discriminative region in the image and then extract features from there to predict image labels, discarding the rest of the image. Studying six popular networks ranging from AlexNet to CLIP, we find that proper framing of the input image can lead to the correct classification of 98.91% of ImageNet images. Furthermore, we uncover positional biases in various datasets, especially a strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA) technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations before making predictions. Our method is more interpretable, accurate, and faster than MEMO, a state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark that challenges SOTA classifiers including large vision-language models even when optimal zooming is allowed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05538v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据设计，图像分类器是信息丢弃机器。然而，这些模型如何丢弃信息仍然是个谜。我们假设图像分类器达到高精度的一种方法是首先缩放到图像中最具鉴别力的区域，然后从中提取特征来预测图像标签，丢弃图像的其余部分。研究了从AlexNet到CLIP的六个流行网络，我们发现正确的输入图像框架可以导致98.91%的ImageNet图像的正确分类。此外，我们发现了各种数据集中的位置偏差，特别是两个流行数据集中的强中心偏差：ImageNet-a和ObjectNet。最后，利用我们对缩放潜力的见解，我们提出了一种测试时间增强（TTA）技术，该技术通过强制模型在进行预测之前显式执行放大操作来提高分类精度。我们的方法比MEMO（一种最先进的（SOTA）TTA方法）更具可解释性、准确性和速度。我们介绍了ImageNet Hard，这是一种新的基准测试，即使在允许最佳缩放的情况下，它也会挑战包括大型视觉语言模型在内的SOTA分类器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05538v3" target="_blank">2304.05538v3</a>
                              </td>
                              <td>ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification</td>
                              <td>Mohammad Reza Taesiri</td>
                              <td>2023-04-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05538v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05538v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_11736v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_11736v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_11736v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_11736v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_11736v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，在学习遵循自然语言指令的机器人操纵策略方面取得了很大进展。这种方法通常从机器人语言数据的语料库中学习，这些数据要么是在考虑到特定任务的情况下收集的，要么是由人类事后用丰富的语言描述昂贵地重新标记的。最近，像CLIP或ViLD这样的大规模预训练视觉语言模型（VLM）已经被应用于机器人，用于学习表示和场景描述符。这些经过预训练的模型能否作为机器人数据的自动标注器，有效地将互联网规模的知识导入现有数据集，使其即使在其基本事实注释中没有反映的任务中也有用？为了实现这一点，我们引入了用于语言条件控制的数据驱动指令增强（DIAL）：我们利用半监督语言标签，利用CLIP的语义理解，将知识传播到未标记的演示数据的大型数据集上，然后在增强的数据集上训练语言条件策略。与昂贵的人工标签相比，这种方法能够更便宜地获取有用的语言描述，从而能够更有效地覆盖大规模数据集的标签。我们将DIAL应用于一个具有挑战性的现实世界机器人操作领域，其中80000个演示中96.5%不包含众包语言注释。DIAL使模仿学习策略能够获得新的能力，并推广到原始数据集中看不到的60条新指令。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.11736v3" target="_blank">2211.11736v3</a>
                              </td>
                              <td>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</td>
                              <td>Ted Xiao</td>
                              <td>2022-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_11736v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.11736v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_03008v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Algorithms for bounding contribution for histogram estimation under user-level privacy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_03008v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_03008v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_03008v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the problem of histogram estimation under user-level differential privacy, where the goal is to preserve the privacy of all entries of any single user. We consider the heterogeneous scenario where the quantity of data can be different for each user. In this scenario, the amount of noise injected into the histogram to obtain differential privacy is proportional to the maximum user contribution, which can be amplified by few outliers. One approach to circumvent this would be to bound (or limit) the contribution of each user to the histogram. However, if users are limited to small contributions, a significant amount of data will be discarded. In this work, we propose algorithms to choose the best user contribution bound for histogram estimation under both bounded and unbounded domain settings. When the size of the domain is bounded, we propose a user contribution bounding strategy that almost achieves a two-approximation with respect to the best contribution bound in hindsight. For unbounded domain histogram estimation, we propose an algorithm that is logarithmic-approximation with respect to the best contribution bound in hindsight. This result holds without any distribution assumptions on the data. Experiments on both real and synthetic datasets verify our theoretical findings and demonstrate the effectiveness of our algorithms. We also show that clipping bias introduced by bounding user contribution may be reduced under mild distribution assumptions, which can be of independent interest.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_03008v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了用户级差分隐私下的直方图估计问题，其中的目标是保护任何单个用户的所有条目的隐私。我们考虑异构场景，其中每个用户的数据量可能不同。在这种情况下，为获得差分隐私而注入直方图的噪声量与最大用户贡献成比例，该贡献可以被少数异常值放大。避免这种情况的一种方法是限制（或限制）每个用户对直方图的贡献。然而，如果用户仅限于少量贡献，大量数据将被丢弃。在这项工作中，我们提出了在有界和无界域设置下为直方图估计选择最佳用户贡献界的算法。当域的大小有界时，我们提出了一种用户贡献边界策略，该策略在事后看来几乎实现了关于最佳贡献边界的二次近似。对于无界域直方图估计，我们提出了一种关于事后最佳贡献界的对数近似算法。这一结果在没有任何数据分布假设的情况下成立。在真实数据集和合成数据集上的实验验证了我们的理论发现，并证明了我们算法的有效性。我们还表明，在温和的分布假设下，由边界用户贡献引入的剪裁偏差可以减少，这可能是独立的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.03008v2" target="_blank">2206.03008v2</a>
                              </td>
                              <td>Algorithms for bounding contribution for histogram estimation under user-level privacy</td>
                              <td>Yuhan Liu</td>
                              <td>2022-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_03008v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.03008v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17659v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-shot Nuclei Detection via Visual-Language Pre-trained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17659v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17659v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17659v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual-language pre-trained models (VLPM) have proven their excellent performance in downstream object detection for natural scenes. However, zero-shot nuclei detection on H\&E images via VLPMs remains underexplored. The large gap between medical images and the web-originated text-image pairs used for pre-training makes it a challenging task. In this paper, we attempt to explore the potential of the object-level VLPM, Grounded Language-Image Pre-training (GLIP) model, for zero-shot nuclei detection. Concretely, an automatic prompts design pipeline is devised based on the association binding trait of VLPM and the image-to-text VLPM BLIP, avoiding empirical manual prompts engineering. We further establish a self-training framework, using the automatically designed prompts to generate the preliminary results as pseudo labels from GLIP and refine the predicted boxes in an iterative manner. Our method achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. Foremost, our work demonstrates that the VLPM pre-trained on natural image-text pairs exhibits astonishing potential for downstream tasks in the medical field as well. Code will be released at https://github.com/wuyongjianCODE/VLPMNuD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17659v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉语言预训练模型（VLPM）已证明其在自然场景的下游对象检测中具有优异的性能。然而，通过VLPM在H\&E图像上进行零样本核检测的研究还不够深入。医学图像和用于预训练的源自网络的文本图像对之间的巨大差距使其成为一项具有挑战性的任务。在本文中，我们尝试探索对象级VLPM，即基于语言的图像预训练（GLIP）模型在零样本核检测中的潜力。具体地，基于VLPM的关联绑定特性和图像到文本的VLPM-BLIP，设计了一个自动提示设计管道，避免了经验手动提示工程。我们进一步建立了一个自训练框架，使用自动设计的提示从GLIP中生成作为伪标签的初步结果，并以迭代的方式细化预测框。我们的方法在无标记细胞核检测方面取得了显著的性能，超过了其他比较方法。最重要的是，我们的工作表明，在自然图像-文本对上预训练的VLPM在医学领域的下游任务中也表现出惊人的潜力。代码将于发布https://github.com/wuyongjianCODE/VLPMNuD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17659v1" target="_blank">2306.17659v1</a>
                              </td>
                              <td>Zero-shot Nuclei Detection via Visual-Language Pre-trained Models</td>
                              <td>Yongjian Wu</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17659v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17659v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16934v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DreamDiffusion: Generating High-Quality Images from Brain EEG Signals</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16934v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16934v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16934v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pre-trained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs. Overall, the proposed method overcomes the challenges of using EEG signals for image generation, such as noise, limited information, and individual differences, and achieves promising results. Quantitative and qualitative results demonstrate the effectiveness of the proposed method as a significant step towards portable and low-cost ``thoughts-to-image'', with potential applications in neuroscience and computer vision. The code is available here \url{https://github.com/bbaaii/DreamDiffusion}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16934v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了DreamDiffusion，这是一种直接从脑电信号中生成高质量图像的新方法，无需将思想转化为文本。DreamDiffusion利用预先训练的文本到图像模型，并采用时间掩蔽信号建模来预先训练EEG编码器，以获得有效和稳健的EEG表示。此外，该方法还利用CLIP图像编码器提供额外的监督，以更好地将EEG、文本和图像嵌入与有限的EEG图像对对准。总体而言，所提出的方法克服了使用EEG信号进行图像生成的挑战，如噪声、有限信息和个体差异，并取得了有希望的结果。定量和定性结果证明了所提出的方法的有效性，这是朝着便携和低成本的“图像思维”迈出的重要一步，在神经科学和计算机视觉中具有潜在的应用。代码在此处可用\url{https://github.com/bbaaii/DreamDiffusion}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16934v2" target="_blank">2306.16934v2</a>
                              </td>
                              <td>DreamDiffusion: Generating High-Quality Images from Brain EEG Signals</td>
                              <td>Yunpeng Bai</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16934v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16934v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17455v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Clipping noise cancellation receiver for the downlink of massive MIMO OFDM system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17455v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17455v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17455v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Massive multiple-input multiple-output (mMIMO) technology is considered a key enabler for the 5G and future wireless networks. In most wireless communication systems, mMIMO is employed together with orthogonal frequency-division multiplexing (OFDM) which exhibits a high peak-to-average-power ratio (PAPR). While passing the OFDM signal through one of the common RF front-ends of limited linearity, significant distortion of the transmitted signal can be expected. In mMIMO systems, this problem is still relevant as in some channels the distortion component is beamformed in the same directions as the desired signal. In this work, we propose a multi-antenna clipping noise cancellation (MCNC) algorithm for the downlink of the mMIMO OFDM system. Computer simulations show it can remove nonlinear distortion even under severe nonlinearity. Next, a simplified version of the algorithm is proposed. It was observed that for the direct visibility channels, its performance is only slightly degraded with respect to the MCNC algorithm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17455v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模多输入多输出（mMIMO）技术被认为是5G和未来无线网络的关键推动者。在大多数无线通信系统中，mMIMO与正交频分复用（OFDM）一起使用，其表现出高的峰均功率比（PAPR）。当使OFDM信号通过有限线性的公共RF前端之一时，可以预期发射信号的显著失真。在mMIMO系统中，这个问题仍然是相关的，因为在一些信道中，失真分量是在与期望信号相同的方向上波束形成的。在这项工作中，我们提出了一种用于mMIMO OFDM系统下行链路的多天线削波噪声消除（MCNC）算法。计算机仿真表明，即使在严重的非线性情况下，它也能消除非线性失真。接下来，提出了该算法的简化版本。据观察，对于直接可见性信道，其性能相对于MCNC算法仅略有下降。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17455v1" target="_blank">2306.17455v1</a>
                              </td>
                              <td>Clipping noise cancellation receiver for the downlink of massive MIMO OFDM system</td>
                              <td>Marcin Wachowiak</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17455v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17455v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_09671v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_09671v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_09671v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_09671v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep image classification models trained on vast amounts of web-scraped data are susceptible to data poisoning - a mechanism for backdooring models. A small number of poisoned samples seen during training can severely undermine a model's integrity during inference. Existing work considers an effective defense as one that either (i) restores a model's integrity through repair or (ii) detects an attack. We argue that this approach overlooks a crucial trade-off: Attackers can increase robustness at the expense of detectability (over-poisoning) or decrease detectability at the cost of robustness (under-poisoning). In practice, attacks should remain both undetectable and robust. Detectable but robust attacks draw human attention and rigorous model evaluation or cause the model to be re-trained or discarded. In contrast, attacks that are undetectable but lack robustness can be repaired with minimal impact on model accuracy. Our research points to intrinsic flaws in current attack evaluation methods and raises the bar for all data poisoning attackers who must delicately balance this trade-off to remain robust and undetectable. To demonstrate the existence of more potent defenders, we propose defenses designed to (i) detect or (ii) repair poisoned models using a limited amount of trusted image-label pairs. Our results show that an attacker who needs to be robust and undetectable is substantially less threatening. Our defenses mitigate all tested attacks with a maximum accuracy decline of 2% using only 1% of clean data on CIFAR-10 and 2.5% on ImageNet. We demonstrate the scalability of our defenses by evaluating large vision-language models, such as CLIP. Attackers who can manipulate the model's parameters pose an elevated risk as they can achieve higher robustness at low detectability compared to data poisoning attackers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_09671v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在大量网络抓取数据上训练的深度图像分类模型容易受到数据中毒的影响——这是一种后门模型的机制。在训练过程中看到的少量中毒样本可能会在推理过程中严重破坏模型的完整性。现有工作将有效防御视为（i）通过修复恢复模型的完整性或（ii）检测到攻击的防御。我们认为，这种方法忽略了一个关键的权衡：攻击者可以以牺牲可检测性为代价（过度中毒）来提高稳健性，或者以牺牲稳健性为代价（中毒不足）来降低可检测性。在实践中，攻击应该保持不可检测和稳健。可检测但稳健的攻击引起了人类的注意和严格的模型评估，或者导致模型被重新训练或丢弃。相反，无法检测但缺乏鲁棒性的攻击可以在对模型准确性影响最小的情况下修复。我们的研究指出了当前攻击评估方法中的内在缺陷，并提高了所有数据中毒攻击者的标准，他们必须谨慎地平衡这种权衡，以保持稳健和不可检测。为了证明更有力的防御者的存在，我们提出了旨在（i）检测或（ii）使用有限数量的可信图像标签对修复中毒模型的防御。我们的研究结果表明，需要强大且不可检测的攻击者的威胁要小得多。我们的防御系统仅使用CIFAR-10上1%的干净数据和ImageNet上2.5%的干净数据，就可以缓解所有测试过的攻击，最大准确率下降2%。我们通过评估大型视觉语言模型（如CLIP）来展示我们防御的可扩展性。能够操纵模型参数的攻击者带来了更高的风险，因为与数据中毒攻击者相比，他们可以在较低的可检测性下实现更高的鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.09671v2" target="_blank">2305.09671v2</a>
                              </td>
                              <td>Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks</td>
                              <td>Nils Lukas</td>
                              <td>2023-05-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_09671v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.09671v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14406v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14406v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14406v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14406v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>When deep neural network has been proposed to assist the dentist in designing the location of dental implant, most of them are targeting simple cases where only one missing tooth is available. As a result, literature works do not work well when there are multiple missing teeth and easily generate false predictions when the teeth are sparsely distributed. In this paper, we are trying to integrate a weak supervision text, the target region, to the implant position regression network, to address above issues. We propose a text condition embedded implant position regression network (TCEIP), to embed the text condition into the encoder-decoder framework for improvement of the regression performance. A cross-modal interaction that consists of cross-modal attention (CMA) and knowledge alignment module (KAM) is proposed to facilitate the interaction between features of images and texts. The CMA module performs a cross-attention between the image feature and the text condition, and the KAM mitigates the knowledge gap between the image feature and the image encoder of the CLIP. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than existing methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14406v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当深度神经网络被提议帮助牙医设计牙齿植入物的位置时，大多数都是针对只有一颗缺失牙齿的简单情况。因此，当存在多个缺失牙齿时，文献工作不能很好地工作，并且当牙齿稀疏分布时，容易产生错误的预测。在本文中，我们试图将一个弱监督文本，目标区域，植入位置回归网络，来解决上述问题。我们提出了一种嵌入文本条件的植入位置回归网络（TCEIP），将文本条件嵌入到编码器-解码器框架中，以提高回归性能。为了促进图像和文本特征之间的交互，提出了一种由跨模态注意力（CMA）和知识对齐模块（KAM）组成的跨模态交互。CMA模块在图像特征和文本条件之间进行交叉关注，KAM缓解了CLIP的图像特征和图像编码器之间的知识差距。通过五次交叉验证在牙科植入物数据集上进行的大量实验表明，所提出的TCEIP比现有方法具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14406v2" target="_blank">2306.14406v2</a>
                              </td>
                              <td>TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</td>
                              <td>Xinquan Yang</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14406v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14406v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16805v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPAG: Towards Generator-Free Text-to-Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16805v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16805v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16805v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation without any generative model, which typically requires huge generators.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16805v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>感知对齐梯度（PAG）是指在鲁棒图像分类模型中观察到的一个有趣的特性，其中它们的输入梯度与人类感知和姿势语义一致。虽然这一现象已经引起了大量的研究关注，但它只是在单模式视觉架构的背景下进行研究的。在这项工作中，我们将PAG的研究扩展到视觉语言架构，这些架构构成了各种图像文本任务和应用程序的基础。通过对CLIP的对抗性鲁棒微调，我们证明了鲁棒视觉语言模型与普通模型相比表现出PAG。这项工作揭示了CLIP与PAG（CLIPAG）在几个视觉语言生成任务中的优点。值得注意的是，我们展示了以“即插即用”方式无缝集成CLIPAG，从而大大改进了视觉语言生成应用程序。此外，利用其PAG特性，CLIPAG能够在没有任何生成模型的情况下实现文本到图像的生成，而生成模型通常需要巨大的生成器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16805v1" target="_blank">2306.16805v1</a>
                              </td>
                              <td>CLIPAG: Towards Generator-Free Text-to-Image Generation</td>
                              <td>Roy Ganz</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16805v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16805v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16741v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16741v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16741v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16741v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downtream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1 for classification, 4.8% Dice for segmentation, and 5.5% F1 for detection) and ST-Adapter (5.9% F1 for classification, 9.6% Dice for segmentation, and 9.9% F1 for detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16741v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型在各种应用中取得了显著的成功，如疾病诊断和文本报告生成。到目前为止，内窥镜视频分析的基础模型仍然缺乏。在本文中，我们提出了Endo FM，这是一个专门使用大量内窥镜视频数据开发的基础模型。首先，我们构建了一个视频转换器，它可以捕获跨空间和时间维度的局部和全局长期依赖关系。其次，我们通过自监督的方式使用全局和局部视图预训练我们的变换器模型，旨在使其对时空变化具有鲁棒性，并在不同场景中具有判别力。为了开发基础模型，我们将中国上海仁济医院宝山分院的9个公开可用的数据集和一个私人收集的数据集相结合，构建了一个大规模的内镜视频数据集。我们的数据集总体上由超过33K个视频片段组成，高达500万帧，包括各种协议、靶器官和疾病类型。我们经过预训练的Endo FM可以作为骨干，通过微调，很容易地用于特定的下游任务。通过对包括分类、分割和检测在内的3种不同类型的下游任务进行实验，我们的Endo FM显著超过了当前最先进的自监督预训练和基于适配器的迁移学习方法，例如VCL（分类用3.1%F1，分割用4.8%骰子，检测用5.5%F1）和ST适配器（分类用5.9%F1，分段用9.6%Dice，检测用9.9%F1）。代码、数据集和模型发布于https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16741v1" target="_blank">2306.16741v1</a>
                              </td>
                              <td>Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</td>
                              <td>Zhao Wang</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16741v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16741v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16533v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16533v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16533v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16533v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \& attributes and actions are joined using correct semantics to form a proper text query. These components (objects \& attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for video retrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that actions and semantics play a minor role compared to objects \& attributes in video understanding. Moreover, video retrieval models that use pre-trained image-text representations (CLIP) have better semantic and compositional understanding as compared to models pre-trained on video-text data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16533v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频检索（VR）涉及从给定文本字幕的视频数据库中检索地面实况视频，反之亦然。复合性的两个重要组成部分：对象\&属性和操作使用正确的语义连接，以形成正确的文本查询。这些组件（对象和属性、动作和语义）在帮助区分视频和检索正确的地面实况视频方面都发挥着重要作用。然而，目前尚不清楚这些组件对视频检索性能的影响。因此，我们进行了一项系统的研究，以评估视频检索模型在标准基准（如MSRVTT、MSVD和DIDEMO）上的组成和语义理解。这项研究是在两类视频检索模型上进行的：（i）在视频-文本对上进行预训练，并在下游视频检索数据集上进行微调（如Frozen in Time、Violet、MCQ等）；（ii）将CLIP等预训练的图像-文本表示用于视频检索（如CLIP4Clip、XCLIP、CLIP2Video等）在视频理解中与对象和属性相比的角色。此外，与在视频文本数据上预先训练的模型相比，使用预先训练的图像文本表示（CLIP）的视频检索模型具有更好的语义和组成理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16533v1" target="_blank">2306.16533v1</a>
                              </td>
                              <td>ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models</td>
                              <td>Avinash Madasu</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16533v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16533v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2112_00845v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Differentially Private SGD via Randomly Sparsified Gradients</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2112_00845v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2112_00845v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2112_00845v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation and utilize it, we propose an efficient and lightweight extension using random sparsification (RS) to strengthen DP-SGD. Experiments with various DP-SGD frameworks show that RS can improve performance. Additionally, the produced sparse gradients of RS exhibit advantages in reducing communication cost and strengthening privacy against reconstruction attacks, which are also key problems in private machine learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2112_00845v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>差分私有随机梯度下降（DP-SGD）已在深度学习中被广泛采用，以提供严格定义的隐私，这需要梯度剪裁来约束单个梯度和加性各向同性高斯噪声的最大范数。通过分析DP-SGD在非凸设置中的收敛速度，我们发现在削波和噪声化之前随机稀疏梯度调整了收敛界的内部分量之间的权衡，并在噪声占主导地位时导致较小的上界。此外，我们的理论分析和经验评估表明，这种权衡并非微不足道，但可能是DP-SGD的一个独特性质，因为消除噪声或梯度削波消除了边界中的权衡。这一观察结果是指示性的，因为它意味着DP-SGD对于（甚至是简单的随机）梯度压缩具有特殊的固有空间。为了验证观察结果并利用它，我们提出了一种使用随机稀疏化（RS）来增强DP-SGD的有效且轻量级的扩展。使用各种DP-SGD框架进行的实验表明，RS可以提高性能。此外，所产生的RS的稀疏梯度在降低通信成本和增强针对重建攻击的隐私方面表现出优势，这也是私有机器学习中的关键问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2112.00845v3" target="_blank">2112.00845v3</a>
                              </td>
                              <td>Improving Differentially Private SGD via Randomly Sparsified Gradients</td>
                              <td>Junyi Zhu</td>
                              <td>2021-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2112_00845v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2112.00845v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_00952v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_00952v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_00952v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_00952v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_00952v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>日常图像可能传达抽象的含义，需要我们记忆并从中推断出深刻的信息。为了鼓励这种类似人类的推理，在这项工作中，我们教机器预测何时何地进行推理，而不是执行传统的分割或分类等基本任务。受Horn的QR理论的启发，我们设计了一个新的QR-CLIP模型，该模型由两个部分组成：1）数量模块首先回顾了更多开放世界知识作为候选语言输入；2） 关联模块仔细估计视觉和语言线索，并推断出位置和时间。实验证明了我们的QR-CLIP的有效性，在位置和时间推理方面，它在每个任务上都比以前的SOTA平均提高了约10%和130%。这项研究为位置和时间推理奠定了技术基础，并表明有效引入开放世界知识是完成任务的灵丹妙药之一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.00952v3" target="_blank">2302.00952v3</a>
                              </td>
                              <td>QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning</td>
                              <td>Weimin Shi</td>
                              <td>2023-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_00952v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.00952v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16048v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16048v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16048v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16048v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to improve their zero-shot capabilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16048v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了在开放环境下，将视觉语言模型（VLM）应用于零样本视觉识别任务的挑战，重点是对比视觉语言模型，如CLIP。我们首先考察了VLM在不同粒度级别的概念上的性能。我们提出了一种在两种实验设置下公平评估性能差异的方法，并发现VLM更善于识别细粒度的概念。此外，我们发现VLM的相似性分数并不能严格反映给定视觉输入的文本输入的正确性。我们提出了一个评估协议来检验我们的假设，即分数可能偏向于更具信息性的描述，而嵌入之间的相似性分数的性质使得VLM很难识别相似但错误的描述之间的正确性。我们的研究强调了在开放环境中使用VLM的挑战，并为未来的研究提供了改进其零样本能力的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16048v1" target="_blank">2306.16048v1</a>
                              </td>
                              <td>Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness</td>
                              <td>Zhenlin Xu</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16048v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08640v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08640v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08640v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08640v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent research on Large Language Models (LLMs) has led to remarkable advancements in general NLP AI assistants. Some studies have further explored the use of LLMs for planning and invoking models or APIs to address more general multi-modal user queries. Despite this progress, complex visual-based tasks still remain challenging due to the diverse nature of visual tasks. This diversity is reflected in two aspects: 1) Reasoning paths. For many real-life applications, it is hard to accurately decompose a query simply by examining the query itself. Planning based on the specific visual content and the results of each step is usually required. 2) Flexible inputs and intermediate results. Input forms could be flexible for in-the-wild cases, and involves not only a single image or video but a mixture of videos and images, e.g., a user-view image with some reference videos. Besides, a complex reasoning process will also generate diverse multimodal intermediate results, e.g., video narrations, segmented video clips, etc. To address such general cases, we propose a multi-modal AI assistant, AssistGPT, with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools. Specifically, the Planner is capable of using natural language to plan which tool in Executor should do next based on the current reasoning progress. Inspector is an efficient memory manager to assist the Planner to feed proper visual information into a specific tool. Finally, since the entire reasoning process is complex and flexible, a Learner is designed to enable the model to autonomously explore and discover the optimal solution. We conducted experiments on A-OKVQA and NExT-QA benchmarks, achieving state-of-the-art results. Moreover, showcases demonstrate the ability of our system to handle questions far more complex than those found in the benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08640v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近对大型语言模型（LLM）的研究在通用NLP人工智能助手方面取得了显著进展。一些研究进一步探讨了使用LLM来规划和调用模型或API，以解决更通用的多模式用户查询。尽管取得了这一进展，但由于视觉任务的多样性，基于视觉的复杂任务仍然具有挑战性。这种多样性体现在两个方面：1）推理路径。对于许多实际应用程序来说，仅仅通过检查查询本身很难准确地分解查询。通常需要根据具体的视觉内容和每个步骤的结果进行规划。2） 灵活的投入和中间结果。输入形式在野外情况下可以是灵活的，并且不仅涉及单个图像或视频，还涉及视频和图像的混合，例如，具有一些参考视频的用户视图图像。此外，复杂的推理过程也会产生不同的多模式中间结果，例如视频叙述、分段视频剪辑等。为了解决这些一般情况，我们提出了一种多模式人工智能助手AssistGPT，它具有一种称为计划、执行、检查和学习（PEIL）的交错代码和语言推理方法，以将LLM与各种工具集成。具体来说，规划师能够使用自然语言，根据当前的推理进度，计划执行器中的哪个工具下一步应该做什么。Inspector是一种高效的内存管理器，可帮助规划器将适当的视觉信息输入到特定的工具中。最后，由于整个推理过程复杂而灵活，因此设计了一个学习者，使模型能够自主探索和发现最优解。我们在A-OKVQA和NExT QA基准上进行了实验，取得了最先进的结果。此外，展示展示了我们的系统处理比基准中发现的问题复杂得多的问题的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08640v2" target="_blank">2306.08640v2</a>
                              </td>
                              <td>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</td>
                              <td>Difei Gao</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08640v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08640v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_03376v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03376v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised object discovery (UOD) refers to the task of discriminating the whole region of objects from the background within a scene without relying on labeled datasets, which benefits the task of bounding-box-level localization and pixel-level segmentation. This task is promising due to its ability to discover objects in a generic manner. We roughly categorise existing techniques into two main directions, namely the generative solutions based on image resynthesis, and the clustering methods based on self-supervised models. We have observed that the former heavily relies on the quality of image reconstruction, while the latter shows limitations in effectively modeling semantic correlations. To directly target at object discovery, we focus on the latter approach and propose a novel solution by incorporating weakly-supervised contrastive learning (WCL) to enhance semantic information exploration. We design a semantic-guided self-supervised learning model to extract high-level semantic features from images, which is achieved by fine-tuning the feature encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we introduce Principal Component Analysis (PCA) to localize object regions. The principal projection direction, corresponding to the maximal eigenvalue, serves as an indicator of the object region(s). Extensive experiments on benchmark unsupervised object discovery datasets demonstrate the effectiveness of our proposed solution. The source code and experimental results are publicly available via our project page at https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03376v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督对象发现（UOD）是指在不依赖标记数据集的情况下，从场景内的背景中区分整个对象区域的任务，这有利于边界框级定位和像素级分割的任务。这项任务很有前途，因为它能够以通用的方式发现对象。我们将现有技术大致分为两个主要方向，即基于图像再合成的生成解决方案和基于自监督模型的聚类方法。我们观察到，前者在很大程度上依赖于图像重建的质量，而后者在有效建模语义相关性方面表现出局限性。为了直接针对对象发现，我们专注于后一种方法，并通过结合弱监督对比学习（WCL）来增强语义信息探索，提出了一种新的解决方案。我们设计了一个语义引导的自监督学习模型来从图像中提取高级语义特征，这是通过WCL微调自监督模型（即DINO）的特征编码器来实现的。随后，我们引入主成分分析（PCA）来定位对象区域。与最大特征值相对应的主投影方向用作对象区域的指示符。在基准无监督对象发现数据集上进行的大量实验证明了我们提出的解决方案的有效性。源代码和实验结果可通过我们的项目页面公开获取，网址为https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03376v1" target="_blank">2307.03376v1</a>
                              </td>
                              <td>Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</td>
                              <td>Yunqiu Lv</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03376v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03376v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08069v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs Beat YOLOs on Real-time Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08069v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08069v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于变压器的端到端检测器（DETR）取得了显著的性能。然而，DETR的高计算成本问题尚未得到有效解决，这限制了它们的实际应用，并使它们无法充分利用无后处理的好处，例如非最大值抑制（NMS）。本文首先分析了现代实时对象检测器中NMS对推理速度的影响，并建立了端到端速度基准。为了避免NMS引起的推理延迟，我们提出了一种实时检测转换器（RT-DETR），这是我们所知的第一个实时端到端对象检测器。具体而言，我们设计了一种高效的混合编码器，通过解耦尺度内交互和跨尺度融合来高效处理多尺度特征，并提出了IoU感知的查询选择，以提高对象查询的初始化。此外，我们提出的检测器支持通过使用不同的解码器层来灵活调整推理速度，而不需要重新训练，这有利于实时对象检测器的实际应用。我们的RT-DETR-L在COCO val2017上实现了53.0%的AP，在T4 GPU上实现了114 FPS，而RT-DETR-X实现了54.8%的AP和74 FPS，在速度和精度方面都优于相同规模的所有YOLO检测器。此外，我们的RT-DETR-R50实现了53.1%的AP和108 FPS，在精度上优于DINO-Deformable-DETR-R5 2.2%的AP和大约21倍的FPS。源代码和预训练模型可在https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08069v2" target="_blank">2304.08069v2</a>
                              </td>
                              <td>DETRs Beat YOLOs on Real-time Object Detection</td>
                              <td>Wenyu Lv</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08069v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08069v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06211v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06211v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06211v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Meta AI Research开发的分段任意模型（SAM）最近引起了人们的极大关注。在超过10亿个掩模的大型分割数据集上训练，SAM能够分割特定图像上的任何对象。在最初的SAM工作中，作者转向零短转移任务（如边缘检测）来评估SAM的性能。最近，许多工作试图研究SAM在各种场景中的性能，以识别和分割对象。此外，通过将SAM与其他模型（如Grounding DINO、Stable Diffusion、ChatGPT等）相结合，已经出现了许多项目来展示SAM作为基础模型的多功能性。随着相关论文和项目呈指数级增长，读者很难跟上SAM的发展。为此，本工作首次对SAM进行了全面的调查。这是一个正在进行的项目，我们打算定期更新手稿。因此，如果读者完成了与SAM相关的新作品，欢迎与我们联系，以便我们将其纳入下一版本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06211v3" target="_blank">2306.06211v3</a>
                              </td>
                              <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06211v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06211v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09165v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09165v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel object detector called DEYOv2, an improved version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate model training and enhance performance. The study delves into the limitations of one-to-one matching in optimization and proposes solutions to effectively address the issue, such as Rank Feature and Greedy Matching. This approach enables the third stage of DEYOv2 to maximize information acquisition from the first and second stages without needing NMS, achieving end-to-end optimization. By combining dense queries, sparse queries, one-to-many matching, and one-to-one matching, DEYOv2 leverages the advantages of each method. It outperforms all existing query-based end-to-end detectors under the same settings. When using ResNet-50 as the backbone and multi-scale features on the COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs, respectively. Compared to the end-to-end model DINO, DEYOv2 provides significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings. To the best of our knowledge, DEYOv2 is the first fully end-to-end object detector that combines the respective strengths of classical detectors and query-based detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09165v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种称为DEYOv2的新型物体检测器，这是第一代DEYO（DETR with YOLO）模型的改进版本。与前代类似，DEYOv2采用渐进式推理方法来加速模型训练并提高性能。该研究深入探讨了一对一匹配在优化中的局限性，并提出了有效解决该问题的解决方案，如秩特征和贪婪匹配。这种方法使DEYOv2的第三阶段能够最大限度地从第一和第二阶段获取信息，而无需NMS，实现端到端优化。通过组合密集查询、稀疏查询、一对多匹配和一对一匹配，DEYOv2充分利用了每种方法的优势。在相同设置下，它的性能优于所有现有的基于查询的端到端检测器。当在COCO数据集上使用ResNet-50作为主干和多尺度特征时，DEYOv2在12个和24个时期分别实现了51.1个AP和51.8个AP。与端到端模型DINO相比，DEYOv2在两个历元设置中提供了2.1 AP和1.4 AP的显著性能提升。据我们所知，DEYOv2是第一个完全端到端的对象检测器，它结合了经典检测器和基于查询的检测器的各自优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09165v2" target="_blank">2306.09165v2</a>
                              </td>
                              <td>DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09165v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09165v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12860v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs with Collaborative Hybrid Assignments Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12860v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervisions on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic Deformable-DETR by 5.8% AP in 12-epoch training and 3.2% AP in 36-epoch training. The state-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 65.6% AP on COCO test-dev, outperforming previous methods with much fewer model sizes. Codes will be available at https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12860v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们观察到，在具有一对一集匹配的DETR中，被分配为正样本的查询太少，导致对编码器输出的稀疏监督，这大大损害了编码器的判别特征学习，反之亦然，影响了解码器中的注意力学习。为了缓解这一问题，我们提出了一种新的协作混合分配训练方案，即Co-DETR，以从通用的标签分配方式中学习更高效、更有效的基于DETR的检测器。这种新的训练方案可以通过训练由一对多标签分配（如ATSS和Faster RCNN）监督的多个并行辅助头，轻松增强编码器在端到端检测器中的学习能力。此外，我们通过从这些辅助头中提取正坐标来进行额外定制的正查询，以提高解码器中正样本的训练效率。在推断中，这些辅助头被丢弃，因此我们的方法没有给原始检测器引入额外的参数和计算成本，同时不需要手工制作的非最大值抑制（NMS）。我们进行了广泛的实验来评估所提出的方法对DETR变体的有效性，包括DAB-DETR、可变形DETR和DINO可变形DETR。具体而言，我们在12个时期的训练中将基本的可变形DETR提高了5.8%AP，在36个时期的培训中提高了3.2%。最先进的带Swin-L的DINO可变形DETR在COCO价值上仍然可以从58.5%提高到59.5%的AP。令人惊讶的是，与ViT-L主干相结合，我们在COCO测试开发上实现了65.6%的AP，优于以前的方法，模型尺寸要小得多。代码将在https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12860v4" target="_blank">2211.12860v4</a>
                              </td>
                              <td>DETRs with Collaborative Hybrid Assignments Training</td>
                              <td>Zhuofan Zong</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12860v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12860v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15472v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taming Detection Transformers for Medical Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15472v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accurate detection of suspicious regions in medical images is an error-prone and time-consuming process required by many routinely performed diagnostic procedures. To support clinicians during this difficult task, several automated solutions were proposed relying on complex methods with many hyperparameters. In this study, we investigate the feasibility of DEtection TRansformer (DETR) models for volumetric medical object detection. In contrast to previous works, these models directly predict a set of objects without relying on the design of anchors or manual heuristics such as non-maximum-suppression to detect objects. We show by conducting extensive experiments with three models, namely DETR, Conditional DETR, and DINO DETR on four data sets (CADA, RibFrac, KiTS19, and LIDC) that these set prediction models can perform on par with or even better than currently existing methods. DINO DETR, the best-performing model in our experiments demonstrates this by outperforming a strong anchor-based one-stage detector, Retina U-Net, on three out of four data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15472v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>医学图像中可疑区域的准确检测是许多常规诊断程序所需的一个容易出错且耗时的过程。为了在这项艰巨的任务中为临床医生提供支持，提出了几种基于具有许多超参数的复杂方法的自动化解决方案。在这项研究中，我们研究了DEtection-TRansformer（DETR）模型用于体积医疗对象检测的可行性。与之前的工作相比，这些模型直接预测一组对象，而不依赖于锚的设计或手动启发式（如非最大值抑制）来检测对象。我们通过在四个数据集（CADA、RibFrac、KiTS19和LIDC）上对三个模型（即DETR、Conditional DETR和DINO DETR）进行广泛的实验表明，这些集合预测模型的性能可以与当前现有的方法相当，甚至更好。DINO DETR，我们实验中性能最好的模型，通过在四分之三的数据集上优于基于强锚的一级检测器Retina U-Net，证明了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15472v1" target="_blank">2306.15472v1</a>
                              </td>
                              <td>Taming Detection Transformers for Medical Object Detection</td>
                              <td>Marc K. Ickler</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15472v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15472v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13723v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social AI and the Challenges of the Human-AI Ecosystem</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13723v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13723v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类与人工智能（AI）系统（包括助手和推荐人，简称AI）互动的大规模社会技术系统的兴起，增加了集体现象和临界点出现的机会，带来了意想不到的、可能是意想不到的后果。例如，如果太多司机在同一条路线上行驶，导航系统的建议可能会造成混乱，而社交媒体上的个性化建议可能会放大两极分化、过滤泡沫和激进化。另一方面，我们可以学习如何培养“群体智慧”和集体行动效应，以应对社会和环境挑战。为了理解人工智能对社会技术系统的影响，并设计下一代人工智能，与人类合作，帮助克服而不是加剧社会问题，我们建议在复杂系统、网络科学和人工智能的交叉点上建立社会人工智能的基础，概述可能的技术和科学挑战，并提出研究途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13723v1" target="_blank">2306.13723v1</a>
                              </td>
                              <td>Social AI and the Challenges of the Human-AI Ecosystem</td>
                              <td>Dino Pedreschi</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13723v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13723v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0% accuracy on ImageNet by linear probing and finetune, outperforming iBOT by 0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3% AP on object detection, 39.7% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on the segme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的自监督学习框架ADCLR：精确和密集的对比表示学习，用于学习准确和密集的视觉表示。为了提取空间敏感信息，ADCLR除了引入全局对比之外，还引入了用于对比的查询补丁。与以前的密集对比方法相比，ADCLR主要有三个优点：i）同时实现全局判别和空间敏感表示，ii）模型有效（除了全局对比基线之外没有额外的参数），以及iii）无对应，因此实现更简单。我们的方法为对比方法实现了最先进的性能。在分类任务上，对于ViT-S，ADCLR在使用线性探测的ImageNet上实现了77.5%的前1级准确率，比没有我们设计的技术作为插件的基线（DINO）高出0.5%。对于ViT-B，ADCLR通过线性探测和微调在ImageNet上分别实现了79.8%和84.0%的准确率，分别比iBOT高出0.3%和0.2%的准确率。对于密集任务，在MS-COCO上，ADCLR在对象检测上实现了44.3%的AP，在实例分割上实现了39.7%的AP，分别比以前的SOTA方法SelfPatch提高了2.2%和1.2%。在ADE20K上，ADCLR在segme上比SelfPatch高出1.0%mIoU和1.2%mAcc</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13337v1" target="_blank">2306.13337v1</a>
                              </td>
                              <td>Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</td>
                              <td>Shaofeng Zhang</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13337v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09346v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rosetta Neurons: Mining the Common Units in a Model Zoo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09346v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call "Rosetta Neurons" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09346v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为各种视觉任务训练的不同神经网络是否有一些共同的表示？在本文中，我们在一系列具有不同架构、不同任务（生成和判别）和不同类型监督（类监督、文本监督、自监督）的模型中证明了我们称之为“罗塞塔神经元”的共同特征的存在。我们提出了一种在几种流行的视觉模型中挖掘罗塞塔神经元字典的算法：Class Supervisored-ResNet50、DINO-ResNet50、DINO-ViT、MAE、CLIP-ResNet50，BigGAN、StyleGAN-2、StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，无论具体任务或架构如何，都可以通过不同的模型学习，而无需使用语义标签。由于我们的分析中包含了生成模型，我们可以直接将共享的概念可视化。罗塞塔神经元促进了模型到模型的翻译，实现了各种基于反转的操作，包括跨类对齐、移位、缩放等，而无需专门训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09346v2" target="_blank">2306.09346v2</a>
                              </td>
                              <td>Rosetta Neurons: Mining the Common Units in a Model Zoo</td>
                              <td>Amil Dravid</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09346v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09346v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06588v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYO: DETR with YOLO for Step-by-Step Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06588v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06588v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体检测是计算机视觉中的一个重要课题，后处理是典型物体检测流水线的重要组成部分，对传统物体检测模型的性能构成了显著的瓶颈。检测转换器（DETR）作为第一个端到端目标检测模型，摒弃了锚和非最大值抑制（NMS）等手动组件的要求，大大简化了目标检测过程。然而，与大多数传统的对象检测模型相比，DETR收敛非常慢，并且查询的含义模糊不清。因此，受分步概念的启发，本文提出了一种新的两阶段目标检测模型，称为DETR with YOLO（DEYO），该模型依靠渐进推理来解决上述问题。DEYO是两阶段架构，包括分别作为第一和第二阶段的经典目标检测模型和类DETR模型。具体而言，第一阶段向第二阶段提供高质量的查询和锚点馈送，与原始DETR模型相比，提高了第二阶段的性能和效率。同时，第二级补偿由第一级检测器的限制引起的性能下降。大量实验表明，DEYO在12个和36个时期分别达到50.6个AP和52.1个AP，同时利用ResNet-50作为COCO数据集上的主干和多尺度特征。与最佳类DETR模型DINO相比，所开发的DEYO模型在两个历元设置中提供了1.6 AP和1.2 AP的显著性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06588v3" target="_blank">2211.06588v3</a>
                              </td>
                              <td>DEYO: DETR with YOLO for Step-by-Step Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2022-11-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06588v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06588v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09345v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Data Attribution for Text-to-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09345v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09345v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型文本到图像模型能够合成“新颖”的图像，但这些图像必然是训练数据的反映。这种模型中的数据归属问题——训练集中的哪些图像对给定生成图像的出现最负责任——是一个困难但重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估归因，该方法将现有的大规模模型调整为给定的示例对象或风格。我们的关键见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到示例的影响。通过我们的新数据集，我们能够评估各种数据归因算法和不同的可能特征空间。此外，通过在我们的数据集上进行训练，我们可以针对归因问题调整标准模型，如DINO、CLIP和ViT。尽管该过程是针对小样本集调整的，但我们显示了对大样本集的推广。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09345v1" target="_blank">2306.09345v1</a>
                              </td>
                              <td>Evaluating Data Attribution for Text-to-Image Models</td>
                              <td>Sheng-Yu Wang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09345v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09345v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07483v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semi-supervised learning made simple with self-supervised clustering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07483v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised principles. In this paper, we propose a conceptually simple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss. This approach may be interpreted as imposing the cluster centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CIFAR100 and ImageNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07483v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习模型已被证明可以在不需要人工注释的情况下学习丰富的视觉表示。然而，在许多现实世界的场景中，标签是部分可用的，这激发了受自我监督原则启发的半监督方法的最新工作。在本文中，我们提出了一种概念简单但经验强大的方法，将基于聚类的自监督方法（如SwAV或DINO）转变为半监督学习者。更准确地说，我们引入了一个多任务框架，将使用基本事实标签的监督目标和依赖于具有单个交叉熵损失的聚类分配的自监督目标合并在一起。这种方法可以被解释为将聚类质心强制作为类原型。尽管它很简单，但我们提供的经验证据表明，我们的方法非常有效，并在CIFAR100和ImageNet上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07483v1" target="_blank">2306.07483v1</a>
                              </td>
                              <td>Semi-supervised learning made simple with self-supervised clustering</td>
                              <td>Enrico Fini</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07483v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07483v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05382v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Automatic Image Blending Algorithm Based on SAM and DINO</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05382v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of image blending has gained popularity in recent years for its ability to create visually stunning content. However, the current image blending algorithm has the following problems: 1) The manual creation of the image blending mask requires a lot of manpower and material resources; 2) The image blending algorithm cannot effectively solve the problems of brightness distortion and low resolution. To this end, we propose a new image blending method: it combines semantic object detection and segmentation with corresponding mask generation to automatically blend images, while a two-stage iterative algorithm based on our proposed new saturation loss and PAN algorithm to fix brightness distortion and low resolution issues. Results on publicly available datasets show that our method outperforms many classic image blending algorithms on various performance metrics such as PSNR and SSIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05382v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，图像融合领域因其创造视觉上令人惊叹的内容的能力而广受欢迎。然而，目前的图像混合算法存在以下问题：1）手动创建图像混合掩模需要大量的人力和物力；2） 图像混合算法不能有效地解决亮度失真和分辨率低的问题。为此，我们提出了一种新的图像混合方法：它将语义对象检测和分割与相应的掩模生成相结合来自动混合图像，而基于我们提出的新饱和度损失和PAN算法的两阶段迭代算法来解决亮度失真和低分辨率问题。在公开数据集上的结果表明，我们的方法在各种性能指标（如PSNR和SSIM）上优于许多经典的图像混合算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05382v2" target="_blank">2306.05382v2</a>
                              </td>
                              <td>Automatic Image Blending Algorithm Based on SAM and DINO</td>
                              <td>Haochen Xue</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05382v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05382v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better 20 understand the success of FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，即特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值转移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为主干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，在实例分割中实现了40.8（+2.3）%AP，和42.1%AP。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的无人机对象检测和DAVIS 2017上的视频实例分割。最后，我们介绍了可视化和各种消融研究，以更好地了解FLSL的成功。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v1" target="_blank">2306.06203v1</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15444v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">10 Security and Privacy Problems in Large Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15444v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation models, including six confidentiality problems, three integrity problems, and one availability problem. For each problem, we discuss potential opportunities and challenges. We hope our book chapter will inspire future research on the security and privacy of foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15444v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，如GPT、CLIP和DINO，在过去几年中取得了革命性的进展，通常被认为是通用人工智能的一种很有前途的方法。特别是，采用自监督学习来使用大量未标记的数据预训练基础模型。预先训练的基础模型就像人工智能生态系统的“操作系统”。具体而言，基础模型可以用作许多下游任务的特征提取器，这些任务很少或没有标记的训练数据。现有的基础模型研究主要集中在预训练一个更好的基础模型，以提高其在非对抗性环境中下游任务的性能，而其在对抗性环境下的安全性和隐私性在很大程度上尚未得到探索。预先训练的基础模型的安全或隐私问题会导致人工智能生态系统的单点故障。在本书的章节中，我们讨论了预训练的基础模型的10个基本安全和隐私问题，包括6个机密性问题、3个完整性问题和1个可用性问题。对于每一个问题，我们都会讨论潜在的机遇和挑战。我们希望本书的章节将启发未来对基金会模型的安全性和隐私性的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15444v3" target="_blank">2110.15444v3</a>
                              </td>
                              <td>10 Security and Privacy Problems in Large Foundation Models</td>
                              <td>Jinyuan Jia</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15444v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15444v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization; none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 16 common metrics for 8 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了各种基于图像的生成模型，这些模型跨越语义不同的数据集，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过对生成模型进行迄今为止最大规模的评估实验，测量了人类对生成样本的图像真实感的感知，并发现没有任何现有指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性和记忆的16个现代指标相比，我们发现，由人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，当前的指标并不能正确地检测记忆；文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块化库，以计算8个不同编码器的16个通用度量https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v1" target="_blank">2306.04675v1</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别比DINO和OpenCLIP高出19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v1" target="_blank">2306.03881v1</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04654v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04654v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective transformer framework for self-supervised learning called DenseDINO to learn dense visual representations. To exploit the spatial information that the dense prediction tasks require but neglected by the existing self-supervised transformers, we introduce point-level supervision across views in a novel token-based way. Specifically, DenseDINO introduces some extra input tokens called reference tokens to match the point-level features with the position prior. With the reference token, the model could maintain spatial consistency and deal with multi-object complex scene images, thus generalizing better on dense prediction tasks. Compared with the vanilla DINO, our approach obtains competitive performance when evaluated on classification in ImageNet and achieves a large margin (+7.2% mIoU) improvement in semantic segmentation on PascalVOC under the linear probing protocol for segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04654v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种简单而有效的自监督学习转换器框架，称为DenseDINO，用于学习密集的视觉表示。为了利用密集预测任务所需但被现有的自监督变换器忽略的空间信息，我们以一种新的基于令牌的方式引入了跨视图的点级监督。具体而言，DenseDINO引入了一些称为参考标记的额外输入标记，以将点级特征与位置先验相匹配。有了参考令牌，该模型可以保持空间一致性，处理多目标复杂场景图像，从而更好地推广密集预测任务。与普通的DINO相比，当在ImageNet中对分类进行评估时，我们的方法获得了有竞争力的性能，并且在用于分割的线性探测协议下，在PascalVOC上的语义分割方面实现了大幅度的改进（+7.2%mIoU）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04654v1" target="_blank">2306.04654v1</a>
                              </td>
                              <td>DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</td>
                              <td>Yike Yuan</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04654v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04654v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07598v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07598v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses Hungarian matching to filter redundant noised queries and $\textit{query alignment}$ to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07598v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着检测变压器（DETR）的变体DINO的发布，检测变压器凭借其端到端设计和可扩展性的优点打破了对象检测基准的记录。然而，DETR向面向对象检测的扩展尚未得到彻底研究，尽管预计其端到端架构会带来更多好处，例如消除NMS和锚相关成本。在本文中，我们提出了第一个基于强DINO的面向对象检测基线。我们发现，直接使用DETR进行定向对象检测并不能保证无重复预测，并提出了一个简单的成本来缓解这一问题。此外，我们引入了$\textit｛动态去噪｝$策略，该策略使用匈牙利匹配来过滤冗余噪声查询，并使用$\textit{查询对齐｝$来保持Transformer解码器层之间的匹配一致性。我们提出的模型优于之前的旋转DETR和其他同行，在DOTA-1.0/v.5/v.20和DIOR-R基准测试中实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07598v3" target="_blank">2305.07598v3</a>
                              </td>
                              <td>RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</td>
                              <td>Hakjin Lee</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07598v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07598v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_09959v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Context Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_09959v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_09959v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了全局上下文视觉转换器（GC-ViT），这是一种提高计算机视觉参数和计算利用率的新架构。我们的方法利用全局上下文自注意模块，结合标准的局部自注意，有效和高效地对长距离和短距离空间交互进行建模，而不需要计算注意力掩码或移动局部窗口等昂贵的操作。此外，我们解决了ViTs中缺乏电感偏置的问题，并建议在我们的架构中利用改进的融合反向残差块。我们提出的GC-ViT在图像分类、对象检测和语义分割任务中取得了最先进的结果。在用于分类的ImageNet-1K数据集上，具有51M、90M和201M参数的GC ViT变体在224图像分辨率和没有任何预训练的情况下分别达到84.3%、85.0%和85.7%的Top-1准确率，因此大大超过了类似规模的现有技术，如基于CNN的ConvNeXt和基于ViT的MaxViT和Swin Transformer。使用MS COCO和ADE20K数据集在对象检测、实例分割和语义分割的下游任务中预先训练的GC-ViT骨干始终优于先前的工作。具体而言，具有4级DINO检测头的GC ViT在MS COCO数据集上实现了58.3的框AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.09959v5" target="_blank">2206.09959v5</a>
                              </td>
                              <td>Global Context Vision Transformers</td>
                              <td>Ali Hatamizadeh</td>
                              <td>2022-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_09959v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.09959v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01398v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01398v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite impressive empirical advances of SSL in solving various tasks, the problem of understanding and characterizing SSL representations learned from input data remains relatively under-explored. We provide a comparative analysis of how the representations produced by SSL models differ when masking parts of the input. Specifically, we considered state-of-the-art SSL pretrained models, such as DINOv2, MAE, and SwaV, and analyzed changes at the representation levels across 4 Image Classification datasets. First, we generate variations of the datasets by applying foreground and background segmentation. Then, we conduct statistical analysis using Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA) to evaluate the robustness of the representations learned in SSL models. Empirically, we show that not all models lead to representations that separate foreground, background, and complete images. Furthermore, we test different masking strategies by occluding the center regions of the images to address cases where foreground and background are difficult. For example, the DTD dataset that focuses on texture rather specific objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01398v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管SSL在解决各种任务方面取得了令人印象深刻的经验进步，但理解和表征从输入数据中学习的SSL表示的问题仍然相对未得到充分探索。我们提供了SSL模型在屏蔽部分输入时产生的表示如何不同的比较分析。具体而言，我们考虑了最先进的SSL预训练模型，如DINOv2、MAE和SwaV，并分析了4个图像分类数据集在表示级别上的变化。首先，我们通过应用前景和背景分割来生成数据集的变化。然后，我们使用标准相关分析（CCA）和中心核对齐（CKA）进行统计分析，以评估SSL模型中学习的表示的稳健性。根据经验，我们表明，并非所有模型都能产生分离前景、背景和完整图像的表示。此外，我们通过遮挡图像的中心区域来测试不同的掩蔽策略，以解决前景和背景困难的情况。例如，专注于纹理而非特定对象的DTD数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01398v1" target="_blank">2306.01398v1</a>
                              </td>
                              <td>Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</td>
                              <td>Xavier F. Cadet</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01398v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_00449v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_00449v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. Further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. The code is available at https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_00449v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着深度神经网络方法的日益普及，外科计算机视觉领域取得了长足的突破。然而，训练此类模型的标准完全监督方法需要大量的注释数据，成本高得令人望而却步；尤其是在临床领域。自监督学习（SSL）方法已经开始在普通计算机视觉社区中获得吸引力，它代表了这些注释成本的潜在解决方案，允许仅从未标记的数据中学习有用的表示。尽管如此，SSL方法在更复杂和更有影响力的领域（如医学和外科）的有效性仍然有限，尚未探索。在这项工作中，我们通过在外科计算机视觉的背景下研究四种最先进的SSL方法（MoCov2、SimCLR、DINO、SwAV）来解决这一关键需求。我们在Cholec80数据集上对这些方法在外科上下文理解、相位识别和工具存在检测这两个基本且流行的任务中的性能进行了广泛的分析。我们检查了它们的参数化，然后检查了它们在半监督设置中相对于训练数据量的行为。正如这项工作中所描述和进行的那样，将这些方法正确地转移到手术中，与SSL的一般用途相比，可以获得实质性的性能提升——在相位识别方面高达7.4%，在工具存在检测方面高达20%——以及最先进的半监督相位识别方法高达14%。在高度多样化的外科手术数据集选择上获得的进一步结果显示出强大的泛化特性。代码位于https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.00449v3" target="_blank">2207.00449v3</a>
                              </td>
                              <td>Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</td>
                              <td>Sanat Ramesh</td>
                              <td>2022-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_00449v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.00449v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_07044v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_07044v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_07044v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自我监督的预训练有可能在没有人为注释的情况下生成表达表征。地球观测（EO）中的大多数预训练都是基于ImageNet或中型标记遥感（RS）数据集。我们共享一个未标记的RS数据集SSL4EO-S12（地球观测的自我监督学习-哨兵-1/2），以收集来自欧空局哨兵-1/2卫星任务的大规模、全球、多模式和多季节卫星图像语料库。对于EO应用，我们展示了SSL4EO-S12在一组方法的自监督预训练中的成功：MoCo-v2、DINO、MAE和data2vec。由此产生的模型产生的下游性能接近或超过监督学习的准确性度量。此外，与现有数据集相比，SSL4EO-S12上的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.07044v2" target="_blank">2211.07044v2</a>
                              </td>
                              <td>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</td>
                              <td>Yi Wang</td>
                              <td>2022-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_07044v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.07044v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11922v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11922v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the ``objectness'' of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase performance with a higher memory requirement. The dataset split and code are available at https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11922v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象实例分割是室内机器人在有许多小对象的杂乱环境中导航的一个关键挑战。3D传感能力的局限性往往使检测每一个可能的物体变得困难。虽然深度学习方法可能对这个问题有效，但手动注释3D数据以进行监督学习是耗时的。在这项工作中，我们探索了RGB-D数据中的零样本实例分割（ZSIS），以语义类别认知的方式识别看不见的对象。我们为桌面对象数据集（TOD-Z）引入了一种零样本分割，以实现这项研究，并提出了一种方法，该方法使用带注释的对象来学习像素的“对象性”，并推广到杂乱室内环境中看不到的对象类别。我们的方法SupeRGB-D基于几何线索将像素分组为小块，并学习以深度聚集聚类的方式合并小块。SupeRGB-D在看不见的对象上优于现有的基线，同时在看到的对象上实现了类似的性能。我们在真实数据集OCID上进一步展示了有竞争力的结果。凭借其轻量级设计（需要0.4 MB内存），我们的方法非常适合移动和机器人应用。额外的DINO功能可以通过更高的内存要求来提高性能。数据集拆分和代码可在https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11922v2" target="_blank">2212.11922v2</a>
                              </td>
                              <td>SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11922v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11922v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上执行与SOTA表示类似的表现。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果出奇地好，使用这些融合特征的最近邻进行零样本评估，与基准数据集上的最新方法相比，性能显著提高，例如SPair-71k、PF-Pascal和TSS。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v1" target="_blank">2305.15347v1</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-vocabulary Segmentation with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练鲁棒和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它严重损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过利用预先训练的基础模型CLIP和DINO的开放词汇多模态知识和对象推理能力，在不需要任何微调的情况下，解决了3D开放词汇分割中的挑战。具体而言，我们将CLIP中的开放词汇视觉和文本知识提取到神经辐射场（NeRF）中，该场有效地将2D特征提升到视图一致的3D分割中。此外，我们引入了相关性分布对齐损失和特征分布对齐损失，以分别减轻CLIP特征的模糊性，并从DINO特征中提取精确的对象边界，从而消除了训练过程中对分割注释的需要。大量实验表明，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v2" target="_blank">2305.14093v2</a>
                              </td>
                              <td>3D Open-vocabulary Segmentation with Foundation Models</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12223v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Makes for Good Visual Tokenizers for Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12223v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12223v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们实证研究了适当的预训练方法来构建良好的视觉标记器，使大型语言模型（LLM）成为强大的多模式大型语言模型。在我们旨在评估MLLMs视觉语义理解和细粒度感知能力的基准中，我们讨论了用主导方法（即DeiT、CLIP、MAE、DINO）预训练的不同视觉标记器，并观察到：i）完全/弱监督模型比自监督模型捕获更多的语义，但通过扩大预训练数据集缩小了差距。ii）自监督模型更擅长细粒度感知，其中补丁级别的监督尤其有效。iii）调整可视化标记器会导致从大规模预训练中获得的语义丢失，这对相对小规模的指令调整数据集是不利的。鉴于这些发现，我们回顾了试图统一语义和细粒度视觉理解的方法，例如，具有语义丰富目标的补丁级特征提取。我们获得了一个有趣的基于面具的洞察策略，这些策略曾经风靡一时，但可能不适用于获得良好的视觉标记器。基于这一关键观察，我们获得了一种新的MLLM，该MLLM配备了定制的良好视觉标记器（GVT），在多个尺度上表现出强大的视觉理解能力。特别是，在不引入额外参数和特定任务微调的情况下，GVT在视觉问答、图像字幕和其他细粒度视觉理解任务（如对象计数和多类识别）上实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12223v2" target="_blank">2305.12223v2</a>
                              </td>
                              <td>What Makes for Good Visual Tokenizers for Large Language Models?</td>
                              <td>Guangzhi Wang</td>
                              <td>2023-05-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12223v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间良好建立的联系的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。说明了它们在各种密度估计和条件密度估计任务中的效用。软件可在https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v1" target="_blank">2305.13552v1</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13291v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Materialistic: Selecting Similar Materials in Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13291v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO features coupled with a proposed Cross-Similarity module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13291v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将图像分离为有意义的底层组件是编辑和理解图像的关键第一步。我们提出了一种方法，能够选择照片中展示与艺术家选择区域相同材料的区域。我们提出的方法对明暗处理、镜面高光和投射阴影都很稳健，可以在真实图像中进行选择。由于我们不依赖于语义分割（不同的木材或金属不应该一起选择），我们将该问题表述为基于用户提供的图像位置的基于相似性的分组问题。特别地，我们建议利用无监督的DINO特征，结合所提出的交叉相似性模块和MLP头来提取图像中的材料相似性。我们在发布的一个新的合成图像数据集上训练我们的模型。我们证明了我们的方法可以很好地推广到真实世界的图像。我们仔细分析了模型在不同材料特性和照明条件下的行为。此外，我们根据50张真实照片的手工注释基准对其进行了评估。我们在一系列应用中进一步展示了我们的模型，包括素材编辑、视频选择和检索具有类似素材的对象照片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13291v1" target="_blank">2305.13291v1</a>
                              </td>
                              <td>Materialistic: Selecting Similar Materials in Images</td>
                              <td>Prafull Sharma</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13291v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13291v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如，CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务上显示出了令人印象深刻的学习和转移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先证明，虽然基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。为此，我们在CLIP模型上提出了一种非常简单的目标数据提取方法，并在所有UniDA基准测试中实现了对基线的一致改进。我们的研究是在一种新提出的通用分类率（UCR）评估指标下进行的，该指标不含阈值和比率，解决了使用现有H-核心指标时遇到的阈值敏感问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v1" target="_blank">2305.11092v1</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08014v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08014v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most invariance-based self-supervised methods rely on single object-centric images (e.g., ImageNet images) for pretraining, learning features that invariant to geometric transformation. However, when images are not object-centric, the semantics of the image can be significantly altered due to cropping. Furthermore, as the model becomes insensitive to geometric transformations, it may struggle to capture location information. For this reason, we propose a Geometric Transformation Sensitive Architecture designed to be sensitive to geometric transformations, specifically focusing on four-fold rotation, random crop, and multi-crop. Our method encourages the student to be sensitive by predicting rotation and using targets that vary with those transformations through pooling and rotating the teacher feature map. Additionally, we use patch correspondence loss to encourage correspondence between patches with similar features. This approach allows us to capture long-term dependencies in a more appropriate way than capturing long-term dependencies by encouraging local-to-global correspondence, which occurs when learning to be insensitive to multi-crop. Our approach demonstrates improved performance when using non-object-centric images as pretraining data compared to other methods that train the model to be insensitive to geometric transformation. We surpass DINO[Caron et al.[2021b]] baseline in tasks including image classification, semantic segmentation, detection, and instance segmentation with improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$. Code and pretrained models are publicly available at: https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08014v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于不变性的自监督方法依赖于以单个对象为中心的图像（例如，ImageNet图像）进行预训练，学习对几何变换不变的特征。然而，当图像不是以对象为中心时，图像的语义可能会因裁剪而发生显著变化。此外，随着模型对几何变换变得不敏感，它可能很难捕捉位置信息。因此，我们提出了一种几何变换敏感架构，该架构设计为对几何变换敏感，特别关注四重旋转、随机裁剪和多重裁剪。我们的方法通过预测旋转，并通过汇集和旋转教师特征图，使用随这些转换而变化的目标，鼓励学生保持敏感。此外，我们使用补丁对应损失来鼓励具有相似特征的补丁之间的对应。这种方法使我们能够以一种比通过鼓励局部到全局的对应关系捕获长期依赖关系更合适的方式捕获长期依赖性，这种对应关系发生在学习对多作物不敏感时。与其他将模型训练为对几何变换不敏感的方法相比，我们的方法在使用非以对象为中心的图像作为预训练数据时提高了性能。我们在包括图像分类、语义分割、检测和实例分割在内的任务中超过了DINO[Caron等人[2021b]]基线，改进了4.9$Top-1Acc$、3.3$mIoU$、3.4$AP^b$和2.7$AP^m$。代码和预训练模型可在以下网站上公开获取：https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08014v7" target="_blank">2304.08014v7</a>
                              </td>
                              <td>Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</td>
                              <td>Taeho Kim</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08014v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08014v7" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06558v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Track Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06558v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This report presents a framework called Segment And Track Anything (SAMTrack) that allows users to precisely and effectively segment and track any object in a video. Additionally, SAM-Track employs multimodal interaction methods that enable users to select multiple objects in videos for tracking, corresponding to their specific requirements. These interaction methods comprise click, stroke, and text, each possessing unique benefits and capable of being employed in combination. As a result, SAM-Track can be used across an array of fields, ranging from drone technology, autonomous driving, medical imaging, augmented reality, to biological analysis. SAM-Track amalgamates Segment Anything Model (SAM), an interactive key-frame segmentation model, with our proposed AOT-based tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022 challenge, to facilitate object tracking in video. In addition, SAM-Track incorporates Grounding-DINO, which enables the framework to support text-based interaction. We have demonstrated the remarkable capabilities of SAM-Track on DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in diverse applications. The project page is available at: https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06558v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本报告提供了一个名为“分段和跟踪任何内容”（SAMTrack）的框架，该框架允许用户精确有效地分段和跟踪视频中的任何对象。此外，SAM Track采用多模式交互方法，使用户能够根据自己的特定要求选择视频中的多个对象进行跟踪。这些交互方法包括点击、笔划和文本，每种方法都具有独特的优点，并且能够组合使用。因此，SAM Track可用于一系列领域，从无人机技术、自动驾驶、医学成像、增强现实到生物分析。SAM Track将交互式关键帧分割模型Segment Anything Model（SAM）与我们提出的基于AOT的跟踪模型（DeAOT）合并，以促进视频中的对象跟踪，该模型在VOT 2022挑战的四个轨道中排名第一。此外，SAM Track结合了Grounding DINO，使框架能够支持基于文本的交互。我们已经在DAVIS-2016 Val（92.0%）和DAVIS-2017 Test（79.2%）上展示了SAM Track的卓越能力及其在各种应用中的实用性。项目页面位于：https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06558v1" target="_blank">2305.06558v1</a>
                              </td>
                              <td>Segment and Track Anything</td>
                              <td>Yangming Cheng</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06558v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06558v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06553v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06553v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce WeLayout, a novel system for segmenting the layout of corporate documents, which stands for WeChat Layout Analysis System. Our approach utilizes a sophisticated ensemble of DINO and YOLO models, specifically developed for the ICDAR 2023 Competition on Robust Layout Segmentation. Our method significantly surpasses the baseline, securing a top position on the leaderboard with a mAP of 70.0. To achieve this performance, we concentrated on enhancing various aspects of the task, such as dataset augmentation, model architecture, bounding box refinement, and model ensemble techniques. Additionally, we trained the data separately for each document category to ensure a higher mean submission score. We also developed an algorithm for cell matching to further improve our performance. To identify the optimal weights and IoU thresholds for our model ensemble, we employed a Bayesian optimization algorithm called the Tree-Structured Parzen Estimator. Our approach effectively demonstrates the benefits of combining query-based and anchor-free models for achieving robust layout segmentation in corporate documents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06553v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了WeLayout，一种新的企业文档布局分割系统，即微信布局分析系统。我们的方法利用了一套复杂的DINO和YOLO模型，专门为ICDAR 2023鲁棒布局分割竞赛开发。我们的方法大大超过了基线，以70.0的mAP稳居排行榜榜首。为了实现这一性能，我们集中精力增强任务的各个方面，如数据集扩充、模型架构、边界框细化和模型集成技术。此外，我们分别为每个文档类别训练数据，以确保更高的平均提交分数。我们还开发了一种用于小区匹配的算法，以进一步提高我们的性能。为了确定我们模型集成的最佳权重和IoU阈值，我们使用了一种称为树结构Parzen估计器的贝叶斯优化算法。我们的方法有效地展示了将基于查询的模型和无锚模型相结合以在公司文档中实现稳健布局分割的好处。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06553v1" target="_blank">2305.06553v1</a>
                              </td>
                              <td>WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</td>
                              <td>Mingliang Zhang</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06553v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on any pre-trained self-supervised model to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear classification performance of state-of-the-art self-supervised models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and enhancing these features through Q-score regularization makes representations more interpretable across all self-supervised models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们习得的表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征大多以正确分类的表示形式出现。使用这些特征，我们可以将表示空间压缩高达$40\%\$，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种模型不可知、无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作任何预先训练的自监督模型的正则化术语，以纠正低质量的表示。与基线相比，使用Q-Score正则化进行微调可以将最先进的自监督模型的线性分类性能在ImageNet-100上提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性强相关，通过Q分数正则化增强这些特征使表示在所有自监督模型中更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v4" target="_blank">2203.01881v4</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14571v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14571v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-trained DINO model (as multiple channels) in the other branch. The results of our experiments on two publicly available medical imaging datasets show that the proposed pipeline outperforms U-Net and the state-of-the-art medical image segmentation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14571v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管纯基于变换器的架构在许多计算机视觉任务中表现出了良好的性能，但引入了许多由CNN和变换器块组成的混合模型来适应更专业的任务。然而，尽管与医学图像分割中的细胞神经网络相比，基于纯变压器和混合变压器的架构都有性能增益，但其高昂的训练成本和复杂性使其在实际场景中使用具有挑战性。在这项工作中，我们提出了基于纯卷积层的简单架构，并表明只要利用从自监督预训练的视觉变换器网络（例如，DINO）获得的注意力图可视化，就可以以更低的计算成本胜过基于复杂变换器的网络。所提出的架构由两个编码器分支组成，其中原始图像在一个分支中作为输入，而来自预训练的DINO模型的多个自注意头的同一图像的注意图可视化（作为多个通道）在另一个分支。我们在两个公开可用的医学图像数据集上的实验结果表明，所提出的流水线优于U-Net和最先进的医学图像分割模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14571v1" target="_blank">2304.14571v1</a>
                              </td>
                              <td>DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</td>
                              <td>Yousef Yeganeh</td>
                              <td>2023-04-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14571v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14571v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13348v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextDeformer: Geometry Manipulation using Text Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13348v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a technique for automatically producing a deformation of an input triangle mesh, guided solely by a text prompt. Our framework is capable of deformations that produce both large, low-frequency shape changes, and small high-frequency details. Our framework relies on differentiable rendering to connect geometry to powerful pre-trained image encoders, such as CLIP and DINO. Notably, updating mesh geometry by taking gradient steps through differentiable rendering is notoriously challenging, commonly resulting in deformed meshes with significant artifacts. These difficulties are amplified by noisy and inconsistent gradients from CLIP. To overcome this limitation, we opt to represent our mesh deformation through Jacobians, which updates deformations in a global, smooth manner (rather than locally-sub-optimal steps). Our key observation is that Jacobians are a representation that favors smoother, large deformations, leading to a global relation between vertices and pixels, and avoiding localized noisy gradients. Additionally, to ensure the resulting shape is coherent from all 3D viewpoints, we encourage the deep features computed on the 2D encoding of the rendering to be consistent for a given vertex from all viewpoints. We demonstrate that our method is capable of smoothly-deforming a wide variety of source mesh and target text prompts, achieving both large modifications to, e.g., body proportions of animals, as well as adding fine semantic details, such as shoe laces on an army boot and fine details of a face.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13348v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种仅通过文本提示自动生成输入三角形网格变形的技术。我们的框架能够产生大的低频形状变化和小的高频细节。我们的框架依赖于可微分渲染来将几何体连接到强大的预训练图像编码器，如CLIP和DINO。值得注意的是，通过可微分渲染采取梯度步骤来更新网格几何体是出了名的具有挑战性，通常会导致变形的网格出现明显的伪影。这些困难被来自CLIP的噪声和不一致的梯度放大了。为了克服这一限制，我们选择通过雅可比变换来表示网格变形，雅可比变换以全局、平滑的方式更新变形（而不是局部次优步骤）。我们的关键观察结果是，雅各宾派是一种倾向于更平滑、大变形的表示，从而导致顶点和像素之间的全局关系，并避免局部噪声梯度。此外，为了确保从所有3D视点得到的形状是一致的，我们鼓励在渲染的2D编码上计算的深度特征对于来自所有视点的给定顶点是一致的。我们证明，我们的方法能够平滑地变形各种源网格和目标文本提示，既可以对动物的身体比例进行大的修改，也可以添加精细的语义细节，如军靴上的鞋带和面部的精细细节。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13348v1" target="_blank">2304.13348v1</a>
                              </td>
                              <td>TextDeformer: Geometry Manipulation using Text Guidance</td>
                              <td>William Gao</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13348v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13348v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13089v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13089v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences explain opposite trends in transfer performance for downstream tasks that require spatial specificity in features. Finally, we address how fine-tuning changes reconstructive representations to enable better transfer, showing that fine-tuning re-organizes the information to be more similar to pre-trained joint embedding models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13089v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于联合嵌入的学习（例如，SimCLR、MoCo、DINO）和基于重建的学习（如，BEiT、SimMIM、MAE）是视觉变换器自我监督学习的两种主要范式，但它们的迁移性能有很大不同。在这里，我们旨在通过分析这些目标对习得表征的结构和可迁移性的影响来解释这些差异。我们的分析表明，基于重建的学习特征与基于联合嵌入的学习特征显著不同，并且以相似目标训练的模型甚至在不同架构中也能学习相似的特征。这些差异出现在网络的早期，主要由注意力和规范化层驱动。我们发现，联合嵌入特征为分类提供了更好的线性探测转移，因为不同的目标驱动学习表示中信息和不变量的不同分布。这些差异解释了需要特征空间特异性的下游任务的转移性能的相反趋势。最后，我们讨论了微调如何改变重建表示以实现更好的传输，表明微调重新组织信息，使其更类似于预训练的联合嵌入模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13089v1" target="_blank">2304.13089v1</a>
                              </td>
                              <td>Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</td>
                              <td>Shashank Shekhar</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13089v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13089v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13027v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Strong and Reproducible Object Detector with Only Public Datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13027v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13027v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13027v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work presents Focal-Stable-DINO, a strong and reproducible object detection model which achieves 64.6 AP on COCO val2017 and 64.8 AP on COCO test-dev using only 700M parameters without any test time augmentation. It explores the combination of the powerful FocalNet-Huge backbone with the effective Stable-DINO detector. Different from existing SOTA models that utilize an extensive number of parameters and complex training techniques on large-scale private data or merged data, our model is exclusively trained on the publicly available dataset Objects365, which ensures the reproducibility of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13027v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作提出了Focal Stable DINO，这是一种强大且可复制的物体检测模型，仅使用700M参数，在没有任何测试时间增加的情况下，就在COCO val2017上实现了64.6 AP，在COCO测试开发中实现了64.8 AP。它探索了强大的FocalNet巨大骨干与有效的稳定DINO检测器的结合。与在大规模私有数据或合并数据上使用大量参数和复杂训练技术的现有SOTA模型不同，我们的模型仅在公开可用的数据集Objects365上进行训练，这确保了我们方法的可重复性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13027v1" target="_blank">2304.13027v1</a>
                              </td>
                              <td>A Strong and Reproducible Object Detector with Only Public Datasets</td>
                              <td>Tianhe Ren</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13027v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13027v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10597v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10597v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10597v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10597v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Through this work, we aim to provide insights into maximizing the applicability of visual FMs in specific contexts with minimal model tuning. The code is available at https://github.com/Douglas2Code/Text2Seg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10597v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（FM）的最新进展，如GPT-4和LLaMA，因其在零样本学习场景中的卓越性能而引起了人们的极大关注。类似地，在视觉学习领域，像Grounding DINO和Segment Anything Model（SAM）这样的模型在开集检测和实例分割任务方面表现出了显著的进步。不可否认，这些FM将深刻影响广泛的现实世界视觉学习任务，为开发此类模型带来新的范式转变。在这项研究中，我们专注于遥感领域，那里的图像与传统场景中的图像明显不同。我们开发了一个管道，利用多个FM来促进由文本提示引导的遥感图像语义分割任务，我们将其表示为Text2Seg。该管道是在几个广泛使用的遥感数据集上进行基准测试的，我们提供了初步结果来证明其有效性。通过这项工作，我们旨在通过最小的模型调整，最大限度地提高视觉FM在特定环境中的适用性。代码位于https://github.com/Douglas2Code/Text2Seg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10597v1" target="_blank">2304.10597v1</a>
                              </td>
                              <td>Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models</td>
                              <td>Jielu Zhang</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10597v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10597v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07527v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Align-DETR: Improving DETR with Simple IoU-aware BCE loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07527v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07527v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07527v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DETR has set up a simple end-to-end pipeline for object detection by formulating this task as a set prediction problem, showing promising potential. However, despite the significant progress in improving DETR, this paper identifies a problem of misalignment in the output distribution, which prevents the best-regressed samples from being assigned with high confidence, hindering the model's accuracy. We propose a metric, recall of best-regressed samples, to quantitively evaluate the misalignment problem. Observing its importance, we propose a novel Align-DETR that incorporates a localization precision-aware classification loss in optimization. The proposed loss, IA-BCE, guides the training of DETR to build a strong correlation between classification score and localization precision. We also adopt the mixed-matching strategy, to facilitate DETR-based detectors with faster training convergence while keeping an end-to-end scheme. Moreover, to overcome the dramatic decrease in sample quality induced by the sparsity of queries, we introduce a prime sample weighting mechanism to suppress the interference of unimportant samples. Extensive experiments are conducted with very competitive results reported. In particular, it delivers a 46 (+3.8)% AP on the DAB-DETR baseline with the ResNet-50 backbone and reaches a new SOTA performance of 50.2% AP in the 1x setting on the COCO validation set when employing the strong baseline DINO. Our code is available at https://github.com/FelixCaae/AlignDETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07527v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DETR通过将这项任务公式化为集合预测问题，建立了一个简单的端到端目标检测流水线，显示出了很好的潜力。然而，尽管在改进DETR方面取得了重大进展，但本文发现了输出分布中的错位问题，这阻碍了最佳回归样本的高置信度分配，阻碍了模型的准确性。我们提出了一个度量，即最佳回归样本的召回，来定量评估错位问题。鉴于其重要性，我们提出了一种新的Align DETR，该方法在优化中引入了定位精度感知的分类损失。所提出的损失IA-BCE指导DETR的训练，以在分类得分和定位精度之间建立强相关性。我们还采用了混合匹配策略，以促进基于DETR的检测器在保持端到端方案的同时具有更快的训练收敛。此外，为了克服查询稀疏导致的样本质量急剧下降的问题，我们引入了一种主样本加权机制来抑制不重要样本的干扰。进行了广泛的实验，报告了非常有竞争力的结果。特别是，当采用强基线DINO时，它在具有ResNet-50主干的DAB-DETR基线上提供了46（+3.8）%AP，并在COCO验证集的1x设置中达到了50.2%AP的新SOTA性能。我们的代码可在https://github.com/FelixCaae/AlignDETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07527v1" target="_blank">2304.07527v1</a>
                              </td>
                              <td>Align-DETR: Improving DETR with Simple IoU-aware BCE loss</td>
                              <td>Zhi Cai</td>
                              <td>2023-04-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07527v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07527v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>