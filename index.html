<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-06-08</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2306_04570v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04570v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In many robotics problems, there is a significant gain in collaborative information sharing between multiple robots, for exploration, search and rescue, tracking multiple targets, or mapping large environments. One of the key implicit assumptions when solving cooperative multi-robot problems is that all robots use the same (homogeneous) underlying algorithm. However, in practice, we want to allow collaboration between robots possessing different capabilities and that therefore must rely on heterogeneous algorithms. We present a system architecture and the supporting theory, to enable collaboration in a decentralized network of robots, where each robot relies on different estimation algorithms. To develop our approach, we focus on multi-robot simultaneous localization and mapping (SLAM) with multi-target tracking. Our theoretical framework builds on our idea of exploiting the conditional independence structure inherent to many robotics applications to separate between each robot's local inference (estimation) tasks and fuse only relevant parts of their non-equal, but overlapping probability density function (pdfs). We present a new decentralized graph-based approach to the multi-robot SLAM and tracking problem. We leverage factor graphs to split between different parts of the problem for efficient data sharing between robots in the network while enabling robots to use different local sparse landmark/dense/metric-semantic SLAM algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04570v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在许多机器人问题中，多个机器人之间的协作信息共享有很大的好处，用于探索、搜索和救援、跟踪多个目标或绘制大型环境地图。解决多机器人协作问题时的一个关键隐含假设是，所有机器人都使用相同（同质）的底层算法。然而，在实践中，我们希望允许具有不同能力的机器人之间进行协作，因此必须依赖于异构算法。我们提出了一种系统架构和支持理论，以实现分散机器人网络中的协作，其中每个机器人依赖于不同的估计算法。为了开发我们的方法，我们专注于具有多目标跟踪的多机器人同时定位和映射（SLAM）。我们的理论框架建立在我们的想法之上，即利用许多机器人应用程序固有的条件独立性结构，在每个机器人的局部推理（估计）任务之间进行分离，并仅融合其不相等但重叠的概率密度函数（pdfs）的相关部分。我们提出了一种新的基于分散图的方法来解决多机器人SLAM和跟踪问题。我们利用因子图在问题的不同部分之间进行划分，以实现网络中机器人之间的有效数据共享，同时使机器人能够使用不同的局部稀疏地标/密集/度量语义SLAM算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04570v1" target="_blank">2306.04570v1</a>
                              </td>
                              <td>Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</td>
                              <td>Ofer Dagan</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04570v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04570v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03953v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03953v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is the task of building a map representation of an unknown environment while it at the same time is used for positioning. A probabilistic interpretation of the SLAM task allows for incorporating prior knowledge and for operation under uncertainty. Contrary to the common practice of computing point estimates of the system states, we capture the full posterior density through approximate Bayesian inference. This dynamic learning task falls under state estimation, where the state-of-the-art is in sequential Monte Carlo methods that tackle the forward filtering problem. In this paper, we introduce a framework for probabilistic SLAM using particle smoothing that does not only incorporate observed data in current state estimates, but it also back-tracks the updated knowledge to correct for past drift and ambiguities in both the map and in the states. Our solution can efficiently handle both dense and sparse map representations by Rao-Blackwellization of conditionally linear and conditionally linearized models. We show through simulations and real-world experiments how the principles apply to radio (BLE/Wi-Fi), magnetic field, and visual SLAM. The proposed solution is general, efficient, and works well under confounding noise.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03953v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是在用于定位的同时构建未知环境的地图表示的任务。SLAM任务的概率解释允许结合先验知识并在不确定性下进行操作。与计算系统状态的点估计的常见做法相反，我们通过近似贝叶斯推理来获取完整的后验密度。这种动态学习任务属于状态估计，其中最先进的是解决前向滤波问题的顺序蒙特卡罗方法。在本文中，我们介绍了一种使用粒子平滑的概率SLAM框架，该框架不仅将观测数据纳入当前状态估计，而且还对更新后的知识进行回溯，以纠正地图和状态中过去的漂移和模糊性。我们的解决方案可以通过条件线性化和条件线性化模型的Rao-Blackwell化有效地处理密集和稀疏映射表示。我们通过模拟和真实世界的实验展示了这些原理如何应用于无线电（BLE/Wi-Fi）、磁场和视觉SLAM。所提出的解决方案是通用的、有效的，并且在混杂噪声下工作良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03953v1" target="_blank">2306.03953v1</a>
                              </td>
                              <td>Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</td>
                              <td>Manon Kok</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03953v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03953v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2102_07448v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2102_07448v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Surround View fisheye cameras are commonly deployed in automated driving for 360\deg{} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2102_07448v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环绕视图鱼眼摄像头通常部署在自动驾驶中，用于车辆周围的360度近场传感。这项工作在未修正的鱼眼图像上提出了一个多任务视觉感知网络，使车辆能够感知周围环境。它由自动驾驶系统所需的六项主要任务组成：深度估计、视觉里程计、语义分割、运动分割、物体检测和镜头污染检测。我们证明，联合训练的模型比各自的单任务版本表现得更好。我们的多任务模型具有一个共享编码器，提供了显著的计算优势，并具有任务相互支持的协同解码器。我们提出了一种新的基于相机几何的自适应机制来对鱼眼失真模型进行训练和推理编码。这对于在WoodScape数据集上进行训练至关重要，该数据集由安装在三辆不同汽车上的12台不同相机收集的来自世界不同地区的数据组成，具有不同的本质和视角。考虑到边界框不是失真鱼眼图像的良好表示，我们还将对象检测扩展到使用具有非均匀采样顶点的多边形。此外，我们还在标准汽车数据集上评估了我们的模型，即KITTI和Cityscapes。我们在深度估计和姿态估计任务的KITTI上获得了最先进的结果，并在其他任务上获得了有竞争力的性能。我们对各种架构选择和任务加权方法进行了广泛的消融研究。上的短视频https://youtu.be/xbSjZ5OfPes提供了定性结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2102.07448v3" target="_blank">2102.07448v3</a>
                              </td>
                              <td>OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2021-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2102_07448v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2102.07448v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2007_06676v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2007_06676v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190$^\circ$ horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet. Qualitative results on a distorted test scene video sequence indicate excellent performance https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2007_06676v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在经典的计算机视觉中，校正是多视图深度估计的一个组成部分。它通常包括对极校正和透镜畸变校正。该过程大大简化了深度估计，因此已被CNN方法所采用。然而，校正有几个副作用，包括视场（FOV）减小、重采样失真和对校准误差的敏感性。在严重失真的情况下，效果尤其明显（例如，广角鱼眼相机）。在本文中，我们提出了一种通用的尺度感知自监督管道，用于从未修正的单目视频中估计深度、欧氏距离和视觉里程。我们在未校正的KITTI数据集上证明了类似的精度水平，桶形失真与校正的KITT数据集相当。直觉是，校正步骤可以隐含地吸收在CNN模型中，CNN模型在不增加复杂性的情况下学习失真模型。我们的方法不会减少视野，并避免了推理时校正的计算成本。为了进一步说明所提出的框架的普遍适用性，我们将其应用于水平视场为190$^\circ$的广角鱼眼相机。训练框架UnRectDepthNet以相机失真模型为自变量，并相应地调整投影和非投影函数。在KITTI校正的数据集上进一步评估了所提出的算法，我们获得了最先进的结果，改进了我们之前的工作FisheyeDistanceNet。失真的测试场景视频序列的定性结果表明性能优异https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2007.06676v4" target="_blank">2007.06676v4</a>
                              </td>
                              <td>UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2020-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2007_06676v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2007.06676v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03660v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PQM: A Point Quality Evaluation Metric for Dense Maps</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03660v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based mapping/reconstruction are important for various applications, but evaluating the quality of the dense maps they produce is challenging. The current methods have limitations, including the inability to capture completeness, structural information, and local variations in error. In this paper, we propose a novel point quality evaluation metric (PQM) that consists of four sub-metrics to provide a more comprehensive evaluation of point cloud quality. The completeness sub-metric evaluates the proportion of missing data, the artifact score sub-metric recognizes and characterizes artifacts, the accuracy sub-metric measures registration accuracy, and the resolution sub-metric quantifies point cloud density. Through an ablation study using a prototype dataset, we demonstrate the effectiveness of each of the sub-metrics and compare them to popular point cloud distance measures. Using three LiDAR SLAM systems to generate maps, we evaluate their output map quality and demonstrate the metrics robustness to noise and artifacts. Our implementation of PQM, datasets and detailed documentation on how to integrate with your custom dense mapping pipeline can be found at github.com/droneslab/pqm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03660v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的地图绘制/重建对于各种应用都很重要，但评估它们生成的密集地图的质量是一项挑战。目前的方法有局限性，包括无法捕获完整性、结构信息和错误的局部变化。在本文中，我们提出了一种新的点质量评估度量（PQM），该度量由四个子度量组成，以提供对点云质量的更全面的评估。完整性子度量评估缺失数据的比例，伪影得分子度量识别和表征伪影，准确性子度量测量配准准确性，分辨率子度量量化点云密度。通过使用原型数据集进行消融研究，我们证明了每个子指标的有效性，并将其与流行的点云距离测量进行了比较。使用三个激光雷达SLAM系统生成地图，我们评估了它们的输出地图质量，并证明了它们对噪声和伪影的鲁棒性。我们的PQM实施、数据集以及关于如何与您的自定义密集映射管道集成的详细文档，请访问github.com/droneslab/PQM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03660v1" target="_blank">2306.03660v1</a>
                              </td>
                              <td>PQM: A Point Quality Evaluation Metric for Dense Maps</td>
                              <td>Yash Turkar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03660v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03660v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02395v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NICE-SLAM with Adaptive Feature Grids</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02395v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02395v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02395v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM is a dense visual SLAM system that combines the advantages of neural implicit representations and hierarchical grid-based scene representation. However, the hierarchical grid features are densely stored, leading to memory explosion problems when adapting the framework to large scenes. In our project, we present sparse NICE-SLAM, a sparse SLAM system incorporating the idea of Voxel Hashing into NICE-SLAM framework. Instead of initializing feature grids in the whole space, voxel features near the surface are adaptively added and optimized. Experiments demonstrated that compared to NICE-SLAM algorithm, our approach takes much less memory and achieves comparable reconstruction quality on the same datasets. Our implementation is available at https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02395v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM是一个密集的视觉SLAM系统，它结合了神经隐式表示和基于层次网格的场景表示的优点。然而，分层网格特征存储密集，导致在将框架适应大型场景时出现内存爆炸问题。在我们的项目中，我们提出了稀疏NICE-SLAM，这是一个稀疏SLAM系统，将体素哈希的思想结合到NICE-SLM框架中。自适应地添加和优化曲面附近的体素特征，而不是初始化整个空间中的特征网格。实验表明，与NICE-SLAM算法相比，我们的方法占用的内存要少得多，并且在相同的数据集上实现了相当的重建质量。我们的实施可在https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02395v1" target="_blank">2306.02395v1</a>
                              </td>
                              <td>NICE-SLAM with Adaptive Feature Grids</td>
                              <td>Ganlin Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02395v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02395v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01891v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01891v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a robust approach for a visual parallel tracking and mapping (PTAM) system that excels in challenging environments. Our proposed method combines the strengths of heterogeneous multi-modal visual sensors, including stereo event-based and frame-based sensors, in a unified reference frame through a novel spatio-temporal synchronization of stereo visual frames and stereo event streams. We employ deep learning-based feature extraction and description for estimation to enhance robustness further. We also introduce an end-to-end parallel tracking and mapping optimization layer complemented by a simple loop-closure algorithm for efficient SLAM behavior. Through comprehensive experiments on both small-scale and large-scale real-world sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM) demonstrates superior performance compared to state-of-the-art methods in terms of robustness and accuracy in adverse conditions. Our implementation's research-based Python API is publicly available on GitHub for further research and development: https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01891v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于视觉并行跟踪和映射（PTAM）系统的鲁棒方法，该系统在具有挑战性的环境中表现出色。我们提出的方法通过立体视觉帧和立体事件流的新颖时空同步，在统一的参考系中结合了异构多模态视觉传感器的优势，包括基于立体事件和基于帧的传感器。我们采用基于深度学习的特征提取和描述进行估计，以进一步增强鲁棒性。我们还引入了一个端到端并行跟踪和映射优化层，并辅以一个简单的闭环算法，以实现高效的SLAM行为。通过对VECtor和TUM-VIE基准的小规模和大规模真实世界序列的综合实验，我们提出的方法（DH-PTAM）在不利条件下的稳健性和准确性方面优于最先进的方法。我们的实现基于研究的Python API在GitHub上公开提供，用于进一步的研究和开发：https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01891v1" target="_blank">2306.01891v1</a>
                              </td>
                              <td>DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</td>
                              <td>Abanob Soliman</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01891v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01891v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01188v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Visual Odometry with Full Temporal Resolution via Continuous-time Gaussian Process Regression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01188v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01188v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01188v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras asynchronously capture individual visual changes in a scene. This makes them more robust than traditional frame-based cameras to highly dynamic motions and poor illumination. It also means that every measurement in a scene can occur at a unique time.   Handling these different measurement times is a major challenge of using event-based cameras. It is often addressed in visual odometry (VO) pipelines by approximating temporally close measurements as occurring at one common time. This grouping simplifies the estimation problem but sacrifices the inherent temporal resolution of event-based cameras.   This paper instead presents a complete stereo VO pipeline that estimates directly with individual event-measurement times without requiring any grouping or approximation. It uses continuous-time trajectory estimation to maintain the temporal fidelity and asynchronous nature of event-based cameras through Gaussian process regression with a physically motivated prior. Its performance is evaluated on the MVSEC dataset, where it achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences, outperforming the existing publicly available event-based stereo VO pipeline by two and four times, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01188v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的摄影机异步捕捉场景中的各个视觉变化。这使得它们比传统的基于帧的相机对高动态运动和较差的照明更具鲁棒性。这也意味着场景中的每一次测量都可以在唯一的时间发生。处理这些不同的测量时间是使用基于事件的相机的一个主要挑战。它通常在视觉里程计（VO）管道中通过将时间上的近距离测量近似为在一个公共时间发生来解决。这种分组简化了估计问题，但牺牲了基于事件的摄像机固有的时间分辨率。相反，本文提出了一个完整的立体VO管道，该管道直接估计单个事件的测量时间，而不需要任何分组或近似。它使用连续时间轨迹估计，通过具有物理动机先验的高斯过程回归来保持基于事件的相机的时间保真度和异步性质。它的性能在MVSEC数据集上进行了评估，在两个独立序列上实现了7.9e-3和5.9e-3 RMS相对误差，分别比现有的公开的基于事件的立体声VO流水线高出两倍和四倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01188v1" target="_blank">2306.01188v1</a>
                              </td>
                              <td>Event-based Visual Odometry with Full Temporal Resolution via Continuous-time Gaussian Process Regression</td>
                              <td>Jianeng Wang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01188v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01188v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01173v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01173v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01173v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01173v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford and Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01173v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了BAMF-SLAM，这是一种新颖的多鱼眼视觉惯性SLAM系统，它利用束平差（BA）和递归场变换（RFT）在具有挑战性的场景中实现精确和鲁棒的状态估计。首先，我们的系统直接对原始鱼眼图像进行操作，使我们能够充分利用鱼眼相机的宽视场（FoV）。其次，为了克服低纹理的挑战，我们通过统一的因子图探索了多相机输入和互补惯性测量的紧密耦合集成，并联合优化姿态和密集深度图。第三，为了全局一致性，鱼眼相机的宽FoV允许系统找到更多潜在的环路闭合，并且在RFT的宽收敛池的支持下，我们的系统可以执行非常宽的基线环路闭合，几乎没有重叠。此外，我们引入了一种半姿态图BA方法来避免昂贵的全全局BA。通过将相对姿态因子与闭环因子相结合，可以在保持高精度的同时，以适度的内存占用有效地调整全局状态。在TUM-VI、Hilti Oxford和Newer College数据集上的评估表明，与先前的工作相比，所提出的系统具有优越的性能。在2022年希尔蒂SLAM挑战赛中，我们的VIO版本获得了第二名。在随后的提交中，我们的完整系统，包括全球BA后端，优于获胜方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01173v1" target="_blank">2306.01173v1</a>
                              </td>
                              <td>BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</td>
                              <td>Wei Zhang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01173v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01173v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00579v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00579v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00579v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00579v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce FMapping, an efficient neural field mapping framework that facilitates the continuous estimation of a colorized point cloud map in real-time dense RGB SLAM. To achieve this challenging goal without depth, a hurdle is how to improve efficiency and reduce the mapping uncertainty of the RGB SLAM system. To this end, we first build up a theoretical analysis by decomposing the SLAM system into tracking and mapping parts, and the mapping uncertainty is explicitly defined within the frame of neural representations. Based on the analysis, we then propose an effective factorization scheme for scene representation and introduce a sliding window strategy to reduce the uncertainty for scene reconstruction. Specifically, we leverage the factorized neural field to decompose uncertainty into a lower-dimensional space, which enhances robustness to noise and improves training efficiency. We then propose the sliding window sampler to reduce uncertainty by incorporating coherent geometric cues from observed frames during map initialization to enhance convergence. Our factorized neural mapping approach enjoys some advantages, such as low memory consumption, more efficient computation, and fast convergence during map initialization. Experiments on two benchmark datasets show that our method can update the map of high-fidelity colorized point clouds around 2 seconds in real time while requiring no customized CUDA kernels. Additionally, it utilizes x20 fewer parameters than the most concise neural implicit mapping of prior methods for SLAM, e.g., iMAP [ 31] and around x1000 fewer parameters than the state-of-the-art approach, e.g., NICE-SLAM [ 42]. For more details, please refer to our project homepage: https://vlis2022.github.io/fmap/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00579v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了FMapping，这是一种高效的神经场映射框架，有助于在实时密集RGB SLAM中连续估计彩色点云图。为了在没有深度的情况下实现这一具有挑战性的目标，一个障碍是如何提高效率并减少RGB SLAM系统的映射不确定性。为此，我们首先通过将SLAM系统分解为跟踪和映射部分来建立理论分析，并在神经表示的框架内明确定义映射的不确定性。在此基础上，我们提出了一种有效的场景表示因子分解方案，并引入了一种滑动窗口策略来减少场景重建的不确定性。具体来说，我们利用因子化的神经场将不确定性分解到低维空间中，这增强了对噪声的鲁棒性，并提高了训练效率。然后，我们提出了滑动窗口采样器，通过在地图初始化期间结合来自观测帧的相干几何线索来提高收敛性，从而降低不确定性。我们的因子分解神经映射方法具有内存消耗低、计算效率高、映射初始化收敛快等优点。在两个基准数据集上的实验表明，我们的方法可以实时更新高保真彩色点云的地图约2秒，而不需要定制CUDA内核。此外，它使用的参数比SLAM的现有方法的最简洁的神经隐式映射（例如iMAP[31]）少x20个，比现有技术的方法（例如NICE-SLAM[42]）少大约x1000个。有关更多详细信息，请参阅我们的项目主页：https://vlis2022.github.io/fmap/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00579v1" target="_blank">2306.00579v1</a>
                              </td>
                              <td>FMapping: Factorized Efficient Neural Field Mapping for Real-Time Dense RGB SLAM</td>
                              <td>Tongyan Hua</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00579v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00579v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_12185v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implementation of a Blind navigation method in outdoors/indoors areas</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_12185v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_12185v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_12185v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>According to WHO statistics, the number of visually impaired people is increasing annually. One of the most critical necessities for visually impaired people is the ability to navigate safely. This paper proposes a navigation system based on the visual slam and Yolo algorithm using monocular cameras. The proposed system consists of three steps: obstacle distance estimation, path deviation detection, and next-step prediction. Using the ORB-SLAM algorithm, the proposed method creates a map from a predefined route and guides the users to stay on the route while notifying them if they deviate from it. Additionally, the system utilizes the YOLO algorithm to detect obstacles along the route and alert the user. The experimental results, obtained by using a laptop camera, show that the proposed system can run in 30 frame per second while guiding the user within predefined routes of 11 meters in indoors and outdoors. The accuracy of the positioning system is 8cm, and the system notifies the users if they deviate from the predefined route by more than 60 cm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_12185v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据世界卫生组织的统计，视障人数每年都在增加。视障人士最重要的必需品之一是安全导航的能力。本文提出了一种基于视觉slam和Yolo算法的单目相机导航系统。所提出的系统包括三个步骤：障碍物距离估计、路径偏差检测和下一步预测。该方法利用ORB-SLAM算法，根据预先定义的路线创建地图，引导用户停留在路线上，同时在偏离路线时通知他们。此外，该系统还利用YOLO算法检测路线上的障碍物并提醒用户。使用笔记本电脑摄像头获得的实验结果表明，所提出的系统可以以每秒30帧的速度运行，同时在室内外11米的预定路线内引导用户。定位系统的精度为8cm，如果用户偏离预定路线超过60cm，系统会通知用户。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.12185v2" target="_blank">2212.12185v2</a>
                              </td>
                              <td>Implementation of a Blind navigation method in outdoors/indoors areas</td>
                              <td>Mohammad Javadian Farzaneh</td>
                              <td>2022-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_12185v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.12185v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11654v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM: A Review On Last Decade</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11654v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11654v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11654v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article presents a novel review of Active SLAM (A-SLAM) research conducted in the last decade. We discuss the formulation, application, and methodology applied in A-SLAM for trajectory generation and control action selection using information theory based approaches. Our extensive qualitative and quantitative analysis highlights the approaches, scenarios, configurations, types of robots, sensor types, dataset usage, and path planning approaches of A-SLAM research. We conclude by presenting the limitations and proposing future research possibilities. We believe that this survey will be helpful to researchers in understanding the various methods and techniques applied to A-SLAM formulation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11654v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对近十年来进行的主动SLAM（a-SLAM）研究进行了新的综述。我们使用基于信息论的方法讨论了A-SLAM中用于轨迹生成和控制动作选择的公式、应用和方法。我们广泛的定性和定量分析强调了A-SLAM研究的方法、场景、配置、机器人类型、传感器类型、数据集使用和路径规划方法。最后，我们介绍了研究的局限性，并提出了未来研究的可能性。我们相信，这项调查将有助于研究人员了解应用于A-SLAM配方的各种方法和技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11654v2" target="_blank">2212.11654v2</a>
                              </td>
                              <td>Active SLAM: A Review On Last Decade</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11654v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11654v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17673v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OSPC: Online Sequential Photometric Calibration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17673v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17673v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17673v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Photometric calibration is essential to many computer vision applications. One of its key benefits is enhancing the performance of Visual SLAM, especially when it depends on a direct method for tracking, such as the standard KLT algorithm. Another advantage could be in retrieving the sensor irradiance values from measured intensities, as a pre-processing step for some vision algorithms, such as shape-from-shading. Current photometric calibration systems rely on a joint optimization problem and encounter an ambiguity in the estimates, which can only be resolved using ground truth information. We propose a novel method that solves for photometric parameters using a sequential estimation approach. Our proposed method achieves high accuracy in estimating all parameters; furthermore, the formulations are linear and convex, which makes the solution fast and suitable for online applications. Experiments on a Visual Odometry system validate the proposed method and demonstrate its advantages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17673v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光度校准对许多计算机视觉应用至关重要。它的一个关键好处是提高了Visual SLAM的性能，尤其是当它依赖于直接的跟踪方法时，例如标准的KLT算法。另一个优点可以是从测量的强度中检索传感器辐照度值，作为一些视觉算法的预处理步骤，例如来自阴影的形状。当前的光度校准系统依赖于联合优化问题，并且在估计中遇到模糊性，这只能使用地面实况信息来解决。我们提出了一种新的方法，使用顺序估计方法解决光度参数。我们提出的方法在估计所有参数方面实现了高精度；此外，该公式具有线性和凸性，使得求解速度快，适合在线应用。在视觉里程计系统上的实验验证了所提出的方法，并证明了其优点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17673v1" target="_blank">2305.17673v1</a>
                              </td>
                              <td>OSPC: Online Sequential Photometric Calibration</td>
                              <td>Jawad Haidar</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17673v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17673v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_03323v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_03323v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_03323v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_03323v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous vehicles must often contend with conflicting planning requirements, e.g., safety and comfort could be at odds with each other if avoiding a collision calls for slamming the brakes. To resolve such conflicts, assigning importance ranking to rules (i.e., imposing a rule hierarchy) has been proposed, which, in turn, induces rankings on trajectories based on the importance of the rules they satisfy. On one hand, imposing rule hierarchies can enhance interpretability, but introduce combinatorial complexity to planning; while on the other hand, differentiable reward structures can be leveraged by modern gradient-based optimization tools, but are less interpretable and unintuitive to tune. In this paper, we present an approach to equivalently express rule hierarchies as differentiable reward structures amenable to modern gradient-based optimizers, thereby, achieving the best of both worlds. We achieve this by formulating rank-preserving reward functions that are monotonic in the rank of the trajectories induced by the rule hierarchy; i.e., higher ranked trajectories receive higher reward. Equipped with a rule hierarchy and its corresponding rank-preserving reward function, we develop a two-stage planner that can efficiently resolve conflicting planning requirements. We demonstrate that our approach can generate motion plans in ~7-10 Hz for various challenging road navigation and intersection negotiation scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_03323v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动驾驶汽车必须经常应对相互冲突的规划要求，例如，如果避免碰撞需要急刹车，安全性和舒适性可能会相互矛盾。为了解决这种冲突，已经提出了将重要性排序分配给规则（即，强加规则层次结构），这反过来又根据它们所满足的规则的重要性对轨迹进行排序。一方面，强加规则层次结构可以提高可解释性，但会给规划带来组合复杂性；而另一方面，现代基于梯度的优化工具可以利用可微的奖励结构，但其可解释性较差，难以调整。在本文中，我们提出了一种方法，将规则层次等效地表示为适用于现代基于梯度的优化器的可微奖励结构，从而实现两全其美。我们通过制定秩保持奖励函数来实现这一点，该函数在由规则层次引起的轨迹的秩中是单调的；即排名更高的轨迹获得更高的奖励。配备了规则层次结构及其相应的保秩奖励函数，我们开发了一个两阶段规划器，可以有效地解决冲突的规划需求。我们证明，我们的方法可以为各种具有挑战性的道路导航和交叉口协商场景生成7-10Hz的运动计划。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.03323v2" target="_blank">2212.03323v2</a>
                              </td>
                              <td>Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</td>
                              <td>Sushant Veer</td>
                              <td>2022-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_03323v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.03323v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_04286v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Simulation of Dynamic Environments for SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_04286v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_04286v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_04286v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simulation engines are widely adopted in robotics. However, they lack either full simulation control, ROS integration, realistic physics, or photorealism. Recently, synthetic data generation and realistic rendering has advanced tasks like target tracking and human pose estimation. However, when focusing on vision applications, there is usually a lack of information like sensor measurements or time continuity. On the other hand, simulations for most robotics tasks are performed in (semi)static environments, with specific sensors and low visual fidelity. To solve this, we introduced in our previous work a fully customizable framework for generating realistic animated dynamic environments (GRADE) [1]. We use GRADE to generate an indoor dynamic environment dataset and then compare multiple SLAM algorithms on different sequences. By doing that, we show how current research over-relies on known benchmarks, failing to generalize. Our tests with refined YOLO and Mask R-CNN models provide further evidence that additional research in dynamic SLAM is necessary. The code, results, and generated data are provided as open-source at https://eliabntt.github.io/grade-rrSimulation of Dynamic Environments for SLAM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_04286v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>仿真引擎在机器人技术中被广泛采用。然而，它们要么缺乏完全的模拟控制，ROS集成，逼真的物理，要么缺乏真实感。近年来，合成数据生成和真实感绘制具有诸如目标跟踪和人体姿态估计等高级任务。然而，当专注于视觉应用时，通常缺乏传感器测量或时间连续性等信息。另一方面，大多数机器人任务的模拟都是在（半）静态环境中进行的，具有特定的传感器和低视觉保真度。为了解决这个问题，我们在之前的工作中引入了一个完全可定制的框架，用于生成逼真的动画动态环境（GRADE）[1]。我们使用GRADE生成室内动态环境数据集，然后比较不同序列上的多种SLAM算法。通过这样做，我们展示了当前的研究是如何过度依赖于已知的基准，而不能概括。我们用改进的YOLO和Mask R-CNN模型进行的测试提供了进一步的证据，表明有必要对动态SLAM进行额外的研究。代码、结果和生成的数据以开源形式提供，网址为https://eliabntt.github.io/grade-rrSimulationSLAM的动态环境</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.04286v2" target="_blank">2305.04286v2</a>
                              </td>
                              <td>Simulation of Dynamic Environments for SLAM</td>
                              <td>Elia Bonetto</td>
                              <td>2023-05-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_04286v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.04286v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14959v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UAV Trajectory Optimization and Tracking for User Localization in Wireless Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14959v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14959v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14959v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we investigate the problem of UAV-aided user localization in wireless networks. Unlike the existing works, we do not assume perfect knowledge of the UAV location, hence we not only need to localize the users but also to track the UAV location. To do so, we utilize the time-of-arrival along with received signal strength radio measurements collected from users using a UAV. A simultaneous localization and mapping (SLAM) framework building on the Expectation-Maximization-based least-squares method is proposed to classify measurements into line-of-sight or non-line-of-sight categories and learn the radio channel, and at the same, localize the users and track the UAV. This framework also allows us to exploit other types of measurements such as the rough estimate of the UAV location available from GPS, and the UAV velocity measured by an inertial measurement unit (IMU) on-board, to achieve better localization accuracy. Moreover, the trajectory of the UAV is optimized which brings considerable improvement to the localization performance. The simulations show the out-performance of the developed algorithm when compared to other approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14959v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了无线网络中无人机辅助用户定位的问题。与现有的工作不同，我们没有假设对无人机位置有完美的了解，因此我们不仅需要定位用户，还需要跟踪无人机位置。为此，我们利用无人机从用户那里收集的到达时间以及接收信号强度无线电测量值。在基于期望最大化的最小二乘法的基础上，提出了一种同时定位和映射（SLAM）框架，将测量分为视线或非视线类别，并学习无线电信道，同时定位用户并跟踪无人机。该框架还允许我们利用其他类型的测量，如GPS对无人机位置的粗略估计，以及机载惯性测量单元（IMU）测量的无人机速度，以实现更好的定位精度。此外，对无人机的轨迹进行了优化，大大提高了定位性能。仿真结果表明，与其他方法相比，所开发的算法具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14959v1" target="_blank">2305.14959v1</a>
                              </td>
                              <td>UAV Trajectory Optimization and Tracking for User Localization in Wireless Networks</td>
                              <td>Omid Esrafilian</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14959v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14959v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14885v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards View-invariant and Accurate Loop Detection Based on Scene Graph</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14885v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14885v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14885v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop detection plays a key role in visual Simultaneous Localization and Mapping (SLAM) by correcting the accumulated pose drift. In indoor scenarios, the richly distributed semantic landmarks are view-point invariant and hold strong descriptive power in loop detection. The current semantic-aided loop detection embeds the topology between semantic instances to search a loop. However, current semantic-aided loop detection methods face challenges in dealing with ambiguous semantic instances and drastic viewpoint differences, which are not fully addressed in the literature. This paper introduces a novel loop detection method based on an incrementally created scene graph, targeting the visual SLAM at indoor scenes. It jointly considers the macro-view topology, micro-view topology, and occupancy of semantic instances to find correct correspondences. Experiments using handheld RGB-D sequence show our method is able to accurately detect loops in drastically changed viewpoints. It maintains a high precision in observing objects with similar topology and appearance. Our method also demonstrates that it is robust in changed indoor scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14885v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环路检测通过校正累积的姿态漂移，在视觉同步定位和映射（SLAM）中发挥着关键作用。在室内场景中，丰富分布的语义地标是视点不变的，并且在循环检测中具有强大的描述能力。当前的语义辅助循环检测将拓扑嵌入语义实例之间以搜索循环。然而，当前的语义辅助循环检测方法在处理模糊的语义实例和激烈的观点差异方面面临挑战，而这些问题在文献中没有得到充分解决。本文介绍了一种新的基于增量创建场景图的循环检测方法，针对室内场景中的视觉SLAM。它联合考虑宏视图拓扑、微观视图拓扑和语义实例的占用，以找到正确的对应关系。使用手持RGB-D序列的实验表明，我们的方法能够在急剧变化的视角下准确地检测环路。它在观察拓扑结构和外观相似的物体时保持了高精度。我们的方法还证明了它在变化的室内场景中是稳健的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14885v1" target="_blank">2305.14885v1</a>
                              </td>
                              <td>Towards View-invariant and Accurate Loop Detection Based on Scene Graph</td>
                              <td>Chuhao Liu</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14885v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14885v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14773v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Imaging Sonar-based Place Recognition and Localization in Underwater Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14773v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14773v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14773v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Place recognition using SOund Navigation and Ranging (SONAR) images is an important task for simultaneous localization and mapping(SLAM) in underwater environments. This paper proposes a robust and efficient imaging SONAR based place recognition, SONAR context, and loop closure method. Unlike previous methods, our approach encodes geometric information based on the characteristics of raw SONAR measurements without prior knowledge or training. We also design a hierarchical searching procedure for fast retrieval of candidate SONAR frames and apply adaptive shifting and padding to achieve robust matching on rotation and translation changes. In addition, we can derive the initial pose through adaptive shifting and apply it to the iterative closest point (ICP) based loop closure factor. We evaluate the performance of SONAR context in the various underwater sequences such as simulated open water, real water tank, and real underwater environments. The proposed approach shows the robustness and improvements of place recognition on various datasets and evaluation metrics. Supplementary materials are available at https://github.com/sparolab/sonar_context.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14773v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用声纳图像进行位置识别是水下环境中同时定位和测绘的一项重要任务。本文提出了一种稳健高效的基于SONAR的成像位置识别、SONAR上下文和闭环方法。与以前的方法不同，我们的方法在没有先验知识或训练的情况下，根据原始SONAR测量的特征对几何信息进行编码。我们还设计了一种用于快速检索候选SONAR帧的分层搜索过程，并应用自适应移位和填充来实现对旋转和平移变化的鲁棒匹配。此外，我们可以通过自适应移位导出初始姿态，并将其应用于基于迭代最近点（ICP）的闭环因子。我们评估了SONAR上下文在各种水下序列中的性能，如模拟开放水域、真实水箱和真实水下环境。所提出的方法显示了位置识别在各种数据集和评估指标上的稳健性和改进。补充材料可在https://github.com/sparolab/sonar_context.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14773v1" target="_blank">2305.14773v1</a>
                              </td>
                              <td>Robust Imaging Sonar-based Place Recognition and Localization in Underwater Environments</td>
                              <td>Hogyun Kim</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14773v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14773v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_04726v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Patch Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_04726v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_04726v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_04726v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose Deep Patch Visual Odometry (DPVO), a new deep learning system for monocular Visual Odometry (VO). DPVO uses a novel recurrent network architecture designed for tracking image patches across time. Recent approaches to VO have significantly improved the state-of-the-art accuracy by using deep networks to predict dense flow between video frames. However, using dense flow incurs a large computational cost, making these previous methods impractical for many use cases. Despite this, it has been assumed that dense flow is important as it provides additional redundancy against incorrect matches. DPVO disproves this assumption, showing that it is possible to get the best accuracy and efficiency by exploiting the advantages of sparse patch-based matching over dense flow. DPVO introduces a novel recurrent update operator for patch based correspondence coupled with differentiable bundle adjustment. On Standard benchmarks, DPVO outperforms all prior work, including the learning-based state-of-the-art VO-system (DROID) using a third of the memory while running 3x faster on average. Code is available at https://github.com/princeton-vl/DPVO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_04726v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的单目视觉Odometry（VO）深度学习系统——深度补丁视觉Odomemetry（DPVO）。DPVO使用了一种新颖的递归网络架构，用于跨时间跟踪图像补丁。最近的VO方法通过使用深度网络来预测视频帧之间的密集流，显著提高了现有技术的准确性。然而，使用密集流会产生巨大的计算成本，使得这些先前的方法在许多用例中不切实际。尽管如此，人们还是认为密集流很重要，因为它为不正确的匹配提供了额外的冗余。DPVO反驳了这一假设，表明通过利用基于稀疏补丁的匹配相对于密集流的优势，可以获得最佳的精度和效率。DPVO为基于补丁的对应关系引入了一种新的递归更新算子，并结合了可微丛调整。在标准基准测试中，DPVO的性能优于所有先前的工作，包括使用三分之一内存的基于学习的最先进VO系统（DROID），同时平均运行速度快3倍。代码位于https://github.com/princeton-vl/DPVO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.04726v2" target="_blank">2208.04726v2</a>
                              </td>
                              <td>Deep Patch Visual Odometry</td>
                              <td>Zachary Teed</td>
                              <td>2022-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_04726v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.04726v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13635v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploiting Radio Fingerprints for Simultaneous Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13635v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13635v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13635v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is paramount for unmanned systems to achieve self-localization and navigation. It is challenging to perform SLAM in large environments, due to sensor limitations, complexity of the environment, and computational resources. We propose a novel approach for localization and mapping of autonomous vehicles using radio fingerprints, for example WiFi (Wireless Fidelity) or LTE (Long Term Evolution) radio features, which are widely available in the existing infrastructure. In particular, we present two solutions to exploit the radio fingerprints for SLAM. In the first solution-namely Radio SLAM, the output is a radio fingerprint map generated using SLAM technique. In the second solution-namely Radio+LiDAR SLAM, we use radio fingerprint to assist conventional LiDAR-based SLAM to improve accuracy and speed, while generating the occupancy map. We demonstrate the effectiveness of our system in three different environments, namely outdoor, indoor building, and semi-indoor environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13635v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）对于无人系统实现自我定位和导航至关重要。由于传感器的限制、环境的复杂性和计算资源，在大型环境中执行SLAM具有挑战性。我们提出了一种使用无线电指纹定位和映射自动驾驶汽车的新方法，例如WiFi（无线保真）或LTE（长期演进）无线电功能，这些功能在现有基础设施中广泛可用。特别地，我们提出了两种利用无线电指纹进行SLAM的解决方案。在第一种解决方案，即无线电SLAM中，输出是使用SLAM技术生成的无线电指纹图。在第二个解决方案，即Radio+LiDAR SLAM中，我们使用无线电指纹来辅助传统的基于LiDAR的SLAM，以提高准确性和速度，同时生成占用图。我们展示了我们的系统在三种不同环境中的有效性，即室外、室内建筑和半室内环境。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13635v1" target="_blank">2305.13635v1</a>
                              </td>
                              <td>Exploiting Radio Fingerprints for Simultaneous Localization and Mapping</td>
                              <td>Ran Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13635v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13635v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14392v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FEDORA: Flying Event Dataset fOr Reactive behAvior</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14392v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14392v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14392v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a first-of-its-kind fully synthetic dataset for vision-based tasks, with ground truths for depth, pose, ego-motion, and optical flow. FEDORA is the first dataset to provide optical flow at three different frequencies - 10Hz, 25Hz, and 50Hz</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14392v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生物体在飞行中用极少数神经元进行复杂的高速操纵的能力和令人难以置信的低故障率突出了这些资源受限的生物系统的功效。近年来，事件驱动硬件已成为在资源受限的环境中实现复杂视觉任务的一种很有前途的途径。基于视觉的自主导航和避障由几个独立但相关的任务组成，如光流估计、深度估计、同时定位和映射（SLAM）、物体检测和识别。为了确保这些任务之间的一致性，必须在单个数据集上对它们进行训练。然而，大多数现有数据集仅提供所需数据的选定子集。这使得网络间的一致性难以实现。现有数据集的另一个限制是它们提供的时间分辨率有限。为了解决这些限制，我们提出了FEDORA，这是第一个用于基于视觉的任务的完全合成数据集，具有深度、姿势、自我运动和光流的基本事实。FEDORA是第一个提供三种不同频率（10Hz、25Hz和50Hz）光流的数据集</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14392v1" target="_blank">2305.14392v1</a>
                              </td>
                              <td>FEDORA: Flying Event Dataset fOr Reactive behAvior</td>
                              <td>Amogh Joshi</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14392v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14392v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13418v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WiROS: WiFi sensing toolbox for robotics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13418v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13418v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13418v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many recent works have explored using WiFi-based sensing to improve SLAM, robot manipulation, or exploration. Moreover, widespread availability makes WiFi the most advantageous RF signal to leverage. But WiFi sensors lack an accurate, tractable, and versatile toolbox, which hinders their widespread adoption with robot's sensor stacks.   We develop WiROS to address this immediate need, furnishing many WiFi-related measurements as easy-to-consume ROS topics. Specifically, WiROS is a plug-and-play WiFi sensing toolbox providing access to coarse-grained WiFi signal strength (RSSI), fine-grained WiFi channel state information (CSI), and other MAC-layer information (device address, packet id's or frequency-channel information). Additionally, WiROS open-sources state-of-art algorithms to calibrate and process WiFi measurements to furnish accurate bearing information for received WiFi signals. The open-sourced repository is: https://github.com/ucsdwcsng/WiROS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13418v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的许多工作都探索了使用基于WiFi的传感来改进SLAM、机器人操纵或探索。此外，广泛的可用性使WiFi成为最有利的RF信号。但WiFi传感器缺乏准确、易操作和通用的工具箱，这阻碍了它们在机器人传感器堆栈中的广泛应用。我们开发了WiROS来满足这一迫切需求，提供了许多与WiFi相关的测量，作为易于消费的ROS主题。具体而言，WiROS是一个即插即用的WiFi传感工具箱，提供对粗粒度WiFi信号强度（RSSI）、细粒度WiFi信道状态信息（CSI）和其他MAC层信息（设备地址、分组id或频率信道信息）的访问。此外，WiROS开源现有技术的算法来校准和处理WiFi测量，以提供接收到的WiFi信号的准确方位信息。开源存储库是：https://github.com/ucsdwcsng/WiROS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13418v1" target="_blank">2305.13418v1</a>
                              </td>
                              <td>WiROS: WiFi sensing toolbox for robotics</td>
                              <td>William Hunter</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13418v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13418v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13147v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PALoc: Robust Prior-assisted Trajectory Generation for Benchmarking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13147v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13147v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13147v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Evaluating simultaneous localization and mapping (SLAM) algorithms necessitates high-precision and dense ground truth (GT) trajectories. But obtaining desirable GT trajectories is sometimes challenging without GT tracking sensors. As an alternative, in this paper, we propose a novel prior-assisted SLAM system to generate a full six-degree-of-freedom ($6$-DOF) trajectory at around $10$Hz for benchmarking under the framework of the factor graph. Our degeneracy-aware map factor utilizes a prior point cloud map and LiDAR frame for point-to-plane optimization, simultaneously detecting degeneration cases to reduce drift and enhancing the consistency of pose estimation. Our system is seamlessly integrated with cutting-edge odometry via a loosely coupled scheme to generate high-rate and precise trajectories. Moreover, we propose a norm-constrained gravity factor for stationary cases, optimizing pose and gravity to boost performance. Extensive evaluations demonstrate our algorithm's superiority over existing SLAM or map-based methods in diverse scenarios in terms of precision, smoothness, and robustness. Our approach substantially advances reliable and accurate SLAM evaluation methods, fostering progress in robotics research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13147v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>评估同时定位和映射（SLAM）算法需要高精度和密集的地面实况（GT）轨迹。但是，在没有GT跟踪传感器的情况下，获得期望的GT轨迹有时是具有挑战性的。作为一种替代方案，在本文中，我们提出了一种新的先验辅助SLAM系统，以在因子图的框架下生成约为$10$Hz的全六自由度（$6$-DOF）轨迹，用于基准测试。我们的退化感知映射因子利用先前的点云映射和激光雷达帧进行点到平面优化，同时检测退化情况以减少漂移并增强姿态估计的一致性。我们的系统通过松散耦合方案与尖端里程计无缝集成，以生成高速率和精确的轨迹。此外，我们为静止情况提出了一个范数约束的重力因子，优化姿态和重力以提高性能。广泛的评估表明，在不同的场景中，我们的算法在精度、平滑度和鲁棒性方面优于现有的SLAM或基于地图的方法。我们的方法大大推进了可靠和准确的SLAM评估方法，促进了机器人研究的进展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13147v1" target="_blank">2305.13147v1</a>
                              </td>
                              <td>PALoc: Robust Prior-assisted Trajectory Generation for Benchmarking</td>
                              <td>Xiangcheng Hu</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13147v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06557v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active View Planning for Visual SLAM in Outdoor Environments Based on Continuous Information Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06557v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06557v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06557v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The visual simultaneous localization and mapping(vSLAM) is widely used in GPS-denied and open field environments for ground and surface robots. However, due to the frequent perception failures derived from lacking visual texture or the {swing} of robot view direction on rough terrains, the accuracy and robustness of vSLAM are still to be enhanced. The study develops a novel view planning approach of actively perceiving areas with maximal information to address the mentioned problem; a gimbal camera is used as the main sensor. Firstly, a map representation based on feature distribution-weighted Fisher information is proposed to completely and effectively represent environmental information richness. With the map representation, a continuous environmental information model is further established to convert the discrete information space into a continuous one for numerical optimization in real-time. Subsequently, the receding horizon optimization is utilized to obtain the optimal informative viewpoints with simultaneously considering the robotic perception, exploration and motion cost based on the continuous environmental model. Finally, several simulations and outdoor experiments are performed to verify the improvement of localization robustness and accuracy by the proposed approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06557v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉同步定位与映射（vSLAM）被广泛应用于地面和地面机器人的GPS拒绝和开阔场地环境中。然而，由于缺乏视觉纹理或机器人在粗糙地形上观察方向的{摆动}而导致的频繁感知失败，vSLAM的准确性和鲁棒性仍有待提高。该研究开发了一种新的视图规划方法，即主动感知具有最大信息的区域，以解决上述问题；万向节照相机被用作主传感器。首先，提出了一种基于特征分布加权Fisher信息的地图表示方法，以完整有效地表示环境信息的丰富度。利用地图表示，进一步建立了连续的环境信息模型，将离散的信息空间转换为连续的信息空间，实时进行数值优化。随后，在连续环境模型的基础上，利用后退视界优化来获得最佳信息视点，同时考虑机器人的感知、探索和运动成本。最后，通过仿真和户外实验验证了该方法在定位鲁棒性和准确性方面的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06557v2" target="_blank">2211.06557v2</a>
                              </td>
                              <td>Active View Planning for Visual SLAM in Outdoor Environments Based on Continuous Information Modeling</td>
                              <td>Zhihao Wang</td>
                              <td>2022-11-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06557v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06557v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12669v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Angle-based SLAM on 5G mmWave Systems: Design, Implementation, and Measurement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12669v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12669v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12669v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is a key technology that provides user equipment (UE) tracking and environment mapping services, enabling the deep integration of sensing and communication. The millimeter-wave (mmWave) communication, with its larger bandwidths and antenna arrays, inherently facilitates more accurate delay and angle measurements than sub-6 GHz communication, thereby providing opportunities for SLAM. However, none of the existing works have realized the SLAM function under the 5G New Radio (NR) standard due to specification and hardware constraints. In this study, we investigate how 5G mmWave communication systems can achieve situational awareness without changing the transceiver architecture and 5G NR standard. We implement 28 GHz mmWave transceivers that deploy OFDM-based 5G NR waveform with 160 MHz channel bandwidth, and we realize beam management following the 5G NR. Furthermore, we develop an efficient successive cancellation-based angle extraction approach to obtain angles of arrival and departure from the reference signal received power measurements. On the basis of angle measurements, we propose an angle-only SLAM algorithm to track UE and map features in the radio environment. Thorough experiments and ray tracing-based computer simulations verify that the proposed angle-based SLAM can achieve sub-meter level localization and mapping accuracy with a single base station and without the requirement of strict time synchronization. Our experiments also reveal many propagation properties critical to the success of SLAM in 5G mmWave communication systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12669v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是提供用户设备（UE）跟踪和环境映射服务的关键技术，实现了传感和通信的深度集成。毫米波通信具有更大的带宽和天线阵列，与低于6GHz的通信相比，固有地促进了更精确的延迟和角度测量，从而为SLAM提供了机会。然而，由于规范和硬件的限制，现有的工作都没有实现5G新无线电（NR）标准下的SLAM功能。在这项研究中，我们研究了5G毫米波通信系统如何在不改变收发器架构和5G NR标准的情况下实现态势感知。我们实现了28 GHz毫米波收发器，该收发器部署了具有160 MHz信道带宽的基于OFDM的5G NR波形，并在5G NR之后实现了波束管理。此外，我们开发了一种高效的基于连续消除的角度提取方法，以从参考信号接收功率测量中获得到达角和离开角。在角度测量的基础上，我们提出了一种仅角度的SLAM算法来跟踪UE并映射无线电环境中的特征。经过深入的实验和基于射线追踪的计算机仿真验证，所提出的基于角度的SLAM可以在单个基站的情况下实现亚米级的定位和映射精度，而不需要严格的时间同步。我们的实验还揭示了许多对SLAM在5G毫米波通信系统中的成功至关重要的传播特性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12669v1" target="_blank">2305.12669v1</a>
                              </td>
                              <td>Angle-based SLAM on 5G mmWave Systems: Design, Implementation, and Measurement</td>
                              <td>Jie Yang</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12669v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_10029v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextSLAM: Visual SLAM with Semantic Planar Text Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_10029v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_10029v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_10029v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel visual SLAM method that integrates text objects tightly by treating them as semantic features via fully exploring their geometric and semantic prior. The text object is modeled as a texture-rich planar patch whose semantic meaning is extracted and updated on the fly for better data association. With the full exploration of locally planar characteristics and semantic meaning of text objects, the SLAM system becomes more accurate and robust even under challenging conditions such as image blurring, large viewpoint changes, and significant illumination variations (day and night). We tested our method in various scenes with the ground truth data. The results show that integrating texture features leads to a more superior SLAM system that can match images across day and night. The reconstructed semantic 3D text map could be useful for navigation and scene understanding in robotic and mixed reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_10029v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉SLAM方法，该方法通过充分探索文本对象的几何和语义先验，将文本对象视为语义特征，从而紧密地集成文本对象。文本对象被建模为纹理丰富的平面补丁，其语义被实时提取和更新以获得更好的数据关联。随着对文本对象局部平面特征和语义的充分探索，即使在图像模糊、大的视点变化和显著的光照变化（白天和晚上）等具有挑战性的条件下，SLAM系统也变得更加准确和稳健。我们用地面实况数据在各种场景中测试了我们的方法。结果表明，集成纹理特征可以获得更优越的SLAM系统，该系统可以匹配昼夜图像。重建的语义3D文本图可用于机器人和混合现实应用中的导航和场景理解。我们的项目页面：https://github.com/SJTU-ViSYS/TextSLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.10029v1" target="_blank">2305.10029v1</a>
                              </td>
                              <td>TextSLAM: Visual SLAM with Semantic Planar Text Features</td>
                              <td>Boying Li</td>
                              <td>2023-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_10029v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.10029v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_09295v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Graph-based Global Robot Simultaneous Localization and Mapping using Architectural Plans</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_09295v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_09295v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_09295v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a solution for graph-based global robot simultaneous localization and mapping (SLAM) using architectural plans. Before the start of the robot operation, the previously available architectural plan of the building is converted into our proposed architectural graph (A-Graph). When the robot starts its operation, it uses its onboard LIDAR and odometry to carry out an online SLAM relying on our situational graph (S-Graph), which includes both, a representation of the environment with multiple levels of abstractions, such as walls or rooms, and their relationships, as well as the robot poses with their associated keyframes. Our novel graph-to-graph matching method is used to relate the aforementioned S-Graph and A-Graph, which are aligned and merged, resulting in our novel informed Situational Graph (iS-Graph). Our iS-Graph not only provides graph-based global robot localization, but it extends the graph-based SLAM capabilities of the S-Graph by incorporating into it the prior knowledge of the environment existing in the architectural plan</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_09295v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种使用架构计划的基于图的全局机器人同时定位和映射（SLAM）的解决方案。在机器人操作开始之前，将先前可用的建筑平面图转换为我们提出的建筑图（A-graph）。当机器人开始运行时，它会使用机载激光雷达和里程计，根据我们的情景图（S-graph）进行在线SLAM，其中包括这两者，即具有多个抽象层次的环境表示，如墙壁或房间，以及它们的关系，以及机器人与相关关键帧的姿势。我们新的图到图匹配方法用于关联前面提到的S图和A图，它们被对齐和合并，从而产生了我们新的知情情景图（is图）。我们的iS-Graph不仅提供了基于图形的全局机器人定位，而且通过将体系结构规划中现有环境的先验知识纳入其中，扩展了S-Graph的基于图形的SLAM功能</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.09295v1" target="_blank">2305.09295v1</a>
                              </td>
                              <td>Graph-based Global Robot Simultaneous Localization and Mapping using Architectural Plans</td>
                              <td>Muhammad Shaheer</td>
                              <td>2023-05-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_09295v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.09295v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08962v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08962v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08962v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08962v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our paper proposes a direct sparse visual odometry method that combines event and RGB-D data to estimate the pose of agile-legged robots during dynamic locomotion and acrobatic behaviors. Event cameras offer high temporal resolution and dynamic range, which can eliminate the issue of blurred RGB images during fast movements. This unique strength holds a potential for accurate pose estimation of agile-legged robots, which has been a challenging problem to tackle. Our framework leverages the benefits of both RGB-D and event cameras to achieve robust and accurate pose estimation, even during dynamic maneuvers such as jumping and landing a quadruped robot, the Mini-Cheetah. Our major contributions are threefold: Firstly, we introduce an adaptive time surface (ATS) method that addresses the whiteout and blackout issue in conventional time surfaces by formulating pixel-wise decay rates based on scene complexity and motion speed. Secondly, we develop an effective pixel selection method that directly samples from event data and applies sample filtering through ATS, enabling us to pick pixels on distinct features. Lastly, we propose a nonlinear pose optimization formula that simultaneously performs 3D-2D alignment on both RGB-based and event-based maps and images, allowing the algorithm to fully exploit the benefits of both data streams. We extensively evaluate the performance of our framework on both public datasets and our own quadruped robot dataset, demonstrating its effectiveness in accurately estimating the pose of agile robots during dynamic movements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08962v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的论文提出了一种直接的稀疏视觉里程计方法，该方法结合事件和RGB-D数据来估计敏捷腿机器人在动态运动和杂技行为中的姿态。事件摄像机提供高时间分辨率和动态范围，可以消除快速移动过程中RGB图像模糊的问题。这种独特的力量为敏捷腿机器人的精确姿态估计提供了潜力，这一直是一个具有挑战性的问题。我们的框架利用RGB-D和事件相机的优势，即使在跳跃和降落四足机器人Mini Cheetah等动态动作过程中，也能实现稳健准确的姿态估计。我们的主要贡献有三方面：首先，我们介绍了一种自适应时间曲面（ATS）方法，该方法通过基于场景复杂性和运动速度制定逐像素衰减率来解决传统时间曲面中的白化和停电问题。其次，我们开发了一种有效的像素选择方法，该方法直接从事件数据中采样，并通过ATS应用样本滤波，使我们能够在不同特征上提取像素。最后，我们提出了一个非线性姿态优化公式，该公式同时对基于RGB和基于事件的地图和图像执行3D-2D对齐，使算法能够充分利用这两种数据流的优势。我们在公共数据集和我们自己的四足机器人数据集上广泛评估了我们的框架的性能，证明了它在准确估计敏捷机器人动态运动过程中的姿态方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08962v1" target="_blank">2305.08962v1</a>
                              </td>
                              <td>Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface</td>
                              <td>Shifan Zhu</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08962v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08962v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_09825v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hilti-Oxford Dataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_09825v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_09825v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_09825v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization and Mapping (SLAM) is being deployed in real-world applications, however many state-of-the-art solutions still struggle in many common scenarios. A key necessity in progressing SLAM research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-Oxford Dataset, to push state-of-the-art SLAM systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassical building with fine details and curved surfaces. To encourage multi-modal SLAM approaches, we designed a data collection platform featuring a lidar, five cameras, and an IMU (Inertial Measurement Unit). With the goal of benchmarking SLAM algorithms for tasks where accuracy and robustness are paramount, we implemented a novel ground truth collection method that enables our dataset to accurately measure SLAM pose errors with millimeter accuracy. To further ensure accuracy, the extrinsics of our platform were verified with a micrometer-accurate scanner, and temporal calibration was managed online using hardware time synchronization. The multi-modality and diversity of our dataset attracted a large field of academic and industrial researchers to enter the second edition of the Hilti SLAM challenge, which concluded in June 2022. The results of the challenge show that while the top three teams could achieve an accuracy of 2cm or better for some sequences, the performance dropped off in more difficult sequences.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_09825v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）正被部署在现实世界的应用程序中，然而许多最先进的解决方案在许多常见场景中仍然很困难。推进SLAM研究的一个关键必要条件是提供高质量的数据集和公平透明的基准测试。为此，我们创建了Hilti Oxford数据集，将最先进的SLAM系统推向极限。该数据集面临着各种挑战，从稀疏而规则的建筑工地到拥有精细细节和曲面的17世纪新古典主义建筑。为了鼓励多模态SLAM方法，我们设计了一个数据收集平台，该平台具有一个激光雷达、五个摄像头和一个IMU（惯性测量单元）。为了将SLAM算法用于精度和鲁棒性至关重要的任务，我们实现了一种新的地面实况收集方法，使我们的数据集能够以毫米精度准确测量SLAM姿态误差。为了进一步确保准确性，我们的平台的外部特性通过微米精度扫描仪进行了验证，并使用硬件时间同步在线管理时间校准。我们数据集的多模态和多样性吸引了大批学术和工业研究人员参加2022年6月结束的第二届Hilti SLAM挑战赛。挑战的结果表明，虽然排名前三的球队在某些序列中可以达到2厘米或更好的精度，但在更困难的序列中，表现有所下降。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.09825v3" target="_blank">2208.09825v3</a>
                              </td>
                              <td>Hilti-Oxford Dataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping</td>
                              <td>Lintong Zhang</td>
                              <td>2022-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_09825v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.09825v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_03102v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Open World NeRF-Based SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_03102v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_03102v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_03102v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) offer versatility and robustness in map representations for Simultaneous Localization and Mapping (SLAM) tasks. This paper extends NICE-SLAM, a recent state-of-the-art NeRF-based SLAM algorithm capable of producing high quality NeRF maps. However, depending on the hardware used, the required number of iterations to produce these maps often makes NICE-SLAM run at less than real time. Additionally, the estimated trajectories fail to be competitive with classical SLAM approaches. Finally, NICE-SLAM requires a grid covering the considered environment to be defined prior to runtime, making it difficult to extend into previously unseen scenes. This paper seeks to make NICE-SLAM more open-world-capable by improving the robustness and tracking accuracy, and generalizing the map representation to handle unconstrained environments. This is done by improving measurement uncertainty handling, incorporating motion information, and modelling the map as having an explicit foreground and background. It is shown that these changes are able to improve tracking accuracy by 85% to 97% depending on the available resources, while also improving mapping in environments with visual information extending outside of the predefined grid.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_03102v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）为同时定位和映射（SLAM）任务提供了地图表示的多功能性和鲁棒性。本文扩展了NICE-SLAM，这是一种最新的基于NeRF的SLAM算法，能够生成高质量的NeRF地图。然而，根据所使用的硬件，生成这些映射所需的迭代次数通常会使NICE-SLAM的运行速度低于实时性。此外，估计的轨迹不能与经典的SLAM方法相竞争。最后，NICE-SLAM要求在运行时之前定义一个覆盖所考虑环境的网格，这使得它很难扩展到以前看不见的场景中。本文试图通过提高鲁棒性和跟踪精度，并将地图表示推广到处理无约束环境，使NICE-SLAM具有更开放的世界能力。这是通过改进测量不确定性处理、结合运动信息以及将地图建模为具有明确的前景和背景来实现的。研究表明，根据可用资源的不同，这些变化能够将跟踪精度提高85%至97%，同时还改善了视觉信息扩展到预定义网格之外的环境中的映射。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.03102v3" target="_blank">2301.03102v3</a>
                              </td>
                              <td>Towards Open World NeRF-Based SLAM</td>
                              <td>Daniil Lisus</td>
                              <td>2023-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_03102v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.03102v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07299v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Object SLAM Framework for Association, Mapping, and High-Level Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07299v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07299v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07299v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object SLAM is considered increasingly significant for robot high-level perception and decision-making. Existing studies fall short in terms of data association, object representation, and semantic mapping and frequently rely on additional assumptions, limiting their performance. In this paper, we present a comprehensive object SLAM framework that focuses on object-based perception and object-oriented robot tasks. First, we propose an ensemble data association approach for associating objects in complicated conditions by incorporating parametric and nonparametric statistic testing. In addition, we suggest an outlier-robust centroid and scale estimation algorithm for modeling objects based on the iForest and line alignment. Then a lightweight and object-oriented map is represented by estimated general object models. Taking into consideration the semantic invariance of objects, we convert the object map to a topological map to provide semantic descriptors to enable multi-map matching. Finally, we suggest an object-driven active exploration strategy to achieve autonomous mapping in the grasping scenario. A range of public datasets and real-world results in mapping, augmented reality, scene matching, relocalization, and robotic manipulation have been used to evaluate the proposed object SLAM framework for its efficient performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07299v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象SLAM被认为对机器人的高级感知和决策越来越重要。现有的研究在数据关联、对象表示和语义映射方面存在不足，并且经常依赖于额外的假设，限制了它们的性能。在本文中，我们提出了一个全面的对象SLAM框架，该框架侧重于基于对象的感知和面向对象的机器人任务。首先，我们提出了一种集成数据关联方法，通过结合参数和非参数统计检验来关联复杂条件下的对象。此外，我们还提出了一种基于iForest和线对齐的异常值鲁棒质心和尺度估计算法，用于建模对象。然后，通过估计的一般对象模型来表示轻量级和面向对象的映射。考虑到对象的语义不变性，我们将对象映射转换为拓扑映射，以提供语义描述符，从而实现多映射匹配。最后，我们提出了一种对象驱动的主动探索策略，以实现抓取场景中的自主映射。映射、增强现实、场景匹配、重定位和机器人操作方面的一系列公共数据集和真实世界结果已被用于评估所提出的对象SLAM框架的有效性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07299v1" target="_blank">2305.07299v1</a>
                              </td>
                              <td>An Object SLAM Framework for Association, Mapping, and High-Level Tasks</td>
                              <td>Yanmin Wu</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07299v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07299v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07154v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07154v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07154v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07154v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D spatial perception is the problem of building and maintaining an actionable and persistent representation of the environment in real-time using sensor data and prior knowledge. Despite the fast-paced progress in robot perception, most existing methods either build purely geometric maps (as in traditional SLAM) or flat metric-semantic maps that do not scale to large environments or large dictionaries of semantic labels. The first part of this paper is concerned with representations: we show that scalable representations for spatial perception need to be hierarchical in nature. Hierarchical representations are efficient to store, and lead to layered graphs with small treewidth, which enable provably efficient inference. We then introduce an example of hierarchical representation for indoor environments, namely a 3D scene graph, and discuss its structure and properties. The second part of the paper focuses on algorithms to incrementally construct a 3D scene graph as the robot explores the environment. Our algorithms combine 3D geometry, topology (to cluster the places into rooms), and geometric deep learning (e.g., to classify the type of rooms the robot is moving across). The third part of the paper focuses on algorithms to maintain and correct 3D scene graphs during long-term operation. We propose hierarchical descriptors for loop closure detection and describe how to correct a scene graph in response to loop closures, by solving a 3D scene graph optimization problem. We conclude the paper by combining the proposed perception algorithms into Hydra, a real-time spatial perception system that builds a 3D scene graph from visual-inertial data in real-time. We showcase Hydra's performance in photo-realistic simulations and real data collected by a Clearpath Jackal robots and a Unitree A1 robot. We release an open-source implementation of Hydra at https://github.com/MIT-SPARK/Hydra.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07154v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D空间感知是使用传感器数据和先验知识实时构建和维护环境的可操作和持久表示的问题。尽管机器人感知进展很快，但大多数现有方法要么构建纯几何映射（如传统的SLAM），要么构建平面度量语义映射，这些映射不能扩展到大型环境或大型语义标签词典。本文的第一部分涉及表示：我们表明，空间感知的可伸缩表示在本质上需要是分层的。分层表示是有效的存储，并导致具有小树宽的分层图，这使得推理能够被证明是有效的。然后，我们介绍了一个室内环境的分层表示示例，即三维场景图，并讨论了其结构和特性。论文的第二部分重点介绍了在机器人探索环境时逐步构建3D场景图的算法。我们的算法结合了3D几何、拓扑结构（将地方聚类为房间）和几何深度学习（例如，对机器人移动的房间类型进行分类）。论文的第三部分重点讨论了在长期运行过程中维护和校正3D场景图的算法。我们提出了用于循环闭合检测的分层描述符，并描述了如何通过解决3D场景图优化问题来校正场景图以响应循环闭合。我们通过将所提出的感知算法结合到Hydra中来结束本文，Hydra是一个实时空间感知系统，它实时根据视觉惯性数据构建3D场景图。我们展示了九头蛇在照片逼真模拟和Clearpath Jackal机器人和Unitree A1机器人收集的真实数据中的表现。我们在上发布了Hydra的开源实现https://github.com/MIT-SPARK/Hydra.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07154v1" target="_blank">2305.07154v1</a>
                              </td>
                              <td>Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems</td>
                              <td>Nathan Hughes</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07154v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07154v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06121v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformer-based model for monocular visual odometry: a video understanding approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06121v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06121v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06121v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating the camera pose given images of a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and it often relies on geometric approaches that require engineering effort for a specific scenario. Deep learning methods have shown to be generalizable after proper training and a considerable amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6-DoF camera's pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI visual odometry dataset, outperforming the DeepVO implementation highly accepted in the visual odometry community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06121v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在移动机器人和自动驾驶汽车中，在给定单个相机图像的情况下估计相机姿态是一项传统任务。这个问题被称为单目视觉里程计，它通常依赖于需要针对特定场景进行工程设计的几何方法。经过适当的训练和大量的可用数据，深度学习方法已被证明是可推广的。基于转换器的架构在自然语言处理和计算机视觉任务（如图像和视频理解）方面占据了最先进的地位。在这项工作中，我们将单眼视觉里程计作为一项视频理解任务来估计6-DoF相机的姿态。我们提出了基于时空自注意机制的TSformer VO模型，以从片段中提取特征并以端到端的方式估计运动。在KITTI视觉里程测量数据集上，与基于几何和基于深度学习的方法相比，我们的方法实现了具有竞争力的最先进性能，优于视觉里程测量社区高度接受的DeepVO实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06121v1" target="_blank">2305.06121v1</a>
                              </td>
                              <td>Transformer-based model for monocular visual odometry: a video understanding approach</td>
                              <td>André O. Françani</td>
                              <td>2023-05-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06121v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06121v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14560v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neural Implicit Dense Semantic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14560v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14560v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14560v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Simultaneous Localization and Mapping (vSLAM) is a widely used technique in robotics and computer vision that enables a robot to create a map of an unfamiliar environment using a camera sensor while simultaneously tracking its position over time. In this paper, we propose a novel RGBD vSLAM algorithm that can learn a memory-efficient, dense 3D geometry, and semantic segmentation of an indoor scene in an online manner. Our pipeline combines classical 3D vision-based tracking and loop closing with neural fields-based mapping. The mapping network learns the SDF of the scene as well as RGB, depth, and semantic maps of any novel view using only a set of keyframes. Additionally, we extend our pipeline to large scenes by using multiple local mapping networks. Extensive experiments on well-known benchmark datasets confirm that our approach provides robust tracking, mapping, and semantic labeling even with noisy, sparse, or no input depth. Overall, our proposed algorithm can greatly enhance scene perception and assist with a range of robot control problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14560v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉同步定位和映射（vSLAM）是机器人和计算机视觉中广泛使用的一种技术，它使机器人能够使用相机传感器创建陌生环境的地图，同时随着时间的推移跟踪其位置。在本文中，我们提出了一种新的RGBD-vSLAM算法，该算法可以在线学习室内场景的高效记忆、密集3D几何和语义分割。我们的管道将经典的基于3D视觉的跟踪和闭环与基于神经场的映射相结合。映射网络仅使用一组关键帧来学习场景的SDF以及任何新颖视图的RGB、深度和语义映射。此外，我们通过使用多个本地映射网络将管道扩展到大型场景。在众所周知的基准数据集上进行的大量实验证实，即使在有噪声、稀疏或无输入深度的情况下，我们的方法也能提供稳健的跟踪、映射和语义标记。总的来说，我们提出的算法可以大大增强场景感知，并有助于解决一系列机器人控制问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14560v2" target="_blank">2304.14560v2</a>
                              </td>
                              <td>Neural Implicit Dense Semantic SLAM</td>
                              <td>Yasaman Haghighi</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14560v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14560v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05313v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding why SLAM algorithms fail in modern indoor environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05313v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05313v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05313v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) algorithms are essential for the autonomous navigation of mobile robots. With the increasing demand for autonomous systems, it is crucial to evaluate and compare the performance of these algorithms in real-world environments. In this paper, we provide an evaluation strategy and real-world datasets to test and evaluate SLAM algorithms in complex and challenging indoor environments. Further, we analysed state-of-the-art (SOTA) SLAM algorithms based on various metrics such as absolute trajectory error, scale drift, and map accuracy and consistency. Our results demonstrate that SOTA SLAM algorithms often fail in challenging environments, with dynamic objects, transparent and reflecting surfaces. We also found that successful loop closures had a significant impact on the algorithm's performance. These findings highlight the need for further research to improve the robustness of the algorithms in real-world scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05313v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）算法是移动机器人自主导航的关键。随着对自主系统的需求不断增加，评估和比较这些算法在现实环境中的性能至关重要。在本文中，我们提供了一种评估策略和真实世界的数据集，以在复杂和具有挑战性的室内环境中测试和评估SLAM算法。此外，我们分析了基于各种指标的最先进（SOTA）SLAM算法，如绝对轨迹误差、比例尺漂移以及地图准确性和一致性。我们的结果表明，SOTA SLAM算法在具有动态对象、透明和反射表面的具有挑战性的环境中经常失败。我们还发现，成功的循环闭包对算法的性能有很大影响。这些发现强调了进一步研究的必要性，以提高算法在现实世界场景中的稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05313v1" target="_blank">2305.05313v1</a>
                              </td>
                              <td>Understanding why SLAM algorithms fail in modern indoor environments</td>
                              <td>Nwankwo Linus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05313v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05313v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_06923v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Know What You Don't Know: Consistency in Sliding Window Filtering with Unobservable States Applied to Visual-Inertial SLAM (Extended Version)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_06923v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_06923v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_06923v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimation algorithms, such as the sliding window filter, produce an estimate and uncertainty of desired states. This task becomes challenging when the problem involves unobservable states. In these situations, it is critical for the algorithm to ``know what it doesn't know'', meaning that it must maintain the unobservable states as unobservable during algorithm deployment. This letter presents general requirements for maintaining consistency in sliding window filters involving unobservable states. The value of these requirements for designing navigation solutions is experimentally shown within the context of visual-inertial SLAM making use of IMU preintegration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_06923v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>估计算法，例如滑动窗口滤波器，产生期望状态的估计和不确定性。当问题涉及不可观测的状态时，这项任务就变得具有挑战性。在这些情况下，算法“知道它不知道的东西”至关重要，这意味着它必须在算法部署期间将不可观察的状态保持为不可观察。这封信提出了在涉及不可观测状态的滑动窗口滤波器中保持一致性的一般要求。在利用IMU预集成的视觉惯性SLAM的背景下，实验显示了这些要求对设计导航解决方案的价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.06923v4" target="_blank">2212.06923v4</a>
                              </td>
                              <td>Know What You Don't Know: Consistency in Sliding Window Filtering with Unobservable States Applied to Visual-Inertial SLAM (Extended Version)</td>
                              <td>Daniil Lisus</td>
                              <td>2022-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_06923v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.06923v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_01627v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ROMR: A ROS-based Open-source Mobile Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_01627v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_01627v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_01627v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Currently, commercially available intelligent transport robots that are capable of carrying up to 90kg of load can cost \$5,000 or even more. This makes real-world experimentation prohibitively expensive and limits the applicability of such systems to everyday home or industrial tasks. Aside from their high cost, the majority of commercially available platforms are either closed-source, platform-specific or use difficult-to-customize hardware and firmware. In this work, we present a low-cost, open-source and modular alternative, referred to herein as "ROS-based Open-source Mobile Robot ($ROMR$)". $ROMR$ utilizes off-the-shelf (OTS) components, additive manufacturing technologies, aluminium profiles, and a consumer hoverboard with high-torque brushless direct current (BLDC) motors. $ROMR$ is fully compatible with the robot operating system (ROS), has a maximum payload of 90kg, and costs less than \$1500. Furthermore, ROMR offers a simple yet robust framework for contextualizing simultaneous localization and mapping (SLAM) algorithms, an essential prerequisite for autonomous robot navigation. The robustness and performance of the $ROMR$ were validated through real-world and simulation experiments. All the design, construction and software files are freely available online under the GNU GPL v3 licence at https://doi.org/10.17605/OSF.IO/K83X7. A descriptive video of $ROMR$ can be found at https://osf.io/ku8ag.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_01627v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前，商业上可买到的智能运输机器人能够承载90公斤的负载，其成本可能高达5000美元甚至更多。这使得现实世界中的实验过于昂贵，并限制了此类系统在日常家庭或工业任务中的适用性。除了高昂的成本外，大多数商用平台要么是封闭源代码的，要么是特定于平台的，要么使用难以定制的硬件和固件。在这项工作中，我们提出了一种低成本、开源和模块化的替代方案，本文称为“基于ROS的开源移动机器人（$ROMR$）”$ROMR$利用现成的（OTS）组件、增材制造技术、铝型材和带有高转矩无刷直流（BLDC）电机的消费气垫板$ROMR$与机器人操作系统（ROS）完全兼容，最大有效载荷为90公斤，成本低于1500美元。此外，ROMR为同时定位和映射（SLAM）算法的上下文化提供了一个简单而稳健的框架，这是自主机器人导航的基本前提。通过真实世界和模拟实验验证了$ROMR$的稳健性和性能。所有的设计、构造和软件文件都可以在GNU GPL v3许可证下免费在线获取，网址为https://doi.org/10.17605/OSF.IO/K83X7.$ROMR$的描述性视频可在https://osf.io/ku8ag.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.01627v2" target="_blank">2210.01627v2</a>
                              </td>
                              <td>ROMR: A ROS-based Open-source Mobile Robot</td>
                              <td>Nwankwo Linus</td>
                              <td>2022-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_01627v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.01627v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03441v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi S-graphs: A Collaborative Semantic SLAM architecture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03441v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03441v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03441v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization and Mapping (CSLAM) is a critical capability for enabling multiple robots to operate in complex environments. Most CSLAM techniques rely on the transmission of low-level features for visual and LiDAR-based approaches, which are used for pose graph optimization. However, these low-level features can lead to incorrect loop closures, negatively impacting map generation.Recent approaches have proposed the use of high-level semantic information in the form of Hierarchical Semantic Graphs to improve the loop closure procedures and overall precision of SLAM algorithms. In this work, we present Multi S-Graphs, an S-graphs [1] based distributed CSLAM algorithm that utilizes high-level semantic information for cooperative map generation while minimizing the amount of information exchanged between robots. Experimental results demonstrate the promising performance of the proposed algorithm in map generation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03441v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协同同时定位和映射（CSLAM）是使多个机器人能够在复杂环境中操作的关键能力。大多数CSLAM技术依赖于视觉和基于激光雷达的方法的低级特征的传输，这些方法用于姿态图优化。然而，这些低级特征可能会导致错误的循环闭合，对地图生成产生负面影响。最近的方法提出了使用层次语义图形式的高级语义信息来提高SLAM算法的循环闭合过程和整体精度。在这项工作中，我们提出了Multi-S-图，这是一种基于S-图[1]的分布式CSLAM算法，它利用高级语义信息进行协作地图生成，同时最小化机器人之间交换的信息量。实验结果表明，该算法在地图生成任务中具有良好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03441v1" target="_blank">2305.03441v1</a>
                              </td>
                              <td>Multi S-graphs: A Collaborative Semantic SLAM architecture</td>
                              <td>Miguel Fernandez-Cortizas</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03441v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03441v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_04797v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Set-Type Belief Propagation with Applications to Mapping, MTT, SLAM, and SLAT</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_04797v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_04797v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_04797v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Belief propagation (BP) is a useful probabilistic inference algorithm for efficiently computing approximate marginal probability densities of random variables. However, in its standard form, BP is applicable to only the vector-type random variables, while certain applications rely on set-type random variables with an unknown number of vector elements. In this paper, we first develop BP rules for set-type random variables and demonstrate that vector-type BP is a special case of set-type BP. We further propose factor graphs with set-factor and set-variable nodes by devising the set-factor nodes that can address the set-variables with random elements and cardinality, while the number of vector elements in vector-type is known. To demonstrate the validity of developed set-type BP, we apply it to the Poisson multi-Bernoulli (PMB) filter for simultaneous localization and mapping (SLAM), which naturally leads to a new set-type BP-SLAM filter. Finally, we reveal connections between the vector-type BP-SLAM filter and the proposed set-type BP-SLAM filter and show a performance gain of the proposed set-type BP-SLAM filter in comparison with the vector-type BP-SLAM filter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_04797v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>置信传播（BP）是一种有效计算随机变量近似边际概率密度的概率推理算法。然而，在其标准形式中，BP仅适用于向量型随机变量，而某些应用依赖于向量元素数量未知的集合型随机变量。本文首先建立了集型随机变量的BP规则，证明了向量型BP是集型BP的一个特例。我们进一步提出了具有集因子和集变量节点的因子图，通过设计集因子节点，可以用随机元素和基数来处理集变量，同时向量类型中向量元素的数量是已知的。为了证明所开发的集合型BP的有效性，我们将其应用于用于同时定位和映射的Poisson-multi-Bernoulli（PMB）滤波器（SLAM），这自然导致了一种新的集合型BP-SLAM滤波器。最后，我们揭示了矢量型BP-SLAM滤波器和所提出的集合型BP-SLAM滤波器之间的联系，并显示了所提出的集型BP-SLAM滤波器与矢量型BP-S LAM滤波器相比的性能增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.04797v1" target="_blank">2305.04797v1</a>
                              </td>
                              <td>Set-Type Belief Propagation with Applications to Mapping, MTT, SLAM, and SLAT</td>
                              <td>Hyowon Kim</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_04797v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.04797v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_07147v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_07147v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_07147v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_07147v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative SLAM is at the core of perception in multi-robot systems as it enables the co-localization of the team of robots in a common reference frame, which is of vital importance for any coordination amongst them. The paradigm of a centralized architecture is well established, with the robots (i.e. agents) running Visual-Inertial Odometry (VIO) onboard while communicating relevant data, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), which then merges and optimizes the joint maps of the agents. While these frameworks have proven to be successful, their capability and performance are highly dependent on the choice of the VIO front-end, thus limiting their flexibility. In this work, we present COVINS-G, a generalized back-end building upon the COVINS framework, enabling the compatibility of the server-back-end with any arbitrary VIO front-end, including, for example, off-the-shelf cameras with odometry capabilities, such as the Realsense T265. The COVINS-G back-end deploys a multi-camera relative pose estimation algorithm for computing the loop-closure constraints allowing the system to work purely on 2D image data. In the experimental evaluation, we show on-par accuracy with state-of-the-art multi-session and collaborative SLAM systems, while demonstrating the flexibility and generality of our approach by employing different front-ends onboard collaborating agents within the same mission. The COVINS-G codebase along with a generalized front-end wrapper to allow any existing VIO front-end to be readily used in combination with the proposed collaborative back-end is open-sourced. Video: https://youtu.be/FoJfXCfaYDw</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_07147v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作SLAM是多机器人系统感知的核心，因为它能够在共同的参考系中实现机器人团队的共同定位，这对它们之间的任何协调都至关重要。集中式体系结构的范例已经很好地建立起来，机器人（即代理）在板上运行视觉惯性里程计（VIO），同时将相关数据（例如，关键帧（KF））传输到中央后端（即服务器），后者随后合并并优化代理的联合映射。虽然这些框架已经被证明是成功的，但它们的能力和性能在很大程度上取决于VIO前端的选择，从而限制了它们的灵活性。在这项工作中，我们介绍了COVINS-G，这是一个基于COVINS框架的通用后端，使服务器后端能够与任何任意的VIO前端兼容，例如，包括具有里程计功能的现成相机，如Realsense T265。COVINS-G后端部署了一种多摄像头相对姿态估计算法，用于计算闭环约束，使系统能够纯粹处理2D图像数据。在实验评估中，我们展示了与最先进的多会话和协作SLAM系统同等的准确性，同时通过在同一任务中使用不同的前端机载协作代理，展示了我们方法的灵活性和通用性。COVINS-G代码库以及一个通用的前端包装器，允许任何现有的VIO前端与所提出的协作后端结合使用，这是开源的。视频：https://youtu.be/FoJfXCfaYDw</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.07147v3" target="_blank">2301.07147v3</a>
                              </td>
                              <td>COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</td>
                              <td>Manthan Patel</td>
                              <td>2023-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_07147v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.07147v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_02645v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Edge-aware Consistent Stereo Video Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_02645v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_02645v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_02645v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video depth estimation is crucial in various applications, such as scene reconstruction and augmented reality. In contrast to the naive method of estimating depths from images, a more sophisticated approach uses temporal information, thereby eliminating flickering and geometrical inconsistencies. We propose a consistent method for dense video depth estimation; however, unlike the existing monocular methods, ours relates to stereo videos. This technique overcomes the limitations arising from the monocular input. As a benefit of using stereo inputs, a left-right consistency loss is introduced to improve the performance. Besides, we use SLAM-based camera pose estimation in the process. To address the problem of depth blurriness during test-time training (TTT), we present an edge-preserving loss function that improves the visibility of fine details while preserving geometrical consistency. We show that our edge-aware stereo video model can accurately estimate the dense depth maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_02645v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频深度估计在各种应用中至关重要，例如场景重建和增强现实。与从图像中估计深度的天真方法相比，更复杂的方法使用时间信息，从而消除闪烁和几何不一致。我们提出了一种用于密集视频深度估计的一致性方法；然而，与现有的单目方法不同，我们的方法涉及立体视频。该技术克服了由单目输入引起的限制。作为使用立体声输入的一个好处，引入了左右一致性损失来提高性能。此外，在该过程中，我们使用了基于SLAM的相机姿态估计。为了解决测试时间训练（TTT）过程中的深度模糊问题，我们提出了一种边缘保持损失函数，该函数在保持几何一致性的同时提高了精细细节的可见性。我们证明了我们的边缘感知立体视频模型可以准确地估计密集的深度图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.02645v1" target="_blank">2305.02645v1</a>
                              </td>
                              <td>Edge-aware Consistent Stereo Video Depth Estimation</td>
                              <td>Elena Kosheleva</td>
                              <td>2023-05-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_02645v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.02645v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13264v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data-Association-Free Landmark-based SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13264v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13264v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13264v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study landmark-based SLAM with unknown data association: our robot navigates in a completely unknown environment and has to simultaneously reason over its own trajectory, the positions of an unknown number of landmarks in the environment, and potential data associations between measurements and landmarks. This setup is interesting since: (i) it arises when recovering from data association failures or from SLAM with information-poor sensors, (ii) it sheds light on fundamental limits (and hardness) of landmark-based SLAM problems irrespective of the front-end data association method, and (iii) it generalizes existing approaches where data association is assumed to be known or partially known. We approach the problem by splitting it into an inner problem of estimating the trajectory, landmark positions and data associations and an outer problem of estimating the number of landmarks. Our approach creates useful and novel connections with existing techniques from discrete-continuous optimization (e.g., k-means clustering), which has the potential to trigger novel research. We demonstrate the proposed approaches in extensive simulations and on real datasets and show that the proposed techniques outperform typical data association baselines and are even competitive against an "oracle" baseline which has access to the number of landmarks and an initial guess for each landmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13264v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了具有未知数据关联的基于地标的SLAM：我们的机器人在完全未知的环境中导航，必须同时推理自己的轨迹、环境中未知数量地标的位置，以及测量和地标之间的潜在数据关联。这种设置很有趣，因为：（i）它是在从数据关联故障或具有信息贫乏传感器的SLAM中恢复时出现的，（ii）它揭示了基于里程碑的SLAM问题的基本极限（和硬度），而与前端数据关联方法无关，以及（iii）它概括了假设数据关联已知或部分已知的现有方法。我们将这个问题分解为估计轨迹、地标位置和数据关联的内部问题和估计地标数量的外部问题。我们的方法与离散连续优化的现有技术（例如，k-means聚类）建立了有用和新颖的联系，这有可能引发新的研究。我们在广泛的模拟和真实数据集上展示了所提出的方法，并表明所提出的技术优于典型的数据关联基线，甚至与可以访问地标数量和每个地标的初始猜测的“预言家”基线相比具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13264v3" target="_blank">2302.13264v3</a>
                              </td>
                              <td>Data-Association-Free Landmark-based SLAM</td>
                              <td>Yihao Zhang</td>
                              <td>2023-02-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13264v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13264v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_10121v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalized Object Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_10121v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_10121v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_10121v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Future collaborative robots must be capable of finding objects. As such a fundamental skill, we expect object search to eventually become an off-the-shelf capability for any robot, similar to e.g., object detection, SLAM, and motion planning. However, existing approaches either make unrealistic compromises (e.g., reduce the problem from 3D to 2D), resort to ad-hoc, greedy search strategies, or attempt to learn end-to-end policies in simulation that are yet to generalize across real robots and environments. This thesis argues that through using Partially Observable Markov Decision Processes (POMDPs) to model object search while exploiting structures in the human world (e.g., octrees, correlations) and in human-robot interaction (e.g., spatial language), a practical and effective system for generalized object search can be achieved. In support of this argument, I develop methods and systems for (multi-)object search in 3D environments under uncertainty due to limited field of view, occlusion, noisy, unreliable detectors, spatial correlations between objects, and possibly ambiguous spatial language (e.g., "The red car is behind Chase Bank"). Besides evaluation in simulators such as PyGame, AirSim, and AI2-THOR, I design and implement a robot-independent, environment-agnostic system for generalized object search in 3D and deploy it on the Boston Dynamics Spot robot, the Kinova MOVO robot, and the Universal Robots UR5e robotic arm, to perform object search in different environments. The system enables, for example, a Spot robot to find a toy cat hidden underneath a couch in a kitchen area in under one minute. This thesis also broadly surveys the object search literature, proposing taxonomies in object search problem settings, methods and systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_10121v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>未来的协作机器人必须能够找到物体。作为这样一项基本技能，我们期望物体搜索最终成为任何机器人的现成能力，类似于例如物体检测、SLAM和运动规划。然而，现有的方法要么做出不切实际的妥协（例如，将问题从3D减少到2D），要么求助于特别的贪婪搜索策略，要么试图在模拟中学习端到端的策略，这些策略尚未在真实的机器人和环境中推广。本文认为，通过使用部分可观测马尔可夫决策过程（POMDP）对对象搜索进行建模，同时利用人类世界中的结构（如八叉树、相关性）和人机交互（如空间语言），可以实现一个实用有效的广义对象搜索系统。为了支持这一论点，我开发了在不确定的3D环境中进行（多）对象搜索的方法和系统，这些不确定是由于有限的视场、遮挡、嘈杂、不可靠的检测器、对象之间的空间相关性以及可能模糊的空间语言（例如，“红色汽车在大通银行后面”）。除了在PyGame、AirSim和AI2-THOR等模拟器中进行评估外，我还设计并实现了一个与机器人无关、与环境无关的三维广义对象搜索系统，并将其部署在Boston Dynamics Spot机器人、Kinova MOVO机器人和Universal Robots UR5e机械臂上，以在不同环境中执行对象搜索。例如，该系统使Spot机器人能够在不到一分钟的时间内找到隐藏在厨房沙发下的玩具猫。本文还对对象搜索文献进行了广泛的综述，提出了对象搜索问题的分类设置、方法和系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.10121v2" target="_blank">2301.10121v2</a>
                              </td>
                              <td>Generalized Object Search</td>
                              <td>Kaiyu Zheng</td>
                              <td>2023-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_10121v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.10121v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14301v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14301v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14301v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14301v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work represents a large step into modern ways of fast 3D reconstruction based on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensor platform that includes an RGB camera and an inertial measurement unit for SLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF) as a neural scene representation in real-time with the acquired data from the HoloLens. The HoloLens is connected via Wifi to a high-performance PC that is responsible for the training and 3D reconstruction. After the data stream ends, the training is stopped and the 3D reconstruction is initiated, which extracts a point cloud of the scene. With our specialized inference algorithm, five million scene points can be extracted within 1 second. In addition, the point cloud also includes radiometry per point. Our method of 3D reconstruction outperforms grid point sampling with NeRFs by multiple orders of magnitude and can be regarded as a complete real-time 3D reconstruction method in a mobile mapping setup.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14301v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作代表着向基于RGB相机图像的快速3D重建的现代方法迈出了一大步。利用Microsoft HoloLens 2作为多传感器平台，包括一个RGB相机和一个惯性测量单元，用于基于SLAM的相机姿态确定，我们使用从HoloLen获取的数据实时训练神经辐射场（NeRF）作为神经场景表示。HoloLens通过Wifi连接到负责训练和3D重建的高性能PC。数据流结束后，停止训练并启动3D重建，提取场景的点云。使用我们专门的推理算法，可以在1秒内提取500万个场景点。此外，点云还包括每个点的辐射测量。我们的三维重建方法在多个数量级上优于NeRF的网格点采样，可以被视为移动映射设置中的一种完整的实时三维重建方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14301v2" target="_blank">2304.14301v2</a>
                              </td>
                              <td>Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping</td>
                              <td>Dennis Haitz</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14301v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14301v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_01843v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Direct LiDAR-Inertial Odometry and Mapping: Perceptive and Connective SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_01843v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_01843v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_01843v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Direct LiDAR-Inertial Odometry and Mapping (DLIOM), a robust SLAM algorithm with an explicit focus on computational efficiency, operational reliability, and real-world efficacy. DLIOM contains several key algorithmic innovations in both the front-end and back-end subsystems to design a resilient LiDAR-inertial architecture that is perceptive to the environment and produces accurate localization and high-fidelity 3D mapping for autonomous robotic platforms. Our ideas spawned after a deep investigation into modern LiDAR SLAM systems and their inabilities to generalize across different operating environments, in which we address several common algorithmic failure points by means of proactive safe-guards to provide long-term operational reliability in the unstructured real world. We detail several important innovations to localization accuracy and mapping resiliency distributed throughout a typical LiDAR SLAM pipeline to comprehensively increase algorithmic speed, accuracy, and robustness. In addition, we discuss insights gained from our ground-up approach while implementing such a complex system for real-time state estimation on resource-constrained systems, and we experimentally show the increased performance of our method as compared to the current state-of-the-art on both public benchmark and self-collected datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_01843v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了直接激光雷达惯性测距和测绘（DLIOM），这是一种稳健的SLAM算法，明确关注计算效率、操作可靠性和真实世界的功效。DLIOM在前端和后端子系统中包含了几个关键的算法创新，以设计一种弹性的激光雷达惯性架构，该架构能够感知环境，并为自主机器人平台提供准确的定位和高保真3D映射。我们的想法是在对现代激光雷达SLAM系统及其在不同操作环境中推广的能力进行深入调查后产生的，在这些系统中，我们通过主动安全保护来解决几个常见的算法故障点，以在非结构化的实际世界中提供长期的操作可靠性。我们详细介绍了分布在典型的激光雷达SLAM管道中的定位精度和映射弹性的几个重要创新，以全面提高算法速度、准确性和稳健性。此外，我们讨论了在资源受限系统上实现这种用于实时状态估计的复杂系统时，从我们的从头开始的方法中获得的见解，并通过实验表明，与当前最先进的公共基准和自收集数据集相比，我们的方法的性能有所提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.01843v1" target="_blank">2305.01843v1</a>
                              </td>
                              <td>Direct LiDAR-Inertial Odometry and Mapping: Perceptive and Connective SLAM</td>
                              <td>Kenny Chen</td>
                              <td>2023-05-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_01843v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.01843v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_01599v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_01599v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_01599v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_01599v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human and environment sensing are two important topics in Computer Vision and Graphics. Human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. We integrate the two techniques together in EgoLocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (IMUs) and a monocular phone camera. On one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. EgoLocate leverages image-based simultaneous localization and mapping (SLAM) techniques to locate the human in the reconstructed scene. On the other hand, SLAM often fails when the visual feature is poor. EgoLocate involves inertial mocap to provide a strong prior for the camera motion. Experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. Our codes are available for research at https://xinyu-yi.github.io/EgoLocate/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_01599v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类和环境感知是计算机视觉和图形学中的两个重要课题。人体运动通常由惯性传感器捕捉，而环境大多是使用相机重建的。我们在EgoLocate中将这两种技术集成在一起，该系统通过稀疏的身体安装传感器实时执行人体运动捕捉（mocap）、定位和映射，包括6个惯性测量单元（IMU）和一个单眼手机摄像头。一方面，由于缺乏全球定位信号，惯性mocap存在较大的平移漂移。EgoLocate利用基于图像的同时定位和映射（SLAM）技术来定位重建场景中的人类。另一方面，当视觉特征较差时，SLAM经常失败。EgoLocate涉及惯性mocap，为相机运动提供强大的先验。实验表明，与这两个领域的现有技术相比，我们的技术在很大程度上改进了定位，这是两个领域面临的关键挑战。我们的代码可在https://xinyu-yi.github.io/EgoLocate/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.01599v1" target="_blank">2305.01599v1</a>
                              </td>
                              <td>EgoLocate: Real-time Motion Capture, Localization, and Mapping with Sparse Body-mounted Sensors</td>
                              <td>Xinyu Yi</td>
                              <td>2023-05-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_01599v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.01599v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_00406v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_00406v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_00406v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_00406v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is critical to the implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms assume a static environment, leading to unreliable localization in dynamic environments. Furthermore, accurate tracking of moving objects is of great significance for the control and planning of autonomous vehicle operation. This study proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial SLAM system capable of accurately estimating the poses of both ego-vehicle and objects. First, we use 3D bounding boxes generated by an object detector to represent all movable objects and perform LiDAR odometry using inertial measurement unit (IMU) pre-integration result. Based on the historical trajectories of tracked objects in a sliding window, we perform robust object association. We propose a trajectory-based dynamic feature filtering method, which filters out features belonging to moving objects by leveraging tracking results. Factor graph-based optimization is then conducted to optimize the bias of the IMU and the poses of both the ego-vehicle and surrounding objects in a sliding window. Experiments conducted on KITTI datasets show that our method achieves better pose and tracking accuracy than our previous work DL-SLOT and other SLAM and multi-object tracking baseline methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_00406v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位与映射（SLAM）是实现自动驾驶的关键。大多数激光雷达惯性SLAM算法假设静态环境，导致在动态环境中定位不可靠。此外，运动物体的精确跟踪对自动驾驶汽车运行的控制和规划具有重要意义。本研究提出了LIMOT，这是一种紧密耦合的多目标跟踪和激光雷达惯性SLAM系统，能够准确估计自我车辆和物体的姿态。首先，我们使用由物体检测器生成的3D边界框来表示所有可移动物体，并使用惯性测量单元（IMU）预积分结果执行激光雷达里程计。基于滑动窗口中被跟踪对象的历史轨迹，我们执行鲁棒的对象关联。我们提出了一种基于轨迹的动态特征过滤方法，该方法通过利用跟踪结果过滤出属于运动对象的特征。然后进行基于因子图的优化，以优化IMU的偏置以及自我车辆和周围物体在滑动窗口中的姿态。在KITTI数据集上进行的实验表明，与我们之前的工作DL-SLOT和其他SLAM和多目标跟踪基线方法相比，我们的方法实现了更好的姿态和跟踪精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.00406v1" target="_blank">2305.00406v1</a>
                              </td>
                              <td>LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</td>
                              <td>Zhongyang Zhu</td>
                              <td>2023-04-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_00406v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.00406v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2301_08422v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用钻爆法的隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，而没有考虑动态障碍物。为了最大限度地提高自主性，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法采用了分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v2" target="_blank">2301.08422v2</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01938v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01938v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01938v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测和匹配是许多计算机视觉问题中的一项基本任务，从形状重建到结构从运动到AR/VR应用和机器人。这是一个研究得很好的问题，取得了显著的成功，如SIFT和最近的深度学习方法。虽然这些技术在噪声、照明变化和刚性运动变换方面表现出了很大的鲁棒性，但对图像失真灵敏度的关注较少。在这项工作中，我们重点关注由用于图像采集的相机的几何形状引起的情况，并考虑鱼眼和投影图像的混合场景之间的关键点检测和匹配问题。我们以最先进的方法为基础，推导出了一种自监督程序，该程序能够训练兴趣点检测器和描述符网络。我们还收集了两个新的数据集，用于在这个未探索的场景中进行额外的训练和测试，我们证明了当前的方法是次优的，因为它们被设计为在传统的投影条件下工作，而所提出的方法被证明是最有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01938v1" target="_blank">2306.01938v1</a>
                              </td>
                              <td>Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</td>
                              <td>Marcela Mera-Trujillo</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01938v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00180v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00180v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00180v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从姿态图像重建三维神经场已成为自监督表示学习的一种很有前途的方法。阻止将这些3D场景学习器部署在大规模视频数据上的关键挑战是，它们依赖于从结构到运动的精确相机姿态，这在规模上运行成本高得令人望而却步。我们提出了一种在线和单次前向联合重建相机姿态和3D神经场景表示的方法。我们通过可微分渲染将帧到帧的光流提升到3D场景流来估计姿态，保持图像处理主干的局部性和平移等变性。然后通过对场景流场的加权最小二乘拟合来执行SE（3）相机姿态估计。该公式使我们能够通过重新渲染输入视频来联合监督姿态估计和可推广的神经场景表示，从而在真实世界的视频数据集上进行端到端和完全自监督的训练。我们证明了我们的方法在不同的真实世界视频上表现稳健，尤其是在传统上对基于优化的姿态估计技术具有挑战性的序列上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00180v1" target="_blank">2306.00180v1</a>
                              </td>
                              <td>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</td>
                              <td>Cameron Smith</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00180v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00180v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16342v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16342v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16342v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征和全局特征对于自动语音识别（ASR）都是必不可少的。最近的许多方法已经证明，简单地结合局部和全局特征可以进一步提高ASR性能。然而，这些方法很少关注局部和全局特征的相互作用，并且它们的串联架构是刚性的，无法反映局部和全局关系。为了解决这些问题，本文提出了用于交互式局部和全局特征融合的InterFormer，以学习ASR的更好表示。具体地说，我们在并行设计中将卷积块与变换器块相结合。此外，我们提出了一个双向特征交互模块（BFIM）和一个选择性融合模块（SFM），分别实现局部和全局特征的交互和融合。在公共ASR数据集上进行的大量实验证明了我们提出的InterFormer的有效性及其优于其他Transformer和Conformer模型的优越性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16342v2" target="_blank">2305.16342v2</a>
                              </td>
                              <td>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</td>
                              <td>Zhi-Hao Lai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16342v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16342v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12036v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIDAR: Synthetic Image Dataset for Alignment & Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12036v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, generating perspective distortions more consistent with real-world scenarios, with homographies closely resembling those of camera projections rather than randomized homographies. For each scene, we provide a sequence of distorted images with corresponding occlusion masks, homographies, and ground-truth labels. The resulting dataset can serve as a training and evaluation set for a multitude of tasks involving image alignment and artifact removal, such as deep homography estimation, dense image matching, 2D bundle adjustment, inpainting, shadow removal, denoising, content retrieval, and background subtraction. Our data generation pipeline is customizable and can be applied to any existing dataset, serving as a data augmentation to further improve the feature learning of any existing method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12036v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像对齐和图像恢复是经典的计算机视觉任务。然而，仍然缺乏提供足够数据来训练和评估端到端深度学习模型的数据集。获得用于图像对准的地面实况数据需要来自运动方法或光流系统的复杂结构，这些运动方法或光学流系统通常不能提供足够的数据方差，即，通常提供大量的图像对应，而在底层图像序列内仅引入很少的风景变化。替代方法利用现有图像数据上的随机透视失真。然而，这只提供了微不足道的扭曲，缺乏现实世界场景的复杂性和多样性。相反，我们提出的数据增强通过使用3D渲染有助于克服数据稀缺的问题：将图像作为纹理添加到平面上，然后将不同的照明条件、阴影和遮挡添加到场景中。场景是从多个视点渲染的，生成的透视扭曲更符合真实世界场景，单应性与相机投影的单应性非常相似，而不是随机单应性。对于每个场景，我们提供一系列扭曲的图像，其中包含相应的遮挡遮罩、单形图和基本事实标签。所得数据集可以作为涉及图像对齐和伪影去除的大量任务的训练和评估集，例如深度单应性估计、密集图像匹配、2D束调整、修复、阴影去除、去噪、内容检索和背景减法。我们的数据生成管道是可定制的，可以应用于任何现有的数据集，作为数据扩充，以进一步改进任何现有方法的特征学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12036v1" target="_blank">2305.12036v1</a>
                              </td>
                              <td>SIDAR: Synthetic Image Dataset for Alignment & Restoration</td>
                              <td>Monika Kwiatkowski</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12036v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08810v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRecon: Automated 3D Object Discovery and Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08810v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08810v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>完全自动化的对象重建管道对于数字内容创建至关重要。虽然3D重建领域已经取得了深刻的发展，但去除背景以获得干净的对象模型仍然依赖于不同形式的手工劳动，如边界框标记、遮罩注释和网格操作。在本文中，我们提出了一个名为AutoRecon的新框架，用于从多视图图像中自动发现和重建对象。我们证明，通过利用自监督2D视觉变换器特征，可以从SfM点云中稳健地定位和分割前景对象。然后，我们在分解的点云提供的密集监督下重建分解的神经场景表示，从而实现精确的对象重建和分割。在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08810v1" target="_blank">2305.08810v1</a>
                              </td>
                              <td>AutoRecon: Automated 3D Object Discovery and Reconstruction</td>
                              <td>Yuang Wang</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08810v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08810v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体而言，我们首先提出了一种空间对齐模块（SAM），用于将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v1" target="_blank">2305.06794v1</a>
                              </td>
                              <td>Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05301v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05301v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of five years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05301v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位在机器人系统在先前访问的环境中的定位和导航中起着重要作用。当访问发生在长时间内时，与季节或昼夜周期相关的环境变化是一个重大挑战。在水下，变异的来源是由于其他因素，如水条件或海洋生物的生长。然而，它仍然是一个主要障碍，也是一个研究较少的障碍，部分原因是缺乏数据。本文提出了一个新的深海数据集，用于对水下长期视觉定位进行基准测试。该数据集由五年内四次访问同一热液喷口建筑物的图像组成。使用导航数据和“运动结构”来估计摄影机姿态和场景的常见几何体。这可作为评估视觉定位技术时的参考。对数据的分析提供了多年来观察到的主要变化的见解。此外，在数据集上评估了几种公认的视觉定位方法，表明水下长期视觉定位仍有改进空间。数据可在https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05301v1" target="_blank">2305.05301v1</a>
                              </td>
                              <td>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</td>
                              <td>Clémentin Boittiaux</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05301v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05301v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05268v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rotation Synchronization via Deep Matrix Factorization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05268v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05268v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了旋转同步问题，其中目标是从成对旋转开始恢复绝对旋转，其中未知数和测度分别表示为图的节点和边。这个问题是结构从运动到同时定位和映射的一个重要任务。我们专注于通过神经网络进行同步的公式化，这是最近才开始在文献中进行探索的。受深度矩阵完备的启发，我们用深度神经网络的矩阵分解来表达旋转同步。我们的公式具有隐式正则化性质，更重要的是，它是无监督的，而以前的深度方法是有监督的。我们的实验表明，在大多数场景中，我们实现了与最接近的竞争对手相当的准确性，同时在较弱的假设下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05268v1" target="_blank">2305.05268v1</a>
                              </td>
                              <td>Rotation Synchronization via Deep Matrix Factorization</td>
                              <td>Gk Tejus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05268v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05268v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，其中机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与准确性之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v3" target="_blank">2210.05020v3</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10664v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10664v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) are trained using a set of camera poses and associated images as input to estimate density and color values for each position. The position-dependent density learning is of particular interest for photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF coordinate system based on the object density. While traditional methods like Structure from Motion are commonly used for camera pose calculation in pre-processing for NeRFs, the HoloLens offers an interesting interface for extracting the required input data directly. We present a workflow for high-resolution 3D reconstructions almost directly from HoloLens data using NeRFs. Thereby, different investigations are considered: Internal camera poses from the HoloLens trajectory via a server application, and external camera poses from Structure from Motion, both with an enhanced variant applied through pose refinement. Results show that the internal camera poses lead to NeRF convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and enable a 3D reconstruction. Pose refinement enables comparable quality compared to external camera poses, resulting in improved training process with a PSNR of 27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform the conventional photogrammetric dense reconstruction using Multi-View Stereo in terms of completeness and level of detail.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10664v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用一组相机姿势和相关图像作为输入来训练神经辐射场（NeRF），以估计每个位置的密度和颜色值。位置相关密度学习对摄影测量特别感兴趣，它通过基于物体密度查询和过滤NeRF坐标系来实现3D重建。虽然在NeRF的预处理中，像“运动结构”这样的传统方法通常用于相机姿态计算，但HoloLens为直接提取所需的输入数据提供了一个有趣的界面。我们提出了一种使用NeRFs几乎直接从HoloLens数据进行高分辨率3D重建的工作流程。因此，考虑了不同的研究：通过服务器应用程序从HoloLens轨迹中获得的内部相机姿势，以及从运动中获得的结构中获得的外部相机姿势，两者都通过姿势细化应用了增强的变体。结果表明，内部相机姿态导致NeRF收敛，PSNR为25dB，绕x轴简单旋转，并实现3D重建。与外部相机姿势相比，姿势细化能够实现相当的质量，从而改进训练过程，PSNR为27\，dB，并实现更好的3D重建。总体而言，NeRF重建在完整性和细节水平方面优于使用多视图立体的传统摄影测量密集重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10664v1" target="_blank">2304.10664v1</a>
                              </td>
                              <td>A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</td>
                              <td>Miriam Jäger</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10664v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10664v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13875v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Geometric Models by Clustering in the Consensus Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13875v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems - at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13875v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的算法来寻找未知数量的几何模型，例如单应性。该问题被形式化为逐步找到主导模型实例，而不形成清晰的点到模型分配。主要实例是通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的合并过程来发现的。通过在一致性空间中进行聚类来发现新的一致性。这种新的公式产生了一种简单的迭代算法，具有最先进的精度，同时实时处理许多视觉问题——在双视图运动估计方面比竞争对手快至少两个数量级。此外，我们提出了一种确定性采样器，反映了真实世界的数据往往形成空间相干结构的事实。采样器返回逐渐加密的邻域图中的连接分量。我们介绍了许多应用，其中使用多个几何模型可以提高精度。这些包括从多个广义单应性的姿态估计；快速移动物体的轨迹估计；并且我们还提出了一种在全局SfM算法中使用多个单应性的方法。源代码：https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13875v2" target="_blank">2103.13875v2</a>
                              </td>
                              <td>Finding Geometric Models by Clustering in the Consensus Space</td>
                              <td>Daniel Barath</td>
                              <td>2021-03-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13875v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13875v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位是机器人、虚拟和增强现实以及仓库货物运输等各种应用中的一项关键任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复图案和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v1" target="_blank">2304.07250v1</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05947v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Localization using Imperfect 3D Models from the Internet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05947v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05947v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是包括增强现实（AR）在内的许多应用中的核心组件。定位算法计算查询图像的相机姿态，该图像通常是根据图像构建的场景表示。这通常需要捕获和存储大量数据，然后运行运动结构（SfM）算法。用于建筑场景表示的一个有趣且未充分探索的数据源是在互联网上容易获得的3D模型，例如手绘CAD模型、从建筑足迹或从航空图像生成的3D模型。这些模型允许立即执行视觉定位，而无需耗时的场景捕捉和模型构建步骤。然而，它也带来了挑战，因为可用的3D模型往往是对现实的不完美反映。例如，模型可能只具有通用纹理或根本没有纹理，可能只提供场景几何体的简单近似，或者可能被拉伸。本文研究了这些模型的缺陷如何影响定位精度。我们为这项任务创建了一个新的基准，并基于每个场景的多个3D模型提供了详细的实验评估。我们表明，来自互联网的3D模型有望成为一种易于获得的场景表示。同时，视觉定位管道还有很大的改进空间。为了促进对这项有趣而富有挑战性的任务的研究，我们在v-pnk.github.io/cadloc上发布了我们的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05947v1" target="_blank">2304.05947v1</a>
                              </td>
                              <td>Visual Localization using Imperfect 3D Models from the Internet</td>
                              <td>Vojtech Panek</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05947v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05947v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像技术已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索从红外图像中通过SfM进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v1" target="_blank">2304.03930v1</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03560v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03560v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.   Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03560v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督多帧深度估计通过计算相邻帧之间像素对应的匹配成本，将几何信息注入网络，实现了高精度。这些像素对应候选是基于帧之间的相对姿态估计来计算的。精确的姿态预测对于精确的匹配成本计算至关重要，因为它们会影响核极几何。此外，改进的深度估计反过来可以用于对准姿态估计。受传统运动结构（SfM）原理的启发，我们提出了DualRefine模型，该模型通过反馈回路将深度和姿态估计紧密耦合。我们新颖的更新管道使用深度平衡模型框架，通过基于核极几何计算局部匹配成本，迭代细化深度估计和特征图的隐藏状态。重要的是，我们使用精细的深度估计和特征图来计算每一步的姿势更新。姿态估计的这种更新在细化过程中缓慢地改变了极线几何结构。KITTI数据集上的实验结果表明，竞争性深度预测和里程计预测性能超过了已公布的自监督基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03560v1" target="_blank">2304.03560v1</a>
                              </td>
                              <td>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</td>
                              <td>Antyanta Bangunharcana</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03560v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03560v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$\widetilde{O}（n^2）$oracle复杂度。然而，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$，这是由于昂贵的子程序，如Lenstra-Lenstra-Lov\asz（LLL）算法[Lenstra，Lenstra，Lov\asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]提出的LLL算法的更快版本、[Vaidya，FOCS 1989]提出的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了这个问题的一个强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\log n）$额外的算术运算。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v1" target="_blank">2304.03426v1</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Validation in Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Structure from Motion (SfM) challenge in computer vision is the process of recovering the 3D structure of a scene from a series of projective measurements that are calculated from a collection of 2D images, taken from different perspectives. SfM consists of three main steps; feature detection and matching, camera motion estimation, and recovery of 3D structure from estimated intrinsic and extrinsic parameters and features.   A problem encountered in SfM is that scenes lacking texture or with repetitive features can cause erroneous feature matching between frames. Semantic segmentation offers a route to validate and correct SfM models by labelling pixels in the input images with the use of a deep convolutional neural network. The semantic and geometric properties associated with classes in the scene can be taken advantage of to apply prior constraints to each class of object. The SfM pipeline COLMAP and semantic segmentation pipeline DeepLab were used. This, along with planar reconstruction of the dense model, were used to determine erroneous points that may be occluded from the calculated camera position, given the semantic label, and thus prior constraint of the reconstructed plane. Herein, semantic segmentation is integrated into SfM to apply priors on the 3D point cloud, given the object detection in the 2D input images. Additionally, the semantic labels of matched keypoints are compared and inconsistent semantically labelled points discarded. Furthermore, semantic labels on input images are used for the removal of objects associated with motion in the output SfM models. The proposed approach is evaluated on a data-set of 1102 images of a repetitive architecture scene. This project offers a novel method for improved validation of 3D SfM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉中的运动结构（SfM）挑战是从一系列投影测量中恢复场景的3D结构的过程，这些投影测量是从不同视角拍摄的2D图像集合中计算出来的。SfM由三个主要步骤组成；特征检测和匹配，相机运动估计，以及从估计的内在和外在参数和特征中恢复3D结构。在SfM中遇到的问题是，缺乏纹理或具有重复特征的场景可能导致帧之间的错误特征匹配。语义分割通过使用深度卷积神经网络标记输入图像中的像素，提供了一种验证和校正SfM模型的途径。可以利用与场景中的类相关联的语义和几何特性来将先验约束应用于每类对象。使用了SfM流水线COLMAP和语义分割流水线DeepLab。这与密集模型的平面重建一起，被用于确定可能被计算的相机位置遮挡的错误点，给定语义标签，从而确定重建平面的先验约束。在此，在给定2D输入图像中的对象检测的情况下，语义分割被集成到SfM中，以在3D点云上应用先验。此外，对匹配的关键点的语义标签进行比较，并丢弃语义上不一致的标记点。此外，输入图像上的语义标签用于去除与输出SfM模型中的运动相关联的对象。所提出的方法是在重复建筑场景的1102幅图像的数据集上进行评估的。该项目为改进三维SfM模型的验证提供了一种新方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02420v1" target="_blank">2304.02420v1</a>
                              </td>
                              <td>Semantic Validation in Structure from Motion</td>
                              <td>Joseph Rowell</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13551v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13551v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13551v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个视图估计密集深度图在几何上是不适定的，最先进的方法依赖于使用深度神经网络学习深度与视觉外观的关系。另一方面，运动结构（SfM）利用多视图约束来生成非常精确但稀疏的地图，因为图像之间的匹配通常受到局部判别纹理的限制。在这项工作中，我们结合了这两种方法的优势，提出了一种新的测试时间细化（TTR）方法，称为SfM-TTR，该方法使用SfM多视图线索在测试时提高了单视图深度网络的性能。具体而言，与现有技术不同的是，我们使用稀疏的SfM点云作为测试时间自监督信号，对网络编码器进行微调，以学习测试场景的更好表示。我们的结果表明，将SfM-TTR添加到几个最先进的自监督和监督网络中，显著提高了它们的性能，优于以前主要基于光度多视图一致性的TTR基线。代码位于https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13551v2" target="_blank">2211.13551v2</a>
                              </td>
                              <td>SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</td>
                              <td>Sergio Izquierdo</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13551v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Line Mapping Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与稀疏关键点相比，少数线段可以简明地对高级场景布局进行编码，因为它们通常描绘主要的结构元素。除了提供强烈的几何线索外，它们还在城市景观和室内场景中无处不在。尽管有明显的优势，但目前基于线的重建方法远远落后于基于点的重建方法。在本文中，我们旨在通过引入LIMAP来缩小这一差距，LIMAP是一个用于3D线图绘制的库，可以从多视图图像中稳健有效地创建3D线图。这是通过重新审视线三角测量的退化问题、精心制作的评分和轨迹构建，以及利用线重合、平行和正交等结构先验来实现的。我们的代码与现有的基于点的运动结构方法无缝集成，可以利用它们的3D点来进一步改进线重建。此外，作为副产品，该方法能够恢复线和点/消失点（VP）之间的3D关联图。在深入的实验中，我们表明LIMAP在3D线映射方面显著优于现有的方法。我们强大的3D折线图也开辟了新的研究方向。我们展示了两个示例应用程序：视觉定位和束调整，其中将线与点一起积分会产生最佳结果。代码位于https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17504v1" target="_blank">2303.17504v1</a>
                              </td>
                              <td>3D Line Mapping Revisited</td>
                              <td>Shaohui Liu</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15069v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15069v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15069v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一种轻量级网络来改进同一图像中关键点的描述符。该网络以原始描述符和关键点的几何特性为输入，并使用基于MLP的自提升级和基于Transformer的交叉提升级来增强描述符。增强的描述符可以是实数描述符，也可以是二进制描述符。我们使用所提出的网络来增强手工制作的（ORB，SIFT）和最先进的基于学习的描述符（SuperPoint，ALIKE），并在图像匹配、视觉定位和运动任务的结构方面对它们进行评估。结果表明，我们的方法显著提高了每个任务的性能，特别是在具有挑战性的情况下，如大的照明变化或重复模式。我们的方法只需要在桌面GPU上3.2ms，在嵌入式GPU上27ms就可以处理2000个特征，这足够快，可以应用于实际系统。代码和训练过的重量可在github.com/SJTU-ViSYS/FeatureBooster上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15069v3" target="_blank">2211.15069v3</a>
                              </td>
                              <td>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</td>
                              <td>Xinjiang Wang</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15069v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15069v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15060v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15060v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15060v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的管道，可以通过一部智能手机在野外获取纹理网格，该智能手机可以访问图像、深度图和有效姿势。我们的方法首先引入了一种基于运动的RGBD辅助结构，该结构可以生成过滤后的深度图，并根据相应的深度细化相机姿态。然后，我们采用了神经隐式曲面重建方法，该方法可以获得高质量的网格，并开发了一种新的训练过程，用于应用经典多视图立体方法提供的正则化。此外，我们应用可微分渲染来微调不完整的纹理贴图，并生成在感知上更接近原始场景的纹理。我们的管道可以应用于现实世界中的任何常见对象，而无需实验室环境或精确的掩模图像。我们展示了具有复杂形状的捕捉对象的结果，并将我们的方法与现有的3D重建和纹理映射方法进行了数值验证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15060v1" target="_blank">2303.15060v1</a>
                              </td>
                              <td>TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</td>
                              <td>Jaehoon Choi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15060v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15060v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12018v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12018v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S$^2$fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12018v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于运动的神经增量结构（SfM）方法，即S$^2$fM级，该方法通过从建立的关键点对应关系中学习隐式表面的坐标MLP和辐射场，从一组未校准的图像中估计相机姿态和场景几何。由于增量SfM管道中不可避免的两视图和少视图配置，我们的新公式提出了一些新的挑战，这使用于具有未知相机姿态的体积神经渲染的坐标MLP的优化变得复杂。然而，我们证明了在2D对应关系中传递的强归纳基有望通过利用射线采样方案之间的关系来解决这些挑战。基于此，我们重新审视了增量SfM的管道，并更新了关键组件，包括两视图几何初始化、相机姿态配准、3D点三角测量和束平差，以基于神经隐式曲面的全新视角。通过通过坐标MLP统一小型MLP网络中的场景几何结构，我们的Level-S$^2$fM将隐式曲面的零级集视为自上而下的信息正则化，以管理重建的3D点，通过查询SDF拒绝对应关系中的异常值，并通过NBA（Neural BA）细化估计的几何结构。我们的S$^2$fM级不仅在相机姿态估计和场景几何重建方面取得了有希望的结果，而且它还为神经隐式渲染提供了一种很有前途的方法，而无需事先了解相机的外在情况。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12018v2" target="_blank">2211.12018v2</a>
                              </td>
                              <td>Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</td>
                              <td>Yuxi Xiao</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12018v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12018v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_14840v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_14840v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{dataset available at https://github.com/Junggy/HAMMER-dataset} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_14840v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>解决密集3D视觉问题的基于学习的方法通常基于3D传感器数据进行训练。分别使用的测量距离的原理提供了优点和缺点。由于缺乏多模态数据集，文献中通常不会对这些数据进行比较或讨论。无纹理区域对运动和立体的结构来说是有问题的，反射材料对主动传感来说是个问题，而半透明物体的距离用现有硬件测量起来很复杂。对不准确或损坏的数据进行培训会导致模型偏差，阻碍泛化能力。如果在评估过程中将传感器测量视为基本事实，则这些影响不会被注意到。本文研究了传感器误差对深度估计和重建的密集三维视觉任务的影响。我们严格展示了传感器特性对学习预测的重大影响，并注意到日常家庭环境中各种技术产生的泛化问题。为了进行评估，我们引入了一个精心设计的数据集\脚注｛数据集，可在https://github.com/Junggy/HAMMER-dataset}包括来自商品传感器的测量，即D-ToF、I-ToF、无源/有源立体声和单目RGB+P。我们的研究量化了相当大的传感器噪声影响，并为改进密集视觉估计和有针对性的数据融合铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.14840v1" target="_blank">2303.14840v1</a>
                              </td>
                              <td>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</td>
                              <td>HyunJun Jung</td>
                              <td>2023-03-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_14840v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.14840v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13543v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13543v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reasoning the 3D structure of a non-rigid dynamic scene from a single moving camera is an under-constrained problem. Inspired by the remarkable progress of neural radiance fields (NeRFs) in photo-realistic novel view synthesis of static scenes, extensions have been proposed for dynamic settings. These methods heavily rely on neural priors in order to regularize the problem. In this work, we take a step back and reinvestigate how current implementations may entail deleterious effects, including limited expressiveness, entanglement of light and density fields, and sub-optimal motion localization. As a remedy, we advocate for a bridge between classic non-rigid-structure-from-motion (\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain the latter. To this end, we propose a framework that factorizes time and space by formulating a scene as a composition of bandlimited, high-dimensional signals. We demonstrate compelling results across complex dynamic scenes that involve changes in lighting, texture and long-range dynamics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13543v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个运动摄像机推断非刚性动态场景的三维结构是一个欠约束问题。受神经辐射场（NeRFs）在静态场景的逼真新颖视图合成中取得的显著进展的启发，提出了动态设置的扩展。这些方法在很大程度上依赖于神经先验来正则化问题。在这项工作中，我们后退一步，重新研究当前的实现可能带来的有害影响，包括有限的表现力、光场和密度场的纠缠以及次优运动定位。作为补救措施，我们主张在经典的非刚性运动结构（\nrsfm）和NeRF之间建立一座桥梁，使前者经过充分研究的先验能够约束后者。为此，我们提出了一个框架，通过将场景公式化为带限高维信号的合成，来分解时间和空间。我们在复杂的动态场景中展示了令人信服的结果，这些场景涉及照明、纹理和长程动力学的变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13543v3" target="_blank">2302.13543v3</a>
                              </td>
                              <td>BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</td>
                              <td>Sameera Ramasinghe</td>
                              <td>2023-02-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13543v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13543v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13805v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13805v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and hybrid rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13805v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们定义了一个新的问题，即恢复被限制在透明外壳中的物体的三维几何结构。我们还提出了一种新的方法来解决这个具有挑战性的问题。透明外壳在不同传播介质（例如空气或玻璃）之间的界面处带来了多重光反射和折射的挑战。这些多重反射和折射会导致严重的图像失真，从而使单一视点假设无效。因此，使用现有的方法，例如传统的运动结构或现代神经重建方法，不能可靠地重建这些物体的3D几何结构。我们通过将场景明确建模为透明外壳内外两个不同的子空间来解决这个问题。我们使用现有的神经重建方法（NeuS），该方法隐式地表示内子空间的几何形状和外观。为了解决复杂的灯光交互，我们开发了一种混合渲染策略，将体积渲染与光线跟踪相结合。然后，我们通过最小化真实渲染图像和混合渲染图像之间的差异来恢复模型的基本几何结构和外观。我们根据合成数据和真实数据对我们的方法进行了评估。实验结果表明，我们的方法优于最先进的（SOTA）方法。代码和数据将在https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13805v1" target="_blank">2303.13805v1</a>
                              </td>
                              <td>Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</td>
                              <td>Jinguang Tong</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13805v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13805v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13791v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Progressively Optimized Local Radiance Fields for Robust View Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13791v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13791v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从单个随意捕捉的视频中重建大规模场景辐射场的算法。这项任务提出了两个核心挑战。首先，大多数现有的辐射场重建方法都依赖于运动结构算法中精确的预估计相机姿态，而这些算法在野外视频中经常失败。其次，在无界场景中，使用具有有限表示能力的单个全局辐射场不会缩放到更长的轨迹。为了处理未知姿态，我们以渐进的方式联合估计具有辐射场的相机姿态。我们表明，渐进优化显著提高了重建的鲁棒性。为了处理大型无界场景，我们动态分配用时间窗口内的帧训练的新的局部辐射场。这进一步提高了鲁棒性（例如，即使在中等姿态漂移的情况下也表现良好），并允许我们缩放到大场景。我们对Tanks and Temples数据集和收集的户外数据集Static Hikes的广泛评估表明，我们的方法与最先进的方法相比是有利的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13791v1" target="_blank">2303.13791v1</a>
                              </td>
                              <td>Progressively Optimized Local Radiance Fields for Robust View Synthesis</td>
                              <td>Andreas Meuleman</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13791v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13791v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_02239v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Dynamic Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_02239v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_02239v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态辐射场重建方法旨在对动态场景的时变结构和外观进行建模。然而，现有的方法假设通过运动结构（SfM）算法可以可靠地估计精确的相机姿态。因此，这些方法是不可靠的，因为SfM算法在具有高度动态对象、纹理较差的表面和旋转相机运动的具有挑战性的视频中经常失败或产生错误的姿势。我们通过联合估计静态和动态辐射场以及相机参数（姿态和焦距）来解决这个鲁棒性问题。我们通过大量的定量和定性实验证明了我们方法的稳健性。与最先进的动态视图合成方法相比，我们的结果显示出良好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.02239v2" target="_blank">2301.02239v2</a>
                              </td>
                              <td>Robust Dynamic Radiance Fields</td>
                              <td>Yu-Lun Liu</td>
                              <td>2023-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_02239v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.02239v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_01160v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">High-Res Facial Appearance Capture from Polarized Smartphone Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_01160v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach. Our method optimizes for high-resolution normal textures, diffuse albedo, and specular albedo using a coarse-to-fine optimization scheme. We show that the optimized textures can be used in a standard rendering pipeline to synthesize high-quality photo-realistic 3D digital humans in novel environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_01160v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从RGB图像中重建高质量面部纹理的新方法，该方法基于我们配备了廉价偏振箔的单个智能手机，使用了一种新的捕获程序。具体来说，我们把手电筒变成一个偏振光源，并在相机顶部添加一个偏振滤光片。利用这种设置，我们用交叉偏振光和平行偏振光捕捉被摄对象的面部。对于每个受试者，我们使用改进的智能手机在不同光偏振的闪光灯照射下，在黑暗环境中记录两个短序列。基于这些观察结果，我们使用运动结构重建了人脸的显式表面网格。然后，我们利用相机和光线在可微分渲染器中的协同定位，使用综合分析方法优化面部纹理。我们的方法使用从粗到细的优化方案优化高分辨率法线纹理、漫射反照率和镜面反照率。我们表明，优化的纹理可以在标准渲染管道中使用，以在新的环境中合成高质量的照片逼真的3D数字人。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.01160v2" target="_blank">2212.01160v2</a>
                              </td>
                              <td>High-Res Facial Appearance Capture from Polarized Smartphone Images</td>
                              <td>Dejan Azinović</td>
                              <td>2022-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_01160v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.01160v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_08695v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_08695v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis (NVS) is a challenging task in computer vision that involves synthesizing new views of a scene from a limited set of input images. Neural Radiance Fields (NeRF) have emerged as a powerful approach to address this problem, but they require accurate knowledge of camera \textit{intrinsic} and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM) and multi-view stereo (MVS) approaches have been used to extract camera parameters, but these methods can be unreliable and may fail in certain cases. In this paper, we propose a novel technique that leverages unposed images from dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera parameters directly from data. Our approach is highly extensible and can be integrated into existing NeRF architectures with minimal modifications. We demonstrate the effectiveness of our method on a variety of static and dynamic scenes and show that it outperforms traditional SfM and MVS approaches. The code for our method is publicly available at \href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}. Our approach offers a promising new direction for improving the accuracy and robustness of NVS using NeRF, and we anticipate that it will be a valuable tool for a wide range of applications in computer vision and graphics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_08695v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>新视图合成（NVS）是计算机视觉中一项具有挑战性的任务，涉及从有限的一组输入图像合成场景的新视图。神经辐射场（NeRF）已成为解决这一问题的一种强大方法，但它们需要准确了解相机的固有参数和外在参数。传统上，运动结构（SfM）和多视图立体（MVS）方法已被用于提取相机参数，但这些方法可能不可靠，并且在某些情况下可能失败。在本文中，我们提出了一种新技术，该技术利用来自动态数据集（如NVIDIA动态场景数据集）的未渲染图像，直接从数据中学习相机参数。我们的方法具有高度的可扩展性，可以集成到现有的NeRF架构中，只需进行最小的修改。我们在各种静态和动态场景中证明了我们的方法的有效性，并表明它优于传统的SfM和MVS方法。我们方法的代码可在\href上公开获取{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}。我们的方法为使用NeRF提高NVS的准确性和稳健性提供了一个有前景的新方向，我们预计它将成为计算机视觉和图形领域广泛应用的宝贵工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.08695v1" target="_blank">2303.08695v1</a>
                              </td>
                              <td>RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</td>
                              <td>Shuja Khalid</td>
                              <td>2023-03-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_08695v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.08695v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05195v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Rotation Averaging: Uncertainties and Robust Losses</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05195v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05195v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05195v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we revisit the rotation averaging problem applied in global Structure-from-Motion pipelines. We argue that the main problem of current methods is the minimized cost function that is only weakly connected with the input data via the estimated epipolar geometries.We propose to better model the underlying noise distributions by directly propagating the uncertainty from the point correspondences into the rotation averaging. Such uncertainties are obtained for free by considering the Jacobians of two-view refinements. Moreover, we explore integrating a variant of the MAGSAC loss into the rotation averaging problem, instead of using classical robust losses employed in current frameworks. The proposed method leads to results superior to baselines, in terms of accuracy, on large-scale public benchmarks. The code is public. https://github.com/zhangganlin/GlobalSfMpy</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05195v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们从运动管道重新审视了应用于全局结构的旋转平均问题。我们认为，当前方法的主要问题是最小化成本函数，该函数仅通过估计的核极几何结构与输入数据弱连接。我们建议通过将点对应的不确定性直接传播到旋转平均中来更好地对潜在的噪声分布建模。这样的不确定性是通过考虑两个视图精化的雅可比派而免费获得的。此外，我们探索将MAGSAC损失的变体集成到旋转平均问题中，而不是使用当前框架中使用的经典鲁棒损失。在大型公共基准上，所提出的方法在精度方面优于基线。该代码是公开的。https://github.com/zhangganlin/GlobalSfMpy</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05195v1" target="_blank">2303.05195v1</a>
                              </td>
                              <td>Revisiting Rotation Averaging: Uncertainties and Robust Losses</td>
                              <td>Ganlin Zhang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05195v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05195v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13201v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DRO: Deep Recurrent Optimizer for Video to Depth</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13201v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13201v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13201v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There are increasing interests of studying the video-to-depth (V2D) problem with machine learning techniques. While earlier methods directly learn a mapping from images to depth maps and camera poses, more recent works enforce multi-view geometry constraints through optimization embedded in the learning framework. This paper presents a novel optimization method based on recurrent neural networks to further exploit the potential of neural networks in V2D. Specifically, our neural optimizer alternately updates the depth and camera poses through iterations to minimize a feature-metric cost, and two gated recurrent units iteratively improve the results by tracing historical information. Extensive experimental results demonstrate that our method outperforms previous methods and is more efficient in computation and memory consumption than cost-volume-based methods. In particular, our self-supervised method outperforms previous supervised methods on the KITTI and ScanNet datasets. Our source code is available at https://github.com/aliyun/dro-sfm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13201v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们对使用机器学习技术研究视频到深度（V2D）问题越来越感兴趣。虽然早期的方法直接学习从图像到深度图和相机姿态的映射，但最近的工作通过嵌入学习框架中的优化来加强多视图几何约束。本文提出了一种新的基于递归神经网络的优化方法，以进一步挖掘神经网络在V2D中的潜力。具体而言，我们的神经优化器通过迭代交替更新深度和相机姿态，以最小化特征度量成本，并且两个门控递归单元通过跟踪历史信息迭代改进结果。大量的实验结果表明，我们的方法优于以前的方法，并且在计算和内存消耗方面比基于成本体积的方法更高效。特别是，我们的自监督方法在KITTI和ScanNet数据集上优于以前的监督方法。我们的源代码可在https://github.com/aliyun/dro-sfm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13201v4" target="_blank">2103.13201v4</a>
                              </td>
                              <td>DRO: Deep Recurrent Optimizer for Video to Depth</td>
                              <td>Xiaodong Gu</td>
                              <td>2021-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13201v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13201v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_14239v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_14239v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_14239v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_14239v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an effective method for the matching of multimodal images. Accurate image matching is the basis of various applications, such as image registration and structure from motion. Conventional matching methods fail when handling noisy multimodal image pairs with severe scale change, rotation, and nonlinear intensity distortion (NID). Toward this need, we introduce an image pyramid strategy to tackle scale change. We put forward an accurate primary orientation estimation approach to reduce the effect of image rotation at any angle. We utilize multi-scale and multi-orientation image filtering results and a feature-to-template matching scheme to ensure effective and accurate matching under large NID. Integrating these improvements significantly increases noise, scale, rotation, and NID invariant capability. Our experimental results confirm the excellent ability to achieve high-quality matches across various multimodal images. The proposed method outperforms the mainstream multimodal image matching methods in qualitative and quantitative evaluations. Our implementation is available at https://github.com/Zhongli-Fan/NISR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_14239v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种有效的多模式图像匹配方法。精确的图像匹配是各种应用的基础，例如图像配准和运动结构。传统的匹配方法在处理具有严重尺度变化、旋转和非线性强度失真（NID）的噪声多模式图像对时失败。为了满足这一需求，我们引入了一种图像金字塔策略来应对规模变化。我们提出了一种精确的主方位估计方法，以减少图像在任何角度旋转的影响。我们利用多尺度和多方位图像滤波结果以及特征到模板匹配方案来确保在大NID下的有效和准确匹配。集成这些改进显著提高了噪声、规模、旋转和NID不变能力。我们的实验结果证实了在各种多模式图像之间实现高质量匹配的卓越能力。该方法在定性和定量评价方面优于主流的多模式图像匹配方法。我们的实施可在https://github.com/Zhongli-Fan/NISR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.14239v1" target="_blank">2302.14239v1</a>
                              </td>
                              <td>Nonlinear Intensity, Scale and Rotation Invariant Matching for Multimodal Images</td>
                              <td>Zhongli Fan</td>
                              <td>2023-02-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_14239v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.14239v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10544v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10544v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10544v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10544v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-Motion is a technology used to obtain scene structure through image collection, which is a fundamental problem in computer vision. For unordered Internet images, SfM is very slow due to the lack of prior knowledge about image overlap. For sequential images, knowing the large overlap between adjacent frames, SfM can adopt a variety of acceleration strategies, which are only applicable to sequential data. To further improve the reconstruction efficiency and break the gap of strategies between these two kinds of data, this paper presents an efficient covisibility-based incremental SfM. Different from previous methods, we exploit covisibility and registration dependency to describe the image connection which is suitable to any kind of data. Based on this general image connection, we propose a unified framework to efficiently reconstruct sequential images, unordered images, and the mixture of these two. Experiments on the unordered images and mixed data verify the effectiveness of the proposed method, which is three times faster than the state of the art on feature matching, and an order of magnitude faster on reconstruction without sacrificing the accuracy. The source code is publicly available at https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10544v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是一种通过图像采集获得场景结构的技术，是计算机视觉中的一个基本问题。对于无序的互联网图像，由于缺乏图像重叠的先验知识，SfM非常慢。对于序列图像，由于知道相邻帧之间有很大的重叠，SfM可以采用各种加速策略，这些策略仅适用于序列数据。为了进一步提高重建效率，打破这两种数据之间的策略差距，本文提出了一种有效的基于共视性的增量SfM。与以往的方法不同，我们利用共视性和配准依赖性来描述适用于任何类型数据的图像连接。基于这种通用的图像连接，我们提出了一个统一的框架来有效地重建序列图像、无序图像以及这两者的混合图像。在无序图像和混合数据上的实验验证了所提出方法的有效性，该方法在特征匹配方面比现有技术快三倍，在不牺牲精度的情况下重建速度快一个数量级。源代码可在https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10544v1" target="_blank">2302.10544v1</a>
                              </td>
                              <td>EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</td>
                              <td>Zhichao Ye</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10544v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10544v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_09208v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_09208v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_09208v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_09208v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, a bridge member damage cause estimation framework is proposed by calculating the image position using Structure from Motion (SfM) and acquiring its information via Visual Question Answering (VQA). For this, a VQA model was developed that uses bridge images for dataset creation and outputs the damage or member name and its existence based on the images and questions. In the developed model, the correct answer rate for questions requiring the member's name and the damage's name were 67.4% and 68.9%, respectively. The correct answer rate for questions requiring a yes/no answer was 99.1%. Based on the developed model, a damage cause estimation method was proposed. In the proposed method, the damage causes are narrowed down by inputting new questions to the VQA model, which are determined based on the surrounding images obtained via SfM and the results of the VQA model. Subsequently, the proposed method was then applied to an actual bridge and shown to be capable of determining damage and estimating its cause. The proposed method could be used to prevent damage causes from being overlooked, and practitioners could determine inspection focus areas, which could contribute to the improvement of maintenance techniques. In the future, it is expected to contribute to infrastructure diagnosis automation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_09208v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，通过使用运动结构（SfM）计算图像位置并通过视觉问答（VQA）获取其信息，提出了一种桥梁构件损伤原因估计框架。为此，开发了一个VQA模型，该模型使用桥梁图像创建数据集，并根据图像和问题输出损坏或成员名称及其存在。在开发的模型中，要求成员姓名和损坏名称的问题的正确回答率分别为67.4%和68.9%。对于需要回答是/否的问题，正确答案率为99.1%。基于所开发的模型，提出了一种损伤原因估计方法。在所提出的方法中，通过向VQA模型输入新的问题来缩小损伤原因，这些问题是基于通过SfM获得的周围图像和VQA模型的结果来确定的。随后，将所提出的方法应用于实际桥梁，并证明能够确定损伤并估计其原因。所提出的方法可用于防止忽视损坏原因，从业者可确定检查重点领域，这有助于改进维护技术。未来，它有望为基础设施诊断自动化做出贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.09208v1" target="_blank">2302.09208v1</a>
                              </td>
                              <td>Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering</td>
                              <td>Tatsuro Yamane</td>
                              <td>2023-02-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_09208v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.09208v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_00523v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uncertainty-Driven Dense Two-View Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_00523v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_00523v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_00523v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces an effective and practical solution to the dense two-view structure from motion (SfM) problem. One vital question addressed is how to mindfully use per-pixel optical flow correspondence between two frames for accurate pose estimation -- as perfect per-pixel correspondence between two images is difficult, if not impossible, to establish. With the carefully estimated camera pose and predicted per-pixel optical flow correspondences, a dense depth of the scene is computed. Later, an iterative refinement procedure is introduced to further improve optical flow matching confidence, camera pose, and depth, exploiting their inherent dependency in rigid SfM. The fundamental idea presented is to benefit from per-pixel uncertainty in the optical flow estimation and provide robustness to the dense SfM system via an online refinement. Concretely, we introduce our uncertainty-driven Dense Two-View SfM pipeline (DTV-SfM), consisting of an uncertainty-aware dense optical flow estimation approach that provides per-pixel correspondence with their confidence score of matching; a weighted dense bundle adjustment formulation that depends on optical flow uncertainty and bidirectional optical flow consistency to refine both pose and depth; a depth estimation network that considers its consistency with the estimated poses and optical flow respecting epipolar constraint. Extensive experiments show that the proposed approach achieves remarkable depth accuracy and state-of-the-art camera pose results superseding SuperPoint and SuperGlue accuracy when tested on benchmark datasets such as DeMoN, YFCC100M, and ScanNet. Code and more materials are available at http://vis.xyz/pub/dtv-sfm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_00523v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作介绍了一个有效和实用的解决密集的两视图结构从运动（SfM）问题。要解决的一个重要问题是，如何谨慎地使用两帧之间的每像素光流对应关系来进行精确的姿态估计——因为即使不是不可能，也很难建立两幅图像之间的完美每像素对应关系。利用仔细估计的相机姿态和预测的每像素光流对应关系，计算出场景的密集深度。随后，引入了迭代细化程序，以进一步提高光流匹配置信度、相机姿态和深度，利用它们在刚性SfM中的固有依赖性。所提出的基本思想是受益于光流估计中的每像素不确定性，并通过在线细化为密集SfM系统提供鲁棒性。具体地，我们介绍了我们的不确定性驱动的密集双视图SfM管道（DTV SfM），该管道由一种不确定性感知的密集光流估计方法组成，该方法提供了与其匹配的置信度分数的每像素对应关系；加权密集光束调整公式，其取决于光流的不确定性和双向光流的一致性以细化姿态和深度；深度估计网络，该深度估计网络考虑其与所估计的姿态和光流的一致性，并考虑对极约束。大量实验表明，在DeMoN、YFCC100M和ScanNet等基准数据集上测试时，所提出的方法实现了显著的深度精度和最先进的相机姿态结果，取代了SuperPoint和SuperGlue的精度。代码和更多材料可在http://vis.xyz/pub/dtv-sfm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.00523v2" target="_blank">2302.00523v2</a>
                              </td>
                              <td>Uncertainty-Driven Dense Two-View Structure from Motion</td>
                              <td>Weirong Chen</td>
                              <td>2023-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_00523v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.00523v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_12135v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_12135v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_12135v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_12135v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive results achieved by many existing Structure from Motion (SfM) approaches, there is still a need to improve the robustness, accuracy, and efficiency on large-scale scenes with many outlier matches and sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive SfM approach that is scalable to large-scale and challenging datasets. Our approach first does a coarse global SfM which improves the reliability of the view graph by leveraging measurements from low-cost sensors such as Inertial Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is divided into sub-scenes that are refined in parallel by a fine local incremental SfM regularised by the result from the coarse global SfM to improve the camera registration accuracy and alleviate scene drifts. Finally, our approach uses a threshold-adaptive strategy to align all local reconstructions to the coordinate frame of global SfM. Extensive experiments on large-scale benchmark datasets show that our approach achieves state-of-the-art accuracy and efficiency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_12135v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管许多现有的运动结构（SfM）方法取得了令人印象深刻的结果，但在具有许多异常匹配和稀疏视图图的大规模场景中，仍然需要提高鲁棒性、准确性和效率。在本文中，我们提出了AdaSfM：一种从粗到细的自适应SfM方法，可扩展到大规模且具有挑战性的数据集。我们的方法首先进行了粗略的全局SfM，通过利用低成本传感器（如惯性测量单元（IMU）和车轮编码器）的测量，提高了视图图的可靠性。随后，视图图被划分为子场景，这些子场景通过由来自粗略全局SfM的结果正则化的精细局部增量SfM并行地细化，以提高相机配准精度并减轻场景漂移。最后，我们的方法使用阈值自适应策略将所有局部重建与全局SfM的坐标系对齐。在大规模基准数据集上进行的大量实验表明，我们的方法实现了最先进的准确性和效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.12135v1" target="_blank">2301.12135v1</a>
                              </td>
                              <td>AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion</td>
                              <td>Yu Chen</td>
                              <td>2023-01-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_12135v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.12135v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_13001v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly and Semi-Supervised Detection, Segmentation and Tracking of Table Grapes with Limited and Noisy Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_13001v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_13001v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_13001v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Detection, segmentation and tracking of fruits and vegetables are three fundamental tasks for precision agriculture, enabling robotic harvesting and yield estimation applications. However, modern algorithms are data hungry and it is not always possible to gather enough data to apply the best performing supervised approaches. Since data collection is an expensive and cumbersome task, the enabling technologies for using computer vision in agriculture are often out of reach for small businesses. Following previous work in this context, where we proposed an initial weakly supervised solution to reduce the data needed to get state-of-the-art detection and segmentation in precision agriculture applications, here we improve that system and explore the problem of tracking fruits in orchards. We present the case of vineyards of table grapes in southern Lazio (Italy) since grapes are a difficult fruit to segment due to occlusion, color and general illumination conditions. We consider the case in which there is some initial labelled data that could work as source data (\eg wine grape data), but it is considerably different from the target data (e.g. table grape data). To improve detection and segmentation on the target data, we propose to train the segmentation algorithm with a weak bounding box label, while for tracking we leverage 3D Structure from Motion algorithms to generate new labels from already labelled samples. Finally, the two systems are combined in a full semi-supervised approach. Comparisons with state-of-the-art supervised solutions show how our methods are able to train new models that achieve high performances with few labelled images and with very simple labelling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_13001v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>水果和蔬菜的检测、分割和跟踪是精准农业的三项基本任务，使机器人收割和产量估算应用成为可能。然而，现代算法需要数据，并且不可能总是收集足够的数据来应用性能最好的监督方法。由于数据收集是一项昂贵而繁琐的任务，小型企业往往无法获得在农业中使用计算机视觉的技术。在这方面的先前工作之后，我们提出了一个初始的弱监督解决方案，以减少在精确农业应用中获得最先进的检测和分割所需的数据，在此我们改进了该系统，并探索了果园中水果的跟踪问题。我们介绍了拉齐奥（意大利）南部的葡萄园，因为葡萄由于闭塞、颜色和一般的光照条件而难以分割。我们考虑的情况是，有一些初始标记的数据可以作为源数据（例如葡萄酒葡萄数据），但它与目标数据（例如餐桌葡萄数据）有很大不同。为了改进对目标数据的检测和分割，我们建议使用弱边界盒标签来训练分割算法，而对于跟踪，我们利用3D Structure from Motion算法从已经标记的样本中生成新的标签。最后，将这两个系统结合在一个完全半监督的方法中。与最先进的监督解决方案的比较表明，我们的方法能够训练新的模型，这些模型通过很少的标记图像和非常简单的标记实现高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.13001v2" target="_blank">2208.13001v2</a>
                              </td>
                              <td>Weakly and Semi-Supervised Detection, Segmentation and Tracking of Table Grapes with Limited and Noisy Data</td>
                              <td>Thomas A. Ciarfuglia</td>
                              <td>2022-08-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_13001v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.13001v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_07673v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_07673v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_07673v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_07673v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new method for object pose estimation without CAD models. The previous feature-matching-based method OnePose has shown promising results under a one-shot setting which eliminates the need for CAD models or object-specific training. However, OnePose relies on detecting repeatable image keypoints and is thus prone to failure on low-textured objects. We propose a keypoint-free pose estimation pipeline to remove the need for repeatable keypoint detection. Built upon the detector-free feature matching method LoFTR, we devise a new keypoint-free SfM method to reconstruct a semi-dense point-cloud model for the object. Given a query image for object pose estimation, a 2D-3D matching network directly establishes 2D-3D correspondences between the query image and the reconstructed point-cloud model without first detecting keypoints in the image. Experiments show that the proposed pipeline outperforms existing one-shot CAD-model-free methods by a large margin and is comparable to CAD-model-based methods on LINEMOD even for low-textured objects. We also collect a new dataset composed of 80 sequences of 40 low-textured objects to facilitate future research on one-shot object pose estimation. The supplementary material, code and dataset are available on the project page: https://zju3dv.github.io/onepose_plus_plus/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_07673v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种在没有CAD模型的情况下进行物体姿态估计的新方法。先前基于特征匹配的方法OnePose在一次性设置下显示出了有希望的结果，这消除了对CAD模型或特定对象训练的需要。然而，OnePose依赖于检测可重复的图像关键点，因此在低纹理对象上容易失败。我们提出了一种无关键点的姿态估计流水线，以消除对可重复关键点检测的需求。在无检测器特征匹配方法LoFTR的基础上，我们设计了一种新的无关键点SfM方法来重建物体的半密集点云模型。给定用于物体姿态估计的查询图像，2D-3D匹配网络直接在查询图像和重构的点云模型之间建立2D-3D对应关系，而无需首先检测图像中的关键点。实验表明，所提出的流水线在很大程度上优于现有的一次CAD无模型方法，即使对于低纹理对象，也可以与基于LINEMOD的CAD模型方法相媲美。我们还收集了一个由40个低纹理物体的80个序列组成的新数据集，以促进未来对一次性物体姿态估计的研究。项目页面上提供了补充材料、代码和数据集：https://zju3dv.github.io/onepose_plus_plus/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.07673v1" target="_blank">2301.07673v1</a>
                              </td>
                              <td>OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models</td>
                              <td>Xingyi He</td>
                              <td>2023-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_07673v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.07673v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00487v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Object at a Time: Accurate and Robust Structure From Motion for Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00487v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00487v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00487v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00487v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注视机器人可以立即、准确、稳健地感知到被注视物体的距离和周围物体的相对位置。我们展示了固定，即在移动时看着一个物体的行为，是如何利用三维空间几何中的规律来获得这些信息的。这些规律引入了旋转-平移耦合，而这种耦合在结构运动中并不常见。为了验证，我们使用了一个带有RGB相机的Franka Emika机器人。我们a）发现，在15厘米的距离上，距离估计的误差小于5毫米，b）展示了在具有挑战性的场景下如何使用相对位置来寻找障碍物。我们将准确的距离估计和障碍物信息结合到反应机器人行为中，该行为能够拾取未知大小的物体，同时受到不可预见的障碍物的阻碍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00487v2" target="_blank">2208.00487v2</a>
                              </td>
                              <td>One Object at a Time: Accurate and Robust Structure From Motion for Robots</td>
                              <td>Aravind Battaje</td>
                              <td>2022-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00487v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00487v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_12721v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Polarimetric Multi-View Inverse Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_12721v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_12721v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_12721v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A polarization camera has great potential for 3D reconstruction since the angle of polarization (AoP) and the degree of polarization (DoP) of reflected light are related to an object's surface normal. In this paper, we propose a novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that effectively exploits geometric, photometric, and polarimetric cues extracted from input multi-view color-polarization images. We first estimate camera poses and an initial 3D model by geometric reconstruction with a standard structure-from-motion and multi-view stereo pipeline. We then refine the initial model by optimizing photometric rendering errors and polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose a novel polarimetric cost function that enables an effective constraint on the estimated surface normal of each vertex, while considering four possible ambiguous azimuth angles revealed from the AoP measurement. The weight for the polarimetric cost is effectively determined based on the DoP measurement, which is regarded as the reliability of polarimetric information. Experimental results using both synthetic and real data demonstrate that our Polarimetric MVIR can reconstruct a detailed 3D shape without assuming a specific surface material and lighting condition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_12721v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于反射光的偏振角（AoP）和偏振度（DoP）与物体的表面法线有关，偏振相机在3D重建方面具有很大的潜力。在本文中，我们提出了一种新的3D重建方法，称为偏振多视图反向渲染（Polarimetric MVIR），该方法有效地利用了从输入多视图彩色偏振图像中提取的几何、光度和偏振线索。我们首先根据运动和多视图立体管道，通过标准结构的几何重建来估计相机姿态和初始3D模型。然后，我们通过使用多视图RGB、AoP和DoP图像优化光度渲染误差和偏振误差来改进初始模型，其中我们提出了一种新的偏振成本函数，该函数能够对每个顶点的估计表面法线进行有效约束，同时考虑AoP测量显示的四个可能的模糊方位角。基于DoP测量有效地确定了极化成本的权重，这被视为极化信息的可靠性。使用合成和真实数据的实验结果表明，我们的偏振MVIR可以在不假设特定表面材料和照明条件的情况下重建详细的3D形状。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.12721v1" target="_blank">2212.12721v1</a>
                              </td>
                              <td>Polarimetric Multi-View Inverse Rendering</td>
                              <td>Jinyu Zhao</td>
                              <td>2022-12-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_12721v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.12721v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_06300v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accidental Turntables: Learning 3D Pose by Watching Objects Turn</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_06300v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_06300v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_06300v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a technique for learning single-view 3D object pose estimation models by utilizing a new source of data -- in-the-wild videos where objects turn. Such videos are prevalent in practice (e.g., cars in roundabouts, airplanes near runways) and easy to collect. We show that classical structure-from-motion algorithms, coupled with the recent advances in instance detection and feature matching, provides surprisingly accurate relative 3D pose estimation on such videos. We propose a multi-stage training scheme that first learns a canonical pose across a collection of videos and then supervises a model for single-view pose estimation. The proposed technique achieves competitive performance with respect to existing state-of-the-art on standard benchmarks for 3D pose estimation, without requiring any pose labels during training. We also contribute an Accidental Turntables Dataset, containing a challenging set of 41,212 images of cars in cluttered backgrounds, motion blur and illumination changes that serves as a benchmark for 3D pose estimation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_06300v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种通过利用一种新的数据源——在物体转向的野生视频中——来学习单视图3D物体姿态估计模型的技术。这样的视频在实践中很普遍（例如，环形交叉路口的汽车、跑道附近的飞机），而且很容易收集。我们展示了来自运动算法的经典结构，再加上实例检测和特征匹配方面的最新进展，在此类视频上提供了令人惊讶的精确相对3D姿态估计。我们提出了一种多阶段训练方案，该方案首先在视频集合中学习规范姿势，然后监督用于单视图姿势估计的模型。所提出的技术在用于3D姿态估计的标准基准上实现了与现有技术相比具有竞争力的性能，而在训练期间不需要任何姿态标签。我们还提供了一个意外转台数据集，其中包含一组41212张杂乱背景、运动模糊和照明变化中的汽车图像，这些图像是3D姿态估计的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.06300v1" target="_blank">2212.06300v1</a>
                              </td>
                              <td>Accidental Turntables: Learning 3D Pose by Watching Objects Turn</td>
                              <td>Zezhou Cheng</td>
                              <td>2022-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_06300v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.06300v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_01768v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Object Aided Self-Supervised Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_01768v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_01768v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_01768v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Monocular depth estimation has been actively studied in fields such as robot vision, autonomous driving, and 3D scene understanding. Given a sequence of color images, unsupervised learning methods based on the framework of Structure-From-Motion (SfM) simultaneously predict depth and camera relative pose. However, dynamically moving objects in the scene violate the static world assumption, resulting in inaccurate depths of dynamic objects. In this work, we propose a new method to address such dynamic object movements through monocular 3D object detection. Specifically, we first detect 3D objects in the images and build the per-pixel correspondence of the dynamic pixels with the detected object pose while leaving the static pixels corresponding to the rigid background to be modeled with camera motion. In this way, the depth of every pixel can be learned via a meaningful geometry model. Besides, objects are detected as cuboids with absolute scale, which is used to eliminate the scale ambiguity problem inherent in monocular vision. Experiments on the KITTI depth dataset show that our method achieves State-of-The-Art performance for depth estimation. Furthermore, joint training of depth, camera motion and object pose also improves monocular 3D object detection performance. To the best of our knowledge, this is the first work that allows a monocular 3D object detection network to be fine-tuned in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_01768v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单目深度估计在机器人视觉、自动驾驶和3D场景理解等领域得到了积极的研究。给定一系列彩色图像，基于运动结构（SfM）框架的无监督学习方法可以同时预测深度和相机相对姿态。然而，场景中动态移动的对象违反了静态世界假设，导致动态对象的深度不准确。在这项工作中，我们提出了一种新的方法，通过单目3D物体检测来解决这种动态物体运动。具体而言，我们首先检测图像中的3D对象，并建立动态像素与检测到的对象姿态的每像素对应关系，同时留下与刚性背景相对应的静态像素以相机运动建模。通过这种方式，可以通过有意义的几何模型来学习每个像素的深度。此外，将物体检测为具有绝对尺度的长方体，用于消除单目视觉中固有的尺度模糊问题。在KITTI深度数据集上的实验表明，我们的方法在深度估计方面达到了最先进的性能。此外，深度、相机运动和物体姿态的联合训练也提高了单目3D物体检测性能。据我们所知，这是第一项允许以自我监督的方式对单目3D对象检测网络进行微调的工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.01768v1" target="_blank">2212.01768v1</a>
                              </td>
                              <td>3D Object Aided Self-Supervised Monocular Depth Estimation</td>
                              <td>Songlin Wei</td>
                              <td>2022-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_01768v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.01768v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2306_04640v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ModuleFormer: Learning Modular Large Language Models From Uncurated Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04640v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04640v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04640v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have achieved remarkable results. But existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model [Gururangan et al., 2021], which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and load concentration losses. ModuleFormer is a modular architecture that includes two different types of modules, new stick-breaking attention heads, and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task, and the task-unrelated modules could be easily pruned for a lightweight deployment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04640v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经取得了显著的成果。但现有模型的训练和部署成本高昂，而且很难在不忘记先前知识的情况下将其知识扩展到预训练数据之外。本文提出了一种新的神经网络架构ModuleFormer，它利用模块化来提高大型语言模型的效率和灵活性。ModuleFormer基于专家稀疏混合（SMoE）。与之前基于SMoE的模块化语言模型[Gururangan et al.，2021]不同，该模型需要领域标记的数据来学习领域特定的专家，ModuleFormer可以通过其新的负载平衡和负载集中损失从未评级的数据中引入模块化。ModuleFormer是一种模块化架构，包括两种不同类型的模块、新的断棒注意力头和前馈专家。在训练和推理过程中，不同的模块是输入令牌上稀疏激活的条件。在我们的实验中，我们发现模块化架构为大型预训练语言模型提供了三项重要能力：1）效率，因为ModuleFormer只为每个输入令牌激活其模块的子集，因此它可以以两倍以上的吞吐量实现与密集LLM相同的性能；2） 可扩展性，ModuleFormer比密集LLM更不受灾难性遗忘的影响，并且可以很容易地用新模块进行扩展，以学习训练数据中不包括的新知识；3） 专业化，微调ModuleFormer可以将模块的子集专门用于微调任务，而与任务无关的模块可以很容易地修剪，以实现轻量级部署。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04640v1" target="_blank">2306.04640v1</a>
                              </td>
                              <td>ModuleFormer: Learning Modular Large Language Models From Uncurated Data</td>
                              <td>Yikang Shen</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04640v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04640v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04634v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Reliability of Watermarks for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04634v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04634v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04634v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight our human study, where we investigate the reliability of watermarking when faced with human paraphrasing. We compare watermark-based detection to other detection strategies, finding overall that watermarking is a reliable solution, especially because of its sample complexity - for all attacks we consider, the watermark evidence compounds the more examples are given, and the watermark is eventually detected.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04634v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）现在被部署到日常使用中，并被定位为在未来十年生成大量文本。机器生成的文本可能会取代互联网上的人工书写文本，并有可能被用于恶意目的，如鱼叉式网络钓鱼攻击和社交媒体机器人。水印是一种简单有效的策略，可以通过检测和记录LLM生成的文本来减轻这种危害。然而，一个关键的问题仍然存在：在野外的现实环境中，水印的可靠性如何？在那里，水印文本可能与其他文本源混合，由人类作家或其他语言模型转述，并用于社会和技术领域的广泛应用。在本文中，我们探索了不同的检测方案，量化了它们在检测水印方面的能力，并确定了在每个场景中需要观察多少机器生成的文本才能可靠地检测水印。我们特别强调了我们的人类研究，在那里我们研究了当面对人类转述时水印的可靠性。我们将基于水印的检测与其他检测策略进行了比较，发现总体而言，水印是一种可靠的解决方案，特别是由于其样本复杂性——对于我们考虑的所有攻击，水印证据都是复合的，给出的例子越多，水印最终就会被检测到。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04634v1" target="_blank">2306.04634v1</a>
                              </td>
                              <td>On the Reliability of Watermarks for Large Language Models</td>
                              <td>John Kirchenbauer</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04634v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04634v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07622v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PALR: Personalization Aware LLMs for Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07622v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07622v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07622v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes retrieval candidates in natural language format as input, with instruction which explicitly asking to select results from input candidates during inference. Our experimental results demonstrate that our solution outperforms state-of-the-art models on various sequential recommendation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07622v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近因其卓越的功能而受到极大的关注。尽管在开发可用于各种自然语言处理（NLP）任务的通用LLM方面做出了广泛的努力，但在推荐系统中探索其潜力的研究却很少。在本文中，我们提出了一个新的框架，名为PALR，旨在将用户历史行为（如点击、购买、评分等）与LLM相结合，以生成用户偏好的项目。具体来说，我们首先使用用户/项目交互作为候选检索的指导。然后，我们采用基于LLM的排名模型来生成推荐项目。与现有的方法不同，现有的方法通常采用通用LLM进行零/少镜头推荐测试或在小型语言模型（参数少于10亿）上进行训练，无法完全激发LLM的推理能力并利用丰富的项目侧参数知识，我们为排名目的微调了70亿个参数LLM。该模型以自然语言格式的检索候选者作为输入，指令明确要求在推理过程中从输入候选者中选择结果。我们的实验结果表明，我们的解决方案在各种顺序推荐任务上优于最先进的模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07622v3" target="_blank">2305.07622v3</a>
                              </td>
                              <td>PALR: Personalization Aware LLMs for Recommendation</td>
                              <td>Fan Yang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07622v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07622v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17491v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Models can Solve Computer Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17491v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17491v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17491v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17491v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够在计算机上执行一般任务的代理可以通过自动化重复任务和协助解决复杂问题来提高效率和生产力。理想情况下，这样的代理应该能够解决通过自然语言命令呈现给他们的新的计算机任务。然而，以前解决这个问题的方法需要大量的专家演示和特定任务的奖励函数，这两种方法对于新任务来说都是不切实际的。在这项工作中，我们展示了一个预先训练的大型语言模型（LLM）代理可以使用一个简单的提示方案执行自然语言指导下的计算机任务，其中代理递归地批评和改进其输出（RCI）。RCI方法在自动化计算机任务方面显著优于现有的LLM方法，并在MiniWoB+基准上超过了监督学习（SL）和强化学习（RL）方法。我们比较了多个LLM，发现RCI与InstructGPT-3+RLHF LLM在MiniWoB+上是最先进的，每个任务只使用少数演示，而不是数万次，并且没有特定任务的奖励功能。此外，我们证明了RCI提示在增强LLM在一系列自然语言推理任务中的推理能力方面的有效性，优于思维链提示。我们发现，RCI与CoT相结合比单独使用效果更好。我们的代码可以在这里找到：https://github.com/posgnu/rci-agent.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17491v2" target="_blank">2303.17491v2</a>
                              </td>
                              <td>Language Models can Solve Computer Tasks</td>
                              <td>Geunwoo Kim</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17491v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17491v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04618v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04618v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04618v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04618v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \url{https://github.com/lifan-yuan/OOD_NLP}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04618v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文重新审视了NLP领域中关于分布外（OOD）鲁棒性的研究。我们发现，先前研究中的分布偏移设置通常缺乏足够的挑战，阻碍了OOD稳健性的准确评估。为了解决这些问题，我们提出了一个基准构建协议，以确保明确的差异化和具有挑战性的分布转变。然后，我们介绍了BOSS，一个用于分布外稳健SS评估的基准套件，涵盖5个任务和20个数据集。基于BOSS，我们对预先训练的语言模型进行了一系列实验，以分析和评估面向对象的鲁棒性。首先，对于vanilla微调，我们研究了分布内（ID）和OOD性能之间的关系。我们确定了三种典型的类型，揭示了内部学习机制，这可能有助于OOD稳健性的预测，与ID数据集的进步相关。然后，我们在BOSS上评估了5种经典方法，发现尽管在特定情况下表现出一些有效性，但与香草微调相比，它们并没有提供显著的改进。此外，我们用各种适应范式评估了5个LLM，发现当有足够的ID数据可用时，微调领域特定模型在ID示例上显著优于LLM。然而，在OOD实例的情况下，通过上下文学习对LLM进行优先级排序会产生更好的结果。我们发现，微调的小型模型和LLM在有效解决下游任务方面都面临挑战。代码在\url上是公共的{https://github.com/lifan-yuan/OOD_NLP}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04618v1" target="_blank">2306.04618v1</a>
                              </td>
                              <td>Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations</td>
                              <td>Lifan Yuan</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04618v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04618v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03901v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03901v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03901v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03901v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03901v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有内存的大型语言模型（LLM）在计算上是通用的。然而，主流LLM并没有充分利用记忆，其设计深受生物大脑的影响。由于其近似性和容易积累错误，传统的神经记忆机制无法支持LLM来模拟复杂的推理。在本文中，我们从现代计算机体系结构中寻求灵感，用符号记忆来增强LLM，用于复杂的多跳推理。这样的符号内存框架被实例化为LLM和一组SQL数据库，其中LLM生成操作SQL数据库的SQL指令。我们在需要复杂推理的合成数据集上验证了所提出的记忆框架的有效性。项目网站位于https://chatdatabase.github.io/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03901v2" target="_blank">2306.03901v2</a>
                              </td>
                              <td>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</td>
                              <td>Chenxu Hu</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03901v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03901v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04610v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Two Word Test: A Semantic Benchmark for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04610v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04610v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04610v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, with both versions. Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the limitations and weaknesses of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing 'true understanding' or AGI to LLMs. TWT is available at: https://github.com/NickRiccardi/two-word-test</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04610v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近表现出了非凡的能力，包括通过高级专业考试和苛刻的基准测试。这一表现使许多人认为，他们接近于实现对语言的类人或“真正”理解，甚至是人工通用智能（AGI）。在这里，我们提供了一个新的开源基准，可以使用两个单词的短语来评估LLM的语义能力，使用的任务可以由人类在没有高级训练的情况下相对容易地执行。将多个单词组合成一个概念是人类语言和智力的一个基本方面。该测试要求对1768个被评为有意义（如男婴）或无意义（如山羊天空）的名词-名词组合进行有意义判断。由150名人工评分员进行评分。我们提供了任务的版本，以0-4的等级调查意义评级以及二元判断。我们在两个版本的GPT-4、GPT-3.5和Bard上使用TWT进行了一系列实验。结果表明，与人类相比，所有模型在评定这些短语的意义方面都表现不佳。GPT-3.5和Bard也无法在合理短语和无意义短语之间进行二元区分。GPT-4在组合短语的二元辨别方面有了实质性的改进，但仍明显低于人类的表现。TWT可用于了解当前LLM的局限性和弱点，并可能对其进行改进。该测试还提醒我们，在将“真正理解”或AGI归因于LLM时需要谨慎。TWT可在以下网址获得：https://github.com/NickRiccardi/two-word-test</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04610v1" target="_blank">2306.04610v1</a>
                              </td>
                              <td>The Two Word Test: A Semantic Benchmark for Large Language Models</td>
                              <td>Nicholas Riccardi</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04610v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04610v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04597v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04597v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04597v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04597v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04597v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预先训练的大型语言模型中存在的社会偏见是一个关键问题，因为这些模型已被证明在无数下游应用中传播偏见，使其对特定人群不公平。由于从头开始对这些模型进行大规模重新训练既费时又计算昂贵，因此之前已经提出了各种方法来消除预训练模型的偏差。虽然目前大多数最先进的去偏方法都侧重于训练制度的改变，但在本文中，我们提出了数据干预策略，作为一种强大而简单的技术，可以减少预训练模型中的性别偏见。具体而言，我们的经验表明，通过仅对10个去偏见（干预）的训练示例微调预训练模型，显著降低了偏向任何性别的倾向。由于我们提出的方法只需要几个训练例子，所以我们的少镜头去偏方法是非常可行和实用的。通过广泛的实验，我们表明，我们的去偏技术比竞争最先进的基线性能更好，语言建模能力损失最小。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04597v1" target="_blank">2306.04597v1</a>
                              </td>
                              <td>Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions</td>
                              <td>Himanshu Thakur</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04597v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04597v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04563v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04563v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04563v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04563v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward "funny" machines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04563v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>幽默是人类交流的一个核心方面，到目前为止，人工智能还没有解决这个问题。大型语言模型（LLM）越来越能够捕捉隐含的和上下文信息。特别是，OpenAI的ChatGPT最近获得了巨大的公众关注。基于GPT3的模型似乎几乎可以在人类层面上进行交流，甚至可以讲笑话。幽默是人类交流的重要组成部分。但是ChatGPT真的很有趣吗？我们测试了ChatGPT的幽默感。在一系列围绕笑话的探索性实验中，即生成、解释和检测，我们试图了解ChatGPT掌握和再现人类幽默的能力。由于模型本身是不可访问的，我们应用了基于提示的实验。我们的经验证据表明，笑话不是硬编码的，但大多也不是模型新生成的。1008个笑话中，90%以上都是25个笑话。该系统准确地解释了有效的笑话，但也为无效的笑话提供了虚构的解释。笑话的典型特征会误导ChatGPT对笑话的分类。ChatGPT还没有解决计算幽默问题，但它可以向“有趣”的机器迈出一大步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04563v1" target="_blank">2306.04563v1</a>
                              </td>
                              <td>ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models</td>
                              <td>Sophie Jentzsch</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04563v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04563v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13040v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13040v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13040v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13040v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken conversation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1% of dialogues. The dataset, code, and leaderboard are available: https://spokenwoz.github.io/SpokenWOZ-github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13040v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以任务为导向的对话模式近年来取得了重大进展。然而，以前的研究主要集中在注释者编写的数据集上，这导致了学术研究与现实世界口语对话场景之间的差距。虽然提出了几个小规模的口语TOD数据集来解决ASR错误等鲁棒性问题，但它们忽略了口语会话中的独特挑战。为了解决这些限制，我们介绍了SpokenWOZ，这是一个用于口语TOD的大型语音文本数据集，包含8个域、203k个转弯、5.7k个对话和249小时的人与人之间口语对话音频。SpokenWOZ进一步融合了常见的口语特征，如口语中的逐字处理和推理。基于这些特点，我们提出了交叉转弯槽和推理槽检测作为新的挑战。我们在各种基线上进行了实验，包括文本模态模型、新提出的双模态模型和LLM，例如ChatGPT。结果表明，当前的模型在口语会话中仍有很大的改进空间，其中最先进的对话状态跟踪器在联合目标准确率上仅达到25.65%，而SOTA端到端模型仅在52.1%的对话中正确完成了用户请求。数据集、代码和排行榜可用：https://spokenwoz.github.io/SpokenWOZ-github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13040v2" target="_blank">2305.13040v2</a>
                              </td>
                              <td>SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains</td>
                              <td>Shuzheng Si</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13040v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13040v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04556v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04556v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04556v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04556v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04556v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>代码LLM正在迅速部署，有证据表明它们可以提高专业程序员的工作效率。当前的代码生成基准衡量模型是否在专家提示下生成正确的程序。在本文中，我们提出了一个新的基准测试，每个问题包含多个提示，由特定的非专家提示者群体编写：初级程序员。StudentEval包含48道问题的1749个提示，由80名只完成了一学期Python编程的学生编写。我们的学生在与CodeLLM交互工作时编写了这些提示，我们观察到成功率参差不齐。我们使用StudentEval评估了5个代码LLM，发现StudentEval是比现有基准更好的模型性能鉴别器。我们分析了提示，发现学生的提示技巧存在显著差异。我们还发现，不确定的LLM抽样可能会误导学生，使他们认为他们的提示比实际效果更有效（或更少），这对如何使用代码LLM进行教学有影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04556v1" target="_blank">2306.04556v1</a>
                              </td>
                              <td>StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code</td>
                              <td>Hannah McLean Babe</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04556v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04556v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04537v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Long-form analogies generated by chatGPT lack human-like psycholinguistic properties</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04537v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04537v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04537v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Psycholinguistic analyses provide a means of evaluating large language model (LLM) output and making systematic comparisons to human-generated text. These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text. In this work, we apply psycholinguistic methods to evaluate individual sentences from long-form analogies about biochemical concepts. We compare analogies generated by human subjects enrolled in introductory biochemistry courses to analogies generated by chatGPT. We perform a supervised classification analysis using 78 features extracted from Coh-metrix that analyze text cohesion, language, and readability (Graesser et. al., 2004). Results illustrate high performance for classifying student-generated and chatGPT-generated analogies. To evaluate which features contribute most to model performance, we use a hierarchical clustering approach. Results from this analysis illustrate several linguistic differences between the two sources.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04537v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>心理语言学分析提供了一种评估大型语言模型（LLM）输出并与人类生成的文本进行系统比较的方法。这些方法可以用来描述LLM输出的心理语言学特性，并说明LLM与人类生成的文本相比不足的地方。在这项工作中，我们运用心理语言学的方法来评估关于生物化学概念的长形式类比中的个别句子。我们将参加生物化学入门课程的人类受试者产生的类比与chatGPT产生的类比进行比较。我们使用从Coh-metrix中提取的78个特征进行监督分类分析，这些特征分析了文本衔接、语言和可读性（Graesser等人，2004）。结果表明，在对学生生成的和聊天GPT生成的类比进行分类方面具有很高的性能。为了评估哪些特征对模型性能的贡献最大，我们使用了分层聚类方法。这项分析的结果说明了这两个来源之间的一些语言差异。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04537v1" target="_blank">2306.04537v1</a>
                              </td>
                              <td>Long-form analogies generated by chatGPT lack human-like psycholinguistic properties</td>
                              <td>S. M. Seals</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04537v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04537v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04528v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04528v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04528v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04528v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04528v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学术界和工业界越来越依赖大型语言模型（LLM），这就需要全面了解它们对提示的鲁棒性。为了满足这一重要需求，我们引入了PromptBench，这是一个稳健性基准，旨在衡量LLM对对抗性提示的弹性。这项研究使用了大量对抗性文本攻击，针对多个层次的提示：字符、单词、句子和语义。然后，这些提示被用于各种任务，如情绪分析、自然语言推理、阅读理解、机器翻译和数学问题解决。我们的研究生成了4032个对抗性提示，在8个任务和13个数据集上进行了仔细评估，共有567084个测试样本。我们的研究结果表明，当代LLM容易受到对抗性提示的影响。此外，我们进行了全面的分析，以了解即时稳健性及其可转移性背后的奥秘。然后，我们提供了有见地的稳健性分析和实用的建议，以便及时撰写，这对研究人员和日常用户都有利。我们将生成对抗性提示的代码、提示和方法公开，从而支持并鼓励在这一关键领域进行合作探索：https://github.com/microsoft/promptbench.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04528v1" target="_blank">2306.04528v1</a>
                              </td>
                              <td>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</td>
                              <td>Kaijie Zhu</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04528v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04528v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04508v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04508v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04508v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04508v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strategy consistently improves LLM's in-context learning performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04508v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管最近出现的像ChatGPT这样的大型语言模型（LLM）表现出了令人印象深刻的总体性能，但在多跨度问答等特定任务上，它与完全监督模型仍有很大差距。先前的研究发现，上下文学习是利用LLM的一种有效方法，通过使用一些与任务相关的标记数据作为演示示例，构建一些提示来回答新问题。一个流行的实现是通过简单的模板将几个问题及其正确答案连接起来，告知LLM所需的输出。在本文中，我们提出了一种使用标记数据的新方法，通过扩展演示示例，对现成模型预测的答案（例如正确、不正确或不完整）进行反馈，使其也向LLM通知一些不希望的输出。在三个多跨度问答数据集和一个关键短语提取数据集上的实验表明，我们的新提示策略持续提高了LLM的上下文学习性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04508v1" target="_blank">2306.04508v1</a>
                              </td>
                              <td>Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering</td>
                              <td>Zixian Huang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04508v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04508v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04504v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04504v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04504v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04504v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04504v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>ChatGPT是由OpenAI开发的一个大型语言模型。尽管它在各种任务中的表现令人印象深刻，但迄今为止还没有任何工作研究它在生物医学领域的能力。为此，本文旨在评估ChatGPT在各种基准生物医学任务上的性能，如关系提取、文档分类、问答和摘要。据我们所知，这是第一项在生物医学领域对ChatGPT进行广泛评估的工作。有趣的是，根据我们的评估，我们发现在训练集较小的生物医学数据集中，零样本ChatGPT甚至优于最先进的微调生成变压器模型，如BioGPT和BioBART。这表明，ChatGPT对大型文本语料库的预训练使其即使在生物医学领域也相当专业。我们的发现表明，ChatGPT有潜力成为生物医学领域中缺乏大量注释数据的各种任务的宝贵工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04504v1" target="_blank">2306.04504v1</a>
                              </td>
                              <td>Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers</td>
                              <td>Israt Jahan</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04504v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13835v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13835v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13835v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13835v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13835v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前的对话研究主要研究成对（两党）对话，而没有涉及两个以上说话者一起交谈的日常环境。在这项工作中，我们收集并评估了多方对话，以研究这一更普遍的情况。我们使用LIGHT环境构建接地气的对话，每个参与者都有一个指定的角色扮演。因此，我们评估了语言模型在此类对话中扮演一个或多个角色的能力。模型需要两项技能，而成对训练的模型似乎缺乏这两项技能：（1）能够决定何时说话；（2） 产生基于多个字符的连贯的话语。我们将在新数据集上训练的模型与现有的成对训练的对话模型以及很少镜头提示的大型语言模型进行比较。我们发现，我们将公开发布的新数据集MultiLIGHT可以帮助显著改善团队设置。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13835v2" target="_blank">2304.13835v2</a>
                              </td>
                              <td>Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models</td>
                              <td>Jimmy Wei</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13835v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13835v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00693v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00693v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00693v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00693v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent upsurge in pre-trained large models (e.g. GPT-4) has swept across the entire deep learning community. Such powerful large language models (LLMs) demonstrate advanced generative ability and multimodal understanding capability, which quickly achieve new state-of-the-art performances on a variety of benchmarks. The pre-trained LLM usually plays the role as a universal AI model that can conduct various tasks, including context reasoning, article analysis and image content comprehension. However, considering the prohibitively high memory and computational cost for implementing such a large model, the conventional models (such as CNN and ViT), are still essential for many visual perception tasks. In this paper, we propose to enhance the representation ability of ordinary vision models for perception tasks (e.g. image classification) by taking advantage of large pre-trained models. We present a new learning paradigm in which the knowledge extracted from large pre-trained models are utilized to help models like CNN and ViT learn enhanced representations and achieve better performance. Firstly, we curate a high quality description set by prompting a multimodal LLM to generate descriptive text for all training images. Furthermore, we feed these detailed descriptions into a pre-trained encoder to extract text embeddings with rich semantic information that encodes the content of images. During training, text embeddings will serve as extra supervising signals and be aligned with image representations learned by vision models. The alignment process helps vision models learn better and achieve higher accuracy with the assistance of pre-trained LLMs. We conduct extensive experiments to verify that the proposed algorithm consistently improves the performance for various vision models with heterogeneous architectures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00693v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，预训练的大型模型（如GPT-4）的热潮席卷了整个深度学习社区。这种强大的大型语言模型（LLM）展示了先进的生成能力和多模态理解能力，可以在各种基准上快速实现最先进的性能。经过预训练的LLM通常扮演通用人工智能模型的角色，可以执行各种任务，包括上下文推理、文章分析和图像内容理解。然而，考虑到实现如此大的模型所需的高内存和计算成本，传统模型（如CNN和ViT）对于许多视觉感知任务来说仍然是必不可少的。在本文中，我们提出通过利用大型预训练模型来增强普通视觉模型对感知任务（如图像分类）的表示能力。我们提出了一种新的学习范式，其中利用从大型预训练模型中提取的知识来帮助CNN和ViT等模型学习增强的表示并获得更好的性能。首先，我们通过提示多模式LLM为所有训练图像生成描述性文本来策划高质量的描述集。此外，我们将这些详细描述输入到预先训练的编码器中，以提取具有丰富语义信息的文本嵌入，从而对图像内容进行编码。在训练过程中，文本嵌入将作为额外的监督信号，并与视觉模型学习的图像表示对齐。在预先训练的LLM的帮助下，对齐过程有助于视觉模型更好地学习并实现更高的精度。我们进行了大量的实验来验证所提出的算法在异构架构的各种视觉模型中始终提高了性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00693v2" target="_blank">2306.00693v2</a>
                              </td>
                              <td>GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?</td>
                              <td>Ning Ding</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00693v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00693v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04441v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">STEPS: A Benchmark for Order Reasoning in Sequential Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04441v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04441v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04441v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Various human activities can be abstracted into a sequence of actions in natural text, i.e. cooking, repairing, manufacturing, etc. Such action sequences heavily depend on the executing order, while disorder in action sequences leads to failure of further task execution by robots or AI agents. Therefore, to verify the order reasoning capability of current neural models in sequential tasks, we propose a challenging benchmark , named STEPS. STEPS involves two subtask settings, focusing on determining the rationality of given next step in recipes and selecting the reasonable step from the multi-choice question, respectively. We describe the data construction and task formulations, and benchmark most of significant Large Language Models (LLMs). The experimental results demonstrate 1) The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs; 2) Prompting method still significantly lags behind tuning-based method on STEPS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04441v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类的各种活动可以抽象为自然文本中的一系列动作，即烹饪、修理、制造等。这些动作序列在很大程度上取决于执行顺序，而动作序列的无序会导致机器人或人工智能代理无法进一步执行任务。因此，为了验证当前神经模型在序列任务中的顺序推理能力，我们提出了一个具有挑战性的基准，称为STEPS。STEPS涉及两个子任务设置，分别侧重于确定配方中给定下一步的合理性和从多选问题中选择合理的步骤。我们描述了数据构造和任务公式，并对大多数重要的大型语言模型（LLM）进行了基准测试。实验结果表明：1）序列任务中动作顺序的常识推理对于LLM来说，通过零样本提示或少快照上下文学习难以解决；2） 提示方法仍然明显落后于基于STEPS的调谐方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04441v1" target="_blank">2306.04441v1</a>
                              </td>
                              <td>STEPS: A Benchmark for Order Reasoning in Sequential Tasks</td>
                              <td>Weizhi Wang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04441v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04441v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_19926v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_19926v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_19926v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_19926v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction. We not only focus on the performance of LLMs, but also explore their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics. Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework. We first focus on evaluating the consistency of personality types exhibited by ChatGPT. Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues. The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts. By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_19926v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在人工智能领域取得了显著的进步，极大地重塑了人机交互。我们不仅关注LLM的表现，而且从心理学的角度探索其特征，承认了解其行为特征的重要性。我们的研究采用特质理论（一种心理学框架）来检验LLM表现出的行为模式。我们首先关注于评估ChatGPT表现出的人格类型的一致性。此外，实验还包括对另外七种语言的跨语言影响，以及对其他六种LLM的调查。此外，该研究还调查了ChatGPT是否会对指令或上下文线索做出反应而表现出个性变化。研究结果表明，无论指令或上下文如何，ChatGPT都能始终如一地保持其ENFJ个性。通过揭示LLM的个性化，我们预计我们的研究将成为该领域进一步研究的催化剂。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.19926v2" target="_blank">2305.19926v2</a>
                              </td>
                              <td>ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models</td>
                              <td>Jen-tse Huang</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_19926v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.19926v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04387v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04387v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04387v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04387v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. To encourage further research, we have open-sourced both the dataset and trained models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04387v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指令调优大大提高了大型语言模型（LLM）（如ChatGPT）的性能，使它们能够在不同的任务中与人工指令保持一致。然而，由于缺乏高质量的指令数据集，开放视觉语言模型（VLM）的进展受到限制。为了应对这一挑战并促进视觉语言领域的研究，我们引入了多模式、多语言指令调整（M$^3$IT）数据集，旨在优化VLM与人工指令的对齐。我们的M$^3$IT数据集包括40个精心策划的数据集，包括240万个实例和400个手动编写的任务指令，这些指令被重新格式化为视觉到文本的结构。通过先进的翻译系统，关键任务被翻译成80种语言，确保更广泛的可访问性。M$^3$IT在任务覆盖率、指令数量和实例规模方面超过了以前的数据集。此外，我们开发了Ying VLM，这是一个在M$^3$IT数据集上训练的VLM模型，展示了它在回答需要世界知识的复杂问题、推广到看不见的视频任务以及理解中文看不见指令方面的潜力。为了鼓励进一步的研究，我们开放了数据集和训练模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04387v1" target="_blank">2306.04387v1</a>
                              </td>
                              <td>M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</td>
                              <td>Lei Li</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04387v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04387v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04384v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multilingual Clinical NER: Translation or Cross-lingual Transfer?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04384v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04384v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04384v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04384v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于缺乏注释数据，临床领域中对非英语文本的命名实体识别（NER）等自然语言任务可能非常耗时和昂贵。跨语言迁移（CLT）是解决这一问题的一种方法，这要归功于多语言大型语言模型能够在一种语言中对特定任务进行微调，并在另一种语言的同一任务中提供高精度。然而，通过翻译训练集或测试集，利用翻译模型的其他方法可以用于在没有目标语言注释数据的情况下执行NER。本文将跨语言迁移与这两种替代方法进行了比较，以在没有任何法语和德语训练数据的情况下用法语和德语进行临床NER。为此，我们发布了MedNERF，这是一个从法国处方中提取的医学NER测试集，并用与英国数据集相同的指南进行了注释。通过在该数据集和德国医学数据集上的广泛实验（Frei和Kramer，2021），我们表明基于翻译的方法可以实现与CLT类似的性能，但在设计时需要更多的注意。尽管他们可以利用单语临床语言模型，但无论是跨语言迁移还是翻译，这些模型都不能保证比大型通用多语言模型更好的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04384v1" target="_blank">2306.04384v1</a>
                              </td>
                              <td>Multilingual Clinical NER: Translation or Cross-lingual Transfer?</td>
                              <td>Xavier Fontaine</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04384v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04384v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04371v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large-Scale Cell Representation Learning via Divide-and-Conquer Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04371v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04371v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04371v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Single-cell RNA sequencing (scRNA-seq) data is a potent tool for comprehending the "language of life" and can provide insights into various downstream biomedical tasks. Large-scale language models (LLMs) are starting to be used for cell representation learning. However, current LLM-based cell representation learning methods depend solely on the BERT architecture, causing an anisotropic embedding space that leads to inefficient semantic representation. Contrastive learning alleviates this problem by distributing the embeddings uniformly. As a larger batch size in contrastive learning results in better representation, the practical application of contrastive learning in cell representation learning is hampered by the high dimensionality of scRNA-seq data and the large parameter volume of LLMs. To address the batch size limitation, we propose a novel divide-and-conquer contrastive learning approach to decouple the batch size from the GPU memory size for cell representation learning. Based on our divide-and-conquer contrastive learning approach, we introduce Single-Cell Language Model CellLM, a large-scale cell representation learning model to handle high-dimensional scRNA-seq data with tens of thousands of genes. CellLM has over 50 million parameters trained with 2 million scRNA-seq data and makes the first attempt to learn cell language models from both normal cells and cancer cells. CellLM achieves new state-of-the-art (SOTA) results in all evaluated downstream tasks: including a 71.8 F_1-score for cell type annotation (a 3.0% absolute improvement over scBERT), an average F_1-score of 88.9 for single-cell drug sensitivity prediction in a few-shot scenario (an 8.3% absolute improvement), and a 93.4 Pearson's correlation for single-omics cell line drug sensitivity prediction (a 6.2% absolute improvement).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04371v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单细胞RNA测序（scRNA-seq）数据是理解“生命语言”的有力工具，可以为各种下游生物医学任务提供见解。大规模语言模型（LLM）开始被用于细胞表征学习。然而，当前基于LLM的细胞表示学习方法仅依赖于BERT架构，导致各向异性的嵌入空间导致低效的语义表示。对比学习通过均匀分布嵌入来缓解这个问题。由于对比学习中较大的批量大小会产生更好的表征，因此scRNA-seq数据的高维度和LLM的大参数量阻碍了对比学习在细胞表征学习中的实际应用。为了解决批量大小的限制，我们提出了一种新的分而治之的对比学习方法，将批量大小与GPU内存大小解耦，用于单元表示学习。基于我们的分而治之的对比学习方法，我们引入了单细胞语言模型CellLM，这是一个大规模的细胞表示学习模型，用于处理具有数万个基因的高维scRNA-seq数据。CellLM使用200万scRNA-seq数据训练了5000多万个参数，并首次尝试从正常细胞和癌症细胞学习细胞语言模型。CellLM在所有评估的下游任务中都取得了新的最先进的（SOTA）结果：包括细胞类型注释的F_1得分为71.8（比scBERT的绝对改善3.0%），单细胞药物敏感性预测的F_1平均得分为88.9（绝对改善8.3%），单组学细胞系药物敏感性预测的Pearson相关性为93.4（绝对改善6.2%）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04371v1" target="_blank">2306.04371v1</a>
                              </td>
                              <td>Large-Scale Cell Representation Learning via Divide-and-Conquer Contrastive Learning</td>
                              <td>Suyuan Zhao</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04371v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04371v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04362v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04362v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04362v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04362v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To promote the development of Vision-Language Pre-training (VLP) and multimodal Large Language Model (LLM) in the Chinese community, we firstly release the largest public Chinese high-quality video-language dataset named Youku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing website, with strict criteria of safety, diversity, and quality. Youku-mPLUG contains 10 million Chinese video-text pairs filtered from 400 million raw videos across a wide range of 45 diverse categories for large-scale pre-training. In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification. Youku-mPLUG can enable researchers to conduct more in-depth multimodal research and develop better applications in the future. Furthermore, we release popular video-language pre-training models, ALPRO and mPLUG-2, and our proposed modularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG. Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1% improvement in video category classification. Besides, mPLUG-video achieves a new state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in video category classification and 68.9 CIDEr score in video captioning, respectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate impressive instruction and video understanding ability. The zero-shot instruction understanding experiment indicates that pretraining with Youku-mPLUG can enhance the ability to comprehend overall and detailed visual semantics, recognize scene text, and leverage open-domain knowledge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04362v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了促进视觉语言预训练（VLP）和多模态大语言模型（LLM）在中文社区的发展，我们首先发布了最大的公共中文高质量视频语言数据集优酷mPLUG，该数据集来自中国知名视频共享网站优酷，具有严格的安全性、多样性和质量标准。优酷mPLUG包含从4亿个原始视频中过滤出来的1000万个中文视频文本对，涵盖45个不同的类别，用于大规模的预训练。此外，为了便于对视频语言模型进行全面评估，我们精心构建了最大的人工注释中文基准，涵盖了跨模态检索、视频字幕和视频类别分类三个流行的视频语言任务。优酷mPLUG可以让研究人员在未来进行更深入的多模态研究，开发更好的应用。此外，我们发布了流行的视频语言预训练模型ALPRO和mPLUG-2，以及我们提出的在优酷mPLUG上预训练的仅模块化解码器模型mPLUG视频。实验表明，在优酷mPLUG上预训练的模型在视频类别分类方面获得了高达23.1%的改进。此外，mPLUG视频在这些基准测试中取得了最先进的成绩，视频类别分类的前1名准确率分别为80.5%，视频字幕的CIDEr得分为68.9。最后，我们将基于仅1.7%可训练参数的冻结Bloomz的mPLUG视频放大为中文多模式LLM，并展示了令人印象深刻的教学和视频理解能力。零样本指令理解实验表明，使用Youku-mPLUG进行预处理可以提高理解整体和详细视觉语义、识别场景文本和利用开放领域知识的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04362v1" target="_blank">2306.04362v1</a>
                              </td>
                              <td>Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</td>
                              <td>Haiyang Xu</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04362v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04362v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18226v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18226v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18226v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18226v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As the use of Large Language Models (LLMs) in text generation tasks proliferates, concerns arise over their potential to compromise academic integrity. The education sector currently tussles with distinguishing student-authored homework assignments from AI-generated ones. This paper addresses the challenge by introducing HowkGPT, designed to identify homework assignments generated by AI. HowkGPT is built upon a dataset of academic assignments and accompanying metadata [17] and employs a pretrained LLM to compute perplexity scores for student-authored and ChatGPT-generated responses. These scores then assist in establishing a threshold for discerning the origin of a submitted assignment. Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection. This study emphasizes the critical need for effective strategies to uphold academic integrity amidst the growing influence of LLMs and provides an approach to ensuring fair and accurate grading in educational institutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18226v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）在文本生成任务中的使用激增，人们开始担心它们可能会损害学术诚信。教育部门目前正在努力区分学生编写的家庭作业和人工智能生成的家庭作业。本文通过引入旨在识别人工智能生成的家庭作业的HowkGPT来应对这一挑战。HowkGPT建立在学术作业和相关元数据的数据集[17]之上，并使用预训练的LLM来计算学生撰写和ChatGPT生成的回答的困惑分数。然后，这些分数有助于建立识别提交作业来源的阈值。鉴于学术工作的特殊性和上下文性质，HowkGPT通过定义从元数据中导出的特定类别阈值来进一步完善其分析，提高了检测的准确性。这项研究强调，在LLM日益增长的影响下，迫切需要有效的策略来维护学术诚信，并提供了一种确保教育机构公平准确评分的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18226v2" target="_blank">2305.18226v2</a>
                              </td>
                              <td>HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis</td>
                              <td>Christoforos Vasilatos</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18226v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18226v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04349v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT Self-Supervision for a Better Data Annotator</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04349v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04349v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04349v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and original data serves as a self-supervision navigator to refine the process. In the generation stage, the optimally selected one-shot sample serves as a template in the prompt and is applied to generating summaries from challenging datasets. The annotation performance is evaluated by tuning several human feedback reward networks and by calculating alignment scores between original and recovered data at both sentence and structure levels. Our self-supervised annotation method consistently achieves competitive scores, convincingly demonstrating its robust strength in various data-to-summary annotation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04349v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将数据注释为简明摘要的任务在各个领域都提出了重大挑战，通常需要人类专家分配大量时间和专业知识。尽管现有的努力是将大型语言模型用于注释任务，但诸如对未标记数据的适用性有限、缺乏自我监督的方法以及缺乏对复杂结构化数据的关注等重大问题仍然存在。在这项工作中，我们提出了一种GPT自监督注释方法。该方法体现了一种生成-恢复范式，该范式利用了生成预训练转换器（GPT）中的一次性学习能力。所提出的方法包括一次调谐阶段，随后是生成阶段。在一次性调优阶段，我们从支持集中采样一个数据，作为GPT生成文本摘要的提示的一部分，然后用于恢复原始数据。恢复的数据和原始数据之间的对齐分数可以作为完善过程的自我监督导航器。在生成阶段，最佳选择的一次性样本用作提示中的模板，并用于从具有挑战性的数据集生成摘要。注释性能是通过调整几个人类反馈奖励网络并计算句子和结构级别上原始数据和恢复数据之间的对齐分数来评估的。我们的自监督注释方法始终获得有竞争力的分数，令人信服地证明了其在各种数据中的强大实力，以总结注释任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04349v1" target="_blank">2306.04349v1</a>
                              </td>
                              <td>GPT Self-Supervision for a Better Data Annotator</td>
                              <td>Xiaohuan Pei</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04349v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04349v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07438v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tractable Control for Autoregressive Language Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07438v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07438v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07438v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution ${\Pr}(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models (TPMs) to impose lexical constraints in autoregressive text generation models, which we refer to as GeLaTo (Generating Language with Tractable Constraints). To demonstrate the effectiveness of this framework, we use distilled hidden Markov models, where we can efficiently compute ${\Pr}(\text{text} | \alpha)$, to guide autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on challenging benchmarks for constrained text generation (e.g., CommonGen), beating various strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive TPMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07438v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管自回归大型语言模型在文本生成中取得了成功，但生成满足复杂约束的文本仍然是一个主要挑战：即使对于最简单的词汇约束$\alpha$，从条件分布${\Pr}（\text{text}|\alpha$$）中采样也是难以解决的。为了克服这一挑战，我们建议使用可跟踪概率模型（TPM）在自回归文本生成模型中施加词汇约束，我们称之为GeLaTo（具有可跟踪约束的生成语言）。为了证明该框架的有效性，我们使用提取的隐马尔可夫模型，其中我们可以有效地计算$｛\Pr｝（\text｛text｝|\alpha）$，以指导GPT2的自回归生成。GeLaTo在具有挑战性的约束文本生成基准（例如CommonGen）上实现了最先进的性能，大大超过了各种强大的基准。我们的工作不仅为控制大型语言模型开辟了新的途径，而且还推动了更具表现力的TPM的开发。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07438v3" target="_blank">2304.07438v3</a>
                              </td>
                              <td>Tractable Control for Autoregressive Language Generation</td>
                              <td>Honghua Zhang</td>
                              <td>2023-04-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07438v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07438v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00739v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00739v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00739v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00739v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other challenging variants of Spider and demonstrate the superior generalization capability of SQL-PaLM. In addition, via extensive case studies, we demonstrate the impressive intelligent capabilities and various success enablers of LLM-based Text-to-SQL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00739v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询、文本转换为SQL的任务，LLM的自适应至关重要，无论是在上下文学习还是微调设置中，都取决于所使用的自适应数据量。在本文中，我们提出了一种基于LLM的文本到SQL模型SQL PaLM，利用PaLM-2，在这两种设置中都推动了最先进的技术。少镜头SQL PaLM基于一种为Text to SQL设计的基于执行的自一致性提示方法，并在Spider上实现了77.3%的测试套件准确率，据我们所知，这是第一个以4%的显著优势进行微调的测试套件。此外，我们还证明了微调后的SQL-PALM的性能比它进一步提高了1%。为了将SQL PaLM应用于真实世界的场景，我们进一步评估了它在Spider的其他具有挑战性的变体上的稳健性，并展示了SQL PaLM卓越的泛化能力。此外，通过广泛的案例研究，我们展示了基于LLM的Text to SQL令人印象深刻的智能功能和各种成功因素。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00739v2" target="_blank">2306.00739v2</a>
                              </td>
                              <td>SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL</td>
                              <td>Ruoxi Sun</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00739v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00739v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04188v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Dataset and Empirical Study for Sentence Simplification in Chinese</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04188v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04188v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04188v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04188v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>句子简化是一种有价值的技巧，对语言学习者和儿童都有很大的好处。然而，目前的研究更多地集中在英语句子的简化上。由于缺乏数据，汉语句子简化的发展相对缓慢。为了缓解这一限制，本文介绍了一个新的评估汉语句子简化的数据集CSS。我们从人类注释者那里收集人工简化，并进行数据分析，以显示英语和汉语句子简化之间的差异。此外，我们在CSS上测试了几种无监督和零/少镜头学习方法，并分析了自动评估和人工评估的结果。最后，通过对大型语言模型在CSS上的评价，探讨了大型语言模型能否成为高质量的汉语句子简化系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04188v1" target="_blank">2306.04188v1</a>
                              </td>
                              <td>A New Dataset and Empirical Study for Sentence Simplification in Chinese</td>
                              <td>Shiping Yang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04188v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04188v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18466v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Test-Time Training on Nearest Neighbors for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18466v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18466v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18466v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are important. Our work establishes a valuable first baseline for implementing test-time training in the context of large language models, opening the door to numerous promising research avenues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18466v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的许多努力旨在利用在测试时从数据库中检索到的相关信息来增强语言模型。我们通过使用其标准训练设置在测试时检索的数据上直接微调模型，避免了快速工程的需要。为此，我们基于Pile数据集的文本嵌入构建了一个大规模的分布式最近邻索引。给定对语言模型的查询，我们的系统检索查询的邻居，并在与这些邻居对应的文本数据上微调模型。令人惊讶的是，在多达20个邻居上检索和训练，每个邻居只进行一次梯度迭代，大大提高了Pile基准测试中20多个语言建模任务的性能。例如，测试时间训练显著缩小了小型GPT2模型和GPTNeo模型之间的性能差距，后者大了十倍以上，专门训练为在桩上收敛。然而，充足的指数质量和规模是重要的。我们的工作为在大型语言模型的背景下实施测试时间训练建立了有价值的第一个基线，为许多有前景的研究途径打开了大门。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18466v2" target="_blank">2305.18466v2</a>
                              </td>
                              <td>Test-Time Training on Nearest Neighbors for Large Language Models</td>
                              <td>Moritz Hardt</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18466v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18466v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04140v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04140v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04140v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04140v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user's domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04140v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）可用于生成用于训练和评估其他模型的文本数据。然而，使用LLM创建高质量的数据集可能具有挑战性。在这项工作中，我们探索了人类与人工智能的伙伴关系，以促进基于LLM的文本数据生成的高度多样性和准确性。我们首先研究了两种使文本生成多样化的方法：1）logit抑制，它最大限度地减少了已经频繁生成的语言的生成；2）温度采样，它使令牌采样概率变平。我们发现，多样化方法可以增加数据的多样性，但往往以数据准确性为代价（即文本和标签适合目标领域）。为了解决这个问题，我们研究了两种人为干预措施，1）标签替换（LR），纠正未对齐的标签，以及2）范围外过滤（OOSF），删除用户感兴趣领域之外或未考虑标签适用的实例。通过oracle研究，我们发现LR将使用多样化数据集训练的模型的绝对准确率提高了14.4%。此外，我们发现，使用LR干预生成的数据训练的一些模型优于基于LLM的少镜头分类。相反，OOSF在提高模型准确性方面并不有效，这意味着未来需要在人在环文本数据生成方面进行工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04140v1" target="_blank">2306.04140v1</a>
                              </td>
                              <td>Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions</td>
                              <td>John Joon Young Chung</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04140v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04140v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04136v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04136v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04136v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04136v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04136v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）能够根据预培训期间存储在参数中的内部知识执行零样本闭卷问答任务。然而，这种内化的知识可能是不充分和不正确的，这可能导致LLM产生事实上错误的答案。此外，微调LLM以更新其知识是昂贵的。为此，我们建议在LLM的输入中直接增加知识。具体来说，我们首先根据问题及其相关事实之间的语义相似性，从知识图中检索输入问题的相关事实。之后，我们以提示的形式将检索到的事实预先添加到输入问题中，然后将其转发给LLM以生成答案。我们的框架Knowledge-Augmented language model PromptING（KAPING）不需要模型训练，因此完全是零样本。我们验证了我们的KAPING框架在知识图问答任务中的性能，该任务旨在基于知识图上的事实回答用户的问题，在不同大小的多个LLM中，我们的KAPING框架平均比相关的零样本基线高出48%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04136v1" target="_blank">2306.04136v1</a>
                              </td>
                              <td>Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering</td>
                              <td>Jinheon Baek</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04136v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04136v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14400v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14400v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14400v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14400v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text -> raster image -> vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text -> vector graphics script) through pretrained large language models. However, these methods still suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to fully exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively and qualitatively. Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14400v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可缩放矢量图形（SVG）是一种流行的矢量图像格式，它为交互性和动画提供了良好的支持。尽管SVG具有吸引人的特性，但由于理解SVG语法或熟悉专业编辑软件需要陡峭的学习曲线，创建自定义SVG内容对用户来说可能很有挑战性。文本到图像生成的最新进展激发了研究人员探索矢量图形合成，使用基于图像的方法（即文本->光栅图像->矢量图形）将文本到图像的生成模型与图像矢量化相结合，或者通过预训练的大语言模型使用基于语言的方法（如文本->矢量图形脚本）。然而，这些方法在生成质量、多样性和灵活性方面仍然存在局限性。在本文中，我们介绍了IconShop，一种使用自回归变换器的文本引导矢量图标合成方法。我们的方法成功的关键是将SVG路径（以及作为指导的文本描述）序列化和标记化为唯一可解码的标记序列。这样，我们就能够充分利用自回归变换器的序列学习能力，同时实现无条件和文本条件的图标合成。通过在大规模矢量图标数据集上预测下一个标记的标准训练，并伴随着文本描述，所提出的IconShop在数量和质量上始终表现出比现有的基于图像和基于语言的方法更好的图标合成能力。同时，我们观察到世代多样性的显著改善，这一点得到了客观的唯一性和新颖性测量的验证。更重要的是，我们通过多种新颖的图标合成任务展示了IconShop的灵活性，包括图标编辑、图标插值、图标语义组合和图标设计自动建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14400v4" target="_blank">2304.14400v4</a>
                              </td>
                              <td>IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers</td>
                              <td>Ronghuan Wu</td>
                              <td>2023-04-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14400v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14400v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04085v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04085v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04085v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04085v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04085v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>跨语言语义解析（CLSP）旨在将多种自然语言（NL）中的查询转换为含义表示（MR），如SQL、lambda演算和逻辑形式。然而，现有的CLSP模型是在有限任务和应用的数据集上单独提出和评估的，阻碍了在各种NL和MR上对CLSP进行全面统一的评估。为此，我们提出了XSemPLR，一个跨语言语义解析的统一基准，通过检查和选择9个现有数据集来覆盖5个任务和164个领域，以22种自然语言和8种含义表示为特征。我们使用XSemPLR对各种多语言模型进行了全面的基准研究，包括基于编码器的模型（mBERT、XLM-R）、编码器-解码器模型（mBART、mT5）和基于解码器的模型（Codex、BLOOM）。我们设计了6个实验设置，涵盖各种语言组合（单语、多语言、跨语言）和学习样本数量（完整数据集、少数快照和零样本）。我们的实验表明，与其他流行的模型相比，编码器-解码器模型（mT5）实现了最高的性能，并且多语言训练可以进一步提高平均性能。值得注意的是，多语言的大型语言模型（例如BLOOM）仍然不足以执行CLSP任务。我们还发现，对于多语言模型，单语训练和跨语迁移学习之间的表现差距仍然很大，尽管跨语少镜头训练可以缓解这一差距。我们的数据集和代码可在https://github.com/psunlpgroup/XSemPLR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04085v1" target="_blank">2306.04085v1</a>
                              </td>
                              <td>XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations</td>
                              <td>Yusen Zhang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04085v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04085v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03872v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deductive Verification of Chain-of-Thought Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03872v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03872v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03872v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03872v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在执行各种推理任务时显著受益于思想链（CoT）提示。虽然CoT允许模型产生更全面的推理过程，但它对中间推理步骤的强调可能会无意中引入幻觉和累积错误，从而限制模型解决复杂推理任务的能力。受人类如何参与细致的演绎逻辑推理过程来解决任务的启发，我们试图使语言模型能够执行明确而严格的演绎推理，并通过自我验证确保其推理过程的可信度。然而，即使使用像ChatGPT这样的高级模型，直接验证整个演绎推理过程的有效性也是具有挑战性的。有鉴于此，我们建议将推理验证过程分解为一系列循序渐进的子过程，每个子过程只接收其必要的上下文和前提。为了促进这一过程，我们提出了自然程序，一种基于自然语言的演绎推理格式。我们的方法使模型能够生成精确的推理步骤，其中后续步骤更严格地基于先前步骤。它还使语言模型能够以逐步的方式进行推理自验证。通过将这个验证过程集成到每个演绎推理阶段，我们显著提高了生成推理步骤的严谨性和可靠性。在这个过程中，我们还提高了复杂推理任务的答案正确性。代码将于发布https://github.com/lz1oceani/verify_cot.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03872v2" target="_blank">2306.03872v2</a>
                              </td>
                              <td>Deductive Verification of Chain-of-Thought Reasoning</td>
                              <td>Zhan Ling</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03872v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03872v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03341v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03341v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03341v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03341v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03341v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了推理时间干预（ITI），这是一种旨在增强大型语言模型（LLM）真实性的技术。ITI通过在推理过程中改变模型激活来运作，遵循有限数量的注意力头上的一组方向。这种干预显著提高了LLaMA模型在TruthfulQA基准上的性能。在一个名为Alpaca的经过微调的LLaMA指令中，ITI将其真实性从32.5%提高到65.1%。我们确定了真实性和帮助性之间的权衡，并展示了如何通过调整干预强度来平衡它。ITI是一种微创且计算成本低廉的技术。此外，该技术是数据高效的：虽然像RLHF这样的方法需要大量的注释，但ITI只使用几百个例子来定位真实的方向。我们的研究结果表明，LLM可能对某些事情是真的可能性有一个内部表征，即使它们表面上会产生虚假信息。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03341v2" target="_blank">2306.03341v2</a>
                              </td>
                              <td>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</td>
                              <td>Kenneth Li</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03341v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03341v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02231v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-Tuning Language Models with Advantage-Induced Policy Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02231v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02231v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02231v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also provide a theoretical justification supporting the design of our loss function.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02231v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从人类反馈中强化学习（RLHF）已经成为将大型语言模型（LLM）与人类偏好相一致的可靠方法。在众多的RLHF技术中，近端策略优化（PPO）是应用最广泛的方法之一。然而，尽管PPO很受欢迎，但它可能会遭受模式崩溃、不稳定和较差的采样效率。我们证明，这些问题可以通过一种新的算法来缓解，我们称之为优势诱导策略对齐（APA），该算法利用了基于估计优势的平方误差损失函数。我们实证证明，当采用单独的奖励模型作为评估者时，APA在语言任务中始终以很大的优势优于PPO。此外，与PPO相比，APA对模型初始策略的偏差提供了更稳定的控制形式，确保模型在不崩溃为确定性输出的情况下提高了性能。除了实证结果外，我们还为我们的损失函数的设计提供了理论依据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02231v2" target="_blank">2306.02231v2</a>
                              </td>
                              <td>Fine-Tuning Language Models with Advantage-Induced Policy Alignment</td>
                              <td>Banghua Zhu</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02231v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02231v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04050v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMZip: Lossless Text Compression using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04050v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04050v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04050v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04050v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们使用大型语言模型LLaMA-7B作为给定过去标记窗口的下一个标记的预测器，提供了英语熵的渐近上界的新估计。这一估计值明显小于目前在\cite{cover1978convergent}、\cite{lutati2023focus}中可用的估计值。一种自然的副产品是用于英语文本无损压缩的算法，该算法将来自大语言模型的预测与无损压缩方案相结合。有限实验的初步结果表明，我们的方案优于最先进的文本压缩方案，如BSC、ZPAQ和paq8h。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04050v1" target="_blank">2306.04050v1</a>
                              </td>
                              <td>LLMZip: Lossless Text Compression using Large Language Models</td>
                              <td>Chandra Shekhara Kaushik Valmeekam</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04050v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04050v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04031v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Certified Reasoning with Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04031v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04031v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04031v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. To tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. A guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. In turn, the model's choices can change the guide's state. We show how a general system for logical reasoning can be used as a guide, which we call LogicGuide. Given a reasoning problem in natural language, a model can formalize its assumptions for LogicGuide and then guarantee that its reasoning steps are sound. In experiments with the PrOntoQA and ProofWriter reasoning datasets, LogicGuide significantly improves the performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35%). LogicGuide also drastically reduces content effects: the interference of prior and current assumptions that both humans and language models have been shown to suffer from. Finally, we explore bootstrapping LLaMA 13B from its own reasoning and find that LogicGuide is critical: by training only on certified self-generated reasoning, LLaMA can self-improve, avoiding learning from its own hallucinations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04031v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在复杂任务中逐步推理时，语言模型通常能获得更高的准确性。然而，他们的推理可能是不健全的、不一致的，或者依赖于不可取的先前假设。为了解决这些问题，我们为语言模型引入了一类称为指南的工具，这些工具使用状态和增量约束来指导生成。模型可以调用指南，以将其自身的生成约束为工具提供的一组有效语句。反过来，模型的选择可以更改向导的状态。我们展示了如何使用逻辑推理的通用系统作为指南，我们称之为LogicGuide。给定自然语言中的推理问题，模型可以为LogicGuide形式化其假设，然后保证其推理步骤是正确的。在PrOntoQA和ProofWriter推理数据集的实验中，LogicGuide显著提高了GPT-3、GPT-3.5 Turbo和LLaMA的性能（准确率提高了35%）。LogicGuide还大大减少了内容影响：人类和语言模型都受到先前和当前假设的干扰。最后，我们从LLaMA 13B自身的推理中探索了自举，并发现LogicGuide是至关重要的：通过只训练认证的自生成推理，LLaMA可以自我改进，避免从自己的幻觉中学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04031v1" target="_blank">2306.04031v1</a>
                              </td>
                              <td>Certified Reasoning with Language Models</td>
                              <td>Gabriel Poesia</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04031v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04031v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03978v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Büyük dil modellerinin Türkçe verisetleri ile eğitilmesi ve ince ayarlanması</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03978v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03978v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03978v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models have advanced enormously, gained vast attraction and are having a phase of intensed research. Some of the developed models and training datasets have been made open-accessible. Hence these may be further fine-tuned with some techniques to obtain specialized models for specific tasks. When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets. In this work, we propose some ideas to mitigate this issue: creating large Turkish datasets, training LLMs with these and fine-tuning pre-trained models with Turkish inputs. We report our findings on Turkish-based trainings with the problems encountered along the way. We conclude with outcomes of these experiments and propose ideas for further works.   --   B\"uy\"uk dil modelleri inan{\i}lmaz \"ol\c{c}\"ude geli\c{s}mekte, b\"uy\"uk ilgi toplayarak ve \"uzerlerinde yo\u{g}un ara\c{s}tirmalarin yapildi\u{g}i bir d\"onemdedirler. Geli\c{s}tirilen modeller ve e\u{g}itimde kullanilan verisetlerinden bazilari a\c{c}ik eri\c{s}imli olarak sunulmaktadir. B\"oylece ince ayarlama teknikleri uygulayarak \"ozelle\c{s}mi\c{s} g\"orevler i\c{c}in \c{c}ali\c{s}abilir modeller elde edilmektedir. T\"urk\c{c}e s\"oz konusu oldu\u{g}unda bu modellerinin kapsayicili\u{g}i yeterli d\"uzeyde de\u{g}ildir. Bu durum, yayimlanan verisetlerinde de g\"ozlemlenebilir. Bunu a\c{s}manin yollari T\"urk\c{c}e i\c{c}erikli b\"uy\"uk verisetlerinin olu\c{s}turulmasi, b\"uy\"uk dil modellerinin bunlarla e\u{g}itilmesi ve \"onceden e\u{g}itilmi\c{s} modellerin T\"urk\c{c}e girdilerle ince ayarlanmalari olabilir. Bu \c{c}ali\c{s}mada a\c{c}ik eri\c{s}imli dil modelleri ve verisetleri \"uzerinde durulmakta ve T\"urk\c{c}e temelli bazi deneyler, kar\c{s}ila\c{s}ilan sorunlar ve sonu\c{c}lar irdelenmektedir.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03978v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型已经取得了巨大的进步，获得了巨大的吸引力，并正处于深入研究的阶段。一些开发的模型和训练数据集已经开放访问。因此，可以使用一些技术对这些模型进行进一步的微调，以获得用于特定任务的专用模型。当谈到土耳其语时，开放获取模式并不能提供令人满意的覆盖范围。在已发布的数据集上也观察到了这一点。在这项工作中，我们提出了一些缓解这一问题的想法：创建大型土耳其数据集，用这些数据集训练LLM，并用土耳其输入微调预先训练的模型。我们报告了我们对土耳其培训的调查结果以及在此过程中遇到的问题。我们总结了这些实验的结果，并提出了进一步工作的想法。——B\“uy”uk dil modelleri inan｛\i｝lmaz \“ol \c c｝\”ude geli \c s｝mekte，B\“uy\”uk ilgi toplayarak ve \“uzerlerinde yo \u｛g｝un ara \c s’tirmalarin yapildi \u｛。Geli\c｛s｝tirilen modeller ve e e｛g｝itimde kullanilan verisetlerinden bazilari a \c \c｝ik eri\c｛。B\“oylece ince ayarlama teknikleri vigulayarak\”ozelle\c｛s｝mi\c｛s｝g\“orevler i \c c｝in \c｝ali\c \s｝abilir modeller elde edilmekedir。T\“urk \c c｛e s”oz konusu oldu \u｛g｝unda bu modellerin kapsayicili\u g｝i yeterli d\”uzeyde de \u。在这段时间里，你可以看到一个叫“Ozlelenebilir”的模型。Bunu a \c｛s｝manin yollari T\“urk \c e i \c｛奥拉比尔。Bu\c ali\c s mada a \c ik eri\c s imli dil modelleri ve verisetleri“uzerinde durulmakta ve T\”urk \c e temelli bazi deneyler，kar\c s ila\c s silan sorunlar ve sonu \c s ilan irdelenmektedir的模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03978v1" target="_blank">2306.03978v1</a>
                              </td>
                              <td>Büyük dil modellerinin Türkçe verisetleri ile eğitilmesi ve ince ayarlanması</td>
                              <td>A. Taha Arslan</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03978v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03978v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03959v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03959v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03959v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03959v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectiveness of our approach on prominent task-oriented dialogue datasets, Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue tasks: action state tracking and workflow discovery. Our results demonstrate that procedural knowledge augmentation improves accuracy predicting in- and out-of-distribution actions while preserving high performance in settings with low or sparse data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03959v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>面向任务的对话通常需要代理制定复杂的多步骤过程，以满足用户的请求。虽然大型语言模型已经成功地在受限的环境中实现了这些对话的自动化，但它们的广泛部署受到训练所需的大量特定任务数据的限制。以下论文提供了一个数据高效的解决方案，用于构建对话系统，利用从代理指南（如公司政策或客户服务手册）中得出的明确指示。我们提出的知识增强对话系统（KADS）将大型语言模型与知识检索模块相结合，该模块在给定用户-代理交互的情况下，从预定义的一组策略中提取概述相关过程的文档。为了训练该系统，我们引入了一种半监督预训练方案，该方案采用了对话文档匹配和部分参数冻结的面向动作的掩蔽语言建模。我们在突出的面向任务的对话数据集、基于行动的对话数据集中和模式引导的对话上评估了我们的方法对两个对话任务的有效性：行动状态跟踪和工作流发现。我们的结果表明，过程知识增强提高了预测分布内和分布外动作的准确性，同时在数据量低或稀疏的环境中保持了高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03959v1" target="_blank">2306.03959v1</a>
                              </td>
                              <td>Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction</td>
                              <td>Julia White</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03959v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03959v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03950v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MISGENDERED: Limits of Large Language Models in Understanding Pronouns</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03950v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03950v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03950v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.   Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce MISGENDERED, a framework for evaluating large language models' ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual's pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03950v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>内容警告：本文包含了误导和删除的例子，这些例子可能具有攻击性和潜在的触发作用。语言技术中的性别偏见已经被广泛研究，但研究大多局限于性别的二元范式。同样重要的是要考虑非二元性别认同，因为将其排除在外可能会对已经被边缘化的群体造成进一步伤害。在本文中，我们全面评估了流行语言模型正确使用英语中性代词（例如单数they，them）和新代词（例如ze，xe，thon）的能力，这些代词是由性别认同不由二元代词表示的个人使用的。我们介绍了MISGENDERED，这是一个评估大型语言模型正确使用首选代词能力的框架，包括（i）声明个人代词的实例，然后是带有缺失代词的句子，以及（ii）使用统一方法评估掩蔽和自回归语言模型的实验设置。当提示开箱即用时，语言模型在正确预测新代词（平均准确率7.6%）和中性代词（平均正确率31.0%）方面表现不佳。这种无法概括的原因是训练数据和记忆联想中缺乏非二元代词的表示。很少有镜头改编，提示中有明确的例子可以提高表现，但新代词的表现仅稳定在45.4%。我们在上发布了完整的数据集、代码和演示https://tamannahossainkay.github.io/misgendered/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03950v1" target="_blank">2306.03950v1</a>
                              </td>
                              <td>MISGENDERED: Limits of Large Language Models in Understanding Pronouns</td>
                              <td>Tamanna Hossain</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03950v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03950v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03917v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Turning large language models into cognitive models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03917v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03917v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03917v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03917v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型是功能强大的系统，擅长许多任务，从翻译到数学推理。然而，与此同时，这些模型往往表现出非人的特征。在本文中，我们解决了这一差距，并询问大型语言模型是否可以转化为认知模型。我们发现，在根据心理学实验的数据对其进行微调后，这些模型提供了人类行为的准确表示，甚至在两个决策领域都优于传统的认知模型。此外，我们还表明，他们的表征包含了在个体主体层面上建模行为所需的信息。最后，我们证明了对多个任务的微调使大型语言模型能够预测以前看不见的任务中的人类行为。总之，这些结果表明，经过预先训练的大型模型可以适应成为广义认知模型，从而开辟新的研究方向，从而改变认知心理学和整个行为科学。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03917v1" target="_blank">2306.03917v1</a>
                              </td>
                              <td>Turning large language models into cognitive models</td>
                              <td>Marcel Binz</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03917v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_10226v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Watermark for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_10226v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_10226v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_10226v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_10226v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的潜在危害可以通过对模型输出进行水印处理来减轻，即将信号嵌入到生成的文本中，这些信号对人类来说是不可见的，但在算法上可以从短时间的令牌中检测到。我们提出了一个用于专有语言模型的水印框架。嵌入水印对文本质量的影响可以忽略不计，并且可以使用高效的开源算法进行检测，而无需访问语言模型API或参数。水印的工作原理是在生成单词之前选择一组随机的“绿色”标记，然后在采样过程中温和地促进绿色标记的使用。我们提出了一种检测具有可解释p值的水印的统计测试，并推导了一个分析水印灵敏度的信息论框架。我们使用来自开放预训练变换器（OPT）家族的数十亿参数模型来测试水印，并讨论了鲁棒性和安全性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.10226v3" target="_blank">2301.10226v3</a>
                              </td>
                              <td>A Watermark for Large Language Models</td>
                              <td>John Kirchenbauer</td>
                              <td>2023-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_10226v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.10226v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03856v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Iterative Translation Refinement with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03856v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03856v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03856v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models have shown surprising performances in understanding instructions and performing natural language tasks. In this paper, we propose iterative translation refinement to leverage the power of large language models for more natural translation and post-editing. We show that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality. Further, human evaluations demonstrate that our method effectively reduces translationese compared to initial GPT translations and even human references, especially for into-English directions. Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03856v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型在理解指令和执行自然语言任务方面表现出了令人惊讶的表现。在本文中，我们提出了迭代翻译精化，以利用大型语言模型的力量进行更自然的翻译和后期编辑。我们表明，通过在迭代过程中简单地涉及一个大型语言模型，输出质量可以提高，而不仅仅是翻译。GPT-3.5的广泛测试场景表明，尽管迭代降低了基于字符串的度量分数，但神经度量表明，即使没有提高翻译质量，也具有可比性。此外，人工评估表明，与最初的GPT翻译甚至人工参考相比，我们的方法有效地减少了翻译酶，尤其是针对英语方向。消融研究强调了将精化过程锚定在源输入和合理的初始翻译上的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03856v1" target="_blank">2306.03856v1</a>
                              </td>
                              <td>Iterative Translation Refinement with Large Language Models</td>
                              <td>Pinzhen Chen</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03856v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03856v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00551v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00551v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00551v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00551v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital textbooks have become an integral part of everyday learning tasks. In this work, we consider the use of digital textbooks for programming classes. Generally, students struggle with utilizing textbooks on programming to the maximum, with a possible reason being that the example programs provided as illustration of concepts in these textbooks don't offer sufficient interactivity for students, and thereby not sufficiently motivating to explore or understand these programming examples better. In our work, we explore the idea of enhancing the navigability of intelligent textbooks with the use of ``counterfactual'' questions, to make students think critically about these programs and enhance possible program comprehension. Inspired from previous works on nudging students on counter factual thinking, we present the possibility to enhance digital textbooks with questions generated using GPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00551v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数字教科书已经成为日常学习任务中不可或缺的一部分。在这项工作中，我们考虑在编程课程中使用数字教科书。一般来说，学生们很难最大限度地利用编程课本，可能的原因是这些课本中作为概念说明的示例程序没有为学生提供足够的互动性，因此没有足够的动力来更好地探索或理解这些编程示例。在我们的工作中，我们探索了通过使用“反事实”问题来提高智能教科书的可导航性的想法，以使学生对这些程序进行批判性思考，并提高可能的程序理解。受之前推动学生进行反事实思维的工作的启发，我们提出了通过使用GPT生成的问题来增强数字教科书的可能性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00551v2" target="_blank">2306.00551v2</a>
                              </td>
                              <td>Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions</td>
                              <td>Arun Balajiee Lekshmi Narayanan</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00551v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03819v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LEACE: Perfect linear concept erasure in closed form</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03819v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03819v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03819v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Concept erasure aims to remove specified features from a representation. It can be used to improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). In this paper, we introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while inflicting the least possible damage to the representation. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate the usefulness of our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03819v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概念擦除旨在从表示中删除指定的特征。它可以用于提高公平性（例如防止分类器使用性别或种族）和可解释性（例如删除概念以观察模型行为的变化）。在本文中，我们介绍了LEAst平方概念擦除（LEACE），这是一种可证明的封闭形式方法，它可以防止所有线性分类器检测到一个概念，同时对表示造成尽可能小的损害。我们通过一种称为“概念清理”的新过程将LEACE应用于大型语言模型，该过程从网络中的每一层删除目标概念信息。我们证明了我们的方法在两项任务上的有用性：测量语言模型对词性信息的依赖性，以及减少BERT嵌入中的性别偏见。代码位于https://github.com/EleutherAI/concept-erasure.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03819v1" target="_blank">2306.03819v1</a>
                              </td>
                              <td>LEACE: Perfect linear concept erasure in closed form</td>
                              <td>Nora Belrose</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03819v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03819v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03809v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can large language models democratize access to dual-use biotechnology?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03809v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03809v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03809v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields. However, these models may also confer easy access to dual-use technologies capable of inflicting great harm. To evaluate this risk, the 'Safeguarding the Future' course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic. In one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic DNA using reverse genetics, supplied the names of DNA synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization. Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training. Promising nonproliferation measures include pre-release evaluations of LLMs by third parties, curating training datasets to remove harmful concepts, and verifiably screening all DNA generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03809v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM），如嵌入“聊天机器人”中的模型，通过提供来自许多不同领域的可理解信息和专业知识，正在加速研究并使其民主化。然而，这些模式也可能使人们容易获得能够造成巨大伤害的两用技术。为了评估这一风险，麻省理工学院的“保护未来”课程要求非科学家学生调查LLM聊天机器人是否会被提示协助非专家引发疫情。在一个小时内，聊天机器人提出了四种潜在的大流行病原体，解释了如何使用反向遗传学从合成DNA中产生这些病原体，提供了不太可能筛选订单的DNA合成公司的名称，确定了详细的协议以及如何对其进行故障排除，并建议任何缺乏反向遗传学技能的人聘请核心机构或合同研究组织。总的来说，这些结果表明，LLM一旦被可靠地识别出来，即使是很少或根本没有受过实验室培训的人，也会让流行病类药物广泛使用。有希望的防扩散措施包括由第三方对LLM进行发布前评估，策划培训数据集以消除有害概念，并可验证地筛选合成供应商生成的或合同研究组织和机器人云实验室用于设计生物体或病毒的所有DNA。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03809v1" target="_blank">2306.03809v1</a>
                              </td>
                              <td>Can large language models democratize access to dual-use biotechnology?</td>
                              <td>Emily H. Soice</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03809v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03809v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03799v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03799v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03799v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03799v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid theoretical foundation for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt "Let's think step by step", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and fundamental theoretical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03799v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>提示工程是通过提供明确和具体的指令来增强大型语言模型（LLM）能力的一项重要技术。它使LLM能够胜任各种任务，如算术推理、问答、摘要、关系提取、机器翻译和情感分析。研究人员一直在积极探索不同的即时工程策略，如思想链（CoT）、零CoT和上下文学习。然而，一个尚未解决的问题是，目前的方法缺乏确定最佳提示的坚实理论基础。为了在即时工程中解决这个问题，我们提出了一种新的有效方法，称为即时空间。我们的方法利用文本嵌入通过矩阵分解获得基向量，然后构建一个表示所有提示的空间。Prompt Space在十个公共推理基准上显著优于最先进的Prompt范式。值得注意的是，在没有CoT方法和提示“让我们一步一步地思考”的帮助下，prompt Space显示出比少镜头方法优越的性能。总的来说，我们的方法为选择简单有效的提示提供了一个稳健而基本的理论框架。这一进步标志着朝着改进LLM中各种应用的即时工程迈出了重要一步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03799v1" target="_blank">2306.03799v1</a>
                              </td>
                              <td>Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models</td>
                              <td>Fobo Shi</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03799v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03799v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2306_04519v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04519v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04519v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04519v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04519v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多任务学习（MTL）可以通过与相关任务共享表示来提高神经网络的泛化性能。尽管如此，MTL也会通过任务之间的有害干扰降低性能。最近的工作追求特定任务的损失加权作为这种干扰的解决方案。然而，现有的算法将任务视为原子，缺乏在任务级别之外明确分离有害和有益信号的能力。为此，我们提出了SLGrad，一种用于带辅助任务的多任务学习的样本级加权算法。通过特定样本的任务权重，SLGrad在训练过程中重塑任务分布，以消除有害的辅助信号并增强有用的任务信号。在（半）合成数据集和常见的监督多任务问题上观察到了显著的泛化性能增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04519v1" target="_blank">2306.04519v1</a>
                              </td>
                              <td>Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks</td>
                              <td>Emilie Grégoire</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04519v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04519v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04226v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Normalization Layers Are All That Sharpness-Aware Minimization Needs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04226v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04226v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04226v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (comprising less than 0.1% of the total parameters) in the adversarial step of SAM outperforms perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness. The code for our experiments is publicly available at https://github.com/mueller-mp/SAM-ON.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04226v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>清晰度感知最小化（SAM）是为了降低最小值的清晰度而提出的，并已被证明可以在各种设置中提高泛化性能。在这项工作中，我们表明，在SAM的对抗性步骤中仅扰动仿射归一化参数（包括小于总参数的0.1%）优于扰动所有参数。这一发现推广到不同的SAM变体以及ResNet（批量规范化）和Vision Transformer（层规范化）架构。我们考虑了替代的稀疏扰动方法，发现这些方法在如此极端的稀疏性水平上无法实现类似的性能增强，这表明这种行为是归一化层独有的。尽管我们的研究结果重申了SAM在提高泛化性能方面的有效性，但他们怀疑这是否仅仅是由清晰度降低引起的。我们实验的代码可在https://github.com/mueller-mp/SAM-ON.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04226v1" target="_blank">2306.04226v1</a>
                              </td>
                              <td>Normalization Layers Are All That Sharpness-Aware Minimization Needs</td>
                              <td>Maximilian Mueller</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04226v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04226v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04178v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimal Transport Model Distributional Robustness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04178v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04178v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04178v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrate that sharpness-aware minimization (SAM) is a specific case of our framework when using a Dirac delta distribution over a single model, while our framework can be viewed as a probabilistic extension of SAM. We conduct extensive experiments to demonstrate the usefulness of our framework in the aforementioned settings, and the results show remarkable improvements in our approaches to the baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04178v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布鲁棒性是一个很有前途的框架，用于训练深度学习模型，这些模型不太容易受到对抗性示例和数据分布变化的影响。以前的工作主要集中在利用数据空间中的分布鲁棒性。在这项工作中，我们在模型空间上探索了一个基于最优传输的分布式鲁棒性框架。具体来说，我们研究了给定中心模型分布的Wasserstein球中的模型分布，该模型分布使损失最大化。我们已经开发了一些理论，使我们能够学习最优鲁棒中心模型分布。有趣的是，通过我们开发的理论，我们可以通过考虑中心模型分布的特定形式，如单个模型上的狄拉克-德尔塔分布、多个模型上的均匀分布和通用贝叶斯神经网络，将清晰度意识的概念灵活地纳入单个模型、集合模型和贝叶斯神经网络的训练中。此外，我们证明了当在单个模型上使用狄拉克-德尔塔分布时，清晰度感知最小化（SAM）是我们框架的一个特定情况，而我们的框架可以被视为SAM的概率扩展。我们进行了大量实验来证明我们的框架在上述设置中的有用性，结果显示，我们对基线的处理方法有了显著改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04178v1" target="_blank">2306.04178v1</a>
                              </td>
                              <td>Optimal Transport Model Distributional Robustness</td>
                              <td>Van-Anh Nguyen</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04178v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04178v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04121v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Matte Anything: Interactive Natural Image Matting with Segment Anything Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04121v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04121v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04121v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural image matting algorithms aim to predict the transparency map (alpha-matte) with the trimap guidance. However, the production of trimaps often requires significant labor, which limits the widespread application of matting algorithms on a large scale. To address the issue, we propose Matte Anything model (MatAny), an interactive natural image matting model which could produce high-quality alpha-matte with various simple hints. The key insight of MatAny is to generate pseudo trimap automatically with contour and transparency prediction. We leverage task-specific vision models to enhance the performance of natural image matting. Specifically, we use the segment anything model (SAM) to predict high-quality contour with user interaction and an open-vocabulary (OV) detector to predict the transparency of any object. Subsequently, a pretrained image matting model generates alpha mattes with pseudo trimaps. MatAny is the interactive matting algorithm with the most supported interaction methods and the best performance to date. It consists of orthogonal vision models without any additional training. We evaluate the performance of MatAny against several current image matting algorithms, and the results demonstrate the significant potential of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04121v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然图像遮片算法旨在利用三分图引导预测透明度图（阿尔法遮片）。然而，三角图的制作通常需要大量的劳动力，这限制了抠图算法的大规模广泛应用。为了解决这个问题，我们提出了Matte Anything模型（MatAny），这是一种交互式自然图像遮片模型，可以通过各种简单的提示生成高质量的alpha遮片。MatAny的关键见解是通过轮廓和透明度预测自动生成伪三分图。我们利用特定任务的视觉模型来增强自然图像抠图的性能。具体来说，我们使用分段任意模型（SAM）来预测具有用户交互的高质量轮廓，并使用开放词汇（OV）检测器来预测任何对象的透明度。随后，预训练的图像遮片模型生成具有伪修剪的阿尔法遮片。MatAny是迄今为止最受支持的交互方法和最佳性能的交互式抠图算法。它由正交视觉模型组成，没有任何额外的训练。我们评估了MatAny相对于当前几种图像抠图算法的性能，结果证明了我们的方法的巨大潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04121v1" target="_blank">2306.04121v1</a>
                              </td>
                              <td>Matte Anything: Interactive Natural Image Matting with Segment Anything Models</td>
                              <td>Jingfeng Yao</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04121v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04121v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03908v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM3D: Segment Anything in 3D Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03908v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03908v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03908v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we propose SAM3D, a novel framework that is able to predict masks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB images without further training or finetuning. For a point cloud of a 3D scene with posed RGB images, we first predict segmentation masks of RGB images with SAM, and then project the 2D masks into the 3D points. Later, we merge the 3D masks iteratively with a bottom-up merging approach. At each step, we merge the point cloud masks of two adjacent frames with the bidirectional merging approach. In this way, the 3D masks predicted from different frames are gradually merged into the 3D masks of the whole 3D scene. Finally, we can optionally ensemble the result from our SAM3D with the over-segmentation results based on the geometric information of the 3D scenes. Our approach is experimented with ScanNet dataset and qualitative results demonstrate that our SAM3D achieves reasonable and fine-grained 3D segmentation results without any training or finetuning of SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03908v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们提出了SAM3D，这是一种新的框架，能够通过利用RGB图像中的分段任意模型（SAM）来预测3D点云中的遮罩，而无需进一步训练或微调。对于具有姿态RGB图像的3D场景的点云，我们首先用SAM预测RGB图像的分割遮罩，然后将2D遮罩投影到3D点中。稍后，我们使用自下而上的合并方法迭代合并3D遮罩。在每一步中，我们都采用双向合并方法对两个相邻帧的点云掩码进行合并。通过这种方式，从不同帧预测的3D掩模逐渐合并到整个3D场景的3D掩膜中。最后，我们可以选择性地将来自我们的SAM3D的结果与基于3D场景的几何信息的过分割结果进行集成。我们的方法在ScanNet数据集上进行了实验，定性结果表明，我们的SAM3D在没有对SAM进行任何训练或微调的情况下实现了合理和细粒度的3D分割结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03908v1" target="_blank">2306.03908v1</a>
                              </td>
                              <td>SAM3D: Segment Anything in 3D Scenes</td>
                              <td>Yunhan Yang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03908v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03908v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03899v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Label-free Scene Understanding by Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03899v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03899v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03899v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. And for nuScenes dataset, our performance is 26.8% with an improvement of 6%. Code will be released (https://github.com/runnanchen/Label-Free-Scene-Understanding).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03899v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比视觉语言预训练（CLIP）和分段任意（SAM）等视觉基础模型在图像分类和分割任务中表现出了令人印象深刻的零样本性能。然而，将CLIP和SAM结合起来进行无标签场景理解还有待探索。在本文中，我们研究了视觉基础模型在使网络能够在没有标记数据的情况下理解2D和3D世界方面的潜力。主要挑战在于在极噪声伪标签下有效地监督网络，这些伪标签由CLIP生成，并在从2D域到3D域的传播过程中进一步加剧。为了应对这些挑战，我们提出了一种新的跨模态噪声监督（CNS）方法，该方法利用CLIP和SAM的优势来同时监督2D和3D网络。特别地，我们引入了一种预测一致性正则化来共同训练2D和3D网络，然后使用SAM的鲁棒特征表示进一步增强网络的潜在空间一致性。在不同的室内和室外数据集上进行的实验证明了我们的方法在理解2D和3D开放环境方面的卓越性能。我们的2D和3D网络在ScanNet上以28.4%和33.5%的mIoU实现了无标签语义分割，分别提高了4.7%和7.9%。对于nuScenes数据集，我们的性能提高了6%，达到26.8%。将发布代码(https://github.com/runnanchen/Label-Free-Scene-Understanding)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03899v1" target="_blank">2306.03899v1</a>
                              </td>
                              <td>Towards Label-free Scene Understanding by Vision Foundation Models</td>
                              <td>Runnan Chen</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03899v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03899v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03775v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Matched Pair Calibration for Ranking Fairness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03775v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03775v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03775v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03775v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于分数的排名系统中的公平性测试，称为匹配对校准。我们的方法构造了一组匹配的项目对，在计算该组的适当排名误差之前，子组之间的混淆差异最小。匹配步骤确保我们比较相同评分项目之间的亚组结果，以便测量的表现差异直接意味着亚组水平暴露的不公平。我们展示了我们的方法如何将校准的公平直觉从二元分类设置推广到排名，并将我们的方法与其他排名公平措施的建议联系起来。此外，我们的策略显示了边际结果测试的逻辑如何扩展到分析师可以访问模型分数的情况。最后，我们提供了一个将匹配对校准应用于真实单词排名数据集的例子，以证明其在检测排名偏差方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03775v1" target="_blank">2306.03775v1</a>
                              </td>
                              <td>Matched Pair Calibration for Ranking Fairness</td>
                              <td>Hannah Korevaar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03775v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03775v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_01513v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Safe AI for health and beyond -- Monitoring to transform a health service</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_01513v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_01513v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_01513v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Machine learning techniques are effective for building predictive models because they identify patterns in large datasets. Development of a model for complex real-life problems often stop at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as patient demographics, systems and clinical practices change. The maintenance and monitoring of predictive model performance post-publication is crucial to enable their safe and effective long-term use. We will assess the infrastructure required to monitor the outputs of a machine learning algorithm, and present two scenarios with examples of monitoring and updates of models, firstly on a breast cancer prognosis model trained on public longitudinal data, and secondly on a neurodegenerative stratification algorithm that is currently being developed and tested in clinic.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_01513v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习技术对于构建预测模型是有效的，因为它们可以识别大型数据集中的模式。针对复杂现实问题的模型的开发通常在发布、概念验证或通过某种部署模式访问时停止。然而，随着患者人口统计、系统和临床实践的变化，医学领域的模型有被淘汰的风险。预测模型发布后性能的维护和监测对于实现其安全有效的长期使用至关重要。我们将评估监测机器学习算法输出所需的基础设施，并提供两种场景，包括监测和更新模型的示例，首先是基于公共纵向数据训练的乳腺癌症预后模型，其次是目前正在开发和临床测试的神经变性分层算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.01513v3" target="_blank">2303.01513v3</a>
                              </td>
                              <td>Safe AI for health and beyond -- Monitoring to transform a health service</td>
                              <td>Mahed Abroshan</td>
                              <td>2023-03-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_01513v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.01513v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02291v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02291v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02291v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02291v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce 3rd place solution for PVUW2023 VSS track. Semantic segmentation is a fundamental task in computer vision with numerous real-world applications. We have explored various image-level visual backbones and segmentation heads to tackle the problem of video semantic segmentation. Through our experimentation, we find that InternImage-H as the backbone and Mask2former as the segmentation head achieves the best performance. In addition, we explore two post-precessing methods: CascadePSP and Segment Anything Model (SAM). Ultimately, our approach obtains 62.60\% and 64.84\% mIoU on the VSPW test set1 and final test set, respectively, securing the third position in the PVUW2023 VSS track.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02291v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了PVUW2023 VSS磁道的第三位解决方案。语义分割是计算机视觉中的一项基本任务，在现实世界中有许多应用。我们已经探索了各种图像级的视觉主干和分割头来解决视频语义分割的问题。通过实验，我们发现InternImage-H作为主干，Mask2former作为分割头可以获得最佳的性能。此外，我们还探讨了两种后进动方法：CascadePSP和Segment Anything Model（SAM）。最终，我们的方法在VSPW测试集1和最终测试集上分别获得62.60\%和64.84\%的mIoU，确保了PVUW2023 VSS轨迹中的第三个位置。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02291v2" target="_blank">2306.02291v2</a>
                              </td>
                              <td>3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW</td>
                              <td>Shijie Chang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02291v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02291v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03263v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient automatic design of robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03263v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03263v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03263v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots are notoriously difficult to design because of complex interdependencies between their physical structure, sensory and motor layouts, and behavior. Despite this, almost every detail of every robot built to date has been manually determined by a human designer after several months or years of iterative ideation, prototyping, and testing. Inspired by evolutionary design in nature, the automated design of robots using evolutionary algorithms has been attempted for two decades, but it too remains inefficient: days of supercomputing are required to design robots in simulation that, when manufactured, exhibit desired behavior. Here we show for the first time de-novo optimization of a robot's structure to exhibit a desired behavior, within seconds on a single consumer-grade computer, and the manufactured robot's retention of that behavior. Unlike other gradient-based robot design methods, this algorithm does not presuppose any particular anatomical form; starting instead from a randomly-generated apodous body plan, it consistently discovers legged locomotion, the most efficient known form of terrestrial movement. If combined with automated fabrication and scaled up to more challenging tasks, this advance promises near instantaneous design, manufacture, and deployment of unique and useful machines for medical, environmental, vehicular, and space-based tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03263v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>众所周知，机器人很难设计，因为它们的物理结构、感官和运动布局以及行为之间存在复杂的相互依赖关系。尽管如此，迄今为止制造的每一个机器人的几乎每一个细节都是由人类设计师在经过几个月或几年的迭代构思、原型设计和测试后手动确定的。受自然界进化设计的启发，使用进化算法的机器人自动化设计已经尝试了20年，但效率仍然很低：设计出在制造时表现出所需行为的模拟机器人需要数天的超级计算。在这里，我们首次展示了机器人结构的从头优化，以在几秒钟内在单一消费级计算机上表现出所需的行为，以及制造的机器人对该行为的保留。与其他基于梯度的机器人设计方法不同，该算法不预设任何特定的解剖形式；相反，从随机生成的apodous身体计划开始，它不断发现腿的运动，这是已知的最有效的陆地运动形式。如果与自动化制造相结合，并扩大规模以完成更具挑战性的任务，这一进步有望为医疗、环境、车辆和天基任务提供近乎即时的设计、制造和部署独特而有用的机器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03263v1" target="_blank">2306.03263v1</a>
                              </td>
                              <td>Efficient automatic design of robots</td>
                              <td>David Matthews</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03263v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03263v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03186v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03186v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03186v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03186v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03186v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种在高维状态空间中进行基于计数的探索的新方法。与之前依赖密度模型的工作不同，我们表明，可以通过对Rademacher分布（或硬币翻转）的样本进行平均来得出计数。这种见解用于建立一个简单的监督学习目标，当优化时，该目标会产生一个状态的访问计数。我们表明，我们的方法在推导地面实况访问次数方面比以前的工作要有效得多；当用作无模型强化学习算法的探索奖励时，它在9项具有挑战性的探索任务中的大多数任务上都优于现有方法，包括雅达利游戏《蒙特祖玛的复仇》。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03186v1" target="_blank">2306.03186v1</a>
                              </td>
                              <td>Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning</td>
                              <td>Sam Lobel</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03186v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03186v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02913v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02913v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02913v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02913v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02913v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分散随机梯度下降（D-SGD）允许在没有中央服务器控制的情况下同时在大规模设备上进行协作学习。然而，现有的理论声称，权力下放必然会破坏泛化。在本文中，我们挑战了传统的信念，并为理解去中心化学习提供了一个全新的视角。我们证明了在一般非凸非$\beta$-光滑设置下，D-SGD隐式地最小化了平均方向清晰度感知最小化（SAM）算法的损失函数。这种令人惊讶的渐近等价揭示了一种内在的正则化优化权衡和去中心化的三个优点：（1）D-SGD中存在一种自由的不确定性评估机制来改进后验估计；（2） D-SGD表现出梯度平滑效应；和（3）D-SGD的锐度正则化效应不会随着总批量的增加而降低，这证明了在大批量场景中，D-SGD相对于集中式SGD（C-SGD）的潜在泛化优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02913v1" target="_blank">2306.02913v1</a>
                              </td>
                              <td>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</td>
                              <td>Tongtian Zhu</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02913v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02913v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_12637v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_12637v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_12637v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_12637v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we examine the recent Segment Anything Model (SAM) on medical images, and report both quantitative and qualitative zero-shot segmentation results on nine medical image segmentation benchmarks, covering various imaging modalities, such as optical coherence tomography (OCT), magnetic resonance imaging (MRI), and computed tomography (CT), as well as different applications including dermatology, ophthalmology, and radiology. Those benchmarks are representative and commonly used in model development. Our experimental results indicate that while SAM presents remarkable segmentation performance on images from the general domain, its zero-shot segmentation ability remains restricted for out-of-distribution images, e.g., medical images. In addition, SAM exhibits inconsistent zero-shot segmentation performance across different unseen medical domains. For certain structured targets, e.g., blood vessels, the zero-shot segmentation of SAM completely failed. In contrast, a simple fine-tuning of it with a small amount of data could lead to remarkable improvement of the segmentation quality, showing the great potential and feasibility of using fine-tuned SAM to achieve accurate medical image segmentation for a precision diagnostics. Our study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_12637v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了医学图像上最新的Segment Anything Model（SAM），并报告了九个医学图像分割基准上的定量和定性零样本分割结果，这些基准涵盖了各种成像方式，如光学相干层析成像（OCT）、磁共振成像（MRI）和计算机断层成像（CT），以及不同的应用，包括皮肤科、眼科和放射学。这些基准具有代表性，通常用于模型开发。我们的实验结果表明，虽然SAM在普通域的图像上具有显著的分割性能，但其零样本分割能力对于分布外的图像（例如医学图像）仍然受到限制。此外，SAM在不同的未知医学领域中表现出不一致的零样本分割性能。对于某些结构化目标，例如血管，SAM的零样本分割完全失败。相反，用少量数据对其进行简单的微调可以显著提高分割质量，这表明使用微调后的SAM实现精确的医学图像分割以进行精确诊断具有巨大的潜力和可行性。我们的研究表明，广义视觉基础模型在医学成像方面的多功能性，以及它们通过精细转向实现所需性能的巨大潜力，并最终解决与访问大型和多样化的医学数据集以支持临床诊断相关的挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.12637v2" target="_blank">2304.12637v2</a>
                              </td>
                              <td>Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation</td>
                              <td>Peilun Shi</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_12637v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.12637v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02656v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02656v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02656v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02656v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The research on extrinsic calibration between Light Detection and Ranging(LiDAR) and camera are being promoted to a more accurate, automatic and generic manner. Since deep learning has been employed in calibration, the restrictions on the scene are greatly reduced. However, data driven method has the drawback of low transfer-ability. It cannot adapt to dataset variations unless additional training is taken. With the advent of foundation model, this problem can be significantly mitigated. By using the Segment Anything Model(SAM), we propose a novel LiDAR-camera calibration method, which requires zero extra training and adapts to common scenes. With an initial guess, we opimize the extrinsic parameter by maximizing the consistency of points that are projected inside each image mask. The consistency includes three properties of the point cloud: the intensity, normal vector and categories derived from some segmentation methods. The experiments on different dataset have demonstrated the generality and comparable accuracy of our method. The code is available at https://github.com/OpenCalib/CalibAnything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02656v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光探测与测距（LiDAR）与相机之间的外部校准研究正朝着更精确、更自动、更通用的方向发展。由于在校准中采用了深度学习，因此大大减少了对场景的限制。然而，数据驱动方法具有传输能力低的缺点。除非进行额外的训练，否则它无法适应数据集的变化。随着基础模型的出现，这个问题可以得到显著缓解。通过使用分段任意模型（SAM），我们提出了一种新的激光雷达相机校准方法，该方法不需要额外的训练，并适用于常见场景。通过初始猜测，我们通过最大化投影在每个图像掩模内的点的一致性来对外部参数进行阿片化。一致性包括点云的三个属性：强度、法向量和从一些分割方法中导出的类别。在不同数据集上的实验证明了我们的方法的通用性和可比的准确性。代码位于https://github.com/OpenCalib/CalibAnything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02656v1" target="_blank">2306.02656v1</a>
                              </td>
                              <td>Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything</td>
                              <td>Zhaotong Luo</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02656v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02656v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02508v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Graph Fourier MMD for Signals on Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02508v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02508v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02508v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While numerous methods have been proposed for computing distances between probability distributions in Euclidean space, relatively little attention has been given to computing such distances for distributions on graphs. However, there has been a marked increase in data that either lies on graph (such as protein interaction networks) or can be modeled as a graph (single cell data), particularly in the biomedical sciences. Thus, it becomes important to find ways to compare signals defined on such graphs. Here, we propose Graph Fourier MMD (GFMMD), a novel distance between distributions and signals on graphs. GFMMD is defined via an optimal witness function that is both smooth on the graph and maximizes difference in expectation between the pair of distributions on the graph. We find an analytical solution to this optimization problem as well as an embedding of distributions that results from this method. We also prove several properties of this method including scale invariance and applicability to disconnected graphs. We showcase it on graph benchmark datasets as well on single cell RNA-sequencing data analysis. In the latter, we use the GFMMD-based gene embeddings to find meaningful gene clusters. We also propose a novel type of score for gene selection called "gene localization score" which helps select genes for cellular state space characterization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02508v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然已经提出了许多方法来计算欧几里得空间中概率分布之间的距离，但相对较少关注计算图上分布的这种距离。然而，位于图上（如蛋白质相互作用网络）或可以建模为图（单细胞数据）的数据显著增加，特别是在生物医学科学中。因此，找到比较在这样的图上定义的信号的方法变得很重要。在这里，我们提出了图傅立叶MMD（GFMMD），这是图上分布和信号之间的一种新距离。GFMMD是通过最优见证函数定义的，该最优见证函数在图上是平滑的，并且使图上的一对分布之间的期望差最大化。我们找到了这个优化问题的解析解，以及由该方法产生的分布的嵌入。我们还证明了该方法的几个性质，包括尺度不变性和对不连通图的适用性。我们在图表基准数据集以及单细胞RNA测序数据分析中展示了它。在后者中，我们使用基于GFMMD的基因嵌入来寻找有意义的基因簇。我们还提出了一种新型的基因选择评分，称为“基因定位评分”，它有助于选择用于细胞状态空间表征的基因。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02508v1" target="_blank">2306.02508v1</a>
                              </td>
                              <td>Graph Fourier MMD for Signals on Graphs</td>
                              <td>Samuel Leone</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02508v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02508v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08203v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An SDE for Modeling SAM: Theory and Insights</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08203v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08203v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08203v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08203v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了SAM（Sharpness Aware Minimization）优化器，它最近吸引了很多人的兴趣，因为它比随机梯度下降的更经典的变体具有更高的性能。我们的主要贡献是推导SAM及其两种变体的连续时间模型（以SDE的形式），包括全批和小批设置。我们证明了这些SDE是真实离散时间算法的严格近似（在弱意义上，随着学习率线性缩放）。使用这些模型，我们解释了为什么SAM更喜欢平坦的极小值而不是尖锐的极小值~--~，通过证明它最小化了具有Hessian相关噪声结构的隐式正则化损失。最后，我们证明了在某些现实条件下SAM被鞍点吸引。我们的理论结果得到了详细实验的支持。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08203v3" target="_blank">2301.08203v3</a>
                              </td>
                              <td>An SDE for Modeling SAM: Theory and Insights</td>
                              <td>Enea Monzio Compagnoni</td>
                              <td>2023-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08203v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08203v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02413v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Continual Learning on a Home Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02413v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02413v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02413v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots in home environments need to be able to learn new skills continuously as data becomes available, becoming ever more capable over time while using as little real-world data as possible. However, traditional robot learning approaches typically assume large amounts of iid data, which is inconsistent with this goal. In contrast, continual learning methods like CLEAR and SANE allow autonomous agents to learn off of a stream of non-iid samples; they, however, have not previously been demonstrated on real robotics platforms. In this work, we show how continual learning methods can be adapted for use on a real, low-cost home robot, and in particular look at the case where we have extremely small numbers of examples, in a task-id-free setting. Specifically, we propose SANER, a method for continuously learning a library of skills, and ABIP (Attention-Based Interaction Policies) as the backbone to support it. We learn four sequential kitchen tasks on a low-cost home robot, using only a handful of demonstrations per task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02413v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>家庭环境中的机器人需要能够随着数据的可用而不断学习新技能，随着时间的推移，在使用尽可能少的真实世界数据的同时变得越来越有能力。然而，传统的机器人学习方法通常假设大量的iid数据，这与这一目标不一致。相反，CLEAR和SANE等持续学习方法允许自主代理从非iid样本流中学习；然而，它们以前还没有在真正的机器人平台上进行过演示。在这项工作中，我们展示了如何将持续学习方法应用于真实、低成本的家用机器人，特别是在没有任务id的环境中，我们有极少数例子的情况。具体来说，我们提出了SANER，这是一种持续学习技能库的方法，ABIP（基于注意力的交互策略）是支持它的主干。我们在一个低成本的家用机器人上学习四个连续的厨房任务，每个任务只使用少量的演示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02413v1" target="_blank">2306.02413v1</a>
                              </td>
                              <td>Evaluating Continual Learning on a Home Robot</td>
                              <td>Sam Powers</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02413v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02413v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02275v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">USD: Unknown Sensitive Detector Empowered by Decoupled Objectness and Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02275v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02275v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02275v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open World Object Detection (OWOD) is a novel and challenging computer vision task that enables object detection with the ability to detect unknown objects. Existing methods typically estimate the object likelihood with an additional objectness branch, but ignore the conflict in learning objectness and classification boundaries, which oppose each other on the semantic manifold and training objective. To address this issue, we propose a simple yet effective learning strategy, namely Decoupled Objectness Learning (DOL), which divides the learning of these two boundaries into suitable decoder layers. Moreover, detecting unknown objects comprehensively requires a large amount of annotations, but labeling all unknown objects is both difficult and expensive. Therefore, we propose to take advantage of the recent Large Vision Model (LVM), specifically the Segment Anything Model (SAM), to enhance the detection of unknown objects. Nevertheless, the output results of SAM contain noise, including backgrounds and fragments, so we introduce an Auxiliary Supervision Framework (ASF) that uses a pseudo-labeling and a soft-weighting strategies to alleviate the negative impact of noise. Extensive experiments on popular benchmarks, including Pascal VOC and MS COCO, demonstrate the effectiveness of our approach. Our proposed Unknown Sensitive Detector (USD) outperforms the recent state-of-the-art methods in terms of Unknown Recall, achieving significant improvements of 14.3\%, 15.5\%, and 8.9\% on the M-OWODB, and 27.1\%, 29.1\%, and 25.1\% on the S-OWODB.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02275v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开放世界物体检测（OWOD）是一项新颖而富有挑战性的计算机视觉任务，它使物体检测能够检测未知物体。现有的方法通常通过额外的对象性分支来估计对象可能性，但忽略了学习对象性和分类边界之间的冲突，这两者在语义流形和训练目标上相互对立。为了解决这个问题，我们提出了一种简单而有效的学习策略，即解耦对象学习（DOL），它将这两个边界的学习划分为合适的解码器层。此外，全面检测未知对象需要大量的注释，但标记所有未知对象既困难又昂贵。因此，我们建议利用最近的大视觉模型（LVM），特别是分段任意模型（SAM），来增强对未知对象的检测。然而，SAM的输出结果包含噪声，包括背景和片段，因此我们引入了一种辅助监督框架（ASF），该框架使用伪标记和软加权策略来减轻噪声的负面影响。在流行的基准测试上进行的大量实验，包括Pascal VOC和MS COCO，证明了我们方法的有效性。我们提出的未知敏感检测器（USD）在未知召回方面优于最近最先进的方法，在M-OWODB上实现了14.3\%、15.5\%和8.9\%的显著改进，在S-OWODB中实现了27.1\%、29.1\%和25.1\%的显着改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02275v1" target="_blank">2306.02275v1</a>
                              </td>
                              <td>USD: Unknown Sensitive Detector Empowered by Decoupled Objectness and Segment Anything Model</td>
                              <td>Yulin He</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02275v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02275v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02245v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02245v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02245v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02245v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. The code is released at https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02245v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型的发展，像ChatGPT这样的许多卓越的语言系统蓬勃发展，并在许多任务上取得了惊人的成功，显示了基础模型令人难以置信的力量。本着释放基础模型在视觉任务上的能力的精神，最近提出了一种用于图像分割的视觉基础模型——Segment Anything Model（SAM），它在许多下游2D任务上表现出强大的零样本能力。然而，SAM是否能够适应3D视觉任务还有待探索，尤其是3D对象检测。受此启发，本文探索将SAM的零样本能力应用于三维目标检测。我们提出了一种SAM驱动的BEV处理流水线来检测对象，并在大规模Waymo开放数据集上获得有希望的结果。作为早期的尝试，我们的方法朝着使用视觉基础模型进行3D对象检测迈出了一步，并提供了在3D视觉任务中释放其力量的机会。代码发布于https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02245v1" target="_blank">2306.02245v1</a>
                              </td>
                              <td>SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</td>
                              <td>Dingyuan Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02245v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02245v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02094v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Meets Semantic Communication</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02094v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02094v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02094v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In light of the diminishing returns of traditional methods for enhancing transmission rates, the domain of semantic communication presents promising new frontiers. Focusing on image transmission, this paper explores the application of foundation models, particularly the Segment Anything Model (SAM) developed by Meta AI Research, to improve semantic communication. SAM is a promptable image segmentation model that has gained attention for its ability to perform zero-shot segmentation tasks without explicit training or domain-specific knowledge. By employing SAM's segmentation capability and lightweight neural network architecture for semantic coding, we propose a practical approach to semantic communication. We demonstrate that this approach retains critical semantic features, achieving higher image reconstruction quality and reducing communication overhead. This practical solution eliminates the resource-intensive stage of training a segmentation model and can be applied to any semantic coding architecture, paving the way for real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02094v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>鉴于提高传输速率的传统方法的回报越来越小，语义通信领域呈现出有前景的新领域。围绕图像传输，本文探讨了基础模型的应用，特别是Meta AI Research开发的分段任意模型（SAM），以改善语义通信。SAM是一种易于实现的图像分割模型，由于其在无需明确训练或特定领域知识的情况下执行零样本分割任务的能力而备受关注。通过利用SAM的分割能力和轻量级神经网络架构进行语义编码，我们提出了一种实用的语义通信方法。我们证明，这种方法保留了关键的语义特征，实现了更高的图像重建质量并减少了通信开销。这种实用的解决方案消除了训练分割模型的资源密集型阶段，可以应用于任何语义编码架构，为现实世界的应用铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02094v1" target="_blank">2306.02094v1</a>
                              </td>
                              <td>Segment Anything Meets Semantic Communication</td>
                              <td>Shehbaz Tariq</td>
                              <td>2023-06-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02094v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02094v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01567v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything in High Quality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01567v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01567v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01567v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 9 diverse segmentation datasets across different downstream tasks, where 7 out of them are evaluated in a zero-shot transfer protocol. Our code and models will be released at https://github.com/SysCV/SAM-HQ.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01567v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近推出的Segment Anything Model（SAM）代表着在扩大细分模型方面的一个巨大飞跃，它支持强大的零样本功能和灵活的提示。尽管使用了11亿个掩码进行训练，SAM的掩码预测质量在许多情况下都达不到要求，尤其是在处理具有复杂结构的对象时。我们提出了HQ-SAM，使SAM具有准确分割任何对象的能力，同时保持SAM最初的快速设计、效率和零样本泛化性。我们的精心设计重用并保留了SAM的预训练模型权重，同时只引入了最小的附加参数和计算。我们设计了一个可学习的高质量输出令牌，该令牌被注入SAM的掩码解码器，并负责预测高质量掩码。我们不是只将其应用于掩模解码器特征，而是首先将其与早期和最终的ViT特征融合，以改进掩模细节。为了训练我们引入的可学习参数，我们从几个来源组成了一个由44K个细粒度掩码组成的数据集。HQ-SAM只接受了引入的44k口罩检测的培训，这在8个GPU上只需要4个小时。我们展示了HQ-SAM在不同下游任务的9个不同分段数据集中的功效，其中7个在零样本传输协议中进行评估。我们的代码和模型将在https://github.com/SysCV/SAM-HQ.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01567v1" target="_blank">2306.01567v1</a>
                              </td>
                              <td>Segment Anything in High Quality</td>
                              <td>Lei Ke</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01567v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01567v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01129v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">White-Box Transformers via Sparse Rate Reduction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01129v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01129v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01129v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01129v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们认为表示学习的目标是将数据的分布（比如令牌集）压缩和转换为非相干子空间上支持的低维高斯分布的混合。最终表示的质量可以通过称为稀疏率缩减的统一目标函数来测量。从这个角度来看，流行的深度网络（如transformer）可以自然地被视为实现迭代方案，以逐步优化这一目标。特别地，我们证明了标准变换器块可以从该目标的互补部分上的交替优化中导出：多头自注意算子可以被视为梯度下降步骤，通过最小化令牌集的有损编码率来压缩令牌集，并且随后的多层感知器可以被视为试图稀疏化令牌的表示。这导致了一系列类似白盒变压器的深度网络架构，这些架构在数学上是完全可解释的。尽管它们很简单，但实验表明，这些网络确实学会了优化设计目标：它们压缩和稀疏了大规模真实世界视觉数据集（如ImageNet）的表示，并实现了与ViT等经过彻底设计的转换器非常接近的性能。代码位于\url{https://github.com/Ma-Lab-Berkeley/CRATE}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01129v1" target="_blank">2306.01129v1</a>
                              </td>
                              <td>White-Box Transformers via Sparse Rate Reduction</td>
                              <td>Yaodong Yu</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01129v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01129v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01102v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01102v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01102v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01102v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01102v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经成为能够完成广泛任务的强大工具。他们的能力跨越了许多领域，其中一个领域他们产生了重大影响，那就是代码生成领域。在这种情况下，我们将LLM视为变异和交叉工具。同时，众所周知，质量多样性（QD）算法可以发现多样且稳健的解决方案。通过将LLM的代码生成能力与QD解决方案的多样性和鲁棒性相结合，我们引入了LLMatic，一种神经结构搜索（NAS）算法。虽然LLM很难直接通过提示进行NAS，但LLMatic使用了一种程序性方法，利用QD进行提示和网络体系结构来创建多样化且高性能的网络。我们在CIFAR-10图像分类基准上测试了LLMatic，证明它只需2000美元的搜索就可以产生有竞争力的网络，即使事先不知道基准领域，也不接触任何以前性能最好的基准模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01102v1" target="_blank">2306.01102v1</a>
                              </td>
                              <td>LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization</td>
                              <td>Muhammad U. Nasir</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01102v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01102v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_12308v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything in 3D with NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_12308v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_12308v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_12308v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the Segment Anything Model (SAM) emerged as a powerful vision foundation model which is capable to segment anything in 2D images. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only required to provide a manual segmentation prompt (e.g., rough points) for the target object in a single view, which is used to generate its 2D mask in this view with SAM. Next, SA3D alternately performs mask inverse rendering and cross-view self-prompting across various views to iteratively complete the 3D mask of the target object constructed with voxel grids. The former projects the 2D mask obtained by SAM in the current view onto 3D mask with guidance of the density distribution learned by the NeRF; The latter extracts reliable prompts automatically as the input to SAM from the NeRF-rendered 2D mask in another view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within minutes. Our research offers a generic and efficient methodology to lift a 2D vision foundation model to 3D, as long as the 2D model can steadily address promptable segmentation across multiple views. The project page is at https://jumpat.github.io/SA3D/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_12308v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，分割任何事物模型（SAM）作为一种强大的视觉基础模型出现了，它能够分割2D图像中的任何事物。本文旨在推广SAM来分割三维对象。我们没有复制3D中成本高昂的数据采集和注释程序，而是设计了一种高效的解决方案，利用神经辐射场（NeRF）作为一种廉价且现成的先验技术，将多视图2D图像连接到3D空间。我们将所提出的解决方案称为SA3D，用于3D中的任何分段。只需要在单个视图中为目标对象提供手动分割提示（例如，粗糙点），用于使用SAM在该视图中生成其2D遮罩。接下来，SA3D在各个视图之间交替执行遮罩反向渲染和跨视图自提示，以迭代完成体素网格构建的目标对象的3D遮罩。前者在NeRF学习的密度分布的指导下，将SAM在当前视图中获得的2D掩模投影到3D掩模上；后者在另一个视图中从NeRF渲染的2D掩模中自动提取可靠提示作为SAM的输入。我们在实验中表明，SA3D能够适应各种场景，并在几分钟内实现3D分割。我们的研究提供了一种通用而有效的方法来将2D视觉基础模型提升到3D，只要2D模型能够稳定地解决多个视图之间的可提示分割问题。项目页面位于https://jumpat.github.io/SA3D/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.12308v3" target="_blank">2304.12308v3</a>
                              </td>
                              <td>Segment Anything in 3D with NeRFs</td>
                              <td>Jiazhong Cen</td>
                              <td>2023-04-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_12308v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.12308v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_19599v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_19599v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_19599v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_19599v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with category labels, and scores the segmented parts by measuring the likelihood of each category appearing in the prompted scene via a large language model, i.e., Vicuna-7B. Additionally, we adopt an assemble reward-ranked learning strategy to enable the integration of multiple reward functions to jointly guide the model training. Adapting results of text-to-image models on the MS-COCO benchmark show that the proposed semantic reward outperforms other baseline reward functions with a considerable margin on both visual quality and semantic similarity with the input prompt. Moreover, by adopting the assemble reward-ranked learning strategy, we further demonstrate that model performance is further improved when adapting under the unifying of the proposed semantic reward with the current image rewards.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_19599v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像扩散模型的最新进展在从给定的文本提示生成高质量、逼真的图像方面取得了显著的成功。然而，由于缺乏成功诊断模态差异的精细语义指导，以前的方法无法在文本概念和生成的图像之间进行准确的模态对齐。在本文中，我们提出了FineRewards，通过引入两种新的细粒度语义奖励：标题奖励和语义段任何东西（SAM）奖励，来改善文本到图像扩散模型中文本和图像之间的对齐。从全局语义角度来看，字幕奖励通过BLIP-2模型生成描述合成图像中所有重要内容的相应详细字幕，然后通过测量生成的字幕与给定提示之间的相似性来计算奖励分数。从局部语义角度来看，SAM奖励将生成的图像分割成具有类别标签的局部部分，并通过大型语言模型（即Vicuna-7B）测量每个类别出现在提示场景中的可能性来对分割的部分进行评分。此外，我们采用了集合奖励排序学习策略，使多个奖励函数能够集成，以共同指导模型训练。基于MS-COCO基准的文本到图像模型的自适应结果表明，所提出的语义奖励优于其他基线奖励函数，在视觉质量和与输入提示的语义相似性方面都有相当大的优势。此外，通过采用组合奖励排序学习策略，我们进一步证明了在将所提出的语义奖励与当前图像奖励相统一的情况下进行自适应时，模型性能得到了进一步提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.19599v2" target="_blank">2305.19599v2</a>
                              </td>
                              <td>Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards</td>
                              <td>Guian Fang</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_19599v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.19599v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_05817v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level Generation Through Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_05817v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_05817v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_05817v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are powerful tools, capable of leveraging their training on natural language to write stories, generate code, and answer questions. But can they generate functional video game levels? Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models. We investigate the use of LLMs to generate levels for the game Sokoban, finding that LLMs are indeed capable of doing so, and that their performance scales dramatically with dataset size. We also perform preliminary experiments on controlling LLM level generators and discuss promising areas for future work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_05817v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）是一种强大的工具，能够利用其在自然语言方面的培训来编写故事、生成代码和回答问题。但他们能生成功能性的电子游戏关卡吗？游戏级别具有复杂的功能约束和不止一个维度的空间关系，与LLM在训练过程中通常看到的数据类型非常不同。游戏级别的数据集也很难获得，这可能会对这些渴望数据的模型的能力造成负担。我们研究了LLM为Sokoban游戏生成关卡的使用，发现LLM确实能够做到这一点，并且它们的性能随着数据集的大小而显著扩展。我们还进行了控制LLM能级发生器的初步实验，并讨论了未来工作的前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.05817v2" target="_blank">2302.05817v2</a>
                              </td>
                              <td>Level Generation Through Large Language Models</td>
                              <td>Graham Todd</td>
                              <td>2023-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_05817v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.05817v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18553v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Controllable Path of Destruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18553v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18553v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18553v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by repairing from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18553v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>破坏路径（PoD）是一种用于学习迭代生成器的自监督方法。核心思想是通过销毁一组工件来生成一个训练集，并为每个销毁步骤创建一个基于相应修复操作的训练实例。在该数据集上训练的生成器可以通过从任意状态进行修复来生成新的工件。就原始训练示例而言，PoD方法是非常数据高效的，并且非常适合于由分类数据组成的功能工件，例如游戏级别和离散3D结构。在本文中，我们扩展了销毁路径方法，允许设计者控制生成工件的各个方面。可控性是通过向组成修复轨迹的状态-动作对添加条件输入来引入的。我们在2D地牢环境中以及在小型3D乐高汽车领域中测试了可控的PoD方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18553v2" target="_blank">2305.18553v2</a>
                              </td>
                              <td>Controllable Path of Destruction</td>
                              <td>Matthew Siper</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18553v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18553v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17191v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17191v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17191v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17191v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17191v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比自监督学习因其能够从大的未标记数据集中创建高质量的表示而受到关注。这些强大的特征能够实现下游任务的数据高效学习的一个关键原因是，它们提供了增强不变性，这通常是一种有用的归纳偏差。然而，首选不变量的数量和类型在先验上是未知的，并且在不同的下游任务中有所不同。因此，我们提出了一种多任务自监督框架（MT-SLVR），该框架以参数有效的方式学习变异和不变特征。我们的多任务表示提供了一个强大而灵活的功能，有利于不同的下游任务。我们在从各种音频领域提取的少量镜头分类任务上评估了我们的方法，并在所有这些任务上展示了改进的分类性能</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17191v1" target="_blank">2305.17191v1</a>
                              </td>
                              <td>MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</td>
                              <td>Calum Heggan</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17191v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17191v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16698v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Any Shadow: Segment Anything for Video Shadow Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16698v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16698v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16698v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) has achieved great success in the field of natural image segmentation. Nevertheless, SAM tends to classify shadows as background, resulting in poor segmentation performance for shadow detection task. In this paper, we propose an simple but effective approach for fine tuning SAM to detect shadows. Additionally, we also combine it with long short-term attention mechanism to extend its capabilities to video shadow detection. Specifically, we first fine tune SAM by utilizing shadow data combined with sparse prompts and apply the fine-tuned model to detect a specific frame (e.g., first frame) in the video with a little user assistance. Subsequently, using the detected frame as a reference, we employ a long short-term network to learn spatial correlations between distant frames and temporal consistency between contiguous frames, thereby achieving shadow information propagation across frames. Extensive experimental results demonstrate that our method outperforms the state-of-the-art techniques, with improvements of 17.2% and 3.3% in terms of MAE and IoU, respectively, validating the effectiveness of our method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16698v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在自然图像分割领域取得了巨大的成功。然而，SAM倾向于将阴影分类为背景，导致阴影检测任务的分割性能较差。在本文中，我们提出了一种简单但有效的方法来微调SAM以检测阴影。此外，我们还将其与长短期注意力机制相结合，将其功能扩展到视频阴影检测。具体而言，我们首先通过利用阴影数据与稀疏提示相结合来微调SAM，并在少量用户帮助下应用微调模型来检测视频中的特定帧（例如，第一帧）。随后，使用检测到的帧作为参考，我们使用长短期网络来学习远距离帧之间的空间相关性和连续帧之间的时间一致性，从而实现阴影信息在帧之间的传播。大量的实验结果表明，我们的方法优于最先进的技术，在MAE和IoU方面分别提高了17.2%和3.3%，验证了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16698v1" target="_blank">2305.16698v1</a>
                              </td>
                              <td>Detect Any Shadow: Segment Anything for Video Shadow Detection</td>
                              <td>Yonghui Wang</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16698v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16698v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16647v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gradient descent-based programming of analog in-memory computing cores</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16647v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16647v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16647v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise programming of crossbar arrays of unit-cells is crucial for obtaining high matrix-vector-multiplication (MVM) accuracy in analog in-memory computing (AIMC) cores. We propose a radically different approach based on directly minimizing the MVM error using gradient descent with synthetic random input data. Our method significantly reduces the MVM error compared with conventional unit-cell by unit-cell iterative programming. It also eliminates the need for high-resolution analog-to-digital converters (ADCs) to read the small unit-cell conductance during programming. Our method improves the experimental inference accuracy of ResNet-9 implemented on two phase-change memory (PCM)-based AIMC cores by 1.26%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16647v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单元交叉阵列的精确编程对于在模拟内存计算（AIMC）核心中获得高矩阵矢量乘法（MVM）精度至关重要。我们提出了一种完全不同的方法，该方法基于使用合成随机输入数据的梯度下降直接最小化MVM误差。与传统的逐单元迭代编程相比，我们的方法显著降低了MVM误差。它还消除了高分辨率模数转换器（ADC）在编程期间读取小单位电池电导的需要。我们的方法将ResNet-9在两个基于相变存储器（PCM）的AIMC核上实现的实验推理精度提高了1.26%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16647v1" target="_blank">2305.16647v1</a>
                              </td>
                              <td>Gradient descent-based programming of analog in-memory computing cores</td>
                              <td>Julian Büchel</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16647v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16647v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10636v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">$ω$PAP Spaces: Reasoning Denotationally About Higher-Order, Recursive Probabilistic and Differentiable Programs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10636v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10636v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10636v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a new setting, the category of $\omega$PAP spaces, for reasoning denotationally about expressive differentiable and probabilistic programming languages. Our semantics is general enough to assign meanings to most practical probabilistic and differentiable programs, including those that use general recursion, higher-order functions, discontinuous primitives, and both discrete and continuous sampling. But crucially, it is also specific enough to exclude many pathological denotations, enabling us to establish new results about both deterministic differentiable programs and probabilistic programs. In the deterministic setting, we prove very general correctness theorems for automatic differentiation and its use within gradient descent. In the probabilistic setting, we establish the almost-everywhere differentiability of probabilistic programs' trace density functions, and the existence of convenient base measures for density computation in Monte Carlo inference. In some cases these results were previously known, but required detailed proofs with an operational flavor; by contrast, all our proofs work directly with programs' denotations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10636v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一个新的设置，$\omega$PAP空间的类别，用于表示可微和概率编程语言的推理。我们的语义足够通用，可以为大多数实用的概率和可微程序赋予意义，包括那些使用一般递归、高阶函数、不连续基元以及离散和连续采样的程序。但至关重要的是，它也足够具体，可以排除许多病理学的外延，使我们能够建立关于确定性可微程序和概率程序的新结果。在确定性设置中，我们证明了自动微分的非常一般的正确性定理及其在梯度下降中的应用。在概率环境中，我们建立了概率程序的迹密度函数的几乎处处可微性，以及蒙特卡罗推理中密度计算的方便基测度的存在性。在某些情况下，这些结果以前是已知的，但需要具有操作性的详细证明；相比之下，我们所有的证明都直接与程序的外延有关。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10636v2" target="_blank">2302.10636v2</a>
                              </td>
                              <td>$ω$PAP Spaces: Reasoning Denotationally About Higher-Order, Recursive Probabilistic and Differentiable Programs</td>
                              <td>Mathieu Huot</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10636v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10636v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16292v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sharpness-Aware Minimization Leads to Low-Rank Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16292v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16292v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16292v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16292v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>清晰度感知最小化（SAM）是最近提出的一种方法，可以最大限度地减少神经网络训练损失的清晰度。虽然它的泛化改进是众所周知的，也是主要的动机，但我们发现了SAM的另一个有趣的影响：在神经网络的不同层发生的特征秩的降低。我们表明，这种低秩效应发生得非常广泛：对于不同的架构，如全连接网络、卷积网络、视觉转换器，以及对于不同的目标，如回归、分类、语言图像对比训练。为了更好地理解这一现象，我们从机理上理解了低秩特征是如何在简单的两层网络中出现的。我们观察到，大量激活被SAM完全修剪，这直接有助于秩的降低。我们从理论上证实了这种影响，并检查了它也可能发生在深度网络中，尽管总体的秩降低机制可能更复杂，特别是对于具有预激活跳过连接和自注意层的深度网络。我们的代码可在https://github.com/tml-epfl/sam-low-rank-features.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16292v1" target="_blank">2305.16292v1</a>
                              </td>
                              <td>Sharpness-Aware Minimization Leads to Low-Rank Features</td>
                              <td>Maksym Andriushchenko</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16292v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16292v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16233v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interactive Segment Anything NeRF with Feature Imitation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16233v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16233v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16233v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: \url{https://me.kiui.moe/san/}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16233v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了用语义增强神经辐射场（NeRF）以扩展其应用的潜力。尽管NeRF已被证明在VR和数字创作等现实世界应用中很有用，但语义的缺乏阻碍了与复杂场景中对象的交互。我们建议模仿离线感知模型的主干特征，使用NeRF实现零样本语义分割。我们的框架通过直接呈现语义特征并仅应用感知模型中的解码器来重新制定分割过程。这消除了对昂贵主干的需求，并有利于3D一致性。此外，我们可以将学习到的语义投影到提取的网格表面上进行实时交互。凭借最先进的分段任意模型（SAM），我们的框架以相当的掩码质量将分段速度提高了16倍。实验结果证明了我们方法的有效性和计算优势。项目页面：\url{https://me.kiui.moe/san/}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16233v1" target="_blank">2305.16233v1</a>
                              </td>
                              <td>Interactive Segment Anything NeRF with Feature Imitation</td>
                              <td>Xiaokang Chen</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16233v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16233v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16220v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Robustness of Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16220v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16220v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16220v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) has presented impressive objectness identification capability with the idea of prompt learning and a new collected large-scale dataset. Given a prompt (e.g., points, bounding boxes, or masks) and an input image, SAM is able to generate valid segment masks for all objects indicated by the prompts, presenting high generalization across diverse scenarios and being a general method for zero-shot transfer to downstream vision tasks. Nevertheless, it remains unclear whether SAM may introduce errors in certain threatening scenarios. Clarifying this is of significant importance for applications that require robustness, such as autonomous vehicles. In this paper, we aim to study the testing-time robustness of SAM under adversarial scenarios and common corruptions. To this end, we first build a testing-time robustness evaluation benchmark for SAM by integrating existing public datasets. Second, we extend representative adversarial attacks against SAM and study the influence of different prompts on robustness. Third, we study the robustness of SAM under diverse corruption types by evaluating SAM on corrupted datasets with different prompts. With experiments conducted on SA-1B and KITTI datasets, we find that SAM exhibits remarkable robustness against various corruptions, except for blur-related corruption. Furthermore, SAM remains susceptible to adversarial attacks, particularly when subjected to PGD and BIM attacks. We think such a comprehensive study could highlight the importance of the robustness issues of SAM and trigger a series of new tasks for SAM as well as downstream vision tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16220v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）以快速学习的思想和新收集的大规模数据集，展现了令人印象深刻的对象性识别能力。给定提示（例如，点、边界框或遮罩）和输入图像，SAM能够为提示指示的所有对象生成有效的分段遮罩，在不同场景中呈现出高度的泛化，是将零样本转移到下游视觉任务的通用方法。然而，尚不清楚SAM是否会在某些威胁场景中引入错误。澄清这一点对于自动驾驶汽车等需要鲁棒性的应用具有重要意义。在本文中，我们旨在研究SAM在对抗性场景和常见损坏情况下的测试时间稳健性。为此，我们首先通过集成现有的公共数据集，为SAM建立了一个测试时间稳健性评估基准。其次，我们扩展了针对SAM的代表性对抗性攻击，并研究了不同提示对稳健性的影响。第三，我们通过在不同提示的损坏数据集上评估SAM，研究了SAM在不同损坏类型下的稳健性。通过在SA-1B和KITTI数据集上进行的实验，我们发现SAM对各种损坏表现出显著的鲁棒性，但与模糊相关的损坏除外。此外，SAM仍然容易受到对抗性攻击，特别是在受到PGD和BIM攻击时。我们认为，这样一项全面的研究可以突出SAM稳健性问题的重要性，并引发SAM的一系列新任务以及下游愿景任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16220v1" target="_blank">2305.16220v1</a>
                              </td>
                              <td>On the Robustness of Segment Anything</td>
                              <td>Yihao Huang</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16220v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16220v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15817v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15817v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15817v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15817v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15817v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>众所周知，深度神经网络（DNNs）的泛化与极小值的平坦性密切相关，这导致了清晰度感知最小化（SAM）的发展，以寻求更平坦的极小值和更好的泛化。在本文中，我们重新审视了SAM的损失，并提出了一种更通用的方法，称为WSAM，通过将锐度作为正则化项。我们通过PAC和Bayes PAC技术的结合证明了它的泛化界，并在各种公共数据集上评估了它的性能。结果表明，与普通优化器、SAM及其变体相比，WSAM实现了改进的泛化能力，或者至少具有很强的竞争力。代码位于https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15817v1" target="_blank">2305.15817v1</a>
                              </td>
                              <td>Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term</td>
                              <td>Yun Yue</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15817v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2304_08451v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Video Action Detection with Token Dropout and Context Refinement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08451v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08451v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08451v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Streaming video clips with large-scale video tokens impede vision transformers (ViTs) for efficient recognition, especially in video action detection where sufficient spatiotemporal representations are required for precise actor identification. In this work, we propose an end-to-end framework for efficient video action detection (EVAD) based on vanilla ViTs. Our EVAD consists of two specialized designs for video action detection. First, we propose a spatiotemporal token dropout from a keyframe-centric perspective. In a video clip, we maintain all tokens from its keyframe, preserve tokens relevant to actor motions from other frames, and drop out the remaining tokens in this clip. Second, we refine scene context by leveraging remaining tokens for better recognizing actor identities. The region of interest (RoI) in our action detector is expanded into temporal domain. The captured spatiotemporal actor identity representations are refined via scene context in a decoder with the attention mechanism. These two designs make our EVAD efficient while maintaining accuracy, which is validated on three benchmark datasets (i.e., AVA, UCF101-24, JHMDB). Compared to the vanilla ViT backbone, our EVAD reduces the overall GFLOPs by 43% and improves real-time inference speed by 40% with no performance degradation. Moreover, even at similar computational costs, our EVAD can improve the performance by 1.1 mAP with higher resolution inputs. Code is available at https://github.com/MCG-NJU/EVAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08451v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有大规模视频令牌的流式视频剪辑阻碍了视觉变换器（ViT）的高效识别，尤其是在视频动作检测中，需要足够的时空表示来精确识别演员。在这项工作中，我们提出了一个基于vanilla ViTs的高效视频动作检测（EVAD）的端到端框架。我们的EVAD由两个专门的视频动作检测设计组成。首先，我们从以关键帧为中心的角度提出了时空令牌丢弃。在视频剪辑中，我们保留其关键帧中的所有标记，保留与其他帧中演员运动相关的标记，并删除该剪辑中的其余标记。其次，我们通过利用剩余的令牌来更好地识别演员身份，从而优化场景上下文。我们的动作检测器中的感兴趣区域（RoI）被扩展到时域。在具有注意力机制的解码器中，通过场景上下文来细化捕获的时空演员身份表示。这两种设计使我们的EVAD在保持准确性的同时具有效率，并在三个基准数据集（即AVA、UCF101-24、JHMDB）上进行了验证。与普通ViT主干相比，我们的EVAD将总体GFLOP减少了43%，并将实时推理速度提高了40%，而不会降低性能。此外，即使在类似的计算成本下，我们的EVAD也可以通过更高分辨率的输入将性能提高1.1mAP。代码位于https://github.com/MCG-NJU/EVAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08451v2" target="_blank">2304.08451v2</a>
                              </td>
                              <td>Efficient Video Action Detection with Token Dropout and Context Refinement</td>
                              <td>Lei Chen</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08451v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08451v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04356v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-Grained Visual Prompting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04356v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04356v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04356v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. The part detection experiments conducted on the PACO dataset further validate the preponderance of FGVP over existing visual prompting techniques. Code and models will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04356v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM），如CLIP，在图像级视觉感知中展示了令人印象深刻的零样本传输能力。然而，这些模型在需要精确定位和识别的实例级任务中表现出有限的性能。先前的工作表明，结合视觉提示，如彩色方框或圆圈，可以提高模型识别感兴趣对象的能力。尽管如此，与语言提示相比，视觉提示设计很少被探索。现有的方法使用粗糙的视觉线索，如彩色方框或圆圈，由于包含了不相关和有噪声的像素，通常会导致次优性能。在本文中，我们通过探索更细粒度的标记，如分割掩模及其变体，仔细研究了视觉提示设计。此外，我们引入了一个新的零样本框架，该框架利用从通用分割模型中获取的像素级注释来实现细粒度的视觉提示。因此，我们的研究表明，在目标掩模之外直接应用模糊，称为模糊反向掩模，表现出非凡的效果。这种提出的提示策略利用精确的掩码注释来减少对弱相关区域的关注，同时保持目标和周围背景之间的空间一致性。我们的精细视觉提示（FGVP）在RefCOCO、RefCOCO+和RefCOCOg基准上的引用表达式的零样本理解方面表现出了卓越的性能。它以3.0%至4.6%的平均优势优于现有方法，在RefCOCO+testA子集上的最大改进为12.5%。在PACO数据集上进行的零件检测实验进一步验证了FGVP相对于现有视觉提示技术的优势。将提供代码和模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04356v1" target="_blank">2306.04356v1</a>
                              </td>
                              <td>Fine-Grained Visual Prompting</td>
                              <td>Lingfeng Yang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04356v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04356v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08685v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08685v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08685v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08685v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Grounding (VG) is a crucial topic in the field of vision and language, which involves locating a specific region described by expressions within an image. To reduce the reliance on manually labeled data, unsupervised methods have been developed to locate regions using pseudo-labels. However, the performance of existing unsupervised methods is highly dependent on the quality of pseudo-labels and these methods always encounter issues with limited diversity. In order to utilize vision and language pre-trained models to address the grounding problem, and reasonably take advantage of pseudo-labels, we propose CLIP-VG, a novel method that can conduct self-paced curriculum adapting of CLIP with pseudo-language labels. We propose a simple yet efficient end-to-end network architecture to realize the transfer of CLIP to the visual grounding. Based on the CLIP-based architecture, we further propose single-source and multi-source curriculum adapting algorithms, which can progressively find more reliable pseudo-labels to learn an optimal model, thereby achieving a balance between reliability and diversity for the pseudo-language labels. Our method outperforms the current state-of-the-art unsupervised method by a significant margin on RefCOCO/+/g datasets in both single-source and multi-source scenarios, with improvements ranging from 6.78% to 10.67% and 11.39% to 14.87%, respectively. Furthermore, our approach even outperforms existing weakly supervised methods. The code and models will be available at https://github.com/linhuixiao/CLIP-VG.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08685v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础（VG）是视觉和语言领域的一个重要课题，它涉及到定位图像中由表情描述的特定区域。为了减少对手动标记数据的依赖，已经开发了无监督的方法来使用伪标签定位区域。然而，现有的无监督方法的性能高度依赖于伪标签的质量，并且这些方法总是遇到多样性有限的问题。为了利用视觉和语言预训练模型来解决基础问题，并合理利用伪标签，我们提出了一种新的方法CLIP-VG，它可以用伪语言标签对CLIP进行自定进度的课程改编。我们提出了一种简单而高效的端到端网络架构，以实现CLIP到视觉基础的转移。在基于CLIP的架构的基础上，我们进一步提出了单源和多源课程自适应算法，可以逐步找到更可靠的伪标签来学习最优模型，从而实现伪语言标签的可靠性和多样性之间的平衡。在单源和多源场景中，我们的方法在RefCOCO/++g数据集上显著优于当前最先进的无监督方法，改进幅度分别从6.78%到10.67%和11.39%到14.87%。此外，我们的方法甚至优于现有的弱监督方法。代码和型号将在https://github.com/linhuixiao/CLIP-VG.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08685v2" target="_blank">2305.08685v2</a>
                              </td>
                              <td>CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding</td>
                              <td>Linhui Xiao</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08685v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08685v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04272v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Generalization of Multi-modal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04272v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04272v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04272v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal contrastive learning (MMCL) has recently garnered considerable interest due to its superior performance in visual tasks, achieved by embedding multi-modal data, such as visual-language pairs. However, there still lack theoretical understandings of how MMCL extracts useful visual representation from multi-modal pairs, and particularly, how MMCL outperforms previous approaches like self-supervised contrastive learning (SSCL). In this paper, by drawing an intrinsic connection between MMCL and asymmetric matrix factorization, we establish the first generalization guarantees of MMCL for visual downstream tasks. Based on this framework, we further unify MMCL and SSCL by showing that MMCL implicitly performs SSCL with (pseudo) positive pairs induced by text pairs. Through this unified perspective, we characterize the advantage of MMCL by showing that text pairs induce more semantically consistent and diverse positive pairs, which, according to our analysis, provably benefit downstream generalization. Inspired by this finding, we propose CLIP-guided resampling methods to significantly improve the downstream performance of SSCL on ImageNet by leveraging multi-modal information. Code is available at https://github.com/PKU-ML/CLIP-Help-SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04272v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态对比学习（MMCL）最近因其在视觉任务中的卓越性能而引起了人们的极大兴趣，它是通过嵌入多模态数据（如视觉语言对）来实现的。然而，对于MMCL如何从多模态对中提取有用的视觉表示，特别是MMCL如何优于自监督对比学习（SSCL）等先前方法，仍然缺乏理论上的理解。在本文中，通过绘制MMCL和非对称矩阵分解之间的内在联系，我们建立了MMCL对视觉下游任务的第一个推广保证。在这个框架的基础上，我们进一步统一了MMCL和SSCL，表明MMCL隐式地用文本对诱导的（伪）正对执行SSCL。通过这种统一的观点，我们通过表明文本对诱导语义一致和多样化的正对来表征MMCL的优势，根据我们的分析，这可证明有利于下游的泛化。受这一发现的启发，我们提出了CLIP引导的重采样方法，通过利用多模态信息显著提高SSCL在ImageNet上的下游性能。代码位于https://github.com/PKU-ML/CLIP-Help-SimCLR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04272v1" target="_blank">2306.04272v1</a>
                              </td>
                              <td>On the Generalization of Multi-modal Contrastive Learning</td>
                              <td>Qi Zhang</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04272v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04272v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_07011v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Modern Look at the Relationship between Sharpness and Generalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_07011v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_07011v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_07011v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlated with generalization depending on the setup. Interestingly, in multiple cases, we observe a consistent negative correlation of sharpness with out-of-distribution error implying that sharper minima can generalize better. Finally, we illustrate on a simple model that the right sharpness measure is highly data-dependent, and that we do not understand well this aspect for realistic data distributions. The code of our experiments is available at https://github.com/tml-epfl/sharpness-vs-generalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_07011v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>极小值的尖锐度是一个很有前途的量，它可以与深度网络中的泛化相关，并且当在训练过程中进行优化时，可以提高泛化能力。然而，在神经网络的重新参数化下，标准清晰度不是不变的，为了解决这一问题，已经提出了重新参数化不变清晰度定义，最突出的是自适应清晰度（Kwon等人，2021）。但它真的能在现代实用环境中捕捉到泛化吗？我们在对自适应清晰度的各种定义的详细研究中全面探讨了这个问题，从在ImageNet和CIFAR-10上从头开始训练到在ImageNet上微调CLIP和在MNLI上微调BERT。我们主要关注变压器，尽管其广泛使用，但在清晰度方面知之甚少。总的来说，我们观察到清晰度与泛化没有很好的相关性，而是与一些训练参数相关，如学习率，根据设置，学习率可以与泛化呈正相关或负相关。有趣的是，在多种情况下，我们观察到锐度与分布外误差的一致负相关，这意味着锐度极小值可以更好地推广。最后，我们在一个简单的模型上说明了正确的清晰度度量是高度依赖于数据的，并且我们对现实数据分布的这一方面没有很好的理解。我们的实验代码可在https://github.com/tml-epfl/sharpness-vs-generalization.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.07011v2" target="_blank">2302.07011v2</a>
                              </td>
                              <td>A Modern Look at the Relationship between Sharpness and Generalization</td>
                              <td>Maksym Andriushchenko</td>
                              <td>2023-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_07011v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.07011v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00595v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00595v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00595v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00595v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We focus on the weakly-supervised audio-visual video parsing task (AVVP), which aims to identify and locate all the events in audio/visual modalities. Previous works only concentrate on video-level overall label denoising across modalities, but overlook the segment-level label noise, where adjacent video segments (i.e., 1-second video clips) may contain different events. However, recognizing events in the segment is challenging because its label could be any combination of events that occur in the video. To address this issue, we consider tackling AVVP from the language perspective, since language could freely describe how various events appear in each segment beyond fixed labels. Specifically, we design language prompts to describe all cases of event appearance for each video. Then, the similarity between language prompts and segments is calculated, where the event of the most similar prompt is regarded as the segment-level label. In addition, to deal with the mislabeled segments, we propose to perform dynamic re-weighting on the unreliable segments to adjust their labels. Experiments show that our simple yet effective approach outperforms state-of-the-art methods by a large margin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00595v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们专注于弱监督视听视频解析任务（AVVP），该任务旨在识别和定位音频/视频模式中的所有事件。先前的工作仅集中于跨模态的视频级整体标签去噪，而忽略了片段级标签噪声，其中相邻的视频片段（即1秒视频剪辑）可能包含不同的事件。然而，识别片段中的事件很有挑战性，因为它的标签可能是视频中发生的事件的任何组合。为了解决这个问题，我们考虑从语言的角度来处理AVVP，因为语言可以自由地描述各种事件如何出现在固定标签之外的每个片段中。具体来说，我们设计语言提示来描述每个视频的事件出现的所有情况。然后，计算语言提示和片段之间的相似性，其中最相似的提示的事件被视为片段级标签。此外，为了处理标记错误的片段，我们建议对不可靠的片段进行动态重新加权，以调整它们的标签。实验表明，我们简单而有效的方法在很大程度上优于最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00595v2" target="_blank">2306.00595v2</a>
                              </td>
                              <td>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</td>
                              <td>Yingying Fan</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00595v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00595v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_09737v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Position-guided Text Prompt for Vision-Language Pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_09737v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_09737v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_09737v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Pre-Training (VLP) has shown promising capabilities to align image and text pairs, facilitating a broad variety of cross-modal learning tasks. However, we observe that VLP models often lack the visual grounding/localization capability which is critical for many downstream tasks such as visual reasoning. In this work, we propose a novel Position-guided Text Prompt (PTP) paradigm to enhance the visual grounding ability of cross-modal models trained with VLP. Specifically, in the VLP phase, PTP divides the image into $N\times N$ blocks, and identifies the objects in each block through the widely used object detector in VLP. It then reformulates the visual grounding task into a fill-in-the-blank problem given a PTP by encouraging the model to predict the objects in the given blocks or regress the blocks of a given object, e.g. filling `P" or ``O" in aPTP ``The block P has a O". This mechanism improves the visual grounding capability of VLP models and thus helps them better handle various downstream tasks. By introducing PTP into several state-of-the-art VLP frameworks, we observe consistently significant improvements across representative cross-modal learning model architectures and several benchmarks, e.g. zero-shot Flickr30K Retrieval (+4.8 in average recall@1) for ViLT \cite{vilt} baseline, and COCO Captioning (+5.3 in CIDEr) for SOTA BLIP \cite{blip} baseline. Moreover, PTP achieves comparable results with object-detector based methods, and much faster inference speed since PTP discards its object detector for inference while the later cannot. Our code and pre-trained weight will be released at \url{https://github.com/sail-sg/ptp}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_09737v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练（VLP）已显示出很有前途的对齐图像和文本对的能力，促进了各种跨模态学习任务。然而，我们观察到，VLP模型通常缺乏视觉基础/定位能力，这对于许多下游任务（如视觉推理）至关重要。在这项工作中，我们提出了一种新的位置引导文本提示（PTP）范式，以增强用VLP训练的跨模态模型的视觉基础能力。具体来说，在VLP阶段，PTP将图像划分为$N\timesN$块，并通过VLP中广泛使用的对象检测器来识别每个块中的对象。然后，它通过鼓励模型预测给定块中的对象或回归给定对象的块，将视觉基础任务重新表述为给定PTP的填空问题，例如在一个PTP中填充“P”或“O”``块P有一个O“.该机制提高了VLP模型的视觉基础能力，从而帮助它们更好地处理各种下游任务。通过将PTP引入几个最先进的VLP框架，我们观察到代表性的跨模式学习模型架构和几个基准的持续显著改进，例如零样本Flickr30K检索（平均+4.8recall@1)ViLT\cite｛ViLT｝基线的COCO字幕（CIDEr中的+5.3）和SOTA BLIP\cite{BLIP｝基线。此外，PTP实现了与基于对象检测器的方法相当的结果，并且推理速度快得多，因为PTP丢弃其对象检测器进行推理，而后者不能。我们的代码和预先训练的体重将在\url上发布{https://github.com/sail-sg/ptp}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.09737v2" target="_blank">2212.09737v2</a>
                              </td>
                              <td>Position-guided Text Prompt for Vision-Language Pre-training</td>
                              <td>Alex Jinpeng Wang</td>
                              <td>2022-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_09737v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.09737v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03514v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Recognize Anything: A Strong Image Tagging Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03514v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03514v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03514v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM can recognize any common category with high accuracy. RAM introduces a new paradigm for image tagging, leveraging large-scale image-text pairs for training instead of manual annotations. The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the caption and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset. We evaluate the tagging capabilities of RAM on numerous benchmarks and observe impressive zero-shot performance, significantly outperforming CLIP and BLIP. Remarkably, RAM even surpasses the fully supervised manners and exhibits competitive performance with the Google API. We are releasing the RAM at \url{https://recognize-anything.github.io/} to foster the advancements of large models in computer vision.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03514v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了识别任何事物模型（RAM）：一个强大的图像标记基础模型。RAM可以高精度识别任何常见类别。RAM引入了一种新的图像标记范式，利用大规模的图像-文本对进行训练，而不是手动注释。RAM的开发包括四个关键步骤。首先，通过自动文本语义解析，大规模地获得无标注的图像标签。随后，通过统一分别由原始文本和解析的标签监督的标题和标记任务，训练用于自动注释的初步模型。第三，使用数据引擎生成额外的注释并清除不正确的注释。最后，使用处理后的数据对模型进行再训练，并使用更小但质量更高的数据集进行微调。我们在众多基准上评估RAM的标记能力，并观察到令人印象深刻的零样本性能，显著优于CLIP和BLIP。值得注意的是，RAM甚至超过了完全受监督的方式，并显示出与谷歌API竞争的性能。我们将在\url发布RAM{https://recognize-anything.github.io/}促进计算机视觉中大型模型的发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03514v2" target="_blank">2306.03514v2</a>
                              </td>
                              <td>Recognize Anything: A Strong Image Tagging Model</td>
                              <td>Youcai Zhang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03514v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03514v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_16582v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SinDDM: A Single Image Denoising Diffusion Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_16582v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_16582v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_16582v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_16582v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>去噪扩散模型（DDM）在图像生成、编辑和恢复方面带来了惊人的性能飞跃。然而，现有的DDM使用非常大的数据集进行训练。在这里，我们介绍了一个用于在单个图像上训练DDM的框架。我们的方法，我们称之为SinDDM，通过使用多尺度扩散过程来学习训练图像的内部统计信息。为了驱动反向扩散过程，我们使用了一个完全卷积的轻量级去噪器，该去噪器以噪声水平和尺度为条件。该体系结构允许以从粗到细的方式生成任意维度的样本。如我们所示，SinDDM生成各种高质量的样本，适用于广泛的任务，包括风格转换和协调。此外，它可以很容易地受到外部监督的指导。特别地，我们演示了使用预先训练的CLIP模型从单个图像进行文本引导生成。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.16582v3" target="_blank">2211.16582v3</a>
                              </td>
                              <td>SinDDM: A Single Image Denoising Diffusion Model</td>
                              <td>Vladimir Kulikov</td>
                              <td>2022-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_16582v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.16582v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03899v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Label-free Scene Understanding by Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03899v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03899v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03899v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. And for nuScenes dataset, our performance is 26.8% with an improvement of 6%. Code will be released (https://github.com/runnanchen/Label-Free-Scene-Understanding).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03899v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比视觉语言预训练（CLIP）和分段任意（SAM）等视觉基础模型在图像分类和分割任务中表现出了令人印象深刻的零样本性能。然而，将CLIP和SAM结合起来进行无标签场景理解还有待探索。在本文中，我们研究了视觉基础模型在使网络能够在没有标记数据的情况下理解2D和3D世界方面的潜力。主要挑战在于在极噪声伪标签下有效地监督网络，这些伪标签由CLIP生成，并在从2D域到3D域的传播过程中进一步加剧。为了应对这些挑战，我们提出了一种新的跨模态噪声监督（CNS）方法，该方法利用CLIP和SAM的优势来同时监督2D和3D网络。特别地，我们引入了一种预测一致性正则化来共同训练2D和3D网络，然后使用SAM的鲁棒特征表示进一步增强网络的潜在空间一致性。在不同的室内和室外数据集上进行的实验证明了我们的方法在理解2D和3D开放环境方面的卓越性能。我们的2D和3D网络在ScanNet上以28.4%和33.5%的mIoU实现了无标签语义分割，分别提高了4.7%和7.9%。对于nuScenes数据集，我们的性能提高了6%，达到26.8%。将发布代码(https://github.com/runnanchen/Label-Free-Scene-Understanding)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03899v1" target="_blank">2306.03899v1</a>
                              </td>
                              <td>Towards Label-free Scene Understanding by Vision Foundation Models</td>
                              <td>Runnan Chen</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03899v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03899v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_07636v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DP-BART for Privatized Text Rewriting under Local Differential Privacy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_07636v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_07636v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_07636v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privacy guarantees, privatization of only individual words, as well as a lack of transparency and reproducibility. In this paper, we propose a new system 'DP-BART' that largely outperforms existing LDP systems. Our approach uses a novel clipping method, iterative pruning, and further training of internal representations which drastically reduces the amount of noise required for DP guarantees. We run experiments on five textual datasets of varying sizes, rewriting them at different privacy guarantees and evaluating the rewritten texts on downstream text classification tasks. Finally, we thoroughly discuss the privatized text rewriting approach and its limitations, including the problem of the strict text adjacency constraint in the LDP paradigm that leads to the high noise requirement.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_07636v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有局部差异隐私的私有化文本重写（LDP）是最近的一种方法，它能够共享敏感的文本文档，同时正式保证对个人的隐私保护。然而，现有的系统面临着几个问题，如形式上的数学缺陷、不切实际的隐私保障、仅对单个单词进行私有化，以及缺乏透明度和可复制性。在本文中，我们提出了一种新的系统“DP-BART”，它在很大程度上优于现有的LDP系统。我们的方法使用了一种新的修剪方法、迭代修剪和内部表示的进一步训练，这大大减少了DP保证所需的噪声量。我们在五个不同大小的文本数据集上进行了实验，在不同的隐私保证下重写它们，并在下游文本分类任务中评估重写的文本。最后，我们深入讨论了私有化文本重写方法及其局限性，包括LDP范式中严格的文本邻接约束导致高噪声要求的问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.07636v2" target="_blank">2302.07636v2</a>
                              </td>
                              <td>DP-BART for Privatized Text Rewriting under Local Differential Privacy</td>
                              <td>Timour Igamberdiev</td>
                              <td>2023-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_07636v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.07636v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03678v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Difference of BERT-style and CLIP-style Text Encoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03678v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03678v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03678v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing, e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent performance on a broad range of vision tasks. However, few studies are dedicated to studying the text encoders learned by CLIP. In this paper, we analyze the difference between BERT-style and CLIP-style text encoders from three experiments: (i) general text understanding, (ii) vision-centric text understanding, and (iii) text-to-image generation. Experimental analyses show that although CLIP-style text encoders underperform BERT-style ones for general text understanding tasks, they are equipped with a unique ability, i.e., synesthesia, for the cross-modal association, which is more similar to the senses of humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03678v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩蔽语言建模（MLM）一直是自然语言处理中最流行的预训练方法之一，例如BERT，它是一种具有代表性的模型。近年来，对比语言图像预训练（CLIP）也引起了人们的关注，尤其是其视觉模型在广泛的视觉任务中取得了优异的性能。然而，很少有研究专门研究CLIP学习的文本编码器。在本文中，我们从三个实验中分析了BERT风格和CLIP风格的文本编码器之间的差异：（i）一般文本理解，（ii）以视觉为中心的文本理解，以及（iii）文本到图像生成。实验分析表明，尽管CLIP风格的文本编码器在一般文本理解任务中表现不如BERT风格的编码器，但它们在跨模态联想方面具有独特的能力，即联觉，这更类似于人类的感官。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03678v1" target="_blank">2306.03678v1</a>
                              </td>
                              <td>On the Difference of BERT-style and CLIP-style Text Encoders</td>
                              <td>Zhihong Chen</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03678v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03678v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03594v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03594v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03594v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03594v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks.~Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03594v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定音频片段和参考人脸图像，会说话的头部生成的目标是生成高保真度的会说话的头视频。尽管一些音频驱动的头部视频生成方法在过去已经取得了一些成就，但它们大多只关注嘴唇和音频同步，缺乏再现目标人物面部表情的能力。为此，我们提出了一种由记忆共享情感特征提取器（MSEF）和基于U-net的注意力增强翻译器（AATU）组成的会说话的头部生成模型。首先，MSEF可以从音频中提取隐含的情感辅助特征，以估计更准确的情感面部标志~其次，AATU充当估计的地标和照片逼真视频帧之间的翻译器。大量的定性和定量实验表明，该方法优于以往的工作。代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03594v1" target="_blank">2306.03594v1</a>
                              </td>
                              <td>Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</td>
                              <td>Jianrong Wang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03594v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03594v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03375v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying Shared Decodable Concepts in the Human Brain Using Image-Language Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03375v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03375v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03375v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a method that takes advantage of high-quality pretrained multimodal representations to explore fine-grained semantic networks in the human brain. Previous studies have documented evidence of functional localization in the brain, with different anatomical regions preferentially activating for different types of sensory input. Many such localized structures are known, including the fusiform face area and parahippocampal place area. This raises the question of whether additional brain regions (or conjunctions of brain regions) are also specialized for other important semantic concepts. To identify such brain regions, we developed a data-driven approach to uncover visual concepts that are decodable from a massive functional magnetic resonance imaging (fMRI) dataset. Our analysis is broadly split into three sections. First, a fully connected neural network is trained to map brain responses to the outputs of an image-language foundation model, CLIP (Radford et al., 2021). Subsequently, a contrastive-learning dimensionality reduction method reveals the brain-decodable components of CLIP space. In the final section of our analysis, we localize shared decodable concepts in the brain using a voxel-masking optimization method to produce a shared decodable concept (SDC) space. The accuracy of our procedure is validated by comparing it to previous localization experiments that identify regions for faces, bodies, and places. In addition to these concepts, whose corresponding brain regions were already known, we localize novel concept representations which are shared across participants to other areas of the human brain. We also demonstrate how this method can be used to inspect fine-grained semantic networks for individual participants. We envisage that this extensible method can also be adapted to explore other questions at the intersection of AI and neuroscience.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03375v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种利用高质量预训练的多模式表示来探索人脑中细粒度语义网络的方法。先前的研究已经记录了大脑功能定位的证据，不同的解剖区域优先激活不同类型的感觉输入。许多这样的局部结构是已知的，包括梭形面区和海马旁区。这就提出了一个问题，即额外的大脑区域（或大脑区域的连接词）是否也专门用于其他重要的语义概念。为了识别这些大脑区域，我们开发了一种数据驱动的方法，以揭示可从大规模功能磁共振成像（fMRI）数据集中解码的视觉概念。我们的分析大致分为三个部分。首先，训练一个完全连接的神经网络，将大脑反应映射到图像语言基础模型CLIP的输出（Radford等人，2021）。随后，对比学习降维方法揭示了CLIP空间的大脑可解码成分。在我们分析的最后一部分，我们使用体素掩蔽优化方法定位大脑中的共享可解码概念，以产生共享可解码理念（SDC）空间。通过将我们的程序与之前识别人脸、身体和位置区域的定位实验进行比较，验证了该程序的准确性。除了这些概念（其对应的大脑区域已经为人所知）之外，我们还将参与者之间共享的新概念表示定位到人脑的其他区域。我们还演示了如何使用此方法来检查各个参与者的细粒度语义网络。我们设想，这种可扩展的方法也可以适用于探索人工智能和神经科学交叉的其他问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03375v1" target="_blank">2306.03375v1</a>
                              </td>
                              <td>Identifying Shared Decodable Concepts in the Human Brain Using Image-Language Foundation Models</td>
                              <td>Cory Efird</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03375v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03375v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_10713v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond Uniform Lipschitz Condition in Differentially Private Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_10713v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_10713v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_10713v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex functions when the Lipschitz constants are unbounded but have bounded moments, i.e., they are heavy-tailed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_10713v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>差分私有随机梯度下降（DP-SGD）的大多数先验结果都是在一致Lipschitzness的简单假设下得出的，即每个样本的梯度是一致有界的。我们通过假设每样本梯度具有样本相关的上界，即每样本Lipschitz常数来推广一致Lipschitzness，其本身可能是无界的。当每样本Lipschitz常数有界时，我们为满足我们的Lipschitzness的一般版本的凸过参数化设置在DP-SGD中选择片段范数提供了原则指导；特别地，我们建议只调整剪辑范数，直到值达到每个样本的最小Lipschitz常数。这在公共数据上预训练的深度网络之上的softmax层的私有训练中得到了应用。我们通过在8个数据集上的实验验证了我们的建议的有效性。此外，当Lipschitz常数是无界的但具有有界矩，即它们是重尾的时，我们提供了DP-SGD在凸函数和非凸函数上的新的收敛结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.10713v2" target="_blank">2206.10713v2</a>
                              </td>
                              <td>Beyond Uniform Lipschitz Condition in Differentially Private Optimization</td>
                              <td>Rudrajit Das</td>
                              <td>2022-06-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_10713v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.10713v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_09172v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hyperbolic Image-Text Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_09172v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_09172v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_09172v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept "dog" entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text datasets. Our results show that MERU learns a highly interpretable and structured representation space while being competitive with CLIP's performance on standard multi-modal tasks like image classification and image-text retrieval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_09172v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉和语言概念自然地将自己组织成一个层次，其中文本概念“狗”包含所有包含狗的图像。尽管是直观的，但当前的大规模视觉和语言模型（如CLIP）并没有明确地捕捉到这种层次结构。我们提出了MERU，这是一个对比模型，可以产生图像和文本的双曲线表示。双曲空间具有合适的几何特性来嵌入树状数据，因此MERU可以更好地捕捉图像-文本数据集中的底层层次结构。我们的结果表明，MERU学习了一个高度可解释和结构化的表示空间，同时在图像分类和图像文本检索等标准多模态任务上与CLIP的性能具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.09172v2" target="_blank">2304.09172v2</a>
                              </td>
                              <td>Hyperbolic Image-Text Representations</td>
                              <td>Karan Desai</td>
                              <td>2023-04-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_09172v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.09172v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12561v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Retrieval-Augmented Multimodal Language Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12561v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12561v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12561v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities, such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12561v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的多模式模型，如DALL-E和CM3，在文本到图像和图像到文本生成方面取得了显著进展。然而，这些模型将所有学习到的知识（例如，埃菲尔铁塔的外观）存储在模型参数中，需要越来越大的模型和训练数据来获取更多的知识。为了以更具可扩展性和模块化的方式集成知识，我们提出了一种检索增强的多模式模型，该模型使基础多模式模型（生成器）能够引用检索器从外部存储器（例如，网络上的文档）中提取的相关文本和图像。具体来说，对于检索器，我们使用预训练的CLIP，对于生成器，我们在LAION数据集上训练CM3 Transformer。我们得到的模型名为检索增强CM3（RA-CM3），是第一个可以检索和生成文本和图像的多模式模型。我们发现，RA-CM3在图像和字幕生成任务上显著优于基线多模式模型，如DALL-E和CM3（MS-COCO的12个FID和17个CIDEr改进），同时训练所需的计算量要少得多（<DALL-E的30%）。此外，我们发现RA-CM3表现出了新的能力，如忠实的图像生成和多模式上下文学习（例如，从演示中生成图像）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12561v2" target="_blank">2211.12561v2</a>
                              </td>
                              <td>Retrieval-Augmented Multimodal Language Modeling</td>
                              <td>Michihiro Yasunaga</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12561v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12561v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_10965v5_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_10965v5_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_10965v5_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_10965v5_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally, crowdsourced user study results are available at Appendix B to further support the effectiveness of our method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_10965v5_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了CLIP Dissect，这是一种自动描述视觉网络中单个隐藏神经元功能的新技术。CLIP Dissect利用多模式视觉/语言模型的最新进展，用开放式概念标记内部神经元，而不需要任何标记的数据或人类例子。我们表明，CLIP Dissect为底层神经元提供了比现有方法更准确的描述，其中基本事实是可用的，并且为隐藏层神经元提供了定性的良好描述。此外，我们的方法非常灵活：它与模型无关，可以轻松处理新概念，并且可以扩展以利用未来更好的多模式模型。最后，CLIP Dissect在计算上是高效的，可以在4分钟内标记五层ResNet-50中的所有神经元，这比现有方法快10倍多。我们的代码可在https://github.com/Trustworthy-ML-Lab/CLIP-dissect.最后，附录B中提供了众包用户研究结果，以进一步支持我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.10965v5" target="_blank">2204.10965v5</a>
                              </td>
                              <td>CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks</td>
                              <td>Tuomas Oikarinen</td>
                              <td>2022-04-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_10965v5_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.10965v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15877v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exponential Smoothing for Off-Policy Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15877v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15877v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15877v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15877v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>非策略学习（OPL）旨在从记录的土匪数据中找到改进的策略，通常是通过最小化风险的反向倾向评分（IPS）估计器。在这项工作中，我们研究了IPS的光滑正则化，为此我们导出了双边PAC-Bayes推广界。绑定是可处理的、可扩展的、可解释的，并提供学习证书。特别地，它对于标准IPS也是有效的，而不需要假设重要性权重是有界的。我们通过一组学习任务来证明我们的方法的相关性及其良好的性能。由于我们对标准IPS的约束成立，我们能够深入了解规范IPS何时有用。也就是说，我们确定了可能不需要正则化的情况。这违背了这样一种观点，即在实践中，剪辑后的IPS通常比OPL中的标准IPS享有良好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15877v2" target="_blank">2305.15877v2</a>
                              </td>
                              <td>Exponential Smoothing for Off-Policy Learning</td>
                              <td>Imad Aouali</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15877v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15877v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14742v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14742v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14742v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14742v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Editing real facial images is a crucial task in computer vision with significant demand in various real-world applications. While GAN-based methods have showed potential in manipulating images especially when combined with CLIP, these methods are limited in their ability to reconstruct real images due to challenging GAN inversion capability. Despite the successful image reconstruction achieved by diffusion-based methods, there are still challenges in effectively manipulating fine-gained facial attributes with textual instructions.To address these issues and facilitate convenient manipulation of real facial images, we propose a novel approach that conduct text-driven image editing in the semantic latent space of diffusion model. By aligning the temporal feature of the diffusion model with the semantic condition at generative process, we introduce a stable manipulation strategy, which perform precise zero-shot manipulation effectively. Furthermore, we develop an interactive system named ChatFace, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latent space. This system enables users to perform complex multi-attribute manipulations through dialogue, opening up new possibilities for interactive image editing. Extensive experiments confirmed that our approach outperforms previous methods and enables precise editing of real facial images, making it a promising candidate for real-world applications. Project page: https://dongxuyue.github.io/chatface/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14742v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>编辑真实的面部图像是计算机视觉中的一项关键任务，在各种现实世界的应用中都有很大的需求。虽然基于GAN的方法在处理图像方面显示出了潜力，尤其是与CLIP相结合时，但由于具有挑战性的GAN反演能力，这些方法重建真实图像的能力有限。尽管基于扩散的方法成功地实现了图像重建，但在用文本指令有效地操纵精细的面部属性方面仍然存在挑战。为了解决这些问题并方便地操纵真实的面部图像，我们提出了一种在扩散模型的语义潜在空间中进行文本驱动的图像编辑的新方法。通过将扩散模型的时间特征与生成过程中的语义条件相结合，我们引入了一种稳定的操作策略，可以有效地执行精确的零样本操作。此外，我们开发了一个名为ChatFace的交互式系统，该系统结合了大型语言模型的零样本推理能力，在扩散语义潜在空间中执行有效操作。该系统使用户能够通过对话进行复杂的多属性操作，为交互式图像编辑开辟了新的可能性。大量实验证实，我们的方法优于以前的方法，能够精确编辑真实的面部图像，使其成为现实世界应用的一个有前途的候选者。项目页面：https://dongxuyue.github.io/chatface/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14742v2" target="_blank">2305.14742v2</a>
                              </td>
                              <td>ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation</td>
                              <td>Dongxu Yue</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14742v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14742v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00974v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Intriguing Properties of Text-guided Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00974v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00974v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00974v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: (i) natural-looking text prompts generating images with the wrong content, or (ii) different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. We make several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, and verify this in a human study. Our study reveals four intriguing properties of TDMs that have not been systematically studied before: (1) We find a variety of natural text prompts producing images that fail to capture the semantics of input texts. We categorize these failures into ten distinct types based on the underlying causes. (2) We find samples in the latent space (which are not outliers) that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that lead to natural-looking images which are unrelated to the text prompt, implying a potential misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to an input prompt we can generate a variety of specified target objects, while only minimally affecting the CLIP score. This demonstrates the fragility of language representations and raises potential safety concerns.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00974v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本引导扩散模型（TDM）被广泛应用，但可能会意外失败。常见的失败包括：（i）生成内容错误的图像的自然文本提示，或（ii）潜在变量的不同随机样本，尽管以相同的文本提示为条件，但会生成截然不同甚至不相关的输出。在这项工作中，我们旨在更详细地研究和理解TDM的故障模式。为了实现这一点，我们提出了SAGE，这是一种对TDM的对抗性攻击，使用图像分类器作为代理损失函数，在TDM的离散提示空间和高维潜在空间上进行搜索，以自动发现图像生成中的意外行为和失败案例。我们做出了一些技术贡献，以确保SAGE发现扩散模型而不是分类器的失败案例，并在人类研究中验证了这一点。我们的研究揭示了TDM的四个有趣的特性，这些特性以前从未被系统地研究过：（1）我们发现各种自然文本提示产生的图像无法捕捉输入文本的语义。我们根据根本原因将这些故障分为十种不同的类型。（2） 我们在潜在空间中发现了导致图像失真的样本（这些样本不是异常值），而与文本提示无关，这表明潜在空间的部分结构不好。（3） 我们还发现潜在样本会导致与文本提示无关的自然图像，这意味着潜在空间和提示空间之间可能存在错位。（4） 通过在输入提示中附加单个对抗性令牌嵌入，我们可以生成各种指定的目标对象，同时对CLIP分数的影响最小。这表明了语言表征的脆弱性，并引发了潜在的安全问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00974v2" target="_blank">2306.00974v2</a>
                              </td>
                              <td>Intriguing Properties of Text-guided Diffusion Models</td>
                              <td>Qihao Liu</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00974v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00974v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16944v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning to Imagine: Visually-Augmented Natural Language Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16944v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16944v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16944v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformerbased architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human evaluation demonstrate the effectiveness of our proposed method. We will release the code, model, and data at the link: https://github.com/RUCAIBox/LIVE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16944v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们经常想象相关的场景来帮助写作过程。在这项工作中，我们的目标是以与人类相同的方式利用视觉信息进行构图。我们提出了一种方法，LIVE，使预先训练的语言模型（PLM）学习想象，用于可视化增强的自然语言gEneration。首先，我们基于文本想象场景：我们使用扩散模型来合成以输入文本为条件的高质量图像。其次，我们使用CLIP来确定文本是否能够以后验的方式唤起想象力。最后，我们的想象力是动态的，我们对每个句子进行合成，而不是对整个段落只生成一个图像。从技术上讲，我们提出了一种新的即插即用融合层，以获得每个文本的视觉增强表示。我们的视觉-文本融合层与基于Transformerbased的架构兼容。我们使用BART和T5在四个生成任务上进行了广泛的实验，自动结果和人工评估证明了我们提出的方法的有效性。我们将在以下链接中发布代码、模型和数据：https://github.com/RUCAIBox/LIVE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16944v2" target="_blank">2305.16944v2</a>
                              </td>
                              <td>Learning to Imagine: Visually-Augmented Natural Language Generation</td>
                              <td>Tianyi Tang</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16944v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16944v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02348v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02348v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02348v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02348v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal embeddings aim to enrich the semantic information in neural representations of language compared to text-only models. While different embeddings exhibit different applicability and performance on downstream tasks, little is known about the systematic representation differences attributed to the visual modality. Our paper compares word embeddings from three vision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three text-only models, with static (FastText) as well as contextual representations (multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the effect of visual grounding on language representations, including 46 semantic parameters. We identify meaning properties and relations that characterize words whose embeddings are most affected by the inclusion of visual modality in the training data; that is, points where visual grounding turns out most important. We find that the effect of visual modality correlates most with denotational semantic properties related to concreteness, but is also detected for several specific semantic classes, as well as for valence, a sentiment-related connotational property of linguistic expressions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02348v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与纯文本模型相比，多模式嵌入旨在丰富语言神经表示中的语义信息。虽然不同的嵌入在下游任务上表现出不同的适用性和性能，但人们对视觉模态导致的系统表示差异知之甚少。我们的论文比较了三种视觉和语言模型（CLIP、OpenCLIP和多语言CLIP）和三种纯文本模型的单词嵌入，以及静态（FastText）和上下文表示（多语言BERT；XLM-RoBERTa）。这是第一次大规模研究视觉基础对语言表征的影响，包括46个语义参数。我们确定了表征单词的含义属性和关系，这些单词的嵌入最受训练数据中视觉模态的影响；也就是说，视觉基础是最重要的。我们发现，视觉模态的影响与与具体性相关的指称语义特性最为相关，但也适用于几个特定的语义类别，以及配价，这是语言表达中与情感相关的内涵特性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02348v1" target="_blank">2306.02348v1</a>
                              </td>
                              <td>Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations</td>
                              <td>Aleksey Tikhonov</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02348v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02348v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02329v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02329v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02329v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02329v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training models to apply common-sense linguistic knowledge and visual concepts from 2D images to 3D scene understanding is a promising direction that researchers have only recently started to explore. However, it still remains understudied whether 2D distilled knowledge can provide useful representations for downstream 3D vision-language tasks such as 3D question answering. In this paper, we propose a novel 3D pre-training Vision-Language method, namely Multi-CLIP, that enables a model to learn language-grounded and transferable 3D scene point cloud representations. We leverage the representational power of the CLIP model by maximizing the agreement between the encoded 3D scene features and the corresponding 2D multi-view image and text embeddings in the CLIP space via a contrastive objective. To validate our approach, we consider the challenging downstream tasks of 3D Visual Question Answering (3D-VQA) and 3D Situated Question Answering (3D-SQA). To this end, we develop novel multi-modal transformer-based architectures and we demonstrate how our pre-training method can benefit their performance. Quantitative and qualitative experimental results show that Multi-CLIP outperforms state-of-the-art works across the downstream tasks of 3D-VQA and 3D-SQA and leads to a well-structured 3D scene feature space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02329v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练模型将2D图像中的常识性语言知识和视觉概念应用于3D场景理解是研究人员最近才开始探索的一个有前途的方向。然而，2D提取的知识是否能为下游的3D视觉语言任务（如3D问答）提供有用的表示，仍有待进一步研究。在本文中，我们提出了一种新的3D预训练视觉语言方法，即Multi-CLIP，该方法使模型能够学习基于语言的可转移3D场景点云表示。我们通过对比目标最大化编码的3D场景特征与CLIP空间中相应的2D多视图图像和文本嵌入之间的一致性，利用了CLIP模型的代表性。为了验证我们的方法，我们考虑了具有挑战性的3D视觉问答（3D-VQA）和3D情境问答（3D-SQA）的下游任务。为此，我们开发了新的基于多模态变换器的架构，并展示了我们的预训练方法如何提高它们的性能。定量和定性实验结果表明，Multi-CLIP在3D-VQA和3D-SQA的下游任务中优于最先进的工作，并导致了结构良好的3D场景特征空间。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02329v1" target="_blank">2306.02329v1</a>
                              </td>
                              <td>Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes</td>
                              <td>Alexandros Delitzas</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02329v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02329v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02252v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02252v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02252v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02252v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce MoviePuzzle, a novel challenge that targets visual narrative reasoning and holistic movie understanding. Despite the notable progress that has been witnessed in the realm of video understanding, most prior works fail to present tasks and models to address holistic video understanding and the innate visual narrative structures existing in long-form videos. To tackle this quandary, we put forth MoviePuzzle task that amplifies the temporal feature learning and structure learning of video models by reshuffling the shot, frame, and clip layers of movie segments in the presence of video-dialogue information. We start by establishing a carefully refined dataset based on MovieNet by dissecting movies into hierarchical layers and randomly permuting the orders. Besides benchmarking the MoviePuzzle with prior arts on movie understanding, we devise a Hierarchical Contrastive Movie Clustering (HCMC) model that considers the underlying structure and visual semantic orders for movie reordering. Specifically, through a pairwise and contrastive learning approach, we train models to predict the correct order of each layer. This equips them with the knack for deciphering the visual narrative structure of movies and handling the disorder lurking in video data. Experiments show that our approach outperforms existing state-of-the-art methods on the \MoviePuzzle benchmark, underscoring its efficacy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02252v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍MoviePuzzle，这是一个针对视觉叙事推理和整体电影理解的新颖挑战。尽管在视频理解领域取得了显著进展，但大多数先前的作品都未能提出解决整体视频理解和长视频中固有的视觉叙事结构的任务和模型。为了解决这一难题，我们提出了MoviePuzzle任务，该任务通过在存在视频对话信息的情况下重新排列电影片段的镜头、帧和剪辑层来放大视频模型的时间特征学习和结构学习。我们首先在MovieNet的基础上，通过将电影分解为分层并随机排列顺序，建立了一个精心完善的数据集。除了在电影理解方面将MoviePuzzle与现有技术进行比较外，我们还设计了一个层次对比电影聚类（HCMC）模型，该模型考虑了电影重新排序的底层结构和视觉语义顺序。具体来说，通过成对和对比学习方法，我们训练模型来预测每一层的正确顺序。这使他们具备了破解电影视觉叙事结构和处理视频数据中隐藏的混乱的技巧。实验表明，我们的方法在\MoviePuzzle基准测试上优于现有的最先进的方法，突出了其有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02252v1" target="_blank">2306.02252v1</a>
                              </td>
                              <td>MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</td>
                              <td>Jianghui Wang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02252v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02252v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02243v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02243v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02243v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02243v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt learning has become a popular approach for adapting large vision-language models, such as CLIP, to downstream tasks. Typically, prompt learning relies on a fixed prompt token or an input-conditional token to fit a small amount of data under full supervision. While this paradigm can generalize to a certain range of unseen classes, it may struggle when domain gap increases, such as in fine-grained classification and satellite image segmentation. To address this limitation, we propose Retrieval-enhanced Prompt learning (RePrompt), which introduces retrieval mechanisms to cache the knowledge representations from downstream tasks. we first construct a retrieval database from training examples, or from external examples when available. We then integrate this retrieval-enhanced mechanism into various stages of a simple prompt learning baseline. By referencing similar samples in the training set, the enhanced model is better able to adapt to new tasks with few samples. Our extensive experiments over 15 vision datasets, including 11 downstream tasks with few-shot setting and 4 domain generalization benchmarks, demonstrate that RePrompt achieves considerably improved performance. Our proposed approach provides a promising solution to the challenges faced by prompt learning when domain gap increases. The code and models will be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02243v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>即时学习已经成为使大型视觉语言模型（如CLIP）适应下游任务的一种流行方法。通常，提示学习依赖于固定的提示令牌或输入条件令牌来在完全监督下适应少量数据。虽然这种范式可以推广到一定范围的看不见的类，但当领域差距增加时，例如在细粒度分类和卫星图像分割中，它可能会遇到困难。为了解决这一限制，我们提出了检索增强提示学习（RePrompt），它引入了检索机制来缓存来自下游任务的知识表示。我们首先根据训练实例或外部实例（如果可用）构建检索数据库。然后，我们将这种检索增强机制集成到简单提示学习基线的各个阶段。通过参考训练集中的相似样本，增强的模型能够更好地适应样本较少的新任务。我们在15个视觉数据集上进行了广泛的实验，包括11个具有少量镜头设置的下游任务和4个领域泛化基准，证明RePrompt实现了显著提高的性能。当领域差距增加时，我们提出的方法为快速学习所面临的挑战提供了一个很有前途的解决方案。将提供代码和型号。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02243v1" target="_blank">2306.02243v1</a>
                              </td>
                              <td>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</td>
                              <td>Jintao Rong</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02243v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02243v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02240v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ProTeCt: Prompt Tuning for Hierarchical Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02240v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02240v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02240v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large visual-language models, like CLIP, learn generalized representations and have shown promising zero-shot performance. Few-shot adaptation methods, based on prompt tuning, have also been shown to further improve performance on downstream datasets. However, these models are not hierarchically consistent. Frequently, they infer incorrect labels at coarser taxonomic class levels, even when the inference at the leaf level (original class labels) is correct. This is problematic, given their support for open set classification and, in particular, open-grained classification, where practitioners define label sets at various levels of granularity. To address this problem, we propose a prompt tuning technique to calibrate the hierarchical consistency of model predictions. A set of metrics of hierarchical consistency, the Hierarchical Consistent Accuracy (HCA) and the Mean Treecut Accuracy (MTA), are first proposed to benchmark model performance in the open-granularity setting. A prompt tuning technique, denoted as Prompt Tuning for Hierarchical Consistency (ProTeCt), is then proposed to calibrate classification across all possible label set granularities. Results show that ProTeCt can be combined with existing prompt tuning methods to significantly improve open-granularity classification performance without degradation of the original classification performance at the leaf level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02240v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型可视化语言模型，如CLIP，学习广义表示，并显示出良好的零样本性能。很少有基于即时调整的镜头自适应方法也被证明可以进一步提高下游数据集的性能。然而，这些模型在层次上并不一致。通常，它们在较粗的分类类级别推断出不正确的标签，即使在叶级别的推断（原始类标签）是正确的。这是有问题的，因为他们支持开放集分类，特别是开放粒度分类，从业者在不同的粒度级别定义标签集。为了解决这个问题，我们提出了一种快速调整技术来校准模型预测的层次一致性。首先提出了一组分层一致性度量，即分层一致性精度（HCA）和平均树精度（MTA），以在开放粒度设置中对模型性能进行基准测试。然后提出了一种即时调整技术，称为分层一致性的即时调整（ProTeCt），用于校准所有可能的标签集粒度的分类。结果表明，ProTeCt可以与现有的即时调整方法相结合，显著提高开放粒度分类性能，而不会降低叶级的原始分类性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02240v1" target="_blank">2306.02240v1</a>
                              </td>
                              <td>ProTeCt: Prompt Tuning for Hierarchical Consistency</td>
                              <td>Tz-Ying Wu</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02240v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02240v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02236v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector Guidance for Multi-Object Text-to-Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02236v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02236v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02236v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models have demonstrated impressive performance in text-to-image generation. They utilize a text encoder and cross-attention blocks to infuse textual information into images at a pixel level. However, their capability to generate images with text containing multiple objects is still restricted. Previous works identify the problem of information mixing in the CLIP text encoder and introduce the T5 text encoder or incorporate strong prior knowledge to assist with the alignment. We find that mixing problems also occur on the image side and in the cross-attention blocks. The noisy images can cause different objects to appear similar, and the cross-attention blocks inject information at a pixel level, leading to leakage of global object understanding and resulting in object mixing. In this paper, we introduce Detector Guidance (DG), which integrates a latent object detection model to separate different objects during the generation process. DG first performs latent object detection on cross-attention maps (CAMs) to obtain object information. Based on this information, DG then masks conflicting prompts and enhances related prompts by manipulating the following CAMs. We evaluate the effectiveness of DG using Stable Diffusion on COCO, CC, and a novel multi-related object benchmark, MRO. Human evaluations demonstrate that DG provides an 8-22\% advantage in preventing the amalgamation of conflicting concepts and ensuring that each object possesses its unique region without any human involvement and additional iterations. Our implementation is available at \url{https://github.com/luping-liu/Detector-Guidance}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02236v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型在文本到图像生成方面表现出了令人印象深刻的性能。它们利用文本编码器和交叉关注块将文本信息注入像素级的图像中。然而，它们生成具有包含多个对象的文本的图像的能力仍然受到限制。先前的工作确定了CLIP文本编码器中的信息混合问题，并介绍了T5文本编码器或结合了强大的先验知识来帮助对齐。我们发现混合问题也发生在图像侧和交叉关注块中。噪声图像会导致不同的对象看起来相似，而交叉关注块在像素级注入信息，导致全局对象理解的泄漏，并导致对象混合。在本文中，我们介绍了探测器制导（DG），它集成了一个潜在物体检测模型，以在生成过程中分离不同的物体。DG首先对交叉注意力图（CAM）进行潜在对象检测以获得对象信息。基于这些信息，DG随后通过操纵以下CAM来屏蔽冲突提示并增强相关提示。我们使用COCO、CC上的稳定扩散和一种新的多相关对象基准MRO来评估DG的有效性。人类评估表明，DG在防止冲突概念的融合方面提供了8-22\%的优势，并确保每个对象都拥有其独特的区域，而无需任何人类参与和额外的迭代。我们的实现可在\url上获得{https://github.com/luping-liu/Detector-Guidance}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02236v1" target="_blank">2306.02236v1</a>
                              </td>
                              <td>Detector Guidance for Multi-Object Text-to-Image Generation</td>
                              <td>Luping Liu</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02236v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02236v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00980v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00980v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00980v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00980v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00980v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型可以从自然语言描述中创造出令人惊叹的图像，与专业艺术家和摄影师的作品相媲美。然而，这些模型很大，具有复杂的网络架构和数十次去噪迭代，这使得它们的计算成本高昂且运行缓慢。因此，需要高端GPU和基于云的推理来大规模运行扩散模型。这是昂贵的，并具有隐私影响，尤其是当用户数据被发送给第三方时。为了克服这些挑战，我们提出了一种通用方法，首次在不到2美元的秒内解锁移动设备上运行的文本到图像扩散模型。我们通过引入高效的网络架构和改进分步蒸馏来实现这一点。具体而言，我们通过识别原始模型的冗余度并通过数据蒸馏减少图像解码器的计算，提出了一种有效的UNet。此外，我们通过探索训练策略和从无分类器引导引入正则化来增强步骤提取。我们在MS-COCO上的大量实验表明，我们的模型具有$8$的去噪步骤，比具有$50$步骤的稳定扩散v$1.5$获得更好的FID和CLIP分数。我们的工作通过将强大的文本到图像扩散模型带给用户，使内容创作民主化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00980v2" target="_blank">2306.00980v2</a>
                              </td>
                              <td>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</td>
                              <td>Yanyu Li</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00980v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00980v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_00097v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_00097v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_00097v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_00097v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users with the support of the trusted execution environment. Experimental results demonstrate the effectiveness of NNSplitter, e.g., by only modifying 275 out of over 11 million (i.e., 0.002%) weights, the accuracy of the obfuscated ResNet-18 model on CIFAR-10 can drop to 10%. Moreover, NNSplitter is stealthy and resilient against norm clipping and fine-tuning attacks, making it an appealing solution for DNN model protection. The code is available at: https://github.com/Tongzhou0101/NNSplitter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_00097v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>作为一种有价值的知识产权，深度神经网络模型受到水印等技术的保护。然而，这种被动的模式保护并不能完全防止模式滥用。在这项工作中，我们提出了一种主动模型IP保护方案，即NNSplitter，它通过将模型分为两部分来主动保护模型：由于权重模糊而表现不佳的模糊模型，以及由模糊权重的索引和原始值组成的模型秘密，只有在可信执行环境的支持下，授权用户才能访问。实验结果证明了NNSplitter的有效性，例如，通过仅修改超过1100万个（即0.002%）权重中的275个，CIFAR-10上模糊的ResNet-18模型的准确性可以降至10%。此外，NNSplitter具有隐蔽性和弹性，可以抵御规范裁剪和微调攻击，使其成为DNN模型保护的一个有吸引力的解决方案。代码位于：https://github.com/Tongzhou0101/NNSplitter.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.00097v2" target="_blank">2305.00097v2</a>
                              </td>
                              <td>NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation</td>
                              <td>Tong Zhou</td>
                              <td>2023-04-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_00097v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.00097v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_00883v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Variance-reduced Clipping for Non-convex Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_00883v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_00883v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_00883v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Gradient clipping is a standard training technique used in deep learning applications such as large-scale language modeling to mitigate exploding gradients. Recent experimental studies have demonstrated a fairly special behavior in the smoothness of the training objective along its trajectory when trained with gradient clipping. That is, the smoothness grows with the gradient norm. This is in clear contrast to the well-established assumption in folklore non-convex optimization, a.k.a. $L$--smoothness, where the smoothness is assumed to be bounded by a constant $L$ globally. The recently introduced $(L_0,L_1)$--smoothness is a more relaxed notion that captures such behavior in non-convex optimization. In particular, it has been shown that under this relaxed smoothness assumption, SGD with clipping requires $O(\epsilon^{-4})$ stochastic gradient computations to find an $\epsilon$--stationary solution. In this paper, we employ a variance reduction technique, namely SPIDER, and demonstrate that for a carefully designed learning rate, this complexity is improved to $O(\epsilon^{-3})$ which is order-optimal. Our designed learning rate comprises the clipping technique to mitigate the growing smoothness. Moreover, when the objective function is the average of $n$ components, we improve the existing $O(n\epsilon^{-2})$ bound on the stochastic gradient complexity to $O(\sqrt{n} \epsilon^{-2} + n)$, which is order-optimal as well. In addition to being theoretically optimal, SPIDER with our designed parameters demonstrates comparable empirical performance against variance-reduced methods such as SVRG and SARAH in several vision tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_00883v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>梯度裁剪是一种标准的训练技术，用于深度学习应用，如大规模语言建模，以缓解梯度的爆炸。最近的实验研究表明，当使用梯度剪裁进行训练时，训练目标沿其轨迹的平滑性具有相当特殊的行为。也就是说，平滑度随梯度范数而增长。这与民间传说中的非凸优化中的既定假设形成了鲜明对比，即$L$——光滑性，其中光滑性被假设为全局受常数$L$的约束。最近引入的$（L_0，L_1）$-光滑性是一个更宽松的概念，它捕捉了非凸优化中的这种行为。特别地，已经表明，在这种松弛的光滑性假设下，具有削波的SGD需要$O（\epsilon^{-4}）$随机梯度计算来找到$\epsilon$-平稳解。在本文中，我们使用了一种方差减少技术，即SPIDE，并证明了对于精心设计的学习率，这种复杂性被提高到$O（ε^{-3}）$，这是阶最优的。我们设计的学习率包括修剪技术，以减轻生长的平滑度。此外，当目标函数是$n$分量的平均值时，我们将随机梯度复杂度上现有的$O（n\epsilon^｛-2｝）$界改进为$O（\sqrt｛n｝\epsilon^{-2｝+n）$，这也是阶最优的。除了在理论上是最优的之外，在几个视觉任务中，具有我们设计参数的SPIDER与SVRG和SARAH等方差减少方法相比，表现出了相当的经验性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.00883v2" target="_blank">2303.00883v2</a>
                              </td>
                              <td>Variance-reduced Clipping for Non-convex Optimization</td>
                              <td>Amirhossein Reisizadeh</td>
                              <td>2023-03-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_00883v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.00883v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01669v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01669v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01669v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01669v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a ``second generation'' of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying prompt modalities, e.g., textual or visual prompts, and learning paradigms. We find that (1) unexplored prompt tuning strategies that iteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5 points in semi-supervised learning, by 28.4 points in transductive zero-shot learning, and by 15.2 points in unsupervised learning, and (2) unlike conventional semi-supervised pseudolabeling, which exacerbates model biases toward classes with higher-quality pseudolabels, prompt tuning leads to a more equitable distribution of per-class accuracy. The code to reproduce the experiments is at github.com/BatsResearch/menghini-enhanceCLIPwithCLIP-code.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01669v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了优化它们的性能，通常需要将像CLIP这样的视觉语言模型（VLM）微调到下游任务。然而，一个主要障碍是标记数据的可用性有限。我们研究了伪标签的使用，即未标记数据的启发式标签，通过即时调整来增强CLIP。传统的伪标记在标记的数据上训练模型，然后为未标记的数据生成标记。VLM的零样本功能支持“第二代”伪标记方法，无需对标记数据进行特定任务培训。通过使用零样本伪标签作为监督源，我们观察到学习范式，如半监督学习、传递式零样本学习和非监督学习都可以被视为优化相同的损失函数。这种统一的观点使得能够制定适用于各种学习范式的通用培训策略。我们在图像分类任务中对它们进行了研究，其中CLIP通过不同的提示模式（如文本或视觉提示）和学习范式表现出局限性。我们发现：（1）迭代细化伪标签的未探索快速调整策略持续提高了CLIP的准确性，半监督学习提高了19.5分，传递式零样本学习提高了28.4分，非监督学习提高15.2分，（2）与传统的半监督伪标签不同，这加剧了模型对具有更高质量伪标签的类的偏见，及时调整导致每类精度的更公平分布。复制实验的代码位于github.com/BatsSearch/enghini-enhanceCLIP with CLIP-code。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01669v1" target="_blank">2306.01669v1</a>
                              </td>
                              <td>Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning</td>
                              <td>Cristina Menghini</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01669v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01533v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhance Temporal Relations in Audio Captioning with Sound Event Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01533v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01533v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01533v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automated audio captioning aims at generating natural language descriptions for given audio clips, not only detecting and classifying sounds, but also summarizing the relationships between audio events. Recent research advances in audio captioning have introduced additional guidance to improve the accuracy of audio events in generated sentences. However, temporal relations between audio events have received little attention while revealing complex relations is a key component in summarizing audio content. Therefore, this paper aims to better capture temporal relationships in caption generation with sound event detection (SED), a task that locates events' timestamps. We investigate the best approach to integrate temporal information in a captioning model and propose a temporal tag system to transform the timestamps into comprehensible relations. Results evaluated by the proposed temporal metrics suggest that great improvement is achieved in terms of temporal relation generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01533v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动音频字幕旨在为给定的音频片段生成自然语言描述，不仅可以检测和分类声音，还可以总结音频事件之间的关系。音频字幕的最新研究进展引入了额外的指导，以提高生成句子中音频事件的准确性。然而，音频事件之间的时间关系很少受到关注，而揭示复杂的关系是总结音频内容的关键组成部分。因此，本文旨在通过声音事件检测（SED）更好地捕捉字幕生成中的时间关系，这是一项定位事件时间戳的任务。我们研究了在字幕模型中集成时间信息的最佳方法，并提出了一个时间标签系统来将时间戳转换为可理解的关系。通过所提出的时间度量评估的结果表明，在时间关系生成方面取得了很大的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01533v1" target="_blank">2306.01533v1</a>
                              </td>
                              <td>Enhance Temporal Relations in Audio Captioning with Sound Event Detection</td>
                              <td>Zeyu Xie</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01533v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01533v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01820v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01820v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01820v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01820v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is trained to detect errors. The proposed CCED scheme has been implemented and evaluated on two widely used large-scale ML models: Contrastive Language Image Pretraining (CLIP) used for image classification and Bidirectional Encoder Representations from Transformers (BERT) used for natural language applications. The results show that more than 95 percent of the errors are detected when using a simple Random Forest classifier that is order of magnitude simpler than CLIP or BERT. These results illustrate the potential of CCED to implement error detection in large-scale ML models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01820v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习（ML）系统的复杂性每年都在增加，目前大型语言模型或文本到图像生成器的实现具有数十亿个参数，需要数十亿次算术运算。随着这些系统的广泛使用，确保其可靠运行成为设计要求。传统的错误检测机制引入了严重影响系统性能的电路或时间冗余。另一种选择是使用与系统并行操作的并发错误检测（CED）方案，并利用其特性来检测错误。CED对于大型ML系统是有吸引力的，因为它可以潜在地降低错误检测的成本。在本文中，我们介绍了并行分类器错误检测（CCED），这是一种在ML系统中使用并行ML分类器检测错误来实现CED的方案。CCED识别主ML系统中的一组检查信号，并将它们馈送到经过训练以检测错误的并发ML分类器。所提出的CCED方案已经在两个广泛使用的大规模ML模型上实现和评估：用于图像分类的对比语言图像预训练（CLIP）和用于自然语言应用的来自变换器的双向编码器表示（BERT）。结果表明，当使用比CLIP或BERT简单一个数量级的简单随机森林分类器时，检测到95%以上的错误。这些结果说明了CCED在大规模ML模型中实现错误检测的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01820v1" target="_blank">2306.01820v1</a>
                              </td>
                              <td>Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems</td>
                              <td>Pedro Reviriego</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01820v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01820v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01293v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01293v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01293v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01293v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel vision-language prompt learning approach for few-shot out-of-distribution (OOD) detection. Few-shot OOD detection aims to detect OOD images from classes that are unseen during training using only a few labeled in-distribution (ID) images. While prompt learning methods such as CoOp have shown effectiveness and efficiency in few-shot ID classification, they still face limitations in OOD detection due to the potential presence of ID-irrelevant information in text embeddings. To address this issue, we introduce a new approach called \textbf{Lo}cal regularized \textbf{Co}ntext \textbf{Op}timization (LoCoOp), which performs OOD regularization that utilizes the portions of CLIP local features as OOD features during training. CLIP's local features have a lot of ID-irrelevant nuisances (e.g., backgrounds), and by learning to push them away from the ID class text embeddings, we can remove the nuisances in the ID class text embeddings and enhance the separation between ID and OOD. Experiments on the large-scale ImageNet OOD detection benchmarks demonstrate the superiority of our LoCoOp over zero-shot, fully supervised detection methods and prompt learning methods. Notably, even in a one-shot setting -- just one label per class, LoCoOp outperforms existing zero-shot and fully supervised detection methods. The code will be available via \url{https://github.com/AtsuMiyai/LoCoOp}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01293v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新颖的视觉语言提示学习方法，用于少镜头分布（OOD）检测。少镜头OOD检测旨在仅使用少数标记分布（ID）图像从训练期间看不见的类中检测OOD图像。尽管CoOp等即时学习方法在少镜头ID分类中显示出了有效性和效率，但由于文本嵌入中可能存在与ID无关的信息，它们在OOD检测中仍然面临限制。为了解决这个问题，我们引入了一种新的方法，称为\textbf｛Lo｝cal正则化\textbf｛Co｝ntext\textbf{Op｝时序化（LoCoOp），它执行OOD正则化，在训练期间利用CLIP局部特征的部分作为OOD特征。CLIP的局部特征有很多与ID无关的干扰（例如背景），通过学习将它们从ID类文本嵌入中推开，我们可以消除ID类文本插入中的干扰，并增强ID和OOD之间的分离。在大规模ImageNet OOD检测基准上的实验证明了我们的LoCoOp相对于零样本、完全监督检测方法和快速学习方法的优越性。值得注意的是，即使在一次性设置中（每个类只有一个标签），LoCoOp也优于现有的零样本和完全监督检测方法。代码将通过\url提供{https://github.com/AtsuMiyai/LoCoOp}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01293v1" target="_blank">2306.01293v1</a>
                              </td>
                              <td>LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning</td>
                              <td>Atsuyuki Miyai</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01293v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01293v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_03565v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_03565v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_03565v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_03565v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Indoor scene synthesis involves automatically picking and placing furniture appropriately on a floor plan, so that the scene looks realistic and is functionally plausible. Such scenes can serve as homes for immersive 3D experiences, or be used to train embodied agents. Existing methods for this task rely on labeled categories of furniture, e.g. bed, chair or table, to generate contextually relevant combinations of furniture. Whether heuristic or learned, these methods ignore instance-level visual attributes of objects, and as a result may produce visually less coherent scenes. In this paper, we introduce an auto-regressive scene model which can output instance-level predictions, using general purpose image embedding based on CLIP. This allows us to learn visual correspondences such as matching color and style, and produce more functionally plausible and aesthetically pleasing scenes. Evaluated on the 3D-FRONT dataset, our model achieves SOTA results in scene synthesis and improves auto-completion metrics by over 50%. Moreover, our embedding-based approach enables zero-shot text-guided scene synthesis and editing, which easily generalizes to furniture not seen during training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_03565v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>室内场景合成包括自动在平面图上适当地挑选和放置家具，使场景看起来逼真，并且在功能上合理。这样的场景可以作为身临其境的3D体验的家，或者用于训练具体化的代理。该任务的现有方法依赖于家具的标记类别，例如床、椅子或桌子，以生成家具的上下文相关组合。无论是启发式的还是学习的，这些方法都忽略了对象的实例级视觉属性，因此可能会产生视觉上不太连贯的场景。在本文中，我们介绍了一种自回归场景模型，该模型可以使用基于CLIP的通用图像嵌入来输出实例级预测。这使我们能够学习视觉上的对应关系，如颜色和风格的匹配，并产生功能上更合理、更美观的场景。在3D-FRONT数据集上进行评估，我们的模型在场景合成中实现了SOTA结果，并将自动完成指标提高了50%以上。此外，我们基于嵌入式的方法可以实现零样本文本引导的场景合成和编辑，这很容易推广到训练中看不到的家具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.03565v2" target="_blank">2303.03565v2</a>
                              </td>
                              <td>CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding</td>
                              <td>Jingyu Liu</td>
                              <td>2023-03-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_03565v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.03565v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01111v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01111v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01111v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01111v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Interstitial lung diseases (ILD) present diagnostic challenges due to their varied manifestations and overlapping imaging features. To address this, we propose a machine learning approach that utilizes CLIP, a multimodal (image and text) self-supervised model, for ILD classification. We extensively integrate zero-shot CLIP throughout our workflow, starting from the initial extraction of image patches from volumetric CT scans and proceeding to ILD classification using "patch montages". Furthermore, we investigate how domain adaptive pretraining (DAPT) CLIP with task-specific images (CT "patch montages" extracted with ILD-specific prompts for CLIP) and/or text (lung-specific sections of radiology reports) affects downstream ILD classification performance. By leveraging CLIP-extracted "patch montages" and DAPT, we achieve strong zero-shot ILD classification results, including an AUROC of 0.893, without the need for any labeled training data. This work highlights the versatility and potential of multimodal models like CLIP for medical image classification tasks where labeled data is scarce.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01111v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>间质性肺病（ILD）由于其多样的表现和重叠的影像学特征，给诊断带来了挑战。为了解决这一问题，我们提出了一种机器学习方法，该方法利用CLIP（一种多模式（图像和文本）自监督模型）进行ILD分类。我们在整个工作流程中广泛集成了零样本CLIP，从体积CT扫描图像补丁的初始提取开始，然后使用“补丁蒙太奇”进行ILD分类。此外，我们研究了具有特定任务图像（用特定于ILD的CLIP提示提取的CT“补丁蒙太奇”）和/或文本（放射学报告的肺部特定部分）的领域自适应预训练（DAPT）CLIP如何影响下游ILD分类性能。通过利用CLIP提取的“补丁蒙太奇”和DAPT，我们实现了强大的零样本ILD分类结果，包括0.893的AUROC，而不需要任何标记的训练数据。这项工作强调了CLIP等多模式模型在标记数据稀缺的医学图像分类任务中的多功能性和潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01111v1" target="_blank">2306.01111v1</a>
                              </td>
                              <td>Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification</td>
                              <td>Cara Van Uden</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01111v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01111v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00984v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00984v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00984v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00984v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is configured with proper classifier-free guidance scale, training self-supervised methods on synthetic images can match or beat the real image counterpart; (2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. When we further add language supervision, StableRep trained with 20M synthetic images achieves better accuracy than CLIP trained with 50M real images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00984v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了使用由文本到图像模型生成的合成图像来学习视觉表示的潜力。鉴于这种模型在生成高质量图像方面的出色性能，这是一个自然的问题。我们特别考虑了稳定扩散，这是领先的开源文本到图像模型之一。我们证明：（1）当生成模型配置适当的无分类器引导尺度时，在合成图像上训练自监督方法可以匹配或击败真实图像；（2） 通过将同一文本提示生成的多个图像视为彼此的阳性图像，我们开发了一种多阳性对比学习方法，称为StableRep。在大规模数据集上，仅使用合成图像，StableRep学习的表示就超过了SimCLR和CLIP使用相同的文本提示集和相应的真实图像学习的表示的性能。当我们进一步添加语言监督时，使用20M合成图像训练的StableRep比使用50M真实图像训练的CLIP实现了更好的准确性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00984v1" target="_blank">2306.00984v1</a>
                              </td>
                              <td>StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</td>
                              <td>Yonglong Tian</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00984v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07437v5_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Continual Vision-Language Representation Learning with Off-Diagonal Information</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07437v5_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07437v5_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07437v5_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale multi-modal contrastive learning frameworks like CLIP typically require a large amount of image-text samples for training. However, these samples are always collected continuously in real scenarios. This paper discusses the feasibility of continual CLIP training using streaming data. Unlike continual learning based on self-supervised learning methods for pure images, which is empirically robust against catastrophic forgetting, CLIP's performance degeneration in the continual setting is significant and non-neglectable. By analyzing the changes in the model's representation space during continual CLIP training from a spatial geometry perspective, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we empirically and theoretically demonstrate how SD leads to a performance decline for CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a new continual vision-language representation learning framework Mod-X: Maintain off-diagonal information-matriX. By selectively aligning the off-diagonal information distribution of contrastive matrices, the Mod-X improves the capability of the multi-modal model by maintaining the multi-modal representation space alignment on the old data domain during continuously fitting the new training data domain. Experiments on commonly used datasets with different scales and scopes have demonstrated the effectiveness of our method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07437v5_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的大规模多模态对比学习框架通常需要大量的图像文本样本进行训练。然而，这些样本总是在真实场景中连续收集的。本文讨论了使用流数据进行连续CLIP训练的可行性。与基于纯图像的自监督学习方法的连续学习不同，CLIP在连续设置中的性能退化是显著的且不可忽视的。通过从空间几何的角度分析连续CLIP训练过程中模型表示空间的变化，我们将这些空间变化总结为空间无序（SD），可分为模态内旋转和模态间偏离。此外，我们从经验和理论上证明了SD如何导致CLIP在跨模态检索任务中的性能下降。为了缓解SD，我们提出了一个新的连续视觉语言表示学习框架Mod-X：保持非对角信息矩阵X。通过选择性地对齐对比矩阵的非对角信息分布，Mod-X通过在连续拟合新的训练数据域期间保持旧数据域上的多模态表示空间对齐，提高了多模态模型的能力。在不同规模和范围的常用数据集上进行的实验证明了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07437v5" target="_blank">2305.07437v5</a>
                              </td>
                              <td>Continual Vision-Language Representation Learning with Off-Diagonal Information</td>
                              <td>Zixuan Ni</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07437v5_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07437v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_19595v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_19595v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_19595v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_19595v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision and Language (VL) models offer an effective method for aligning representation spaces of images and text, leading to numerous applications such as cross-modal retrieval, visual question answering, captioning, and more. However, the aligned image-text spaces learned by all the popular VL models are still suffering from the so-called `object bias' - their representations behave as `bags of nouns', mostly ignoring or downsizing the attributes, relations, and states of objects described/appearing in texts/images. Although some great attempts at fixing these `compositional reasoning' issues were proposed in the recent literature, the problem is still far from being solved. In this paper, we uncover two factors limiting the VL models' compositional reasoning performance. These two factors are properties of the paired VL dataset used for finetuning and pre-training the VL model: (i) the caption quality, or in other words `image-alignment', of the texts; and (ii) the `density' of the captions in the sense of mentioning all the details appearing on the image. We propose a fine-tuning approach for automatically treating these factors leveraging a standard VL dataset (CC3M). Applied to CLIP, we demonstrate its significant compositional reasoning performance increase of up to $\sim27\%$ over the base model, up to $\sim20\%$ over the strongest baseline, and by $6.7\%$ on average.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_19595v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉和语言（VL）模型为对齐图像和文本的表示空间提供了一种有效的方法，从而带来了许多应用，如跨模态检索、视觉问答、字幕等。然而，所有流行的VL模型学习到的对齐图像-文本空间仍然存在所谓的“对象偏见”——它们的表示表现为“名词袋”，大多忽略或缩小了文本/图像中描述/出现的对象的属性、关系和状态。尽管最近的文献中提出了一些解决这些“组合推理”问题的伟大尝试，但这个问题仍然远未解决。在本文中，我们揭示了限制VL模型组合推理性能的两个因素。这两个因素是用于微调和预训练VL模型的配对VL数据集的特性：（i）文本的字幕质量，或者换句话说，“图像对齐”；以及（ii）在提及图像上出现的所有细节的意义上的字幕的“密度”。我们提出了一种利用标准VL数据集（CC3M）自动处理这些因素的微调方法。将其应用于CLIP，我们证明了其显著的组合推理性能比基本模型提高了$\sim27\%$，比最强基线提高了$\sim20\%$，平均提高了$6.7\%$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.19595v2" target="_blank">2305.19595v2</a>
                              </td>
                              <td>Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models</td>
                              <td>Sivan Doveh</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_19595v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.19595v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00813v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00813v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00813v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00813v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in vision-language pre-training have enabled machines to perform better in multimodal object discrimination (e.g., image-text semantic alignment) and image synthesis (e.g., text-to-image generation). On the other hand, fine-tuning pre-trained models with discriminative or generative capabilities such as CLIP and Stable Diffusion on domain-specific datasets has shown to be effective in various tasks by adapting to specific domains. However, few studies have explored the possibility of learning both discriminative and generative capabilities and leveraging their synergistic effects to create a powerful and personalized multimodal model during fine-tuning. This paper presents UniDiff, a unified multi-modal model that integrates image-text contrastive learning (ITC), text-conditioned image synthesis learning (IS), and reciprocal semantic consistency modeling (RSC). UniDiff effectively learns aligned semantics and mitigates the issue of semantic collapse during fine-tuning on small datasets by leveraging RSC on visual features from CLIP and diffusion models, without altering the pre-trained model's basic architecture. UniDiff demonstrates versatility in both multi-modal understanding and generative tasks. Experimental results on three datasets (Fashion-man, Fashion-woman, and E-commercial Product) showcase substantial enhancements in vision-language retrieval and text-to-image generation, illustrating the advantages of combining discriminative and generative fine-tuning. The proposed UniDiff model establishes a robust pipeline for personalized modeling and serves as a benchmark for future comparisons in the field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00813v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练的最新进展使机器能够在多模式对象识别（例如，图像-文本语义对齐）和图像合成（例如，文本到图像生成）方面表现得更好。另一方面，通过适应特定领域，在特定领域数据集上微调具有判别或生成能力的预训练模型，如CLIP和稳定扩散，已被证明在各种任务中是有效的。然而，很少有研究探索在微调过程中学习辨别能力和生成能力，并利用它们的协同效应创建强大且个性化的多模式模型的可能性。本文提出了统一的多模态模型UniDiff，它集成了图像-文本对比学习（ITC）、文本条件图像合成学习（IS）和交互语义一致性建模（RSC）。UniDiff通过利用CLIP和扩散模型的视觉特征上的RSC，在不改变预先训练的模型的基本架构的情况下，有效地学习对齐的语义，并缓解了在小数据集上微调期间的语义崩溃问题。UniDiff在多模态理解和生成任务方面都表现出了多功能性。在三个数据集（时尚男、时尚女和电子商务产品）上的实验结果显示，视觉语言检索和文本到图像生成方面有了显著的增强，说明了区分和生成微调相结合的优势。所提出的UniDiff模型为个性化建模建立了一个稳健的管道，并作为该领域未来比较的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00813v1" target="_blank">2306.00813v1</a>
                              </td>
                              <td>UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning</td>
                              <td>Xiao Dong</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00813v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00813v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_07773v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Temporal Logic Motion Planning with Convex Optimization via Graphs of Convex Sets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_07773v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_07773v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_07773v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Temporal logic is a concise way of specifying complex tasks. But motion planning to achieve temporal logic specifications is difficult, and existing methods struggle to scale to complex specifications and high-dimensional system dynamics. In this paper, we cast Linear Temporal Logic (LTL) motion planning as a shortest path problem in a Graph of Convex Sets (GCS) and solve it with convex optimization. This approach brings together the best of modern optimization-based temporal logic planners and older automata-theoretic methods, addressing the limitations of each: we avoid clipping and passthrough by representing paths with continuous Bezier curves; computational complexity is polynomial (not exponential) in the number of sample points; global optimality can be certified (though it is not guaranteed); soundness and probabilistic completeness are guaranteed under mild assumptions; and most importantly, the method scales to complex specifications and high-dimensional systems, including a 30-DoF humanoid. Open-source code is available at https://github.com/vincekurtz/ltl_gcs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_07773v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>时态逻辑是指定复杂任务的一种简明方法。但是，实现时间逻辑规范的运动规划是困难的，并且现有的方法难以扩展到复杂的规范和高维系统动力学。本文将线性时序逻辑（LTL）运动规划问题转化为凸集图（GCS）中的最短路径问题，并用凸优化方法进行求解。这种方法融合了现代基于优化的时间逻辑规划器和旧的自动机理论方法的优点，解决了每种方法的局限性：我们通过用连续的贝塞尔曲线表示路径来避免剪裁和通过；计算复杂度是采样点数量的多项式（而不是指数）；可以证明全局最优性（尽管不能保证）；稳健性和概率完整性在温和的假设下得到保证；最重要的是，该方法可扩展到复杂的规格和高维系统，包括一个30 DoF的人形机器人。开放源代码可在https://github.com/vincekurtz/ltl_gcs.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.07773v2" target="_blank">2301.07773v2</a>
                              </td>
                              <td>Temporal Logic Motion Planning with Convex Optimization via Graphs of Convex Sets</td>
                              <td>Vince Kurtz</td>
                              <td>2023-01-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_07773v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.07773v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00450v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Open-Vocabulary Semantic Segmentation without Human Labels</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00450v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00450v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00450v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic segmentation is a crucial task in computer vision that involves segmenting images into semantically meaningful regions at the pixel level. However, existing approaches often rely on expensive human annotations as supervision for model training, limiting their scalability to large, unlabeled datasets. To address this challenge, we present ZeroSeg, a novel method that leverages the existing pretrained vision-language (VL) model (e.g. CLIP) to train open-vocabulary zero-shot semantic segmentation models. Although acquired extensive knowledge of visual concepts, it is non-trivial to exploit knowledge from these VL models to the task of semantic segmentation, as they are usually trained at an image level. ZeroSeg overcomes this by distilling the visual concepts learned by VL models into a set of segment tokens, each summarizing a localized region of the target image. We evaluate ZeroSeg on multiple popular segmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO, in a zero-shot manner (i.e., no training or adaption on target segmentation datasets). Our approach achieves state-of-the-art performance when compared to other zero-shot segmentation methods under the same training data, while also performing competitively compared to strongly supervised methods. Finally, we also demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation, through both human studies and qualitative visualizations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00450v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割是计算机视觉中的一项关键任务，它涉及在像素级将图像分割成具有语义意义的区域。然而，现有的方法通常依赖于昂贵的人工注释作为模型训练的监督，这限制了它们对大型未标记数据集的可扩展性。为了应对这一挑战，我们提出了ZeroSeg，这是一种利用现有预处理视觉语言（VL）模型（例如CLIP）来训练开放词汇表零样本语义分割模型的新方法。尽管获得了视觉概念的广泛知识，但利用这些VL模型的知识进行语义分割并不是一件小事，因为它们通常是在图像级别进行训练的。ZeroSeg通过将VL模型学习到的视觉概念提取为一组片段标记来克服这一点，每个片段标记概括目标图像的局部区域。我们以零样本的方式（即不训练或适应目标分割数据集）在多个流行的分割基准上评估ZeroSeg，包括PASCAL VOC 2012、PASCAL Context和COCO。与相同训练数据下的其他零样本分割方法相比，我们的方法实现了最先进的性能，同时与强监督方法相比，性能也具有竞争力。最后，我们还通过人类研究和定性可视化证明了ZeroSeg在开放式词汇分割方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00450v1" target="_blank">2306.00450v1</a>
                              </td>
                              <td>Exploring Open-Vocabulary Semantic Segmentation without Human Labels</td>
                              <td>Jun Chen</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00450v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00450v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00393v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00393v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00393v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00393v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capable of generating stable and accurate soft labels to replace the output of the teacher model. This method circumvents the problem of knowledge misleading caused by inaccurate predictions of the teacher model and avoids the computational overhead of loading the teacher model for knowledge distillation. Extensive experiments demonstrate the advantages of our method, yielding significant performance improvements while utilizing only half the resolution of video clips in the incremental phases as input compared to recent state-of-the-art methods. Moreover, our method surpasses the performance of joint training when employing four times the number of samples in episodic memory.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00393v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基于视频的社交媒体越来越受欢迎，新类别的视频不断产生，迫切需要强大的增量学习技术来理解视频。这项任务中最大的挑战之一是灾难性遗忘，即网络在学习新类别时往往会忘记以前学习过的数据。为了克服这个问题，知识提取是一种广泛使用的基于排练的视频增量学习技术，包括在不同类别之间传递关于相似性的重要信息，以增强学生模型。因此，最好有一个强大的教师模式来指导学生。然而，网络本身的有限性能和灾难性遗忘的发生会导致教师网络对一些记忆样本做出不准确的预测，最终限制学生网络的性能。基于这些观察，我们提出了一种教师代理，能够生成稳定准确的软标签来代替教师模型的输出。该方法避免了教师模型预测不准确导致的知识误导问题，避免了加载教师模型进行知识提炼的计算开销。大量的实验证明了我们的方法的优势，与最近最先进的方法相比，在增量阶段仅使用视频片段一半的分辨率作为输入的同时，产生了显著的性能改进。此外，当在情景记忆中使用四倍数量的样本时，我们的方法超过了联合训练的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00393v1" target="_blank">2306.00393v1</a>
                              </td>
                              <td>Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning</td>
                              <td>Shengqin Jiang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00393v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00393v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16223v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16223v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16223v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16223v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image (T2I) research has grown explosively in the past year, owing to the large-scale pre-trained diffusion models and many emerging personalization and editing approaches. Yet, one pain point persists: the text prompt engineering, and searching high-quality text prompts for customized results is more art than science. Moreover, as commonly argued: "an image is worth a thousand words" - the attempt to describe a desired image with texts often ends up being ambiguous and cannot comprehensively cover delicate visual details, hence necessitating more additional controls from the visual domain. In this paper, we take a bold step forward: taking "Text" out of a pre-trained T2I diffusion model, to reduce the burdensome prompt engineering efforts for users. Our proposed framework, Prompt-Free Diffusion, relies on only visual inputs to generate new images: it takes a reference image as "context", an optional image structural conditioning, and an initial noise, with absolutely no text prompt. The core architecture behind the scene is Semantic Context Encoder (SeeCoder), substituting the commonly used CLIP-based or LLM-based text encoder. The reusability of SeeCoder also makes it a convenient drop-in component: one can also pre-train a SeeCoder in one T2I model and reuse it for another. Through extensive experiments, Prompt-Free Diffusion is experimentally found to (i) outperform prior exemplar-based image synthesis approaches; (ii) perform on par with state-of-the-art T2I models using prompts following the best practice; and (iii) be naturally extensible to other downstream applications such as anime figure generation and virtual try-on, with promising quality. Our code and models are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16223v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于大规模的预先训练的扩散模型和许多新兴的个性化和编辑方法，文本到图像（T2I）研究在过去一年中呈爆炸式增长。然而，有一个痛点仍然存在：文本提示工程，搜索高质量的文本提示以获得定制结果，与其说是科学，不如说是艺术。此外，正如人们普遍认为的那样：“一幅图像胜过千言万语”——试图用文本描述所需的图像往往会变得模棱两可，无法全面涵盖微妙的视觉细节，因此需要从视觉领域进行更多的额外控制。在本文中，我们向前迈出了大胆的一步：将“文本”从预先训练的T2I扩散模型中删除，以减少用户繁重的即时工程工作。我们提出的框架Prompt Free Diffusion仅依靠视觉输入来生成新图像：它将参考图像作为“上下文”，一个可选的图像结构条件和一个初始噪声，而绝对没有文本提示。该场景背后的核心架构是语义上下文编码器（SeeCoder），取代了常用的基于CLIP或基于LLM的文本编码器。SeeCoder的可重用性也使其成为一个方便的插入组件：您还可以在一个T2I模型中预训练SeeCoder，并将其重新用于另一个模型。通过大量实验，实验发现Prompt Free Diffusion（i）优于现有的基于样本的图像合成方法；（ii）使用遵循最佳实践的提示，与最先进的T2I模型不相上下；以及（iii）可以自然地扩展到其他下游应用程序，如动漫人物生成和虚拟试穿，具有良好的质量。我们的代码和模型开源于https://github.com/SHI-Labs/Prompt-Free-Diffusion.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16223v2" target="_blank">2305.16223v2</a>
                              </td>
                              <td>Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</td>
                              <td>Xingqian Xu</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16223v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00228v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00228v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00228v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00228v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP and BLIP visual QA model gradients. Our experiments demonstrate that the performance of BLIP model variants can be significantly improved through human cropping, and automatic cropping methods can produce comparable benefits. A deeper dive into our findings indicates that the performance enhancement is more pronounced in zero-shot models than in fine-tuned models and more salient with smaller bounding boxes than larger ones. We perform case studies to connect quantitative differences with qualitative observations across question types and datasets. Finally, we see that the cropping enhancement is robust, as we gain an improvement of 4.59% (absolute) in the general VQA-random task by simply inputting a concatenation of the original and gradient-based cropped images. We make our code available to facilitate further innovation on visual cropping methods for question answering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00228v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉问答是一项具有挑战性的任务，因为它需要感知、语言和背景知识系统之间的无缝交互。虽然像BLIP这样的视觉和自然语言模型的最新进展提高了这项任务的性能，但我们对这些模型处理不同类型的问题和推理类型的能力缺乏了解。由于我们对BLIP家族模型的初步分析揭示了回答精细细节问题的困难，我们研究了以下问题：视觉裁剪可以用来提高最先进的视觉问答模型在精细细节问题上的性能吗？鉴于BLIP系列模型最近取得的成功，我们研究了零样本和微调BLIP模型。我们定义了流行的VQA-v2基准的三个受控子集，以衡量裁剪是否有助于建模性能。除了人工裁剪之外，我们还设计了两种基于CLIP和BLIP视觉QA模型梯度的多模式嵌入的自动裁剪策略。我们的实验表明，通过人工种植可以显著提高BLIP模型变体的性能，自动种植方法可以产生类似的效益。深入研究我们的发现表明，在零样本模型中，性能增强比在微调模型中更为显著，边界框越小，性能增强越显著。我们进行案例研究，将问题类型和数据集之间的定量差异与定性观察联系起来。最后，我们看到裁剪增强是稳健的，因为我们通过简单地输入原始和基于梯度的裁剪图像的级联，在一般的VQA随机任务中获得了4.59%（绝对）的改进。我们提供了我们的代码，以促进对问题回答的视觉裁剪方法的进一步创新。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00228v1" target="_blank">2306.00228v1</a>
                              </td>
                              <td>Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models</td>
                              <td>Jiarui Zhang</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00228v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00228v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00204v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Toward Understanding Why Adam Converges Faster Than SGD for Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00204v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00204v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00204v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding up the convergence of optimization algorithms under various settings. We show that coordinate-wise clipping improves the local loss reduction when only a small fraction of the coordinates has bad sharpness. We conclude that the sharpness reduction effect of adaptive coordinate-wise scaling is the reason for Adam's success in practice and suggest the use of coordinate-wise clipping as a universal technique to speed up deep learning optimization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00204v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然随机梯度下降（SGD）仍然是深度学习中最流行的优化算法，但Adam等自适应算法在一些深度学习应用（如训练转换器）中已经建立了优于SGD的经验优势。然而，在这些情况下，为什么Adam的收敛速度明显快于SGD，这仍然是一个问题。在本文中，我们使用一个新的概念——方向锐度——来解释为什么Adam比SGD收敛得更快。我们认为优化算法的性能与更新步骤的方向清晰度密切相关，并表明与自适应算法相比，SGD的方向清晰度要差得多。我们进一步观察到，只有一小部分坐标会导致SGD的清晰度差和收敛慢，并建议使用坐标方向剪裁作为SGD和其他优化算法的解决方案。我们展示了在各种设置下，按坐标剪裁对锐度降低和加速优化算法收敛的影响。我们表明，当只有一小部分坐标具有较差的锐度时，按坐标进行剪裁可以提高局部损耗的降低。我们得出结论，自适应坐标缩放的锐度降低效应是Adam在实践中取得成功的原因，并建议使用坐标裁剪作为一种通用技术来加快深度学习优化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00204v1" target="_blank">2306.00204v1</a>
                              </td>
                              <td>Toward Understanding Why Adam Converges Faster Than SGD for Transformers</td>
                              <td>Yan Pan</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00204v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00204v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2306_03881v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别比DINO和OpenCLIP高出19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v1" target="_blank">2306.03881v1</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07598v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07598v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses Hungarian matching to filter redundant noised queries and $\textit{query alignment}$ to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07598v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着检测变压器（DETR）的变体DINO的发布，检测变压器凭借其端到端设计和可扩展性的优点打破了对象检测基准的记录。然而，DETR向面向对象检测的扩展尚未得到彻底研究，尽管预计其端到端架构会带来更多好处，例如消除NMS和锚相关成本。在本文中，我们提出了第一个基于强DINO的面向对象检测基线。我们发现，直接使用DETR进行定向对象检测并不能保证无重复预测，并提出了一个简单的成本来缓解这一问题。此外，我们引入了$\textit｛动态去噪｝$策略，该策略使用匈牙利匹配来过滤冗余噪声查询，并使用$\textit{查询对齐｝$来保持Transformer解码器层之间的匹配一致性。我们提出的模型优于之前的旋转DETR和其他同行，在DOTA-1.0/v.5/v.20和DIOR-R基准测试中实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07598v3" target="_blank">2305.07598v3</a>
                              </td>
                              <td>RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</td>
                              <td>Hakjin Lee</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07598v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07598v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_09959v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Context Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_09959v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_09959v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了全局上下文视觉转换器（GC-ViT），这是一种提高计算机视觉参数和计算利用率的新架构。我们的方法利用全局上下文自注意模块，结合标准的局部自注意，有效和高效地对长距离和短距离空间交互进行建模，而不需要计算注意力掩码或移动局部窗口等昂贵的操作。此外，我们解决了ViTs中缺乏电感偏置的问题，并建议在我们的架构中利用改进的融合反向残差块。我们提出的GC-ViT在图像分类、对象检测和语义分割任务中取得了最先进的结果。在用于分类的ImageNet-1K数据集上，具有51M、90M和201M参数的GC ViT变体在224图像分辨率和没有任何预训练的情况下分别达到84.3%、85.0%和85.7%的Top-1准确率，因此大大超过了类似规模的现有技术，如基于CNN的ConvNeXt和基于ViT的MaxViT和Swin Transformer。使用MS COCO和ADE20K数据集在对象检测、实例分割和语义分割的下游任务中预先训练的GC-ViT骨干始终优于先前的工作。具体而言，具有4级DINO检测头的GC ViT在MS COCO数据集上实现了58.3的框AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.09959v5" target="_blank">2206.09959v5</a>
                              </td>
                              <td>Global Context Vision Transformers</td>
                              <td>Ali Hatamizadeh</td>
                              <td>2022-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_09959v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.09959v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01398v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01398v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite impressive empirical advances of SSL in solving various tasks, the problem of understanding and characterizing SSL representations learned from input data remains relatively under-explored. We provide a comparative analysis of how the representations produced by SSL models differ when masking parts of the input. Specifically, we considered state-of-the-art SSL pretrained models, such as DINOv2, MAE, and SwaV, and analyzed changes at the representation levels across 4 Image Classification datasets. First, we generate variations of the datasets by applying foreground and background segmentation. Then, we conduct statistical analysis using Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA) to evaluate the robustness of the representations learned in SSL models. Empirically, we show that not all models lead to representations that separate foreground, background, and complete images. Furthermore, we test different masking strategies by occluding the center regions of the images to address cases where foreground and background are difficult. For example, the DTD dataset that focuses on texture rather specific objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01398v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管SSL在解决各种任务方面取得了令人印象深刻的经验进步，但理解和表征从输入数据中学习的SSL表示的问题仍然相对未得到充分探索。我们提供了SSL模型在屏蔽部分输入时产生的表示如何不同的比较分析。具体而言，我们考虑了最先进的SSL预训练模型，如DINOv2、MAE和SwaV，并分析了4个图像分类数据集在表示级别上的变化。首先，我们通过应用前景和背景分割来生成数据集的变化。然后，我们使用标准相关分析（CCA）和中心核对齐（CKA）进行统计分析，以评估SSL模型中学习的表示的稳健性。根据经验，我们表明，并非所有模型都能产生分离前景、背景和完整图像的表示。此外，我们通过遮挡图像的中心区域来测试不同的掩蔽策略，以解决前景和背景困难的情况。例如，专注于纹理而非特定对象的DTD数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01398v1" target="_blank">2306.01398v1</a>
                              </td>
                              <td>Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</td>
                              <td>Xavier F. Cadet</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01398v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_00449v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_00449v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. Further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. The code is available at https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_00449v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着深度神经网络方法的日益普及，外科计算机视觉领域取得了长足的突破。然而，训练此类模型的标准完全监督方法需要大量的注释数据，成本高得令人望而却步；尤其是在临床领域。自监督学习（SSL）方法已经开始在普通计算机视觉社区中获得吸引力，它代表了这些注释成本的潜在解决方案，允许仅从未标记的数据中学习有用的表示。尽管如此，SSL方法在更复杂和更有影响力的领域（如医学和外科）的有效性仍然有限，尚未探索。在这项工作中，我们通过在外科计算机视觉的背景下研究四种最先进的SSL方法（MoCov2、SimCLR、DINO、SwAV）来解决这一关键需求。我们在Cholec80数据集上对这些方法在外科上下文理解、相位识别和工具存在检测这两个基本且流行的任务中的性能进行了广泛的分析。我们检查了它们的参数化，然后检查了它们在半监督设置中相对于训练数据量的行为。正如这项工作中所描述和进行的那样，将这些方法正确地转移到手术中，与SSL的一般用途相比，可以获得实质性的性能提升——在相位识别方面高达7.4%，在工具存在检测方面高达20%——以及最先进的半监督相位识别方法高达14%。在高度多样化的外科手术数据集选择上获得的进一步结果显示出强大的泛化特性。代码位于https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.00449v3" target="_blank">2207.00449v3</a>
                              </td>
                              <td>Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</td>
                              <td>Sanat Ramesh</td>
                              <td>2022-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_00449v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.00449v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_07044v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_07044v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_07044v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自我监督的预训练有可能在没有人为注释的情况下生成表达表征。地球观测（EO）中的大多数预训练都是基于ImageNet或中型标记遥感（RS）数据集。我们共享一个未标记的RS数据集SSL4EO-S12（地球观测的自我监督学习-哨兵-1/2），以收集来自欧空局哨兵-1/2卫星任务的大规模、全球、多模式和多季节卫星图像语料库。对于EO应用，我们展示了SSL4EO-S12在一组方法的自监督预训练中的成功：MoCo-v2、DINO、MAE和data2vec。由此产生的模型产生的下游性能接近或超过监督学习的准确性度量。此外，与现有数据集相比，SSL4EO-S12上的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.07044v2" target="_blank">2211.07044v2</a>
                              </td>
                              <td>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</td>
                              <td>Yi Wang</td>
                              <td>2022-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_07044v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.07044v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11922v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11922v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the ``objectness'' of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase performance with a higher memory requirement. The dataset split and code are available at https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11922v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象实例分割是室内机器人在有许多小对象的杂乱环境中导航的一个关键挑战。3D传感能力的局限性往往使检测每一个可能的物体变得困难。虽然深度学习方法可能对这个问题有效，但手动注释3D数据以进行监督学习是耗时的。在这项工作中，我们探索了RGB-D数据中的零样本实例分割（ZSIS），以语义类别认知的方式识别看不见的对象。我们为桌面对象数据集（TOD-Z）引入了一种零样本分割，以实现这项研究，并提出了一种方法，该方法使用带注释的对象来学习像素的“对象性”，并推广到杂乱室内环境中看不到的对象类别。我们的方法SupeRGB-D基于几何线索将像素分组为小块，并学习以深度聚集聚类的方式合并小块。SupeRGB-D在看不见的对象上优于现有的基线，同时在看到的对象上实现了类似的性能。我们在真实数据集OCID上进一步展示了有竞争力的结果。凭借其轻量级设计（需要0.4 MB内存），我们的方法非常适合移动和机器人应用。额外的DINO功能可以通过更高的内存要求来提高性能。数据集拆分和代码可在https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11922v2" target="_blank">2212.11922v2</a>
                              </td>
                              <td>SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11922v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11922v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上执行与SOTA表示类似的表现。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果出奇地好，使用这些融合特征的最近邻进行零样本评估，与基准数据集上的最新方法相比，性能显著提高，例如SPair-71k、PF-Pascal和TSS。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v1" target="_blank">2305.15347v1</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-vocabulary Segmentation with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练鲁棒和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它严重损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过利用预先训练的基础模型CLIP和DINO的开放词汇多模态知识和对象推理能力，在不需要任何微调的情况下，解决了3D开放词汇分割中的挑战。具体而言，我们将CLIP中的开放词汇视觉和文本知识提取到神经辐射场（NeRF）中，该场有效地将2D特征提升到视图一致的3D分割中。此外，我们引入了相关性分布对齐损失和特征分布对齐损失，以分别减轻CLIP特征的模糊性，并从DINO特征中提取精确的对象边界，从而消除了训练过程中对分割注释的需要。大量实验表明，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v2" target="_blank">2305.14093v2</a>
                              </td>
                              <td>3D Open-vocabulary Segmentation with Foundation Models</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12223v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Makes for Good Visual Tokenizers for Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12223v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12223v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们实证研究了适当的预训练方法来构建良好的视觉标记器，使大型语言模型（LLM）成为强大的多模式大型语言模型。在我们旨在评估MLLMs视觉语义理解和细粒度感知能力的基准中，我们讨论了用主导方法（即DeiT、CLIP、MAE、DINO）预训练的不同视觉标记器，并观察到：i）完全/弱监督模型比自监督模型捕获更多的语义，但通过扩大预训练数据集缩小了差距。ii）自监督模型更擅长细粒度感知，其中补丁级别的监督尤其有效。iii）调整可视化标记器会导致从大规模预训练中获得的语义丢失，这对相对小规模的指令调整数据集是不利的。鉴于这些发现，我们回顾了试图统一语义和细粒度视觉理解的方法，例如，具有语义丰富目标的补丁级特征提取。我们获得了一个有趣的基于面具的洞察策略，这些策略曾经风靡一时，但可能不适用于获得良好的视觉标记器。基于这一关键观察，我们获得了一种新的MLLM，该MLLM配备了定制的良好视觉标记器（GVT），在多个尺度上表现出强大的视觉理解能力。特别是，在不引入额外参数和特定任务微调的情况下，GVT在视觉问答、图像字幕和其他细粒度视觉理解任务（如对象计数和多类识别）上实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12223v2" target="_blank">2305.12223v2</a>
                              </td>
                              <td>What Makes for Good Visual Tokenizers for Large Language Models?</td>
                              <td>Guangzhi Wang</td>
                              <td>2023-05-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12223v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间良好建立的联系的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。说明了它们在各种密度估计和条件密度估计任务中的效用。软件可在https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v1" target="_blank">2305.13552v1</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13291v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Materialistic: Selecting Similar Materials in Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13291v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO features coupled with a proposed Cross-Similarity module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13291v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将图像分离为有意义的底层组件是编辑和理解图像的关键第一步。我们提出了一种方法，能够选择照片中展示与艺术家选择区域相同材料的区域。我们提出的方法对明暗处理、镜面高光和投射阴影都很稳健，可以在真实图像中进行选择。由于我们不依赖于语义分割（不同的木材或金属不应该一起选择），我们将该问题表述为基于用户提供的图像位置的基于相似性的分组问题。特别地，我们建议利用无监督的DINO特征，结合所提出的交叉相似性模块和MLP头来提取图像中的材料相似性。我们在发布的一个新的合成图像数据集上训练我们的模型。我们证明了我们的方法可以很好地推广到真实世界的图像。我们仔细分析了模型在不同材料特性和照明条件下的行为。此外，我们根据50张真实照片的手工注释基准对其进行了评估。我们在一系列应用中进一步展示了我们的模型，包括素材编辑、视频选择和检索具有类似素材的对象照片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13291v1" target="_blank">2305.13291v1</a>
                              </td>
                              <td>Materialistic: Selecting Similar Materials in Images</td>
                              <td>Prafull Sharma</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13291v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13291v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如，CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务上显示出了令人印象深刻的学习和转移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先证明，虽然基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。为此，我们在CLIP模型上提出了一种非常简单的目标数据提取方法，并在所有UniDA基准测试中实现了对基线的一致改进。我们的研究是在一种新提出的通用分类率（UCR）评估指标下进行的，该指标不含阈值和比率，解决了使用现有H-核心指标时遇到的阈值敏感问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v1" target="_blank">2305.11092v1</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08014v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08014v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most invariance-based self-supervised methods rely on single object-centric images (e.g., ImageNet images) for pretraining, learning features that invariant to geometric transformation. However, when images are not object-centric, the semantics of the image can be significantly altered due to cropping. Furthermore, as the model becomes insensitive to geometric transformations, it may struggle to capture location information. For this reason, we propose a Geometric Transformation Sensitive Architecture designed to be sensitive to geometric transformations, specifically focusing on four-fold rotation, random crop, and multi-crop. Our method encourages the student to be sensitive by predicting rotation and using targets that vary with those transformations through pooling and rotating the teacher feature map. Additionally, we use patch correspondence loss to encourage correspondence between patches with similar features. This approach allows us to capture long-term dependencies in a more appropriate way than capturing long-term dependencies by encouraging local-to-global correspondence, which occurs when learning to be insensitive to multi-crop. Our approach demonstrates improved performance when using non-object-centric images as pretraining data compared to other methods that train the model to be insensitive to geometric transformation. We surpass DINO[Caron et al.[2021b]] baseline in tasks including image classification, semantic segmentation, detection, and instance segmentation with improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$. Code and pretrained models are publicly available at: https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08014v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于不变性的自监督方法依赖于以单个对象为中心的图像（例如，ImageNet图像）进行预训练，学习对几何变换不变的特征。然而，当图像不是以对象为中心时，图像的语义可能会因裁剪而发生显著变化。此外，随着模型对几何变换变得不敏感，它可能很难捕捉位置信息。因此，我们提出了一种几何变换敏感架构，该架构设计为对几何变换敏感，特别关注四重旋转、随机裁剪和多重裁剪。我们的方法通过预测旋转，并通过汇集和旋转教师特征图，使用随这些转换而变化的目标，鼓励学生保持敏感。此外，我们使用补丁对应损失来鼓励具有相似特征的补丁之间的对应。这种方法使我们能够以一种比通过鼓励局部到全局的对应关系捕获长期依赖关系更合适的方式捕获长期依赖性，这种对应关系发生在学习对多作物不敏感时。与其他将模型训练为对几何变换不敏感的方法相比，我们的方法在使用非以对象为中心的图像作为预训练数据时提高了性能。我们在包括图像分类、语义分割、检测和实例分割在内的任务中超过了DINO[Caron等人[2021b]]基线，改进了4.9$Top-1Acc$、3.3$mIoU$、3.4$AP^b$和2.7$AP^m$。代码和预训练模型可在以下网站上公开获取：https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08014v7" target="_blank">2304.08014v7</a>
                              </td>
                              <td>Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</td>
                              <td>Taeho Kim</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08014v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08014v7" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06558v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Track Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06558v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This report presents a framework called Segment And Track Anything (SAMTrack) that allows users to precisely and effectively segment and track any object in a video. Additionally, SAM-Track employs multimodal interaction methods that enable users to select multiple objects in videos for tracking, corresponding to their specific requirements. These interaction methods comprise click, stroke, and text, each possessing unique benefits and capable of being employed in combination. As a result, SAM-Track can be used across an array of fields, ranging from drone technology, autonomous driving, medical imaging, augmented reality, to biological analysis. SAM-Track amalgamates Segment Anything Model (SAM), an interactive key-frame segmentation model, with our proposed AOT-based tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022 challenge, to facilitate object tracking in video. In addition, SAM-Track incorporates Grounding-DINO, which enables the framework to support text-based interaction. We have demonstrated the remarkable capabilities of SAM-Track on DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in diverse applications. The project page is available at: https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06558v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本报告提供了一个名为“分段和跟踪任何内容”（SAMTrack）的框架，该框架允许用户精确有效地分段和跟踪视频中的任何对象。此外，SAM Track采用多模式交互方法，使用户能够根据自己的特定要求选择视频中的多个对象进行跟踪。这些交互方法包括点击、笔划和文本，每种方法都具有独特的优点，并且能够组合使用。因此，SAM Track可用于一系列领域，从无人机技术、自动驾驶、医学成像、增强现实到生物分析。SAM Track将交互式关键帧分割模型Segment Anything Model（SAM）与我们提出的基于AOT的跟踪模型（DeAOT）合并，以促进视频中的对象跟踪，该模型在VOT 2022挑战的四个轨道中排名第一。此外，SAM Track结合了Grounding DINO，使框架能够支持基于文本的交互。我们已经在DAVIS-2016 Val（92.0%）和DAVIS-2017 Test（79.2%）上展示了SAM Track的卓越能力及其在各种应用中的实用性。项目页面位于：https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06558v1" target="_blank">2305.06558v1</a>
                              </td>
                              <td>Segment and Track Anything</td>
                              <td>Yangming Cheng</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06558v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06558v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06553v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06553v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce WeLayout, a novel system for segmenting the layout of corporate documents, which stands for WeChat Layout Analysis System. Our approach utilizes a sophisticated ensemble of DINO and YOLO models, specifically developed for the ICDAR 2023 Competition on Robust Layout Segmentation. Our method significantly surpasses the baseline, securing a top position on the leaderboard with a mAP of 70.0. To achieve this performance, we concentrated on enhancing various aspects of the task, such as dataset augmentation, model architecture, bounding box refinement, and model ensemble techniques. Additionally, we trained the data separately for each document category to ensure a higher mean submission score. We also developed an algorithm for cell matching to further improve our performance. To identify the optimal weights and IoU thresholds for our model ensemble, we employed a Bayesian optimization algorithm called the Tree-Structured Parzen Estimator. Our approach effectively demonstrates the benefits of combining query-based and anchor-free models for achieving robust layout segmentation in corporate documents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06553v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了WeLayout，一种新的企业文档布局分割系统，即微信布局分析系统。我们的方法利用了一套复杂的DINO和YOLO模型，专门为ICDAR 2023鲁棒布局分割竞赛开发。我们的方法大大超过了基线，以70.0的mAP稳居排行榜榜首。为了实现这一性能，我们集中精力增强任务的各个方面，如数据集扩充、模型架构、边界框细化和模型集成技术。此外，我们分别为每个文档类别训练数据，以确保更高的平均提交分数。我们还开发了一种用于小区匹配的算法，以进一步提高我们的性能。为了确定我们模型集成的最佳权重和IoU阈值，我们使用了一种称为树结构Parzen估计器的贝叶斯优化算法。我们的方法有效地展示了将基于查询的模型和无锚模型相结合以在公司文档中实现稳健布局分割的好处。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06553v1" target="_blank">2305.06553v1</a>
                              </td>
                              <td>WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</td>
                              <td>Mingliang Zhang</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06553v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on any pre-trained self-supervised model to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear classification performance of state-of-the-art self-supervised models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and enhancing these features through Q-score regularization makes representations more interpretable across all self-supervised models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们习得的表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征大多以正确分类的表示形式出现。使用这些特征，我们可以将表示空间压缩高达$40\%\$，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种模型不可知、无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作任何预先训练的自监督模型的正则化术语，以纠正低质量的表示。与基线相比，使用Q-Score正则化进行微调可以将最先进的自监督模型的线性分类性能在ImageNet-100上提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性强相关，通过Q分数正则化增强这些特征使表示在所有自监督模型中更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v4" target="_blank">2203.01881v4</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14571v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14571v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-trained DINO model (as multiple channels) in the other branch. The results of our experiments on two publicly available medical imaging datasets show that the proposed pipeline outperforms U-Net and the state-of-the-art medical image segmentation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14571v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管纯基于变换器的架构在许多计算机视觉任务中表现出了良好的性能，但引入了许多由CNN和变换器块组成的混合模型来适应更专业的任务。然而，尽管与医学图像分割中的细胞神经网络相比，基于纯变压器和混合变压器的架构都有性能增益，但其高昂的训练成本和复杂性使其在实际场景中使用具有挑战性。在这项工作中，我们提出了基于纯卷积层的简单架构，并表明只要利用从自监督预训练的视觉变换器网络（例如，DINO）获得的注意力图可视化，就可以以更低的计算成本胜过基于复杂变换器的网络。所提出的架构由两个编码器分支组成，其中原始图像在一个分支中作为输入，而来自预训练的DINO模型的多个自注意头的同一图像的注意图可视化（作为多个通道）在另一个分支。我们在两个公开可用的医学图像数据集上的实验结果表明，所提出的流水线优于U-Net和最先进的医学图像分割模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14571v1" target="_blank">2304.14571v1</a>
                              </td>
                              <td>DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</td>
                              <td>Yousef Yeganeh</td>
                              <td>2023-04-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14571v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14571v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13348v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextDeformer: Geometry Manipulation using Text Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13348v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a technique for automatically producing a deformation of an input triangle mesh, guided solely by a text prompt. Our framework is capable of deformations that produce both large, low-frequency shape changes, and small high-frequency details. Our framework relies on differentiable rendering to connect geometry to powerful pre-trained image encoders, such as CLIP and DINO. Notably, updating mesh geometry by taking gradient steps through differentiable rendering is notoriously challenging, commonly resulting in deformed meshes with significant artifacts. These difficulties are amplified by noisy and inconsistent gradients from CLIP. To overcome this limitation, we opt to represent our mesh deformation through Jacobians, which updates deformations in a global, smooth manner (rather than locally-sub-optimal steps). Our key observation is that Jacobians are a representation that favors smoother, large deformations, leading to a global relation between vertices and pixels, and avoiding localized noisy gradients. Additionally, to ensure the resulting shape is coherent from all 3D viewpoints, we encourage the deep features computed on the 2D encoding of the rendering to be consistent for a given vertex from all viewpoints. We demonstrate that our method is capable of smoothly-deforming a wide variety of source mesh and target text prompts, achieving both large modifications to, e.g., body proportions of animals, as well as adding fine semantic details, such as shoe laces on an army boot and fine details of a face.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13348v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种仅通过文本提示自动生成输入三角形网格变形的技术。我们的框架能够产生大的低频形状变化和小的高频细节。我们的框架依赖于可微分渲染来将几何体连接到强大的预训练图像编码器，如CLIP和DINO。值得注意的是，通过可微分渲染采取梯度步骤来更新网格几何体是出了名的具有挑战性，通常会导致变形的网格出现明显的伪影。这些困难被来自CLIP的噪声和不一致的梯度放大了。为了克服这一限制，我们选择通过雅可比变换来表示网格变形，雅可比变换以全局、平滑的方式更新变形（而不是局部次优步骤）。我们的关键观察结果是，雅各宾派是一种倾向于更平滑、大变形的表示，从而导致顶点和像素之间的全局关系，并避免局部噪声梯度。此外，为了确保从所有3D视点得到的形状是一致的，我们鼓励在渲染的2D编码上计算的深度特征对于来自所有视点的给定顶点是一致的。我们证明，我们的方法能够平滑地变形各种源网格和目标文本提示，既可以对动物的身体比例进行大的修改，也可以添加精细的语义细节，如军靴上的鞋带和面部的精细细节。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13348v1" target="_blank">2304.13348v1</a>
                              </td>
                              <td>TextDeformer: Geometry Manipulation using Text Guidance</td>
                              <td>William Gao</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13348v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13348v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13089v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13089v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences explain opposite trends in transfer performance for downstream tasks that require spatial specificity in features. Finally, we address how fine-tuning changes reconstructive representations to enable better transfer, showing that fine-tuning re-organizes the information to be more similar to pre-trained joint embedding models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13089v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于联合嵌入的学习（例如，SimCLR、MoCo、DINO）和基于重建的学习（如，BEiT、SimMIM、MAE）是视觉变换器自我监督学习的两种主要范式，但它们的迁移性能有很大不同。在这里，我们旨在通过分析这些目标对习得表征的结构和可迁移性的影响来解释这些差异。我们的分析表明，基于重建的学习特征与基于联合嵌入的学习特征显著不同，并且以相似目标训练的模型甚至在不同架构中也能学习相似的特征。这些差异出现在网络的早期，主要由注意力和规范化层驱动。我们发现，联合嵌入特征为分类提供了更好的线性探测转移，因为不同的目标驱动学习表示中信息和不变量的不同分布。这些差异解释了需要特征空间特异性的下游任务的转移性能的相反趋势。最后，我们讨论了微调如何改变重建表示以实现更好的传输，表明微调重新组织信息，使其更类似于预训练的联合嵌入模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13089v1" target="_blank">2304.13089v1</a>
                              </td>
                              <td>Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</td>
                              <td>Shashank Shekhar</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13089v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13089v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13027v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Strong and Reproducible Object Detector with Only Public Datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13027v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13027v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13027v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work presents Focal-Stable-DINO, a strong and reproducible object detection model which achieves 64.6 AP on COCO val2017 and 64.8 AP on COCO test-dev using only 700M parameters without any test time augmentation. It explores the combination of the powerful FocalNet-Huge backbone with the effective Stable-DINO detector. Different from existing SOTA models that utilize an extensive number of parameters and complex training techniques on large-scale private data or merged data, our model is exclusively trained on the publicly available dataset Objects365, which ensures the reproducibility of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13027v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作提出了Focal Stable DINO，这是一种强大且可复制的物体检测模型，仅使用700M参数，在没有任何测试时间增加的情况下，就在COCO val2017上实现了64.6 AP，在COCO测试开发中实现了64.8 AP。它探索了强大的FocalNet巨大骨干与有效的稳定DINO检测器的结合。与在大规模私有数据或合并数据上使用大量参数和复杂训练技术的现有SOTA模型不同，我们的模型仅在公开可用的数据集Objects365上进行训练，这确保了我们方法的可重复性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13027v1" target="_blank">2304.13027v1</a>
                              </td>
                              <td>A Strong and Reproducible Object Detector with Only Public Datasets</td>
                              <td>Tianhe Ren</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13027v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13027v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10597v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10597v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10597v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10597v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Through this work, we aim to provide insights into maximizing the applicability of visual FMs in specific contexts with minimal model tuning. The code is available at https://github.com/Douglas2Code/Text2Seg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10597v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（FM）的最新进展，如GPT-4和LLaMA，因其在零样本学习场景中的卓越性能而引起了人们的极大关注。类似地，在视觉学习领域，像Grounding DINO和Segment Anything Model（SAM）这样的模型在开集检测和实例分割任务方面表现出了显著的进步。不可否认，这些FM将深刻影响广泛的现实世界视觉学习任务，为开发此类模型带来新的范式转变。在这项研究中，我们专注于遥感领域，那里的图像与传统场景中的图像明显不同。我们开发了一个管道，利用多个FM来促进由文本提示引导的遥感图像语义分割任务，我们将其表示为Text2Seg。该管道是在几个广泛使用的遥感数据集上进行基准测试的，我们提供了初步结果来证明其有效性。通过这项工作，我们旨在通过最小的模型调整，最大限度地提高视觉FM在特定环境中的适用性。代码位于https://github.com/Douglas2Code/Text2Seg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10597v1" target="_blank">2304.10597v1</a>
                              </td>
                              <td>Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models</td>
                              <td>Jielu Zhang</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10597v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10597v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08069v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs Beat YOLOs on Real-time Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08069v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08069v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08069v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, end-to-end transformer-based detectors (DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. Source code and pretrained models will be available at PaddleDetection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08069v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，端到端的基于变压器的检测器（DETR）已经取得了显著的性能。然而，DETR的高计算成本问题尚未得到有效解决，这限制了它们的实际应用，并使它们无法充分利用无后处理的好处，例如非最大值抑制（NMS）。本文首先分析了现代实时对象检测器中NMS对推理速度的影响，并建立了端到端速度基准。为了避免NMS引起的推理延迟，我们提出了一种实时检测转换器（RT-DETR），这是我们所知的第一个实时端到端对象检测器。具体而言，我们设计了一种高效的混合编码器，通过解耦尺度内交互和跨尺度融合来高效处理多尺度特征，并提出了IoU感知的查询选择，以提高对象查询的初始化。此外，我们提出的检测器支持通过使用不同的解码器层来灵活调整推理速度，而不需要重新训练，这有利于实时对象检测器的实际应用。我们的RT-DETR-L在COCO val2017上实现了53.0%的AP，在T4 GPU上实现了114 FPS，而RT-DETR-X实现了54.8%的AP和74 FPS，在速度和精度方面都优于相同规模的所有YOLO检测器。此外，我们的RT-DETR-R50实现了53.1%的AP和108 FPS，在精度上优于DINO-Deformable-DETR-R5 2.2%的AP和大约21倍的FPS。PaddleDetection将提供源代码和预训练模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08069v1" target="_blank">2304.08069v1</a>
                              </td>
                              <td>DETRs Beat YOLOs on Real-time Object Detection</td>
                              <td>Wenyu Lv</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08069v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08069v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07527v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Align-DETR: Improving DETR with Simple IoU-aware BCE loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07527v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07527v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07527v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DETR has set up a simple end-to-end pipeline for object detection by formulating this task as a set prediction problem, showing promising potential. However, despite the significant progress in improving DETR, this paper identifies a problem of misalignment in the output distribution, which prevents the best-regressed samples from being assigned with high confidence, hindering the model's accuracy. We propose a metric, recall of best-regressed samples, to quantitively evaluate the misalignment problem. Observing its importance, we propose a novel Align-DETR that incorporates a localization precision-aware classification loss in optimization. The proposed loss, IA-BCE, guides the training of DETR to build a strong correlation between classification score and localization precision. We also adopt the mixed-matching strategy, to facilitate DETR-based detectors with faster training convergence while keeping an end-to-end scheme. Moreover, to overcome the dramatic decrease in sample quality induced by the sparsity of queries, we introduce a prime sample weighting mechanism to suppress the interference of unimportant samples. Extensive experiments are conducted with very competitive results reported. In particular, it delivers a 46 (+3.8)% AP on the DAB-DETR baseline with the ResNet-50 backbone and reaches a new SOTA performance of 50.2% AP in the 1x setting on the COCO validation set when employing the strong baseline DINO. Our code is available at https://github.com/FelixCaae/AlignDETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07527v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DETR通过将这项任务公式化为集合预测问题，建立了一个简单的端到端目标检测流水线，显示出了很好的潜力。然而，尽管在改进DETR方面取得了重大进展，但本文发现了输出分布中的错位问题，这阻碍了最佳回归样本的高置信度分配，阻碍了模型的准确性。我们提出了一个度量，即最佳回归样本的召回，来定量评估错位问题。鉴于其重要性，我们提出了一种新的Align DETR，该方法在优化中引入了定位精度感知的分类损失。所提出的损失IA-BCE指导DETR的训练，以在分类得分和定位精度之间建立强相关性。我们还采用了混合匹配策略，以促进基于DETR的检测器在保持端到端方案的同时具有更快的训练收敛。此外，为了克服查询稀疏导致的样本质量急剧下降的问题，我们引入了一种主样本加权机制来抑制不重要样本的干扰。进行了广泛的实验，报告了非常有竞争力的结果。特别是，当采用强基线DINO时，它在具有ResNet-50主干的DAB-DETR基线上提供了46（+3.8）%AP，并在COCO验证集的1x设置中达到了50.2%AP的新SOTA性能。我们的代码可在https://github.com/FelixCaae/AlignDETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07527v1" target="_blank">2304.07527v1</a>
                              </td>
                              <td>Align-DETR: Improving DETR with Simple IoU-aware BCE loss</td>
                              <td>Zhi Cai</td>
                              <td>2023-04-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07527v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07527v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07314v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07314v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07314v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07314v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training strategies have recently shown impressive results for training general-purpose feature extraction backbones in computer vision. In combination with the Vision Transformer architecture, the DINO self-distillation technique has interesting emerging properties, such as unsupervised clustering in the latent space and semantic correspondences of the produced features without using explicit human-annotated labels. The STEGO method for unsupervised semantic segmentation contrastively distills feature correspondences of a DINO-pre-trained Vision Transformer and recently set a new state of the art. However, the detailed workings of STEGO have yet to be disentangled, preventing its usage in safety-critical applications. This paper provides a deeper understanding of the STEGO architecture and training strategy by conducting studies that uncover the working mechanisms behind STEGO, reproduce and extend its experimental validation, and investigate the ability of STEGO to transfer to different datasets. Results demonstrate that the STEGO architecture can be interpreted as a semantics-preserving dimensionality reduction technique.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07314v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督预训练策略最近在训练计算机视觉中的通用特征提取骨干方面显示出令人印象深刻的结果。与视觉转换器架构相结合，DINO自蒸馏技术具有有趣的新兴特性，例如在潜在空间中的无监督聚类，以及在不使用明确的人类注释标签的情况下产生的特征的语义对应。用于无监督语义分割的STEGO方法对比地提取了DINO预训练的视觉转换器的特征对应关系，最近开创了一个新的技术状态。然而，STEGO的详细工作原理尚未解开，阻碍了其在安全关键应用中的使用。本文通过开展研究，揭示STEGO背后的工作机制，复制和扩展其实验验证，并研究STEGO转移到不同数据集的能力，对STEGO架构和训练策略有了更深入的了解。结果表明，STEGO体系结构可以被解释为一种保留语义的降维技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07314v1" target="_blank">2304.07314v1</a>
                              </td>
                              <td>Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation</td>
                              <td>Alexander Koenig</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07314v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07314v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07193v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINOv2: Learning Robust Visual Features without Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07193v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07193v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07193v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07193v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，用于对大量数据进行模型预训练的自然语言处理取得了突破，为计算机视觉中的类似基础模型开辟了道路。这些模型可以通过产生通用的视觉特征，即在不进行微调的情况下跨图像分布和任务工作的特征，极大地简化图像在任何系统中的使用。这项工作表明，如果在来自不同来源的足够精心策划的数据上进行训练，现有的预训练方法，特别是自监督方法，可以产生这样的特征。我们重新审视现有的方法，并结合不同的技术，在数据和模型大小方面扩大我们的预训练。大多数技术贡献旨在加速和稳定大规模培训。在数据方面，我们提出了一种自动管道，以建立一个专门的、多样化的和精心策划的图像数据集，而不是像自我监督文献中通常做的那样，建立未经处理的数据集。在模型方面，我们训练了一个具有1B参数的ViT模型（Dosovitskiy et al.，2020），并将其提取为一系列较小的模型，这些模型在图像和像素级别的大多数基准上超过了可用的最佳通用功能OpenCLIP（Ilharco et al.，2021）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07193v1" target="_blank">2304.07193v1</a>
                              </td>
                              <td>DINOv2: Learning Robust Visual Features without Supervision</td>
                              <td>Maxime Oquab</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07193v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07193v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05754v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Learning with Cluster-Aware-DINO for High-Performance Robust Speaker Verification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05754v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05754v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05754v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic speaker verification task has made great achievements using deep learning approaches with the large-scale manually annotated dataset. However, it's very difficult and expensive to collect a large amount of well-labeled data for system building. In this paper, we propose a novel and advanced self-supervised learning framework which can construct a high performance speaker verification system without using any labeled data. To avoid the impact of false negative pairs, we adopt the self-distillation with no labels (DINO) framework as the initial model, which can be trained without exploiting negative pairs. Then, we introduce a cluster-aware training strategy for DINO to improve the diversity of data. In the iteration learning stage, due to a mass of unreliable labels from clustering, the quality of pseudo labels is important for the system training. This motivates us to propose dynamic loss-gate and label correction (DLG-LC) methods to alleviate the performance degradation caused by unreliable labels. More specifically, we model the loss distribution with GMM and obtain the loss-gate threshold dynamically to distinguish the reliable and unreliable labels. Besides, we adopt the model predictions to correct the unreliable label, for better utilizing the unreliable data rather than dropping them directly. Moreover, we extend the DLG-LC to multi-modality to further improve the performance. The experiments are performed on the commonly used Voxceleb dataset. Compared to the best-known self-supervised speaker verification system, our proposed method obtain 22.17%, 27.94% and 25.56% relative EER improvement on Vox-O, Vox-E and Vox-H test sets, even with fewer iterations, smaller models, and simpler clustering methods. More importantly, the newly proposed system even achieves comparable results with the fully supervised system, but without using any human labeled data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05754v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大规模手动注释数据集的深度学习方法，自动说话人验证任务取得了巨大成就。然而，为系统构建收集大量标记良好的数据是非常困难和昂贵的。在本文中，我们提出了一种新颖而先进的自监督学习框架，该框架可以在不使用任何标记数据的情况下构建一个高性能的说话人验证系统。为了避免假阴性对的影响，我们采用无标签自蒸馏（DINO）框架作为初始模型，该模型可以在不利用阴性对的情况下进行训练。然后，我们为DINO引入了一种集群感知训练策略，以提高数据的多样性。在迭代学习阶段，由于聚类中存在大量不可靠的标签，伪标签的质量对系统训练至关重要。这促使我们提出动态损失门和标签校正（DLG-LC）方法，以缓解不可靠标签导致的性能下降。更具体地说，我们用GMM对损失分布进行建模，并动态获得损失门阈值，以区分可靠和不可靠标签。此外，我们采用模型预测来校正不可靠的标签，以便更好地利用不可靠的数据，而不是直接丢弃它们。此外，我们将DLG-LC扩展到多模态，以进一步提高性能。实验是在常用的Voxceleb数据集上进行的。与最著名的自监督说话人验证系统相比，我们提出的方法在Vox-O、Vox-E和Vox-H测试集上获得了22.17%、27.94%和25.56%的相对EER改进，即使迭代次数更少、模型更小、聚类方法更简单。更重要的是，新提出的系统甚至实现了与完全监督系统相当的结果，但没有使用任何人工标记的数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05754v1" target="_blank">2304.05754v1</a>
                              </td>
                              <td>Self-Supervised Learning with Cluster-Aware-DINO for High-Performance Robust Speaker Verification</td>
                              <td>Bing Han</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05754v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05754v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05163v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervision for medical image classification: state-of-the-art performance with ~100 labeled training samples per class</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05163v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05163v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05163v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Is self-supervised deep learning (DL) for medical image analysis already a serious alternative to the de facto standard of end-to-end trained supervised DL? We tackle this question for medical image classification, with a particular focus on one of the currently most limiting factors of the field: the (non-)availability of labeled data. Based on three common medical imaging modalities (bone marrow microscopy, gastrointestinal endoscopy, dermoscopy) and publicly available data sets, we analyze the performance of self-supervised DL within the self-distillation with no labels (DINO) framework. After learning an image representation without use of image labels, conventional machine learning classifiers are applied. The classifiers are fit using a systematically varied number of labeled data (1-1000 samples per class). Exploiting the learned image representation, we achieve state-of-the-art classification performance for all three imaging modalities and data sets with only a fraction of between 1% and 10% of the available labeled data and about 100 labeled samples per class.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05163v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于医学图像分析的自监督深度学习（DL）是否已经是端到端训练的监督DL的事实标准的一个重要替代方案？我们解决了医学图像分类的这个问题，特别关注该领域目前最受限制的因素之一：标记数据的（非）可用性。基于三种常见的医学成像模式（骨髓显微镜、胃肠镜和皮肤镜）和公开的数据集，我们在无标签自蒸馏（DINO）框架内分析了自监督DL的性能。在不使用图像标签的情况下学习图像表示之后，应用传统的机器学习分类器。分类器使用系统变化数量的标记数据（每类1-1000个样本）进行拟合。利用学习的图像表示，我们对所有三种成像模态和数据集实现了最先进的分类性能，每类仅占可用标记数据的1%至10%之间的一小部分，约100个标记样本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05163v1" target="_blank">2304.05163v1</a>
                              </td>
                              <td>Self-supervision for medical image classification: state-of-the-art performance with ~100 labeled training samples per class</td>
                              <td>Maximilian Nielsen</td>
                              <td>2023-04-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05163v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05163v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_04742v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detection Transformer with Stable Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_04742v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_04742v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_04742v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper is concerned with the matching stability problem across different decoder layers in DEtection TRansformers (DETR). We point out that the unstable matching in DETR is caused by a multi-optimization path problem, which is highlighted by the one-to-one matching design in DETR. To address this problem, we show that the most important design is to use and only use positional metrics (like IOU) to supervise classification scores of positive examples. Under the principle, we propose two simple yet effective modifications by integrating positional metrics to DETR's classification loss and matching cost, named position-supervised loss and position-modulated cost. We verify our methods on several DETR variants. Our methods show consistent improvements over baselines. By integrating our methods with DINO, we achieve 50.4 and 51.5 AP on the COCO detection benchmark using ResNet-50 backbones under 12 epochs and 24 epochs training settings, achieving a new record under the same setting. We achieve 63.8 AP on COCO detection test-dev with a Swin-Large backbone. Our code will be made available at https://github.com/IDEA-Research/Stable-DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_04742v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了DEDetection-Transformer（DETR）中不同解码器层之间的匹配稳定性问题。我们指出，DETR中的不稳定匹配是由多优化路径问题引起的，DETR的一对一匹配设计突出了这一问题。为了解决这个问题，我们展示了最重要的设计是使用并且仅使用位置度量（如IOU）来监督正面示例的分类分数。在该原理下，我们通过将位置度量与DETR的分类损失和匹配成本相结合，提出了两种简单而有效的修改，称为位置监督损失和位置调制成本。我们在几个DETR变体上验证了我们的方法。我们的方法显示出与基线相比的持续改进。通过将我们的方法与DINO相结合，我们在12个时期和24个时期的训练设置下，使用ResNet-50骨干在COCO检测基准上分别获得了50.4和51.5的AP，在相同设置下创下了新纪录。我们使用Swin-Large主干在COCO检测测试开发中实现了63.8 AP。我们的代码将在https://github.com/IDEA-Research/Stable-DINO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.04742v1" target="_blank">2304.04742v1</a>
                              </td>
                              <td>Detection Transformer with Stable Matching</td>
                              <td>Shilong Liu</td>
                              <td>2023-04-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_04742v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.04742v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_09981v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weighted Ensemble Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_09981v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_09981v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_09981v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_09981v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>嵌入已被证明是一种在监督学习中提高模型性能、不确定性估计和鲁棒性的强大技术。自我监督学习（SSL）的进步使得能够利用大型未标记语料库来实现最先进的少镜头和监督学习性能。在本文中，我们探讨了集成方法如何通过开发一个允许数据相关加权交叉熵损失的框架来改进最近的SSL技术。我们不把代表权的骨干混为一谈；这种选择产生了一种高效的集成方法，该方法产生了较小的训练成本，并且不需要架构更改或下游评估的计算开销。我们的方法的有效性通过两种最先进的SSL方法来证明，即DINO（Caron等人，2021）和MSN（Assran等人，2022）。我们的方法在ImageNet-1K上的多个评估指标方面都优于这两种方法，尤其是在少镜头设置方面。我们探索了几种加权方案，发现那些增加系综头部多样性的方案会带来更好的下游评估结果。彻底的实验产生了改进的现有技术基线，我们的方法仍然超越了这些基线；例如，我们使用MSN ViT-B/16对一次学习的总体改进为3.9 p.p。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.09981v3" target="_blank">2211.09981v3</a>
                              </td>
                              <td>Weighted Ensemble Self-Supervised Learning</td>
                              <td>Yangjun Ruan</td>
                              <td>2022-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_09981v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.09981v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03140v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From Saliency to DINO: Saliency-guided Vision Transformer for Few-shot Keypoint Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03140v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03140v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03140v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unlike current deep keypoint detectors that are trained to recognize limited number of body parts, few-shot keypoint detection (FSKD) attempts to localize any keypoints, including novel or base keypoints, depending on the reference samples. FSKD requires the semantically meaningful relations for keypoint similarity learning to overcome the ubiquitous noise and ambiguous local patterns. One rescue comes with vision transformer (ViT) as it captures long-range relations well. However, ViT may model irrelevant features outside of the region of interest due to the global attention matrix, thus degrading similarity learning between support and query features. In this paper, we present a novel saliency-guided vision transformer, dubbed SalViT, for few-shot keypoint detection. Our SalViT enjoys a uniquely designed masked self-attention and a morphology learner, where the former introduces saliency map as a soft mask to constrain the self-attention on foregrounds, while the latter leverages the so-called power normalization to adjust morphology of saliency map, realizing ``dynamically changing receptive field''. Moreover, as salinecy detectors add computations, we show that attentive masks of DINO transformer can replace saliency. On top of SalViT, we also investigate i) transductive FSKD that enhances keypoint representations with unlabelled data and ii) FSKD under occlusions. We show that our model performs well on five public datasets and achieves ~10% PCK higher than the normally trained model under severe occlusions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03140v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与当前训练为识别有限数量身体部位的深度关键点检测器不同，很少有射击关键点检测（FSKD）尝试根据参考样本定位任何关键点，包括新的或基本的关键点。FSKD要求关键点相似性学习具有语义意义的关系，以克服普遍存在的噪声和模糊的局部模式。一个救援方案是视觉转换器（ViT），因为它很好地捕捉到了长期关系。然而，由于全局注意力矩阵，ViT可能对感兴趣区域之外的不相关特征进行建模，从而降低支持和查询特征之间的相似性学习。在本文中，我们提出了一种新的显著性引导的视觉转换器，称为SalViT，用于少镜头关键点检测。我们的SalViT具有独特设计的掩蔽自注意和形态学学习器，前者引入显著图作为软掩蔽来约束前景上的自注意，而后者利用所谓的功率归一化来调整显著图的形态学，实现“动态变化的感受野”。此外，随着盐度检测器增加计算，我们表明DINO变换器的注意掩码可以取代显著性。除SalViT外，我们还研究了i）用未标记数据增强关键点表示的转导FSKD和ii）闭塞下的FSKD。我们表明，我们的模型在五个公共数据集上表现良好，在严重闭塞情况下比正常训练的模型高出约10%的PCK。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03140v1" target="_blank">2304.03140v1</a>
                              </td>
                              <td>From Saliency to DINO: Saliency-guided Vision Transformer for Few-shot Keypoint Detection</td>
                              <td>Changsheng Lu</td>
                              <td>2023-04-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03140v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03140v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_12252v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Adversarial Transferability using Dynamic Cues</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_12252v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_12252v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_12252v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. Our attack results indicate that the attacker does not need specialized architectures, \eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at https://bit.ly/3Xd9gRQ</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_12252v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对抗性扰动在图像模型之间的可转移性已经得到了广泛的研究。在这种情况下，攻击是从已知的代理（例如ImageNet训练的模型）生成的，并被转移以改变在图像数据集上训练的未知（黑匣子）模型的决策。然而，由于图像模型中缺乏时间线索，从图像模型生成的攻击无法捕捉运动对象或变化场景的动态特性。这导致对抗性攻击从表示丰富的\emph｛image｝模型（如监督视觉变换器（ViTs）、自监督ViTs（例如，DINO）和视觉语言模型（例如，CLIP））到黑盒\emph｝视频｝模型的可转移性降低。在这项工作中，我们在图像模型中引入动态提示，而不会牺牲它们在图像上的原始性能。为此，我们通过冻结图像模型来优化emph｛时间提示｝，以捕捉运动动力学。我们的时间提示是可学习变换的结果，该变换允许在对抗性攻击期间优化时间梯度，以欺骗运动动力学。具体而言，我们通过任务特定提示在同一源模型中引入空间（图像）和时间（视频）线索。使用针对图像模型设计的攻击，攻击这样的提示最大限度地提高了从图像到视频和从图像到图像模型的对抗性可转移性。我们的攻击结果表明，攻击者不需要专门的架构，例如，针对不同数据模式的划分时空注意力、3D卷积或多视图卷积网络。图像模型是在随着时间的推移而变化的环境中优化对抗性攻击以欺骗黑盒模型的有效代理。代码位于https://bit.ly/3Xd9gRQ</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.12252v2" target="_blank">2302.12252v2</a>
                              </td>
                              <td>Boosting Adversarial Transferability using Dynamic Cues</td>
                              <td>Muzammal Naseer</td>
                              <td>2023-02-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_12252v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.12252v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13396v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-guidance Segmentation Using Zero Segment Labels</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13396v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13396v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13396v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP has enabled new and exciting joint vision-language applications, one of which is open-vocabulary segmentation, which can locate any segment given an arbitrary text query. In our research, we ask whether it is possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically? We propose a novel problem zero-guidance segmentation and the first baseline that leverages two pre-trained generalist models, DINO and CLIP, to solve this problem without any fine-tuning or segmentation dataset. The general idea is to first segment an image into small over-segments, encode them into CLIP's visual-language space, translate them into text labels, and merge semantically similar segments together. The key challenge, however, is how to encode a visual segment into a segment-specific embedding that balances global and local context information, both useful for recognition. Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. With CLIP's innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd. Project page: https://zero-guide-seg.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13396v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP实现了新的、令人兴奋的联合视觉语言应用，其中之一是开放式词汇分割，它可以在给定任意文本查询的情况下定位任何片段。在我们的研究中，我们问是否有可能在没有任何用户指导的情况下以文本查询或预定义类的形式发现语义片段，并使用自然语言自动标记它们？我们提出了一种新的问题零引导分割和第一个基线，该基线利用两个预先训练的多面手模型DINO和CLIP来解决这个问题，而不需要任何微调或分割数据集。一般的想法是首先将图像分割成小的过度片段，将它们编码到CLIP的视觉语言空间中，将它们翻译成文本标签，并将语义相似的片段合并在一起。然而，关键的挑战是如何将视觉片段编码为特定片段的嵌入，以平衡全局和局部上下文信息，这两种信息都对识别有用。我们的主要贡献是一种新颖的注意力掩蔽技术，通过分析CLIP内部的注意力层来平衡这两种上下文。我们还介绍了用于评估这项新任务的几个指标。凭借CLIP与生俱来的知识，我们的方法可以在博物馆人群中准确定位蒙娜丽莎的画作。项目页面：https://zero-guide-seg.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13396v2" target="_blank">2303.13396v2</a>
                              </td>
                              <td>Zero-guidance Segmentation Using Zero Segment Labels</td>
                              <td>Pitchaporn Rewatbowornwong</td>
                              <td>2023-03-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13396v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13396v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12786v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12786v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12786v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12786v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent works on generalizable NeRFs have shown promising results on novel view synthesis from single or few images. However, such models have rarely been applied on other downstream tasks beyond synthesis such as semantic understanding and parsing. In this paper, we propose a novel framework named FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D pre-trained foundation models to 3D space via neural rendering, and then extract deep features for 3D query points from NeRF MLPs. Consequently, it allows to map 2D images to continuous 3D semantic feature volumes, which can be used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D semantic keypoint transfer and 2D/3D object part segmentation. Our extensive experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D semantic feature extractor. Our project page is available at https://jianglongye.com/featurenerf/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12786v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近关于可推广NeRFs的工作在从单个或少量图像合成新视图方面显示出了有希望的结果。然而，这种模型很少应用于合成之外的其他下游任务，如语义理解和解析。在本文中，我们提出了一个名为FeatureNeRF的新框架，通过提取预先训练的视觉基础模型（例如，DINO、潜在扩散）来学习可推广的NeRF。FeatureNeRF通过神经渲染将2D预训练的基础模型利用到3D空间，然后从NeRF MLP中提取3D查询点的深层特征。因此，它允许将2D图像映射到连续的3D语义特征量，这可以用于各种下游任务。我们在2D/3D语义关键点转移和2D/3D对象部分分割任务上评估了FeatureNeRF。我们的大量实验证明了FeatureNeRF作为一种可推广的3D语义特征提取器的有效性。我们的项目页面可在https://jianglongye.com/featurenerf/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12786v1" target="_blank">2303.12786v1</a>
                              </td>
                              <td>FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models</td>
                              <td>Jianglong Ye</td>
                              <td>2023-03-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12786v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12786v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12860v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs with Collaborative Hybrid Assignments Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12860v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12860v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12860v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervisions on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS, FCOS, and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic Deformable-DETR by 5.8% in 12-epoch training and 3.2% in 36-epoch training. The state-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from 58.5% to 59.5%. Surprisingly, incorporated with the large-scale backbone MixMIM-g with 1-Billion parameters, we achieve the 64.5% mAP on MS COCO test-dev, achieving superior performance with much fewer extra data sizes. Codes will be available at https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12860v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们观察到，在具有一对一集匹配的DETR中，被分配为正样本的查询太少，导致对编码器输出的稀疏监督，这大大损害了编码器的判别特征学习，反之亦然，影响了解码器中的注意力学习。为了缓解这一问题，我们提出了一种新的协作混合分配训练方案，即Co-DETR，以从通用的标签分配方式中学习更高效、更有效的基于DETR的检测器。这种新的训练方案可以通过训练由一对多标签分配（如ATSS、FCOS和Faster RCNN）监督的多个并行辅助头，轻松增强编码器在端到端检测器中的学习能力。此外，我们通过从这些辅助头中提取正坐标来进行额外定制的正查询，以提高解码器中正样本的训练效率。在推断中，这些辅助头被丢弃，因此我们的方法没有给原始检测器引入额外的参数和计算成本，同时不需要手工制作的非最大值抑制（NMS）。我们进行了广泛的实验来评估所提出的方法对DETR变体的有效性，包括DAB-DETR、可变形DETR和DINO可变形DETR。具体而言，我们在12个时期的训练中将基本的可变形DETR提高了5.8%，在36个时期的培训中提高了3.2%。最先进的带Swin-L的DINO可变形DETR仍然可以从58.5%提高到59.5%。令人惊讶的是，与具有10亿个参数的大规模骨干MixMIM-g相结合，我们在MS COCO测试开发中实现了64.5%的mAP，以更少的额外数据大小实现了卓越的性能。代码将在https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12860v3" target="_blank">2211.12860v3</a>
                              </td>
                              <td>DETRs with Collaborative Hybrid Assignments Training</td>
                              <td>Zhuofan Zong</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12860v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12860v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05499v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05499v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05499v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05499v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \url{https://github.com/IDEA-Research/GroundingDINO}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05499v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种开放集对象检测器，称为Grounding DINO，通过将基于Transformer的检测器DINO与Grounding预训练相结合，该检测器可以检测具有人类输入（如类别名称或引用表达式）的任意对象。开集对象检测的关键解决方案是将语言引入到闭集检测器中，用于开集概念的泛化。为了有效地融合语言和视觉模态，我们在概念上将闭集检测器分为三个阶段，并提出了一种紧密融合解决方案，其中包括特征增强器、语言引导的查询选择和用于跨模态融合的跨模态解码器。虽然以前的工作主要评估新类别上的开集对象检测，但我们建议也对用属性指定的对象的指称表达理解进行评估。接地DINO在所有三种设置上都表现得非常好，包括COCO、LVIS、ODinW和RefCOCO/+/g上的基准测试。接地DINO在COCO检测零样本传输基准上达到$52.5$AP，即没有来自COCO的任何培训数据。它在ODinW零样本基准测试中创下了平均26.1美元的新纪录。代码将在\url中提供{https://github.com/IDEA-Research/GroundingDINO}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05499v4" target="_blank">2303.05499v4</a>
                              </td>
                              <td>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</td>
                              <td>Shilong Liu</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05499v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05499v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04788v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Representation Learning by Detecting Incorrect Location Embeddings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04788v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04788v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04788v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our method on ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks. Moreover, we show that when downstream tasks are strongly reliant on shape (such as in the YOGA-82 pose dataset), our pre-trained features yield a significant gain over prior work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04788v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了一种用于图像表示学习的新型自监督学习（SSL）损失。人们越来越相信，深度神经网络的泛化能力与其辨别物体形状的能力有关。由于物体的形状与其部分的位置有关，我们建议检测那些被人为放错位置的物体。我们用图像标记表示对象部分，并训练ViT来检测哪个标记与错误的位置嵌入相结合。然后，我们在输入中引入稀疏性，使模型对遮挡更具鲁棒性，并加快训练速度。我们将我们的方法称为DILEMMA，它代表具有MAsked输入的错误位置检测EM床。我们将DILEMMA应用于MoCoV3、DINO和SimCLR，并在相同的训练时间下，通过在ImageNet-1K上的线性探测转移，它们的性能分别提高了4.41%、3.97%和0.5%。我们还在ImageNet-100上展示了MAE与我们的方法相结合的全面微调改进。我们通过对常见SSL基准进行微调来评估我们的方法。此外，我们表明，当下游任务强烈依赖于形状时（例如在YOGA-82姿势数据集中），我们预先训练的特征比先前的工作产生了显著的增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04788v2" target="_blank">2204.04788v2</a>
                              </td>
                              <td>Representation Learning by Detecting Incorrect Location Embeddings</td>
                              <td>Sepehr Sameni</td>
                              <td>2022-04-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04788v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04788v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_06670v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_06670v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_06670v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_06670v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Due to the costly nature of remote sensing image labeling and the large volume of available unlabeled imagery, self-supervised methods that can learn feature representations without manual annotation have received great attention. While prior works have explored self-supervised learning in remote sensing tasks, pretext tasks based on local-global view alignment remain underexplored. Inspired by DINO, which employs an effective representation learning structure with knowledge distillation based on global-local view alignment, we formulate two pretext tasks for use in self-supervised learning on remote sensing imagery (SSLRS). Using these tasks, we explore the effectiveness of positive temporal contrast as well as multi-sized views on SSLRS. Moreover, we extend DINO and propose DINO-MC which uses local views of various sized crops instead of a single fixed size. Our experiments demonstrate that even when pre-trained on only 10% of the dataset, DINO-MC performs on par or better than existing state of the art SSLRS methods on multiple remote sensing tasks, while using less computational resources. All codes, models and results are available at https://github.com/WennyXY/DINO-MC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_06670v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于遥感图像标记的昂贵性质和大量可用的未标记图像，可以在无需手动注释的情况下学习特征表示的自监督方法受到了极大的关注。虽然先前的工作已经探索了遥感任务中的自我监督学习，但基于局部全局视图对齐的借口任务仍然没有得到充分的探索。受DINO的启发，我们制定了两个借口任务，用于遥感图像的自监督学习（SSLRS）。利用这些任务，我们探索了正时间对比以及多尺度视图对SSLRS的有效性。此外，我们扩展了DINO并提出了DINO-MC，它使用了各种大小作物的局部视图，而不是单一的固定大小。我们的实验表明，即使仅在10%的数据集上进行预训练，DINO-MC在多个遥感任务上的表现也与现有技术的SSLRS方法相当或更好，同时使用更少的计算资源。所有代码、模型和结果均可在https://github.com/WennyXY/DINO-MC.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.06670v1" target="_blank">2303.06670v1</a>
                              </td>
                              <td>DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops</td>
                              <td>Xinye Wanyan</td>
                              <td>2023-03-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_06670v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.06670v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_15369v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">No Reason for No Supervision: Improved Generalization in Supervised Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_15369v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_15369v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_15369v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https://europe.naverlabs.com/t-rex</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_15369v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑在给定的分类任务上训练深度神经网络的问题，例如ImageNet-1K（IN1K），以便它在训练任务和其他（未来）转移任务上都很出色。这两个看似矛盾的特性在提高模型的泛化能力和保持其在原始任务上的性能之间进行了权衡。在迁移学习方面，用自我监督学习训练的模型往往比监督的模型更容易概括；然而，它们仍然落后于IN1K上的监督模型。在本文中，我们提出了一种监督学习设置，它利用了两个世界的最佳效果。我们广泛分析了使用多尺度作物进行数据扩充和一次性投影仪头的监督训练，并揭示了投影仪的设计使我们能够控制训练任务的性能和可转移性之间的权衡。我们进一步用使用记忆库动态计算的类原型替换了最后一层类权重，并导出了两个模型：t-ReX，它实现了迁移学习的新技术，并在IN1K上优于DINO和PAWS等顶级方法，以及t-ReX*，它与IN1K中高度优化的RSB-A1模型相匹配，同时在迁移任务上表现更好。编码和预训练模型：https://europe.naverlabs.com/t-rex</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.15369v2" target="_blank">2206.15369v2</a>
                              </td>
                              <td>No Reason for No Supervision: Improved Generalization in Supervised Models</td>
                              <td>Mert Bulent Sariyildiz</td>
                              <td>2022-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_15369v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.15369v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05475v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05475v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05475v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05475v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked Autoencoders (MAE) have been popular paradigms for large-scale vision representation pre-training. However, MAE solely reconstructs the low-level RGB signals after the decoder and lacks supervision upon high-level semantics for the encoder, thus suffering from sub-optimal learned representations and long pre-training epochs. To alleviate this, previous methods simply replace the pixel reconstruction targets of 75% masked tokens by encoded features from pre-trained image-image (DINO) or image-language (CLIP) contrastive learning. Different from those efforts, we propose to Mimic before Reconstruct for Masked Autoencoders, named as MR-MAE, which jointly learns high-level and low-level representations without interference during pre-training. For high-level semantics, MR-MAE employs a mimic loss over 25% visible tokens from the encoder to capture the pre-trained patterns encoded in CLIP and DINO. For low-level structures, we inherit the reconstruction loss in MAE to predict RGB pixel values for 75% masked tokens after the decoder. As MR-MAE applies high-level and low-level targets respectively at different partitions, the learning conflicts between them can be naturally overcome and contribute to superior visual representations for various downstream tasks. On ImageNet-1K, the MR-MAE base pre-trained for only 400 epochs achieves 85.8% top-1 accuracy after fine-tuning, surpassing the 1600-epoch MAE base by +2.2% and the previous state-of-the-art BEiT V2 base by +0.3%. Code and pre-trained models will be released at https://github.com/Alpha-VL/ConvMAE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05475v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模自动编码器（MAE）已经成为大规模视觉表示预训练的流行范例。然而，MAE仅在解码器之后重建低水平RGB信号，并且缺乏对编码器的高水平语义的监督，因此受到次优学习表示和长的预训练时期的影响。为了缓解这种情况，以前的方法简单地用来自预训练图像图像（DINO）或图像语言（CLIP）对比学习的编码特征来代替75%掩蔽标记的像素重建目标。与这些努力不同的是，我们在重构之前向Mimic提出了用于掩蔽自动编码器的方法，称为MR-MAE，它在预训练期间在没有干扰的情况下联合学习高级和低级表示。对于高级语义，MR-MAE使用来自编码器的超过25%的可见令牌的模拟丢失来捕获CLIP和DINO中编码的预先训练的模式。对于低级结构，我们继承MAE中的重建损失，以预测解码器后75%掩码令牌的RGB像素值。由于MR-MAE分别在不同的分区应用高级和低级目标，因此可以自然地克服它们之间的学习冲突，并有助于为各种下游任务提供更好的视觉表示。在ImageNet-1K上，仅为400个历元预训练的MR-MAE基础在微调后达到85.8%的前1级精度，超过1600个历元的MAE基础+2.2%和之前最先进的BEiT V2基础+0.3%。代码和预训练模型将于https://github.com/Alpha-VL/ConvMAE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05475v1" target="_blank">2303.05475v1</a>
                              </td>
                              <td>Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking</td>
                              <td>Peng Gao</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05475v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05475v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_03956v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neural Congealing: Aligning Images to a Joint Semantic Atlas</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_03956v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_03956v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_03956v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Neural Congealing -- a zero-shot self-supervised framework for detecting and jointly aligning semantically-common content across a given set of images. Our approach harnesses the power of pre-trained DINO-ViT features to learn: (i) a joint semantic atlas -- a 2D grid that captures the mode of DINO-ViT features in the input set, and (ii) dense mappings from the unified atlas to each of the input images. We derive a new robust self-supervised framework that optimizes the atlas representation and mappings per image set, requiring only a few real-world images as input without any additional input information (e.g., segmentation masks). Notably, we design our losses and training paradigm to account only for the shared content under severe variations in appearance, pose, background clutter or other distracting objects. We demonstrate results on a plethora of challenging image sets including sets of mixed domains (e.g., aligning images depicting sculpture and artwork of cats), sets depicting related yet different object categories (e.g., dogs and tigers), or domains for which large-scale training data is scarce (e.g., coffee mugs). We thoroughly evaluate our method and show that our test-time optimization approach performs favorably compared to a state-of-the-art method that requires extensive training on large-scale datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_03956v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了神经凝聚（Neural Congealing）——一种零样本自监督框架，用于检测和联合对齐给定图像集的语义公共内容。我们的方法利用预先训练的DINO-ViT特征的力量来学习：（i）联合语义图谱——捕捉输入集中DINO-ViT特征模式的2D网格，以及（ii）从统一图谱到每个输入图像的密集映射。我们推导了一种新的鲁棒自监督框架，该框架优化了每个图像集的图谱表示和映射，只需要几个真实世界的图像作为输入，而不需要任何额外的输入信息（例如，分割掩码）。值得注意的是，我们设计的损失和训练范式只考虑在外观、姿势、背景混乱或其他分散注意力的物体发生严重变化的情况下共享的内容。我们展示了大量具有挑战性的图像集的结果，包括混合域集（例如，对齐描绘猫的雕塑和艺术品的图像）、描绘相关但不同的对象类别的集（例如狗和老虎），或缺乏大规模训练数据的域（例如，咖啡杯）。我们彻底评估了我们的方法，并表明与需要在大规模数据集上进行广泛训练的最先进方法相比，我们的测试时间优化方法表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.03956v2" target="_blank">2302.03956v2</a>
                              </td>
                              <td>Neural Congealing: Aligning Images to a Joint Semantic Atlas</td>
                              <td>Dolev Ofri-Amar</td>
                              <td>2023-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_03956v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.03956v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>