<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-11-08</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2311_03722v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03722v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03722v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计和同步定位与映射（SLAM）已被研究为计算机视觉和机器人领域最重要的任务之一，为自主导航和增强现实系统做出贡献。在基于特征的里程计/SLAM的情况下，移动的视觉传感器从不同的视点观察一组3D点，通常通过特征跟踪和匹配来建立每个图像中投影的2D点之间的对应关系。然而，由于对应点可能是错误的和有噪声的，可靠的不确定性估计可以提高里程计/SLAM方法的准确性。此外，惯性测量单元用于帮助视觉传感器进行视觉惯性融合。在本文中，我们提出了一种使用惯性制导来估计特征对应的不确定性的方法，该惯性制导对运动模糊、照明变化和遮挡引起的图像退化具有鲁棒性。对制导分布进行建模，以采样可能的对应关系，我们将该分布拟合为基于图像误差的能量函数，产生比传统方法更稳健的不确定性。我们还通过将我们的方法纳入最近的一种用于公共数据集的视觉惯性里程计/SLAM算法来证明我们的方法的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03722v1" target="_blank">2311.03722v1</a>
                              </td>
                              <td>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</td>
                              <td>Seongwook Yoon</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03722v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03722v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02831v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02831v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02831v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02831v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02831v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环作为SLAM的关键组成部分之一，在纠正累积误差方面起着至关重要的作用。传统的基于外观的方法，如单词袋模型，通常受到局部2D特征和训练数据量的限制，这使得它们在现实世界场景中的通用性和鲁棒性较差，导致环路闭合中的漏检测或误报检测。为了解决这些问题，我们首先提出了一种基于多级验证的对象级数据关联方法，该方法可以将当前帧的2D语义特征与地图的3D对象地标相关联。接下来，利用这些关联关系，我们介绍了一种基于二次对象映射拓扑的语义闭环方法，该方法通过对象的拓扑图来表示场景，并通过比较拓扑图中的差异来实现宽视场下的精确闭环。最后，我们将这两种方法集成到一个完整的对象感知SLAM系统中。定性实验和消融研究证明了所提出的对象级数据关联算法的有效性和稳健性。定量实验表明，我们的语义闭环方法在精度、召回率和定位精度指标方面优于现有的最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02831v2" target="_blank">2311.02831v2</a>
                              </td>
                              <td>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</td>
                              <td>Zhenzhong Cao</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02831v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02831v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03484v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03484v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Semi-autonomous mapping with GPS-guided aerial platforms that fly preplanned missions is already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and mission planning. This paper presents Osprey, an autonomous aerial mapping system with state-of-the-art multi-session mapping capabilities. It enables a non-expert operator to specify a bounded target area that the aerial platform can then map autonomously, over multiple flights if necessary. Field experiments with Osprey demonstrate that this system can achieve greater map coverage of large industrial sites than manual surveys with a pilot-flown aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in separate missions using $112$ minutes of autonomous flight time. True colour maps were created from images captured by Osprey using pointcloud and NeRF reconstruction methods. These maps provide useful data for structural inspection tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03484v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>航空测绘系统对于许多测量应用（例如工业检查或农业监测）都很重要。使用GPS引导的空中平台执行预先计划的任务的半自主地图已经广泛可用，但完全自主的系统可以显著提高效率。自动绘制复杂的三维结构需要一个执行在线绘制和任务规划的系统。本文介绍了Osprey，这是一个具有最先进的多会话地图功能的自主航空地图系统。它使非专家操作员能够指定一个有界目标区域，然后空中平台可以在必要时通过多次飞行自主绘制该区域的地图。Osprey的现场实验表明，与使用飞行员飞行的空中平台或地面激光扫描仪（TLS）进行手动测量相比，该系统可以实现更大的大型工业场地地图覆盖范围。三个地点的总地面覆盖面积为7085美元，最高高度为2700美元，在单独的任务中使用112美元的自主飞行时间绘制了地图。使用点云和NeRF重建方法，从鱼鹰捕获的图像中创建了真实的彩色地图。这些地图为结构检查任务提供了有用的数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03484v1" target="_blank">2311.03484v1</a>
                              </td>
                              <td>Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</td>
                              <td>Rowan Border</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03484v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03484v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02327v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02327v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640*480, 346*260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations. The dataset is available at https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02327v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用多个传感器可以增强复杂的环境感知，并提高对不同亮度条件和高速运动模式的弹性，从而实现精确的定位和映射。本文提出了ECMD，这是一个以事件为中心的多传感器数据集，包含81个序列，覆盖了200多公里的各种具有挑战性的驾驶场景，包括高速运动、重复场景、动态物体等。ECMD提供了来自两组不同分辨率的立体事件相机（640*480346*260）、立体工业相机、红外相机，一个顶部安装的机械激光雷达，带有两个倾斜的激光雷达、两个消费者级GNSS接收器和一个机载IMU。同时，使用厘米级高精度GNSS-RTK/INS导航系统获得了车辆的地面实况。所有传感器都经过了良好的校准，并在硬件级别上进行了时间同步，同时记录数据。我们还评估了几种最先进的SLAM算法，用于对视觉和激光雷达SLAM进行基准测试，并确定其局限性。数据集位于https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02327v1" target="_blank">2311.02327v1</a>
                              </td>
                              <td>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</td>
                              <td>Peiyu Chen</td>
                              <td>2023-11-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02327v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02327v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18917v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18917v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18917v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>先前将神经辐射场（NeRF）集成到同步定位和映射（SLAM）框架中的尝试要么依赖于静态场景的假设，要么将动态对象视为异常值。然而，现实世界中的大多数场景都是动态的。在本文中，我们提出了一种时变表示来跟踪和重建动态场景。我们的系统同时维护两个过程，跟踪过程和映射过程。对于跟踪过程，对整个输入图像进行均匀采样，并对RGB图像的训练进行自监督。对于映射过程，我们利用已知遮罩来区分动态对象和静态背景，并对两种类型的区域应用不同的采样策略。两个过程的参数优化由两个阶段组成，第一阶段将时间与3D位置相关联，以将变形场转换为规范场。第二种方法将时间与规范场中的三维位置相关联，以获得颜色和符号距离函数（SDF）。此外，我们还提出了一种新的基于重叠率的关键帧选择策略。我们在两个公开可用的合成数据集上评估了我们的方法，并验证了与当前最先进的动态映射方法相比，我们的方法更有效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18917v2" target="_blank">2310.18917v2</a>
                              </td>
                              <td>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</td>
                              <td>Chengyao Duan</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18917v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18917v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展了我们以前只关注全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v1" target="_blank">2311.00928v1</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00276v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00276v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent decades, the field of robotic mapping has witnessed widespread research and development in LiDAR (Light Detection And Ranging)-based simultaneous localization and mapping (SLAM) techniques. In this paper, we review the state-of-the-art in LiDAR-based SLAM and explore the remaining challenges that still require attention to satisfy the needs of contemporary applications. A distinctive aspect of this study lies in its literature survey, which specifically investigates the application of various types and configurations of LiDAR, setting it apart from prior reviews. Furthermore, several representative comparisons of LiDAR-based SLAM algorithms are presented, which can serve as a point of reference. Finally, the paper concludes with an insightful discussion on the emergence of new frontiers in the domain of LiDAR-based SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00276v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近几十年来，基于激光雷达（Light Detection and Ranging）的同时定位和测绘（SLAM）技术在机器人测绘领域得到了广泛的研究和发展。在本文中，我们回顾了基于激光雷达的SLAM的最新技术，并探讨了满足当代应用需求仍需关注的剩余挑战。这项研究的一个独特之处在于其文献调查，该调查专门调查了各种类型和配置的激光雷达的应用，使其与先前的综述不同。此外，还对基于激光雷达的SLAM算法进行了一些有代表性的比较，可供参考。最后，本文对基于激光雷达的SLAM领域出现的新前沿进行了深入的讨论。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00276v1" target="_blank">2311.00276v1</a>
                              </td>
                              <td>LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</td>
                              <td>Xiangdi Yue</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00276v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00276v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_02257v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Perception System for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_02257v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent surge in interest in autonomous driving stems from its rapidly developing capacity to enhance safety, efficiency, and convenience. A pivotal aspect of autonomous driving technology is its perceptual systems, where core algorithms have yielded more precise algorithms applicable to autonomous driving, including vision-based Simultaneous Localization and Mapping (SLAMs), object detection, and tracking algorithms. This work introduces a visual-based perception system for autonomous driving that integrates trajectory tracking and prediction of moving objects to prevent collisions, while addressing autonomous driving's localization and mapping requirements. The system leverages motion cues from pedestrians to monitor and forecast their movements and simultaneously maps the environment. This integrated approach resolves camera localization and the tracking of other moving objects in the scene, subsequently generating a sparse map to facilitate vehicle navigation. The performance, efficiency, and resilience of this approach are substantiated through comprehensive evaluations of both simulated and real-world datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_02257v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近人们对自动驾驶的兴趣激增，源于其快速发展的提高安全性、效率和便利性的能力。自动驾驶技术的一个关键方面是其感知系统，其中的核心算法产生了适用于自动驾驶的更精确的算法，包括基于视觉的同步定位和映射（SLAM）、物体检测和跟踪算法。这项工作介绍了一种基于视觉的自动驾驶感知系统，该系统集成了运动物体的轨迹跟踪和预测，以防止碰撞，同时满足自动驾驶的定位和映射要求。该系统利用行人的运动提示来监测和预测他们的运动，同时绘制环境地图。这种集成方法解决了摄像机定位和场景中其他移动物体的跟踪问题，随后生成稀疏地图以便于车辆导航。通过对模拟和真实世界数据集的全面评估，证实了这种方法的性能、效率和弹性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.02257v2" target="_blank">2303.02257v2</a>
                              </td>
                              <td>Visual Perception System for Autonomous Driving</td>
                              <td>Qi Zhang</td>
                              <td>2023-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_02257v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.02257v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_05927v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_05927v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature extraction and matching are the basic parts of many robotic vision tasks, such as 2D or 3D object detection, recognition, and registration. As known, 2D feature extraction and matching have already been achieved great success. Unfortunately, in the field of 3D, the current methods fail to support the extensive application of 3D LiDAR sensors in robotic vision tasks, due to the poor descriptiveness and inefficiency. To address this limitation, we propose a novel 3D feature representation method: Linear Keypoints representation for 3D LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully considers the characteristics (such as the sparsity, and complexity of scenes) of LiDAR point clouds, and represents the keypoint with its robust neighbor keypoints, which provide strong distinction in the description of the keypoint. The proposed LinK3D has been evaluated on two public datasets (i.e., KITTI, Steven VLP16), and the experimental results show that our method greatly outperforms the state-of-the-art in matching performance. More importantly, LinK3D shows excellent real-time performance, faster than the sensor frame rate at 10 Hz of a typical rotating LiDAR sensor. LinK3D only takes an average of 32 milliseconds to extract features from the point cloud collected by a 64-beam LiDAR, and takes merely about 8 milliseconds to match two LiDAR scans when executed in a notebook with an Intel Core i7 @2.2 GHz processor. Moreover, our method can be widely extended to various 3D vision applications. In this paper, we apply the proposed LinK3D to the LiDAR odometry and place recognition task of LiDAR SLAM. The experimental results show that our method can improve the efficiency and accuracy of LiDAR SLAM system.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_05927v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征提取和匹配是许多机器人视觉任务的基本部分，例如2D或3D对象检测、识别和配准。众所周知，二维特征提取和匹配已经取得了巨大的成功。不幸的是，在3D领域，由于描述性差和效率低，目前的方法无法支持3D激光雷达传感器在机器人视觉任务中的广泛应用。为了解决这一限制，我们提出了一种新的3D特征表示方法：3D激光雷达点云的线性关键点表示，称为LinK3D。LinK3D的新颖之处在于，它充分考虑了激光雷达点云的特性（如场景的稀疏性和复杂性），并用其稳健的邻居关键点来表示关键点，这在关键点的描述中提供了很强的区分。所提出的LinK3D已经在两个公共数据集（即KITTI、Steven VLP16）上进行了评估，实验结果表明，我们的方法在匹配性能上大大优于最先进的方法。更重要的是，LinK3D显示出出色的实时性能，比典型旋转激光雷达传感器在10Hz下的传感器帧速率更快。LinK3D从64束激光雷达收集的点云中提取特征平均只需32毫秒，在使用英特尔酷睿i7@2.2 GHz处理器的笔记本电脑中执行时，匹配两次激光雷达扫描仅需约8毫秒。此外，我们的方法可以广泛扩展到各种三维视觉应用中。在本文中，我们将所提出的LinK3D应用于激光雷达SLAM的测距和位置识别任务。实验结果表明，该方法可以提高激光雷达SLAM系统的效率和精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.05927v2" target="_blank">2206.05927v2</a>
                              </td>
                              <td>LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</td>
                              <td>Yunge Cui</td>
                              <td>2022-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_05927v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.05927v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19400v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed multi-agent magnetic field norm SLAM with Gaussian processes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19400v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19400v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19400v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately estimating the positions of multi-agent systems in indoor environments is challenging due to the lack of Global Navigation Satelite System (GNSS) signals. Noisy measurements of position and orientation can cause the integrated position estimate to drift without bound. Previous research has proposed using magnetic field simultaneous localization and mapping (SLAM) to compensate for position drift in a single agent. Here, we propose two novel algorithms that allow multiple agents to apply magnetic field SLAM using their own and other agents measurements.   Our first algorithm is a centralized approach that uses all measurements collected by all agents in a single extended Kalman filter. This algorithm simultaneously estimates the agents position and orientation and the magnetic field norm in a central unit that can communicate with all agents at all times. In cases where a central unit is not available, and there are communication drop-outs between agents, our second algorithm is a distributed approach that can be employed.   We tested both algorithms by estimating the position of magnetometers carried by three people in an optical motion capture lab with simulated odometry and simulated communication dropouts between agents. We show that both algorithms are able to compensate for drift in a case where single-agent SLAM is not. We also discuss the conditions for the estimate from our distributed algorithm to converge to the estimate from the centralized algorithm, both theoretically and experimentally.   Our experiments show that, for a communication drop-out rate of 80 percent, our proposed distributed algorithm, on average, provides a more accurate position estimate than single-agent SLAM. Finally, we demonstrate the drift-compensating abilities of our centralized algorithm on a real-life pedestrian localization problem with multiple agents moving inside a building.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19400v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于缺乏全球导航卫星系统（GNSS）信号，准确估计多智能体系统在室内环境中的位置具有挑战性。位置和方向的噪声测量可能导致积分位置估计漂移而不受约束。先前的研究已经提出使用磁场同时定位和映射（SLAM）来补偿单个智能体中的位置漂移。在这里，我们提出了两种新的算法，允许多个代理使用他们自己和其他代理的测量来应用磁场SLAM。我们的第一个算法是一种集中式方法，它使用所有代理在单个扩展卡尔曼滤波器中收集的所有测量值。该算法同时估计可以在任何时候与所有代理通信的中央单元中的代理位置和方向以及磁场范数。在中央单元不可用，并且代理之间存在通信中断的情况下，我们的第二种算法是可以使用的分布式方法。我们在光学运动捕捉实验室中用模拟里程计和模拟特工之间的通信中断来估计三个人携带的磁力计的位置，从而测试了这两种算法。我们证明，在单代理SLAM不可用的情况下，这两种算法都能够补偿漂移。我们还从理论和实验上讨论了分布式算法的估计收敛于集中式算法的估计的条件。我们的实验表明，对于80%的通信丢失率，我们提出的分布式算法平均比单代理SLAM提供了更准确的位置估计。最后，我们在多个智能体在建筑物内移动的真实行人定位问题上展示了我们的集中式算法的漂移补偿能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19400v1" target="_blank">2310.19400v1</a>
                              </td>
                              <td>Distributed multi-agent magnetic field norm SLAM with Gaussian processes</td>
                              <td>Frida Viset</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19400v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19400v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18697v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">KernelGPA: A Globally Optimal Solution to Deformable SLAM in Closed-form</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18697v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18697v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18697v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the generalized Procrustes analysis (GPA), as a minimal formulation to the simultaneous localization and mapping (SLAM) problem. We propose KernelGPA, a novel global registration technique to solve SLAM in the deformable environment. We propose the concept of deformable transformation which encodes the entangled pose and deformation. We define deformable transformations using a kernel method, and show that both the deformable transformations and the environment map can be solved globally in closed-form, up to global scale ambiguities. We solve the scale ambiguities by an optimization formulation that maximizes rigidity. We demonstrate KernelGPA using the Gaussian kernel, and validate the superiority of KernelGPA with various datasets. Code and data are available at \url{https://bitbucket.org/FangBai/deformableprocrustes}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18697v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了广义Procrustes分析（GPA），作为同时定位和映射（SLAM）问题的一个极小公式。我们提出了KernelGPA，一种新的全局配准技术来解决可变形环境中的SLAM。我们提出了可变形变换的概念，它对纠缠的姿态和变形进行编码。我们使用核方法定义了可变形变换，并表明可变形变换和环境映射都可以以闭合形式全局求解，直到全局尺度的模糊度。我们通过使刚度最大化的优化公式来解决尺度模糊性。我们使用高斯核演示了KernelGPA，并用各种数据集验证了KernelGPA的优越性。代码和数据位于\url{https://bitbucket.org/FangBai/deformableprocrustes}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18697v1" target="_blank">2310.18697v1</a>
                              </td>
                              <td>KernelGPA: A Globally Optimal Solution to Deformable SLAM in Closed-form</td>
                              <td>Fang Bai</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18697v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18697v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_17879v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Split Covariance Intersection Filter Based Visual Localization With Accurate AprilTag Map For Warehouse Robot Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_17879v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_17879v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_17879v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate and efficient localization with conveniently-established map is the fundamental requirement for mobile robot operation in warehouse environments. An accurate AprilTag map can be conveniently established with the help of LiDAR-based SLAM. It is true that a LiDAR-based system is usually not commercially competitive in contrast with a vision-based system, yet fortunately for warehouse applications, only a single LiDAR-based SLAM system is needed to establish an accurate AprilTag map, whereas a large amount of visual localization systems can share this established AprilTag map for their own operations. Therefore, the cost of a LiDAR-based SLAM system is actually shared by the large amount of visual localization systems, and turns to be acceptable and even negligible for practical warehouse applications. Once an accurate AprilTag map is available, visual localization is realized as recursive estimation that fuses AprilTag measurements (i.e. AprilTag detection results) and robot motion data. AprilTag measurements may be nonlinear partial measurements; this can be handled by the well-known extended Kalman filter (EKF) in the spirit of local linearization. AprilTag measurements tend to have temporal correlation as well; however, this cannot be reasonably handled by the EKF. The split covariance intersection filter (Split CIF) is adopted to handle temporal correlation among AprilTag measurements. The Split CIF (in the spirit of local linearization) can also handle AprilTag nonlinear partial measurements. The Split CIF based visual localization system incorporates a measurement adaptive mechanism to handle outliers in AprilTag measurements and adopts a dynamic initialization mechanism to address the kidnapping problem. A comparative study in real warehouse environments demonstrates the potential and advantage of the Split CIF based visual localization solution.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_17879v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用方便的地图进行准确高效的定位是移动机器人在仓库环境中操作的基本要求。借助基于激光雷达的SLAM，可以方便地建立准确的AprilTag地图。的确，与基于视觉的系统相比，基于激光雷达的系统通常在商业上没有竞争力，但幸运的是，对于仓库应用来说，只需要一个基于激光DAR的SLAM系统就可以建立准确的AprilTag地图，而大量的视觉定位系统可以共享这个建立的April Tag地图来进行自己的操作。因此，基于激光雷达的SLAM系统的成本实际上由大量的视觉定位系统分担，并且对于实际的仓库应用来说是可以接受的，甚至可以忽略不计。一旦精确的AprilTag地图可用，视觉定位就被实现为融合AprilTag测量（即AprilTag-检测结果）和机器人运动数据的递归估计。AprilTag测量可以是非线性部分测量；这可以通过众所周知的扩展卡尔曼滤波器（EKF）本着局部线性化的精神来处理。AprilTag测量也往往具有时间相关性；然而，EKF无法合理地处理这一问题。采用分割协方差交集滤波器（split CIF）来处理AprilTag测量之间的时间相关性。Split CIF（本着局部线性化的精神）也可以处理AprilTag非线性部分测量。基于分割CIF的视觉定位系统结合了测量自适应机制来处理AprilTag测量中的异常值，并采用动态初始化机制来解决绑架问题。在真实仓库环境中进行的比较研究表明了基于拆分CIF的视觉定位解决方案的潜力和优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.17879v1" target="_blank">2310.17879v1</a>
                              </td>
                              <td>Split Covariance Intersection Filter Based Visual Localization With Accurate AprilTag Map For Warehouse Robot Navigation</td>
                              <td>Susu Fang</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_17879v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.17879v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15023v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15023v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15023v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets will be made public to facilitate further development in the field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15023v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过一种利用学习特征进行声纳图像对应的新方法来解决水下SLAM的数据关联这一具有挑战性的问题。我们介绍了SONIC（SONar图像对应），这是一种姿态监督网络，旨在产生能够承受视点变化的鲁棒特征对应。水下环境固有的复杂性源于动态和经常有限的能见度条件，将视野限制在几米的范围内，通常没有特征。这使得基于摄像头的系统在大多数开放水域应用场景中都不理想。因此，多波束成像声纳成为感知传感器的首选。然而，它们也并非没有局限性。虽然与相机相比，成像声纳提供了卓越的远程可见性，但它们的测量结果在不同的视角下可能会有所不同。这种固有的可变性给数据关联带来了巨大的挑战，尤其是对于基于特征的方法。我们的方法在为声纳图像生成对应关系方面表现出了明显更好的性能，这将为更准确的闭环约束和基于声纳的位置识别铺平道路。代码以及模拟和真实世界的数据集将公开，以促进该领域的进一步发展。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15023v1" target="_blank">2310.15023v1</a>
                              </td>
                              <td>SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</td>
                              <td>Samiran Gode</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15023v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15023v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_07241v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ConceptFusion: Open-set Multimodal 3D Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_07241v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_07241v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.   We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.   For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_07241v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>构建环境的3D地图是机器人导航、规划以及与场景中对象交互的核心。大多数现有的将语义概念与3D地图集成的方法在很大程度上仍然局限于闭集设置：它们只能推理在训练时预定义的有限概念集。此外，只能使用类标签或在最近的工作中使用文本提示来查询这些映射。我们使用ConceptFusion解决了这两个问题，ConceptFusion-一种场景表示，它（1）基本上是开放集，能够超越封闭的概念集进行推理，（2）本质上是多模态的，能够对3D地图进行各种可能的查询，从语言到图像，从音频到3D几何，所有这些都协同工作。ConceptFusion利用当今在互联网规模数据上预先训练的基础模型的开放集功能，对自然语言、图像和音频等模态的概念进行推理。我们证明了像素对齐的开放集特征可以通过传统的SLAM和多视图融合方法融合到3D地图中。这实现了有效的零样本空间推理，不需要任何额外的训练或微调，并比监督方法更好地保留了长尾概念，在3D IoU上比它们高出40%以上。我们在许多真实世界的数据集、模拟家庭环境、真实世界的桌面操作任务和自动驾驶平台上广泛评估了ConceptFusion。我们展示了将基础模型与3D开放集多模式映射相结合的新途径。有关更多信息，请访问我们的项目页面https://concept-fusion.github.io或观看我们的5分钟解说视频https://www.youtube.com/watch?v=rkXgws8fiDs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.07241v3" target="_blank">2302.07241v3</a>
                              </td>
                              <td>ConceptFusion: Open-set Multimodal 3D Mapping</td>
                              <td>Krishna Murthy Jatavallabhula</td>
                              <td>2023-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_07241v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.07241v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14924v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Converting Depth Images and Point Clouds for Feature-based Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14924v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14924v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14924v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, depth sensors have become more and more affordable and have found their way into a growing amount of robotic systems. However, mono- or multi-modal sensor registration, often a necessary step for further processing, faces many challenges on raw depth images or point clouds. This paper presents a method of converting depth data into images capable of visualizing spatial details that are basically hidden in traditional depth images. After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion. Compared to Bearing Angle images, our method yields brighter, higher-contrast images with more visible contours and more details. We tested feature-based pose estimation of both conversions in a visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT, and SURF, our new Flexion images yield better results than Bearing Angle images and show great potential to bridge the gap between depth data and classical computer vision. Source code is available here: https://rlsch.github.io/depth-flexion-conversion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14924v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，深度传感器变得越来越便宜，并已进入越来越多的机器人系统。然而，单模态或多模态传感器配准通常是进一步处理的必要步骤，在原始深度图像或点云上面临许多挑战。本文提出了一种将深度数据转换为能够可视化传统深度图像中基本隐藏的空间细节的图像的方法。在去除噪声之后，点的邻域形成两个法向量，其差值被编码到这个新的转换中。与方位角图像相比，我们的方法产生了更明亮、对比度更高、轮廓更清晰、细节更多的图像。我们在视觉里程计任务和RGB-D SLAM中测试了两种转换的基于特征的姿态估计。对于所有测试的特征，AKAZE、ORB、SIFT和SURF，我们的新Flexion图像比方位角图像产生更好的结果，并显示出弥合深度数据和经典计算机视觉之间差距的巨大潜力。此处提供源代码：https://rlsch.github.io/depth-flexion-conversion.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14924v1" target="_blank">2310.14924v1</a>
                              </td>
                              <td>Converting Depth Images and Point Clouds for Feature-based Pose Estimation</td>
                              <td>Robert Lösch</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14924v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14924v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_15268v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ObVi-SLAM: Long-Term Object-Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_15268v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_15268v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_15268v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robots responsible for tasks over long time scales must be able to localize consistently and scalably amid geometric, viewpoint, and appearance changes. Existing visual SLAM approaches rely on low-level feature descriptors that are not robust to such environmental changes and result in large map sizes that scale poorly over long-term deployments. In contrast, object detections are robust to environmental variations and lead to more compact representations, but most object-based SLAM systems target short-term indoor deployments with close objects. In this paper, we introduce ObVi-SLAM to overcome these challenges by leveraging the best of both approaches. ObVi-SLAM uses low-level visual features for high-quality short-term visual odometry; and to ensure global, long-term consistency, ObVi-SLAM builds an uncertainty-aware long-term map of persistent objects and updates it after every deployment. By evaluating ObVi-SLAM on data from 16 deployment sessions spanning different weather and lighting conditions, we empirically show that ObVi-SLAM generates accurate localization estimates consistent over long-time scales in spite of varying appearance conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_15268v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>负责长时间尺度任务的机器人必须能够在几何、视点和外观变化中一致且可缩放地进行定位。现有的视觉SLAM方法依赖于低级别的特征描述符，这些特征描述符对这种环境变化不具有鲁棒性，并导致在长期部署中难以扩展的大地图大小。相比之下，对象检测对环境变化是鲁棒的，并导致更紧凑的表示，但大多数基于对象的SLAM系统针对的是具有近距离对象的短期室内部署。在本文中，我们介绍了ObVi SLAM，通过利用这两种方法中的最佳方法来克服这些挑战。ObVi SLAM使用低水平的视觉特征进行高质量的短期视觉里程计；为了确保全局、长期的一致性，ObVi SLAM构建了一个持久对象的不确定性感知长期映射，并在每次部署后进行更新。通过对跨越不同天气和照明条件的16个部署会话的数据进行ObVi SLAM评估，我们的经验表明，尽管外观条件不同，ObVi SLAM在长时间尺度上产生了一致的准确定位估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.15268v2" target="_blank">2309.15268v2</a>
                              </td>
                              <td>ObVi-SLAM: Long-Term Object-Visual SLAM</td>
                              <td>Amanda Adkins</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_15268v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.15268v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08856v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08856v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08856v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08856v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel framework for RGB-based category-level 6D object pose and size estimation. Our approach relies on the prediction of normalized object coordinate space (NOCS), which serves as an efficient and effective object canonical representation that can be extracted from RGB images. Unlike previous approaches that heavily relied on additional depth readings as input, our novelty lies in leveraging multi-view information, which is commonly available in practical scenarios where a moving camera continuously observes the environment. By introducing multi-view constraints, we can obtain accurate camera pose and depth estimation from a monocular dense SLAM framework. Additionally, by incorporating constraints on the camera relative pose, we can apply trimming strategies and robust pose averaging on the multi-view object poses, resulting in more accurate and robust estimations of category-level object poses even in the absence of direct depth readings. Furthermore, we introduce a novel NOCS prediction network that significantly improves performance. Our experimental results demonstrate the strong performance of our proposed method, even comparable to state-of-the-art RGB-D methods across public dataset sequences. Additionally, we showcase the generalization ability of our method by evaluating it on self-collected datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08856v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于RGB的类别级6D对象姿态和大小估计框架。我们的方法依赖于归一化对象坐标空间（NOCS）的预测，它是一种有效的对象规范表示，可以从RGB图像中提取。与以前严重依赖额外深度读数作为输入的方法不同，我们的新颖之处在于利用多视图信息，这在移动相机连续观察环境的实际场景中很常见。通过引入多视图约束，我们可以从单目密集SLAM框架中获得准确的相机姿态和深度估计。此外，通过结合对相机相对姿态的约束，我们可以对多视图对象姿态应用修剪策略和稳健的姿态平均，即使在没有直接深度读数的情况下，也可以对类别级对象姿态进行更准确和稳健的估计。此外，我们还介绍了一种新的NOCS预测网络，该网络显著提高了性能。我们的实验结果证明了我们提出的方法的强大性能，甚至可以与公共数据集序列中最先进的RGB-D方法相媲美。此外，我们通过在自己收集的数据集上评估我们的方法，展示了它的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08856v2" target="_blank">2308.08856v2</a>
                              </td>
                              <td>MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose and Size Estimation</td>
                              <td>Jiaqi Yang</td>
                              <td>2023-08-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08856v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08856v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13768v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PACE: Human and Camera Motion Estimation from in-the-wild Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13768v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13768v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13768v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13768v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从移动摄像机中估计全局场景中人类运动的方法。由于视频中人和摄像机运动的耦合，这是一项极具挑战性的任务。为了解决这个问题，我们提出了一个联合优化框架，该框架使用前景人类运动先验和背景场景特征来解开人类和相机的运动。与使用SLAM作为初始化的现有方法不同，我们建议在受束调整启发的优化中紧密集成SLAM和人体运动先验。具体来说，我们优化人体和相机的运动，以匹配观察到的人体姿势和场景特征。这种设计结合了SLAM和运动先验的优势，显著改进了人体和相机的运动估计。我们还引入了一种适用于批量优化的运动先验，使我们的方法比现有方法更有效。最后，我们提出了一个新的合成数据集，该数据集除了可以从动态视频中评估人体运动外，还可以评估相机运动。在合成和真实世界的RICH数据集上的实验表明，我们的方法在恢复人体和相机运动方面大大优于现有技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13768v1" target="_blank">2310.13768v1</a>
                              </td>
                              <td>PACE: Human and Camera Motion Estimation from in-the-wild Videos</td>
                              <td>Muhammed Kocabas</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13768v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13768v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13324v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs' Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13324v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13324v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13324v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Perception is necessary for autonomous navigation in an unknown area crowded with obstacles. It's challenging for a robot to navigate safely without any sensors that can sense the environment, resulting in a $\textit{blind}$ robot, and becomes more difficult when comes to a group of robots. However, it could be costly to equip all robots with expensive perception or SLAM systems. In this paper, we propose a novel system named $\textbf{ColAG}$, to solve the problem of autonomous navigation for a group of $\textit{blind}$ UGVs by introducing cooperation with one UAV, which is the only robot that has full perception capabilities in the group. The UAV uses SLAM for its odometry and mapping while sharing this information with UGVs via limited relative pose estimation. The UGVs plan their trajectories in the received map and predict possible failures caused by the uncertainty of its wheel odometry and unknown risky areas. The UAV dynamically schedules waypoints to prevent UGVs from collisions, formulated as a Vehicle Routing Problem with Time Windows to optimize the UAV's trajectories and minimize time when UGVs have to wait to guarantee safety. We validate our system through extensive simulation with up to 7 UGVs and real-world experiments with 3 UGVs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13324v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>感知是在充满障碍物的未知区域进行自主导航所必需的。对于一个机器人来说，在没有任何可以感知环境的传感器的情况下安全导航是一项挑战，这导致了$\textit｛blind｝$机器人，当涉及到一组机器人时，这变得更加困难。然而，为所有机器人配备昂贵的感知或SLAM系统可能成本高昂。在本文中，我们提出了一个名为$\textbf｛ColAG｝$的新系统，通过引入与一个无人机的合作来解决一组$\textit｛blind｝$无人值守地面飞行器的自主导航问题，该无人机是该组无人机中唯一具有完全感知能力的机器人。无人机使用SLAM进行里程测量和测绘，同时通过有限的相对姿态估计与UGV共享这些信息。UGV在收到的地图中规划其轨迹，并预测其车轮里程计的不确定性和未知风险区域可能导致的故障。无人机动态调度航路点，以防止无人值守地面飞行器发生碰撞，该问题被定义为具有时间窗口的车辆路线问题，以优化无人机的轨迹，并在无人值守地面车辆必须等待以确保安全时将时间减至最少。我们通过对多达7辆无人值守地面车辆的广泛模拟和对3辆无人值守地下车辆的真实世界实验来验证我们的系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13324v1" target="_blank">2310.13324v1</a>
                              </td>
                              <td>ColAG: A Collaborative Air-Ground Framework for Perception-Limited UGVs' Navigation</td>
                              <td>Zhehan Li</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13324v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13324v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13256v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Higher or Lower: Challenges in Object based SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13256v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13256v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13256v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping, as a fundamental task in computer vision, has gained higher demands for performance in recent years due to the rapid development of autonomous driving and unmanned aerial vehicles. Traditional SLAM algorithms highly rely on basic geometry features such as points and lines, which are susceptible to environment. Conversely, higher-level object features offer richer information that is crucial for enhancing the overall performance of the framework. However, the effective utilization of object features necessitates careful consideration of various challenges, including complexity and process velocity. Given the advantages and disadvantages of both high-level object feature and low-level geometry features, it becomes essential to make informed choices within the SLAM framework. Taking these factors into account, this paper provides a thorough comparison between geometry features and object features, analyzes the current mainstream application methods of object features in SLAM frameworks, and presents a comprehensive overview of the main challenges involved in object-based SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13256v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着自动驾驶和无人机的快速发展，同时定位和绘制地图作为计算机视觉的一项基础任务，对性能提出了更高的要求。传统的SLAM算法高度依赖于点和线等基本几何特征，这些特征容易受到环境的影响。相反，更高级的对象特征提供了更丰富的信息，这对于提高框架的整体性能至关重要。然而，有效利用对象特征需要仔细考虑各种挑战，包括复杂性和处理速度。考虑到高级对象特征和低级几何特征的优缺点，在SLAM框架内做出明智的选择变得至关重要。考虑到这些因素，本文对几何特征和对象特征进行了深入的比较，分析了SLAM框架中当前主流的对象特征应用方法，并全面概述了基于对象的SLAM所面临的主要挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13256v1" target="_blank">2310.13256v1</a>
                              </td>
                              <td>Higher or Lower: Challenges in Object based SLAM</td>
                              <td>Zhihe Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13256v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13256v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11645v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11645v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11645v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11645v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given that a conventional laparoscope only provides a two-dimensional (2-D) view, the detection and diagnosis of medical ailments can be challenging. To overcome the visual constraints associated with laparoscopy, the use of laparoscopic images and videos to reconstruct the three-dimensional (3-D) anatomical structure of the abdomen has proven to be a promising approach. Neural Radiance Fields (NeRFs) have recently gained attention thanks to their ability to generate photorealistic images from a 3-D static scene, thus facilitating a more comprehensive exploration of the abdomen through the synthesis of new views. This distinguishes NeRFs from alternative methods such as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this paper, we present a comprehensive examination of NeRFs in the context of laparoscopy surgical videos, with the goal of rendering abdominal scenes in 3-D. Although our experimental results are promising, the proposed approach encounters substantial challenges, which require further exploration in future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11645v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>考虑到传统腹腔镜仅提供二维（2-D）视图，对医疗疾病的检测和诊断可能具有挑战性。为了克服与腹腔镜相关的视觉限制，使用腹腔镜图像和视频重建腹部的三维解剖结构已被证明是一种很有前途的方法。神经辐射场（NeRF）最近因其能够从三维静态场景中生成真实感图像而受到关注，从而通过合成新视图促进对腹部的更全面探索。这将NeRF与诸如同时定位和映射（SLAM）和深度估计之类的替代方法区分开来。在本文中，我们在腹腔镜手术视频的背景下对NeRFs进行了全面的检查，目的是在3D中呈现腹部场景。尽管我们的实验结果很有希望，但所提出的方法遇到了巨大的挑战，需要在未来的研究中进一步探索。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11645v1" target="_blank">2310.11645v1</a>
                              </td>
                              <td>Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs</td>
                              <td>Khoa Tuan Nguyen</td>
                              <td>2023-10-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11645v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11645v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11598v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11598v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11598v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11598v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11598v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习神经隐式表示在多视图图像的三维重建中取得了显著的性能。当前的方法使用体积渲染将隐式表示渲染为RGB或深度图像，这些图像由多视图地面实况监督。然而，每次渲染视图都会遇到孔的深度不完整以及深度监督对遮挡结构的不了解，这严重影响了通过体绘制进行几何推断的准确性。为了解决这个问题，我们建议通过具有注意深度融合先验的体绘制，从多视图RGBD图像中学习神经隐式表示。我们的先验允许神经网络从从可用于渲染的所有深度图像中融合的截断有符号距离函数（TSDF）中感知粗略的3D结构。TSDF能够访问一个深度图像上孔的缺失深度以及当前视图中不可见的遮挡部分。通过引入一种新的注意力机制，我们允许神经网络直接使用具有推断占用率的深度融合先验作为学习的内隐函数。在同步定位和映射（SLAM）的背景下，我们的注意力机制与表示整个场景的一次性融合TSDF或表示部分场景的增量融合TSDF一起工作。我们对广泛使用的基准测试（包括合成扫描和真实世界扫描）的评估表明，我们优于最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11598v1" target="_blank">2310.11598v1</a>
                              </td>
                              <td>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</td>
                              <td>Pengchong Hu</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11598v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11598v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2104_08634v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2104_08634v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2104_08634v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2104_08634v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Drones will revolutionize 3D modeling. A 3D model represents an accurate reconstruction of an object or structure. This paper explores the design and implementation of ARES, which provides near real-time, accurate reconstruction of 3D models using a drone-mounted LiDAR; such a capability can be useful to document construction or check aircraft integrity between flights. Accurate reconstruction requires high drone positioning accuracy, and, because GPS can be in accurate, ARES uses SLAM. However, in doing so it must deal with several competing constraints: drone battery and compute resources, SLAM error accumulation, and LiDAR resolution. ARES uses careful trajectory design to find a sweet spot in this constraint space, a fast reconnaissance flight to narrow the search area for structures, and offloads expensive computations to the cloud by streaming compressed LiDAR data over LTE. ARES reconstructs large structures to within 10s of cms and incurs less than 100 ms compute latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2104_08634v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无人机将彻底改变3D建模。3D模型表示对象或结构的精确重建。本文探讨了ARES的设计和实现，该系统使用无人机安装的激光雷达提供近实时、准确的三维模型重建；这种能力可用于记录结构或检查飞行之间的飞机完整性。精确的重建需要高的无人机定位精度，而且由于GPS可以精确，ARES使用SLAM。然而，在这样做的过程中，它必须处理几个相互竞争的约束：无人机电池和计算资源、SLAM误差积累和激光雷达分辨率。ARES使用仔细的轨迹设计来在这个约束空间中找到一个最佳点，通过快速侦察飞行来缩小结构的搜索区域，并通过LTE上的压缩LiDAR数据流将昂贵的计算转移到云端。ARES将大型结构重建到10厘米以内，计算延迟小于100毫秒。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2104.08634v3" target="_blank">2104.08634v3</a>
                              </td>
                              <td>ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones</td>
                              <td>Fawad Ahmad</td>
                              <td>2021-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2104_08634v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2104.08634v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10931v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10931v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10931v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10931v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a new benchmark dataset, Open-Structure, for evaluating visual odometry and SLAM methods, which directly equips point and line measurements, correspondences, structural associations, and co-visibility factor graphs instead of providing raw images. Based on the proposed benchmark dataset, these 2D or 3D data can be directly input to different stages of SLAM pipelines to avoid the impact of the data preprocessing modules in ablation experiments. First, we propose a dataset generator for real-world and simulated scenarios. In real-world scenes, it maintains the same observations and occlusions as actual feature extraction results. Those generated simulation sequences enhance the dataset's diversity by introducing various carefully designed trajectories and observations. Second, a SLAM baseline is proposed using our dataset to evaluate widely used modules in camera pose tracking, parametrization, and optimization modules. By evaluating these state-of-the-art algorithms across different scenarios, we discern each module's strengths and weaknesses within the camera tracking and optimization process. Our dataset and baseline are available at \url{https://github.com/yanyan-li/Open-Structure}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10931v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一个新的基准数据集Open Structure，用于评估视觉里程计和SLAM方法，该数据集直接配备点和线测量、对应关系、结构关联和共视因子图，而不是提供原始图像。基于所提出的基准数据集，这些2D或3D数据可以直接输入到SLAM管道的不同阶段，以避免消融实验中数据预处理模块的影响。首先，我们提出了一个用于真实世界和模拟场景的数据集生成器。在真实世界的场景中，它保持与实际特征提取结果相同的观察和遮挡。这些生成的模拟序列通过引入各种精心设计的轨迹和观测结果，增强了数据集的多样性。其次，使用我们的数据集提出了SLAM基线，以评估相机姿态跟踪、参数化和优化模块中广泛使用的模块。通过在不同场景中评估这些最先进的算法，我们可以在相机跟踪和优化过程中辨别出每个模块的优势和劣势。我们的数据集和基线位于\url{https://github.com/yanyan-li/Open-Structure}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10931v1" target="_blank">2310.10931v1</a>
                              </td>
                              <td>Open-Structure: a Structural Benchmark Dataset for SLAM Algorithms</td>
                              <td>Yanyan Li</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10931v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10931v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10862v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10862v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10862v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10862v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a system for creating building-scale, easily navigable 3D maps using mainstream smartphones. In our approach, we formulate the 3D-mapping problem as an instance of Graph SLAM and infer the position of both building landmarks (fiducial markers) and navigable paths through the environment (phone poses). Our results demonstrate the system's ability to create accurate 3D maps. Further, we highlight the importance of careful selection of mapping hyperparameters and provide a novel technique for tuning these hyperparameters to adapt our algorithm to new environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10862v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提供了一个使用主流智能手机创建建筑规模、易于导航的3D地图的系统。在我们的方法中，我们将3D映射问题公式化为Graph SLAM的一个实例，并推断建筑地标（基准标记）和环境中的可导航路径（电话姿势）的位置。我们的结果证明了该系统能够创建准确的3D地图。此外，我们强调了仔细选择映射超参数的重要性，并提供了一种新的技术来调整这些超参数，以使我们的算法适应新的环境。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10862v1" target="_blank">2310.10862v1</a>
                              </td>
                              <td>The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</td>
                              <td>Paul Ruvolo</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10862v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10862v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10290v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Autonomous Mapping and Navigation using Fiducial Markers and Pan-Tilt Camera for Assisting Indoor Mobility of Blind and Visually Impaired People</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10290v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10290v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10290v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large indoor spaces have complex layouts making them difficult to navigate. Indoor spaces in hospitals, universities, shopping complexes, etc., carry multi-modal information in the form of text and symbols. Hence, it is difficult for Blind and Visually Impaired (BVI) people to independently navigate such spaces. Indoor environments are usually GPS-denied; therefore, Bluetooth-based, WiFi-based, or Range-based methods are used for localization. These methods have high setup costs, lesser accuracy, and sometimes need special sensing equipment. We propose a Visual Assist (VA) system for the indoor navigation of BVI individuals using visual Fiducial markers for localization. State-of-the-art (SOTA) approaches for visual localization using Fiducial markers use fixed cameras having a narrow field of view. These approaches stop tracking the markers when they are out of sight. We employ a Pan-Tilt turret-mounted camera which enhances the field of view to 360{\deg} for enhanced marker tracking. We, therefore, need fewer markers for mapping and navigation. The efficacy of the proposed VA system is measured on three metrics, i.e., RMSE (Root Mean Square Error), ADNN (Average Distance to Nearest Neighbours), and ATE (Absolute Trajectory Error). Our system outperforms Hector-SLAM, ORB-SLAM3, and UcoSLAM. The proposed system achieves localization accuracy within $\pm8cm$ compared to $\pm12cm$ and $\pm10cm$ for ORB-SLAM3 and UcoSLAM, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10290v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型室内空间布局复杂，难以导航。医院、大学、购物中心等的室内空间以文本和符号的形式承载着多模态信息。因此，盲人和视障人士（BVI）很难独立导航这些空间。室内环境通常被GPS拒绝；因此，使用基于蓝牙、基于WiFi或基于范围的方法进行定位。这些方法的设置成本高，精度低，有时需要特殊的传感设备。我们提出了一种视觉辅助（VA）系统，用于BVI个人的室内导航，使用视觉基准标记进行定位。使用基准标记进行视觉定位的现有技术（SOTA）方法使用具有窄视场的固定相机。这些方法在标记不在视线范围内时停止跟踪标记。我们采用了一种安装在云台上的摄像机，它可以将视野提高到360度，以增强标记跟踪。因此，我们需要更少的标记来绘制地图和导航。所提出的VA系统的功效是在三个度量上测量的，即RMSE（均方根误差）、ADNN（到最近邻居的平均距离）和ATE（绝对轨迹误差）。我们的系统性能优于Hector SLAM、ORB-SLAM3和UcoSLAM。与ORB-SLAM3和UcoSLAM的$\pm12cm$和$\pm10cm$相比，所提出的系统在$\pm8cm$内实现了定位精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10290v1" target="_blank">2310.10290v1</a>
                              </td>
                              <td>Autonomous Mapping and Navigation using Fiducial Markers and Pan-Tilt Camera for Assisting Indoor Mobility of Blind and Visually Impaired People</td>
                              <td>Dharmateja Adapa</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10290v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10290v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16585v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16585v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16585v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16585v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be released on project page: http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16585v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时语义的可用性极大地提高了SLAM系统的核心几何功能，实现了许多机器人和AR/VR应用。我们提出了一种从RGB-D序列进行实时语义映射的新方法，该方法将2D神经网络和基于SLAM系统的3D网络与3D占用映射相结合。在分割新帧时，我们基于可微分渲染从先前帧执行潜在特征重新投影。与独立处理图像的基线相比，将来自先前帧的重新投影的特征图与当前帧特征融合可以大大提高图像分割质量。对于3D地图处理，我们提出了一种新的几何准平面过分割方法，该方法根据曲面法线对可能属于相同语义类的3D地图元素进行分组。我们还描述了一种用于轻量级语义图后处理的新型神经网络设计。我们的系统在基于2D-3D网络的系统中实现了最先进的语义映射质量，并在三个真实的室内数据集上匹配3D卷积网络的性能，同时实时工作。此外，与3D细胞神经网络相比，它显示出更好的跨传感器泛化能力，能够使用不同的深度传感器进行训练和推理。代码和数据将在项目页面上发布：http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16585v2" target="_blank">2306.16585v2</a>
                              </td>
                              <td>SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</td>
                              <td>Jingwen Wang</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16585v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16585v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_06950v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Active Metric-Semantic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_06950v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_06950v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_06950v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we address the problem of exploration and metric-semantic mapping of multi-floor GPS-denied indoor environments using Size Weight and Power (SWaP) constrained aerial robots. Most previous work in exploration assumes that robot localization is solved. However, neglecting the state uncertainty of the agent can ultimately lead to cascading errors both in the resulting map and in the state of the agent itself. Furthermore, actions that reduce localization errors may be at direct odds with the exploration task. We propose a framework that balances the efficiency of exploration with actions that reduce the state uncertainty of the agent. In particular, our algorithmic approach for active metric-semantic SLAM is built upon sparse information abstracted from raw problem data, to make it suitable for SWaP-constrained robots. Furthermore, we integrate this framework within a fully autonomous aerial robotic system that achieves autonomous exploration in cluttered, 3D environments. From extensive real-world experiments, we showed that by including Semantic Loop Closure (SLC), we can reduce the robot pose estimation errors by over 90% in translation and approximately 75% in yaw, and the uncertainties in pose estimates and semantic maps by over 70% and 65%, respectively. Although discussed in the context of indoor multi-floor exploration, our system can be used for various other applications, such as infrastructure inspection and precision agriculture where reliable GPS data may not be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_06950v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们讨论了使用尺寸、重量和功率（SWaP）约束的空中机器人对多层GPS拒绝的室内环境进行探索和度量语义映射的问题。以前的大多数探索工作都假设机器人定位已经解决。然而，忽略代理的状态不确定性最终会导致在生成的映射和代理本身的状态中出现级联错误。此外，减少定位误差的动作可能与探索任务直接不一致。我们提出了一个框架，该框架平衡了探索的效率和减少代理状态不确定性的行动。特别是，我们的主动度量语义SLAM算法方法建立在从原始问题数据中提取的稀疏信息的基础上，使其适用于SWaP约束的机器人。此外，我们将该框架集成在一个完全自主的空中机器人系统中，该系统可以在杂乱的3D环境中实现自主探索。从大量的真实世界实验中，我们表明，通过包括语义环闭合（SLC），我们可以将机器人姿态估计误差在平移时降低90%以上，在偏航时降低约75%，将姿态估计和语义图的不确定性分别降低70%和65%以上。尽管在室内多层勘探的背景下进行了讨论，但我们的系统可用于各种其他应用，如基础设施检查和精密农业，在这些应用中可能无法获得可靠的GPS数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.06950v2" target="_blank">2309.06950v2</a>
                              </td>
                              <td>3D Active Metric-Semantic SLAM</td>
                              <td>Yuezhan Tao</td>
                              <td>2023-09-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_06950v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.06950v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08082v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Jointly Optimized Global-Local Visual Localization of UAVs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08082v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08082v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08082v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), exhibit certain limitations in furnishing absolute coordinates and mitigating error accumulation. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching with ortho satellite images. However, doing so cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08082v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当全球导航卫星系统（GNSS）中断且不可靠时，无人机的导航和定位面临挑战。传统技术，如同时定位和映射（SLAM）和视觉里程计（VO），在提供绝对坐标和减少误差积累方面表现出一定的局限性。现有的视觉定位方法通过与卫星正射图像的匹配，实现了无误差积累的自主视觉定位。然而，由于匹配过程复杂，这样做并不能保证实时性能。为了应对这些挑战，我们提出了一种新的全球局部视觉定位（GLVL）网络。我们的GLVL网络是一种两阶段视觉定位方法，结合了一个大型检索模块和一个细粒度匹配模块，前者可以查找与无人机飞行场景相似的区域，后者可以定位精确的无人机坐标，实现实时精确的定位。以端到端的方式对训练过程进行联合优化，以进一步增强模型能力。在包括纹理丰富和纹理稀疏区域的六个无人机飞行场景上的实验证明了我们的模型能够实现无人机的实时精确定位要求。特别地，在具有稀疏纹理特征的村庄场景中，我们的方法在0.48秒内实现了仅2.39米的定位误差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08082v1" target="_blank">2310.08082v1</a>
                              </td>
                              <td>Jointly Optimized Global-Local Visual Localization of UAVs</td>
                              <td>Haoling Li</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08082v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08082v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_04466v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GRADE: Generating Realistic Animated Dynamic Environments for Robotics Research</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_04466v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_04466v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_04466v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, computer vision tasks like target tracking and human pose estimation have immensely benefited from synthetic data generation and novel rendering techniques. On the other hand, methods in robotics, especially for robot perception, have been slow to leverage these techniques. This is because state-of-the-art simulation frameworks for robotics lack either complete control, integration with the Robot Operating System (ROS), realistic physics or photorealism. To solve this, we present a fully customizable framework for generating realistic animated dynamic environments (GRADE) for robotics research, focused primarily at robot perception. The framework can be used either to generate ground truth data for robotic vision-related tasks and offline processing, or to experiment with robots online in dynamic environments. We build upon the Nvidia Isaac Sim to allow control of custom robots. We provide methods to include assets, populate and control the simulation, and process the data. Using autonomous robots in GRADE, we generate video datasets of an indoor dynamic environment. First, we use it to demonstrate the framework's visual realism by evaluating the sim-to-real gap through experiments with YOLO and Mask R-CNN. Second, we benchmark dynamic SLAM algorithms with this dataset. This not only shows that GRADE can significantly improve training performance and generalization to real sequences, but also highlights how current dynamic SLAM methods over-rely on known benchmarks, failing to generalize. We also introduce a method to precisely repeat a previously recorded experiment, while allowing changes in the surroundings of the robot. Code and data are provided as open-source at https://grade.is.tue.mpg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_04466v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，目标跟踪和人体姿态估计等计算机视觉任务极大地受益于合成数据生成和新颖的渲染技术。另一方面，机器人技术中的方法，特别是机器人感知的方法，在利用这些技术方面进展缓慢。这是因为最先进的机器人模拟框架缺乏完整的控制、与机器人操作系统（ROS）的集成、逼真的物理或真实感。为了解决这个问题，我们提出了一个完全可定制的框架，用于为机器人研究生成逼真的动画动态环境（GRADE），主要关注机器人感知。该框架可以用于为机器人视觉相关任务和离线处理生成地面实况数据，也可以用于在动态环境中对机器人进行在线实验。我们以英伟达Isaac Sim为基础，实现对定制机器人的控制。我们提供了包括资产、填充和控制模拟以及处理数据的方法。使用GRADE中的自主机器人，我们生成了室内动态环境的视频数据集。首先，我们通过YOLO和Mask R-CNN的实验评估模拟到真实的差距，用它来展示框架的视觉真实性。其次，我们用这个数据集对动态SLAM算法进行了基准测试。这不仅表明GRADE可以显著提高训练性能和对真实序列的泛化能力，还突出了当前动态SLAM方法如何过度依赖已知基准，而无法泛化。我们还介绍了一种方法，可以精确地重复之前记录的实验，同时允许机器人周围环境的变化。代码和数据以开源形式提供，网址为https://grade.is.tue.mpg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.04466v2" target="_blank">2303.04466v2</a>
                              </td>
                              <td>GRADE: Generating Realistic Animated Dynamic Environments for Robotics Research</td>
                              <td>Elia Bonetto</td>
                              <td>2023-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_04466v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.04466v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景中的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v2" target="_blank">2307.07763v2</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_05167v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with Negative Imaging Plane on Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_05167v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_05167v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_05167v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in the fields of autonomous driving and robotics. One crucial component of visual SLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a wider range of surrounding elements and features to be perceived. However, when the FoV of the camera reaches the negative half-plane, traditional methods for representing image feature points using [u,v,1]^T become ineffective. While the panoramic FoV is advantageous for loop closure, its benefits are not easily realized under large-attitude-angle differences where loop-closure frames cannot be easily matched by existing methods. As loop closure on wide-FoV panoramic data further comes with a large number of outliers, traditional outlier rejection methods are not directly applicable. To address these issues, we propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with extremely Large FoV with loop closure. A three-dimensional vector with unit length is introduced to effectively represent feature points even on the negative half-plane. The attitude information of the SLAM system is leveraged to guide the feature point detection of the loop closure. Additionally, a new outlier rejection method based on the unit length representation is integrated into the loop closure module. We collect the PALVIO dataset using a Panoramic Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}~120{\deg}) and an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to address the lack of panoramic SLAM datasets. Experiments on the established PALVIO and public datasets show that the proposed LF-VISLAM outperforms state-of-the-art SLAM methods. Our code will be open-sourced at https://github.com/flysoaryun/LF-VISLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_05167v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同步定位与映射（SLAM）已成为自动驾驶和机器人领域的一个重要方面。视觉SLAM的一个关键组成部分是相机的视场（FoV），因为更大的视场可以感知更广泛的周围元素和特征。然而，当相机的FoV到达负半平面时，使用[u，v，1]^T表示图像特征点的传统方法变得无效。虽然全景FoV有利于闭环，但在现有方法无法轻松匹配闭环框架的大姿态角差异下，其优点不容易实现。由于宽视场全景数据的闭环进一步带来了大量的异常值，传统的异常值剔除方法不直接适用。为了解决这些问题，我们提出了LF-VISLAM，这是一种视觉惯性SLAM框架，适用于具有环形闭合的超大视场的相机。引入单位长度的三维矢量，即使在负半平面上也能有效地表示特征点。利用SLAM系统的姿态信息来指导闭环的特征点检测。此外，在闭环模块中集成了一种基于单位长度表示的新的异常值抑制方法。我们使用全景环形透镜（PAL）系统和用于视觉惯性里程计（VIO）的惯性测量单元（IMU）收集PALVIO数据集，全景环形透镜系统的整个FoV为360｛\deg｝x（40｛\dig｝~120｛\deg｝），以解决全景SLAM数据集的缺乏问题。在已建立的PALVIO和公共数据集上的实验表明，所提出的LF-VISLAM优于最先进的SLAM方法。我们的代码将在https://github.com/flysoaryun/LF-VISLAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.05167v3" target="_blank">2209.05167v3</a>
                              </td>
                              <td>LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with Negative Imaging Plane on Mobile Agents</td>
                              <td>Ze Wang</td>
                              <td>2022-09-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_05167v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.05167v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07844v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07844v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07844v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07844v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel angular velocity estimation method to increase the robustness of Simultaneous Localization And Mapping (SLAM) algorithms against gyroscope saturations induced by aggressive motions. Field robotics expose robots to various hazards, including steep terrains, landslides, and staircases, where substantial accelerations and angular velocities can occur if the robot loses stability and tumbles. These extreme motions can saturate sensor measurements, especially gyroscopes, which are the first sensors to become inoperative. While the structural integrity of the robot is at risk, the resilience of the SLAM framework is oftentimes given little consideration. Consequently, even if the robot is physically capable of continuing the mission, its operation will be compromised due to a corrupted representation of the world. Regarding this problem, we propose a way to estimate the angular velocity using accelerometers during extreme rotations caused by tumbling. We show that our method reduces the median localization error by 71.5 % in translation and 65.5 % in rotation and reduces the number of SLAM failures by 73.3 % on the collected data. We also propose the Tumbling-Induced Gyroscope Saturation (TIGS) dataset, which consists of outdoor experiments recording the motion of a lidar subject to angular velocities four times higher than other available datasets. The dataset is available online at https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07844v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的角速度估计方法，以提高同步定位和映射（SLAM）算法对侵略性运动引起的陀螺仪饱和的鲁棒性。野外机器人使机器人面临各种危险，包括陡峭的地形、山体滑坡和楼梯，如果机器人失去稳定性并摔倒，可能会出现大幅加速度和角速度。这些极端运动会使传感器测量饱和，尤其是陀螺仪，因为陀螺仪是第一个不工作的传感器。虽然机器人的结构完整性面临风险，但SLAM框架的弹性往往很少得到考虑。因此，即使机器人在物理上有能力继续执行任务，其操作也会因对世界的破坏而受到影响。关于这个问题，我们提出了一种在翻滚引起的极端旋转过程中使用加速度计估计角速度的方法。我们表明，在收集的数据中，我们的方法在平移和旋转中分别将中值定位误差降低了71.5%和65.5%，并将SLAM故障次数降低了73.3%。我们还提出了翻滚诱导陀螺仪饱和（TIGS）数据集，该数据集由室外实验组成，记录激光雷达在角速度比其他可用数据集高四倍的情况下的运动。数据集可在线获取，网址为https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07844v1" target="_blank">2310.07844v1</a>
                              </td>
                              <td>Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</td>
                              <td>Simon-Pierre Deschênes</td>
                              <td>2023-10-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07844v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07844v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09531v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09531v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09531v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09531v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local geometric information, i.e. normal and distribution of points, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constraints for data association, which further determines the direction of optimization and ultimately affects the accuracy of localization. However, estimating normal and distribution of points are time-consuming tasks even with the assistance of kdtree or volumetric maps. To achieve fast normal estimation, we look into the structure of LiDAR scan and propose a ring-based fast approximate least squares (Ring FALS) method. With the Ring structural information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update the distribution of points in each voxel incrementally while maintaining its consistency with the normal estimation. We further fix the distribution after its convergence to balance the time consumption and the correctness of representation. Based on the extracted and maintained local geometric information, we devise a robust and accurate hierarchical data association scheme where point-to-surfel association is prioritized over point-to-plane. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-of-the-art methods. Our open source implementation is available at https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09531v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部几何信息，即点的法线和分布，对于基于激光雷达的同时定位和映射（SLAM）至关重要，因为它为数据关联提供了约束，从而进一步决定了优化的方向，并最终影响定位的准确性。然而，即使在kdtree或体积图的帮助下，估计点的正态和分布也是耗时的任务。为了实现快速正态估计，我们研究了激光雷达扫描的结构，并提出了一种基于环的快速近似最小二乘法（ring-FALS）。利用环形结构信息，当新的扫描到达时，估计法线只需要点的范围信息。为了有效地估计点的分布，我们扩展了ikd树来管理体素中的映射，并在保持其与正常估计的一致性的同时逐步更新每个体素中点的分布。我们在收敛后进一步固定分布，以平衡时间消耗和表示的正确性。基于提取和维护的局部几何信息，我们设计了一种稳健而准确的分层数据关联方案，其中点到表面的关联优先于点到平面。在不同的公共数据集上进行的大量实验表明，与其他最先进的方法相比，我们的系统具有优势。我们的开源实现可在https://github.com/tiev-tongji/LOG-LIO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09531v3" target="_blank">2307.09531v3</a>
                              </td>
                              <td>LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</td>
                              <td>Kai Huang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09531v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09531v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06765v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Graduated Non-Convexity for Pose Graph Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06765v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06765v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06765v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate its efficacy through its application in robust pose graph optimization, a key component in SLAM backends. Traditional GNC methods often rely on heuristic methods for GNC schedule, updating control parameter {\mu} for escalating the non-convexity. In contrast, our approach leverages the properties of convex functions and convex optimization to identify the boundary points beyond which convexity is no longer guaranteed, thereby eliminating redundant optimization steps in existing methodologies and enhancing both speed and robustness. We show that our method outperforms the state-of-the-art method in terms of speed and accuracy when used for robust back-end pose graph optimization via GNC. Our work builds upon and enhances the open-source riSAM framework. Our implementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06765v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的分级非凸性（GNC）方法，并通过将其应用于SLAM后端的关键组件——鲁棒姿态图优化中来证明其有效性。传统的GNC方法通常依赖于GNC调度的启发式方法，更新控制参数以升级非凸性。相反，我们的方法利用凸函数和凸优化的性质来识别超出其凸性不再得到保证的边界点，从而消除了现有方法中的冗余优化步骤，并提高了速度和鲁棒性。我们表明，当通过GNC用于稳健的后端姿态图优化时，我们的方法在速度和精度方面优于最先进的方法。我们的工作建立在开源riSAM框架的基础上并对其进行了增强。我们的实施可以访问：https://github.com/SNU-DLLAB/EGNC-PGO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06765v1" target="_blank">2310.06765v1</a>
                              </td>
                              <td>Efficient Graduated Non-Convexity for Pose Graph Optimization</td>
                              <td>Wonseok Kang</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06765v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06765v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06385v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06385v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06385v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06385v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The existence of variable factors within the environment can cause a decline in camera localization accuracy, as it violates the fundamental assumption of a static environment in Simultaneous Localization and Mapping (SLAM) algorithms. Recent semantic SLAM systems towards dynamic environments either rely solely on 2D semantic information, or solely on geometric information, or combine their results in a loosely integrated manner. In this research paper, we introduce 3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic and geometric constraints sequentially. We designed a 3D part-aware hybrid transformer for point cloud-based object detection to identify dynamic objects. Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering to extract objects with significant absolute depth differences. When compared against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the performance of the other four leading SLAM systems designed for dynamic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06385v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环境中可变因素的存在可能会导致相机定位精度的下降，因为这违反了同步定位和映射（SLAM）算法中静态环境的基本假设。最近针对动态环境的语义SLAM系统要么仅依赖于2D语义信息，要么仅依赖几何信息，或者以松散集成的方式组合它们的结果。在这篇研究论文中，我们介绍了3DS-SLAM，3D语义SLAM，它是为具有视觉3D对象检测的动态场景量身定制的。3DS-SLAM是一种严格耦合的算法，可以顺序地解决语义和几何约束。我们设计了一个3D零件感知混合转换器，用于基于点云的对象检测，以识别动态对象。随后，我们提出了一种基于HDBSCAN聚类的动态特征滤波器来提取具有显著绝对深度差异的对象。与ORB-SLAM2相比，3DS-SLAM在TUM RGB-D数据集的动态序列中表现出98.01%的平均改进。此外，它的性能超过了为动态环境设计的其他四个领先的SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06385v1" target="_blank">2310.06385v1</a>
                              </td>
                              <td>3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</td>
                              <td>Ghanta Sai Krishna</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06385v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06385v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_05249v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Visual Odometry Methods for Autonomous Driving in Rain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_05249v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_05249v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_05249v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The increasing demand for autonomous vehicles has created a need for robust navigation systems that can also operate effectively in adverse weather conditions. Visual odometry is a technique used in these navigation systems, enabling the estimation of vehicle position and motion using input from onboard cameras. However, visual odometry accuracy can be significantly impacted in challenging weather conditions, such as heavy rain, snow, or fog. In this paper, we evaluate a range of visual odometry methods, including our DROID-SLAM based heuristic approach. Specifically, these algorithms are tested on both clear and rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, the Oxford Robotcar dataset from Oxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different visual odometry algorithms for both monocular and stereo camera setups using the Absolute Trajectory Error (ATE). From the range of approaches evaluated, our findings suggest that the Depth and Flow for Visual Odometry (DF-VO) algorithm with monocular setup performed the best for short range distances (< 500m) and our proposed DROID-SLAM based heuristic approach for the stereo setup performed relatively well for long-term localization. Both VO algorithms suggested a need for a more robust sensor fusion based approach for localization in rain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_05249v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对自动驾驶汽车日益增长的需求产生了对鲁棒导航系统的需求，该系统也可以在恶劣天气条件下有效运行。视觉里程计是这些导航系统中使用的一种技术，能够使用车载摄像头的输入来估计车辆的位置和运动。然而，在大雨、雪或雾等具有挑战性的天气条件下，视觉里程计的准确性可能会受到显著影响。在本文中，我们评估了一系列视觉里程计方法，包括我们基于DROID-SLAM的启发式方法。具体来说，这些算法在晴朗和雨天的城市驾驶数据上进行了测试，以评估其稳健性。我们汇编了一个数据集，其中包括来自不同城市的一系列降雨天气条件。这包括来自牛津的Oxford Robotcar数据集、来自慕尼黑的4Seasons数据集和在新加坡收集的内部数据集。我们使用绝对轨迹误差（ATE）评估了单眼和立体相机设置的不同视觉里程计算法。从评估的方法范围来看，我们的研究结果表明，具有单目设置的视觉Odometry的深度和流量（DF-VO）算法在短距离（<500m）中表现最好，并且我们提出的基于DROID-SLAM的立体设置启发式方法在长期定位中表现相对较好。两种VO算法都表明，需要一种更稳健的基于传感器融合的方法来进行降雨定位。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.05249v2" target="_blank">2309.05249v2</a>
                              </td>
                              <td>Evaluating Visual Odometry Methods for Autonomous Driving in Rain</td>
                              <td>Yu Xiang Tan</td>
                              <td>2023-09-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_05249v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.05249v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06249v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">l-dyno: framework to learn consistent visual features using robot's motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06249v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06249v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06249v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robot's transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robot's relative pose transformations. This approach enables us to incorporate information from the robot's perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI & EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06249v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从历史上看，基于特征的方法已被广泛用于基于相机的机器人感知任务，如定位、映射、跟踪等。这些方法中的一些还结合了其他传感器（例如惯性传感）来执行组合状态估计。我们的工作重新思考了这种方法；我们提出了一种表示学习机制，该机制识别最符合由外部信号估计的机器人运动的视觉特征。具体来说，我们通过外部信号（例如惯性传感）利用机器人的变换，并关注与外部信号最一致的图像空间。我们使用成对一致性度量作为表示，以通过与机器人的相对姿势变换的序列来保持视觉特征的一致性。这种方法使我们能够从机器人的角度整合信息，而不是仅仅依赖于图像属性。我们在KITTI和EuRoC等真实世界的数据集上评估了我们的方法，并将改进后的特征与现有的特征描述符进行了比较。我们还使用真实的机器人实验来评估我们的方法。我们注意到，在不影响轨迹估计精度的情况下，图像搜索空间平均减少了49%。我们的方法将视觉里程计的执行时间减少了4.3%，还减少了重投影误差。我们证明了只选择最重要特征的必要性，并使用各种特征检测基线来显示竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06249v1" target="_blank">2310.06249v1</a>
                              </td>
                              <td>l-dyno: framework to learn consistent visual features using robot's motion</td>
                              <td>Kartikeya Singh</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06249v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06249v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06160v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Entropy Based Multi-robot Active SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06160v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06160v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06160v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article, we present an efficient multi-robot active SLAM framework that involves a frontier-sharing method for maximum exploration of an unknown environment. It encourages the robots to spread into the environment while weighting the goal frontiers with the pose graph SLAM uncertainly and path entropy. Our approach works on a limited number of frontier points and weights the goal frontiers with a utility function that encapsulates both the SLAM and map uncertainties, thus providing an efficient and not computationally expensive solution. Our approach has been tested on publicly available simulation environments and on real robots. An accumulative 31% more coverage than similar state-of-the-art approaches has been obtained, proving the capability of our approach for efficient environment exploration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06160v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种高效的多机器人主动SLAM框架，该框架涉及一种最大限度地探索未知环境的边界共享方法。它鼓励机器人扩散到环境中，同时用姿态图SLAM不确定性和路径熵对目标边界进行加权。我们的方法在有限数量的边界点上工作，并使用封装SLAM和地图不确定性的效用函数对目标边界进行加权，从而提供了一种高效且计算成本不高的解决方案。我们的方法已经在公开的模拟环境和真实的机器人上进行了测试。已经获得了比类似的最先进方法多31%的累积覆盖率，证明了我们的方法在有效的环境勘探方面的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06160v1" target="_blank">2310.06160v1</a>
                              </td>
                              <td>Entropy Based Multi-robot Active SLAM</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06160v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06160v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05766v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated TSDF-Mapping Backend for NVIDIA Jetson Boards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05766v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05766v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05766v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents FeatSense, a feature-based GPU-accelerated SLAM system for high resolution LiDARs, combined with a map generation algorithm for real-time generation of large Truncated Signed Distance Fields (TSDFs) on embedded hardware. FeatSense uses LiDAR point cloud features for odometry estimation and point cloud registration. The registered point clouds are integrated into a global Truncated Signed Distance Field (TSDF) representation. FeatSense is intended to run on embedded systems with integrated GPU-accelerator like NVIDIA Jetson boards. In this paper, we present a real-time capable TSDF-SLAM system specially tailored for close coupled CPU/GPU systems. The implementation is evaluated in various structured and unstructured environments and benchmarked against existing reference datasets. The main contribution of this paper is the ability to register up to 128 scan lines of an Ouster OS1-128 LiDAR at 10Hz on a NVIDIA AGX Xavier while achieving a TSDF map generation speedup by a factor of 100 compared to previous work on the same power budget.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05766v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于高分辨率激光雷达的基于特征的GPU加速SLAM系统FeathSense，并结合了一种在嵌入式硬件上实时生成大型截断符号距离场（TSDF）的地图生成算法。FeathSense使用激光雷达点云特征进行里程计估计和点云配准。注册的点云被集成到全局截断有符号距离场（TSDF）表示中。FeathSense旨在运行在具有集成GPU加速器的嵌入式系统上，如NVIDIA Jetson板。在本文中，我们提出了一个实时TSDF-SLAM系统，专门为紧密耦合的CPU/GPU系统量身定制。该实现在各种结构化和非结构化环境中进行了评估，并根据现有的参考数据集进行了基准测试。本文的主要贡献是能够在NVIDIA AGX Xavier上以10Hz的频率注册Ouster OS1-128 LiDAR的多达128条扫描线，同时与以前在相同功率预算下的工作相比，TSDF地图生成速度提高了100倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05766v1" target="_blank">2310.05766v1</a>
                              </td>
                              <td>FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated TSDF-Mapping Backend for NVIDIA Jetson Boards</td>
                              <td>Julian Gaal</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05766v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05766v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05600v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05600v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05600v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05600v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As labor shortage increases in the health sector, the demand for assistive robotics grows. However, the needed test data to develop those robots is scarce, especially for the application of active 3D object detection, where no real data exists at all. This short paper counters this by introducing such an annotated dataset of real environments. The captured environments represent areas which are already in use in the field of robotic health care research. We further provide ground truth data within one room, for assessing SLAM algorithms running directly on a health care robot.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05600v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着卫生部门劳动力短缺的加剧，对辅助机器人的需求也在增长。然而，开发这些机器人所需的测试数据很少，尤其是在完全不存在真实数据的主动三维物体检测应用中。这篇简短的论文通过引入这样一个真实环境的注释数据集来反驳这一点。捕捉到的环境代表了机器人医疗保健研究领域中已经在使用的领域。我们进一步在一个房间内提供地面实况数据，用于评估直接在医疗机器人上运行的SLAM算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05600v1" target="_blank">2310.05600v1</a>
                              </td>
                              <td>Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments</td>
                              <td>Michael G. Adam</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05600v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05600v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16772v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XVO: Generalized Visual Odometry via Cross-Modal Self-Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16772v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16772v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16772v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to significantly enhance the semi-supervised learning process while alleviating noisy pseudo-labels, particularly in highly dynamic and out-of-domain video data. Our proposed teacher network achieves state-of-the-art performance on the commonly used KITTI benchmark despite no multi-frame optimization or knowledge of camera parameters. Combined with the proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, and Argoverse without fine-tuning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16772v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了XVO，这是一种半监督学习方法，用于在不同的数据集和设置中训练具有鲁棒非自操作的广义单目视觉Odometry（VO）模型。与通常在单个数据集中研究已知校准的标准单目VO方法相比，XVO有效地学习从视觉场景语义中恢复具有真实世界尺度的相对姿态，即不依赖于任何已知的相机参数。我们通过从YouTube上提供的大量无约束和异构的行车记录仪视频中进行自我训练来优化运动估计模型。我们的主要贡献是双重的。首先，我们实证证明了半监督训练对学习通用直接VO回归网络的好处。其次，我们演示了多模态监督，包括分割、流、深度和音频辅助预测任务，以便于VO任务的广义表示。具体来说，我们发现音频预测任务可以显著增强半监督学习过程，同时减轻噪声伪标签，特别是在高度动态和域外视频数据中。尽管没有多帧优化或相机参数知识，但我们提出的教师网络在常用的KITTI基准上实现了最先进的性能。结合所提出的半监督步骤，XVO展示了在KITTI、nuScenes和Argovere上跨不同条件的现成知识转移，而无需微调。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16772v3" target="_blank">2309.16772v3</a>
                              </td>
                              <td>XVO: Generalized Visual Odometry via Cross-Modal Self-Training</td>
                              <td>Lei Lai</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16772v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16772v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04802v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hierarchical Unsupervised Topological SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04802v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04802v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04802v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we present a novel framework for unsupervised topological clustering resulting in improved loop. In this paper we present a novel framework for unsupervised topological clustering resulting in improved loop detection and closure for SLAM. A navigating mobile robot clusters its traversal into visually similar topologies where each cluster (topology) contains a set of similar looking images typically observed from spatially adjacent locations. Each such set of spatially adjacent and visually similar grouping of images constitutes a topology obtained without any supervision. We formulate a hierarchical loop discovery strategy that first detects loops at the level of topologies and subsequently at the level of images between the looped topologies. We show over a number of traversals across different Habitat environments that such a hierarchical pipeline significantly improves SOTA image based loop detection and closure methods. Further, as a consequence of improved loop detection, we enhance the loop closure and backend SLAM performance. Such a rendering of a traversal into topological segments is beneficial for downstream tasks such as navigation that can now build a topological graph where spatially adjacent topological clusters are connected by an edge and navigate over such topological graphs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04802v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的无监督拓扑聚类框架，从而改进了循环。在本文中，我们提出了一种新的无监督拓扑聚类框架，从而改进了SLAM的环路检测和闭合。导航移动机器人将其遍历聚类为视觉上相似的拓扑，其中每个聚类（拓扑）包含一组通常从空间相邻位置观察到的相似图像。每一组这样的空间上相邻且视觉上相似的图像分组构成了在没有任何监督的情况下获得的拓扑。我们制定了一种分层循环发现策略，该策略首先在拓扑级别检测循环，然后在循环拓扑之间的图像级别检测循环。我们在不同生境环境中的多次遍历中表明，这种分层管道显著改进了基于SOTA图像的环路检测和闭合方法。此外，由于改进了循环检测，我们增强了循环闭合和后端SLAM性能。将遍历呈现为拓扑段对于诸如导航之类的下游任务是有益的，导航现在可以构建拓扑图，其中空间相邻的拓扑簇通过边连接，并在这样的拓扑图上导航。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04802v1" target="_blank">2310.04802v1</a>
                              </td>
                              <td>Hierarchical Unsupervised Topological SLAM</td>
                              <td>Ayush Sharma</td>
                              <td>2023-10-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04802v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04802v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04787v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04787v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04787v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04787v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we present a neural field-based real-time monocular mapping framework for accurate and dense Simultaneous Localization and Mapping (SLAM). Recent neural mapping frameworks show promising results, but rely on RGB-D or pose inputs, or cannot run in real-time. To address these limitations, our approach integrates dense-SLAM with neural implicit fields. Specifically, our dense SLAM approach runs parallel tracking and global optimization, while a neural field-based map is constructed incrementally based on the latest SLAM estimates. For the efficient construction of neural fields, we employ multi-resolution grid encoding and signed distance function (SDF) representation. This allows us to keep the map always up-to-date and adapt instantly to global updates via loop closing. For global consistency, we propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach to run online loop closing and mitigate the pose and scale drift. To enhance depth accuracy further, we incorporate learned monocular depth priors. We propose a novel joint depth and scale adjustment (JDSA) module to solve the scale ambiguity inherent in depth priors. Extensive evaluations across synthetic and real-world datasets validate that our approach outperforms existing methods in accuracy and map completeness while preserving real-time performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04787v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们提出了一个基于神经场的实时单目映射框架，用于精确和密集的同时定位和映射（SLAM）。最近的神经映射框架显示出有希望的结果，但依赖于RGB-D或姿势输入，或者无法实时运行。为了解决这些局限性，我们的方法将密集SLAM与神经隐式场相结合。具体来说，我们的密集SLAM方法运行并行跟踪和全局优化，而基于神经场的映射是基于最新的SLAM估计逐步构建的。为了有效地构造神经场，我们采用了多分辨率网格编码和符号距离函数（SDF）表示。这使我们能够始终保持地图的最新状态，并通过循环关闭立即适应全球更新。为了全局一致性，我们提出了一种有效的基于Sim（3）的姿态图束调整（PGBA）方法来运行在线闭环并减轻姿态和尺度漂移。为了进一步提高深度精度，我们结合了学习的单目深度先验。我们提出了一种新的深度和尺度联合调整（JDSA）模块来解决深度先验中固有的尺度模糊性。对合成和真实世界数据集的广泛评估验证了我们的方法在准确性和地图完整性方面优于现有方法，同时保持了实时性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04787v1" target="_blank">2310.04787v1</a>
                              </td>
                              <td>HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</td>
                              <td>Wei Zhang</td>
                              <td>2023-10-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04787v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04787v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04162v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on Graph-Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04162v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04162v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04162v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization and Mapping (SLAM) plays an important role in robot autonomy. Reliability and efficiency are the two most valued features for applying SLAM in robot applications. In this paper, we consider achieving a reliable LiDAR-based SLAM function in computation-limited platforms, such as quadrotor UAVs based on graph-based point cloud association. First, contrary to most works selecting salient features for point cloud registration, we propose a non-conspicuous feature selection strategy for reliability and robustness purposes. Then a two-stage correspondence selection method is used to register the point cloud, which includes a KD-tree-based coarse matching followed by a graph-based matching method that uses geometric consistency to vote out incorrect correspondences. Additionally, we propose an odometry approach where the weight optimizations are guided by vote results from the aforementioned geometric consistency graph. In this way, the optimization of LiDAR odometry rapidly converges and evaluates a fairly accurate transformation resulting in the back-end module efficiently finishing the mapping task. Finally, we evaluate our proposed framework on the KITTI odometry dataset and real-world environments. Experiments show that our SLAM system achieves a comparative level or higher level of accuracy with more balanced computation efficiency compared with the mainstream LiDAR-based SLAM solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04162v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同步定位与映射（SLAM）在机器人自主中发挥着重要作用。可靠性和效率是SLAM在机器人应用中最有价值的两个特征。在本文中，我们考虑在计算有限的平台上实现可靠的基于激光雷达的SLAM功能，例如基于图的点云关联的四旋翼无人机。首先，与大多数为点云配准选择显著特征的工作相反，出于可靠性和鲁棒性的目的，我们提出了一种非显著特征选择策略。然后使用两阶段对应关系选择方法来注册点云，该方法包括基于KD树的粗略匹配，然后是基于图的匹配方法，该方法使用几何一致性来投票排除不正确的对应关系。此外，我们提出了一种里程计方法，其中权重优化由上述几何一致性图的投票结果指导。通过这种方式，激光雷达里程计的优化快速收敛并评估相当准确的变换，从而使后端模块有效地完成映射任务。最后，我们在KITTI里程计数据集和真实世界环境中评估了我们提出的框架。实验表明，与主流的基于激光雷达的SLAM解决方案相比，我们的SLAM系统以更平衡的计算效率实现了相当水平或更高水平的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04162v1" target="_blank">2310.04162v1</a>
                              </td>
                              <td>Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on Graph-Matching</td>
                              <td>Shiquan Yi</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04162v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04162v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04111v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Random Texture Detection using Beta Distribution Statistics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04111v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04111v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04111v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This note describes a method for detecting dense random texture using fully connected points sampled on image edges. An edge image is randomly sampled with points, the standard L2 distance is calculated between all connected points in a neighbourhood. For each point, a check is made if the point intersects with an image edge. If this is the case, a unity value is added to the distance, otherwise zero. From this an edge excess index is calculated for the fully connected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The ratio can be interpreted as a sampled Bernoulli process with unknown probability. The Bayesian posterior estimate of the probability can be associated with its conjugate prior which is a Beta($\alpha$, $\beta$) distribution, with hyper parameters $\alpha$ and $\beta$ related to the number of edge crossings. Low values of $\beta$ indicate a texture rich area, higher values less rich. The method has been applied to real-time SLAM-based moving object detection, where points are confined to tracked boxes (rois).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04111v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文描述了一种使用在图像边缘采样的全连通点来检测密集随机纹理的方法。边缘图像用点随机采样，计算邻域中所有连接点之间的标准L2距离。对于每个点，都会检查该点是否与图像边缘相交。如果是这种情况，则会向距离添加一个单位值，否则为零。由此计算出在[1.0..2.0]范围内的全连通边图的边过剩指数，其中1.0表示没有边。该比率可以解释为具有未知概率的采样伯努利过程。概率的贝叶斯后验估计可以与其共轭先验相关联，共轭先验是Beta（$\alpha$，$\Beta$）分布，超参数$\alpha$和$\Beta$与边缘交叉的数量有关。$\beta$的值越低表示纹理丰富的区域，值越高表示不太丰富。该方法已应用于基于SLAM的实时运动目标检测，其中点被限制在跟踪框（roi）中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04111v1" target="_blank">2310.04111v1</a>
                              </td>
                              <td>Dense Random Texture Detection using Beta Distribution Statistics</td>
                              <td>Soeren Molander</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04111v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04111v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_00406v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_00406v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_00406v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_00406v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is critical to the implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms assume a static environment, leading to unreliable localization in dynamic environments. Moreover, the accurate tracking of moving objects is of great significance for the control and planning of autonomous vehicles. This study proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial odometry system that is capable of accurately estimating the poses of both ego-vehicle and objects. We propose a trajectory-based dynamic feature filtering method, which filters out features belonging to moving objects by leveraging tracking results before scan-matching. Factor graph-based optimization is then conducted to optimize the bias of the IMU and the poses of both the ego-vehicle and surrounding objects in a sliding window. Experiments conducted on the KITTI tracking dataset and self-collected dataset show that our method achieves better pose and tracking accuracy than our previous work DL-SLOT and other baseline methods. Our open-source implementation is available at https://github.com/tiev-tongji/LIMOT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_00406v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）对自动驾驶的实现至关重要。大多数激光雷达惯性SLAM算法假设静态环境，导致在动态环境中定位不可靠。此外，运动物体的精确跟踪对自动驾驶汽车的控制和规划具有重要意义。本研究提出了LIMOT，这是一种紧密耦合的多目标跟踪和激光雷达惯性里程计系统，能够准确估计自我车辆和物体的姿态。我们提出了一种基于轨迹的动态特征滤波方法，该方法通过在扫描匹配之前利用跟踪结果来滤除属于运动对象的特征。然后进行基于因子图的优化，以优化IMU的偏置以及自我车辆和周围物体在滑动窗口中的姿态。在KITTI跟踪数据集和自收集数据集上进行的实验表明，与我们之前的工作DL-SLOT和其他基线方法相比，我们的方法实现了更好的姿态和跟踪精度。我们的开源实现可在https://github.com/tiev-tongji/LIMOT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.00406v2" target="_blank">2305.00406v2</a>
                              </td>
                              <td>LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</td>
                              <td>Zhongyang Zhu</td>
                              <td>2023-04-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_00406v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.00406v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02650v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02650v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02650v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02650v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02650v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与让每个新部署的机器人创建自己的周围环境地图不同，SLAM设备的可用性不断增加，提供了在另一个机器人或设备的地图中简单定位的选项。在多机器人或人机协作等情况下，甚至有必要将所有代理定位在同一地图中。然而，由于视点的变化，在无人机或头戴式MR耳机的地图中定位例如地面机器人会带来独特的挑战。这项工作研究了如何使用主动视觉定位来克服视点变化的挑战。具体来说，我们关注在给定位置选择最佳视点的问题。我们将文献中现有的方法与额外提出的基线进行了比较，并提出了一种新的数据驱动方法。结果表明，与现有方法相比，无论是在受控模拟实验还是在现实世界部署中，数据驱动方法都具有优越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02650v1" target="_blank">2310.02650v1</a>
                              </td>
                              <td>Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach</td>
                              <td>Matthew Hanlon</td>
                              <td>2023-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02650v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02650v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的、真实感的模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们提出了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。该代码位于：https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>根据内窥镜视频生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对一种自监督的鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是一些计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从偶然图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单目相机重建的最先进技术主要依赖于运动结构（SfM）流水线。然而，这种方法通常会产生缺乏关键尺度信息的重建结果，并且随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，以追求测绘结果中的精确缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节级别。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05134v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05134v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05134v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是移动机器人的一项关键任务，研究人员正在不断开发新的方法来提高其效率。在本文中，我们提出了一种利用运动结构（SfM）技术提高视觉定位精度的新方法。我们强调了全局SfM的局限性，它具有高延迟，以及局部SfM所面临的挑战，后者需要大型图像数据库才能进行精确重建。为了解决这些问题，我们建议利用神经辐射场（NeRF），而不是图像数据库，来减少存储所需的空间。我们建议，对先前查询位置周围的参考图像进行采样可以带来进一步的改进。我们评估了我们提出的方法相对于使用激光雷达和高级激光雷达实时测距和测绘（A-LOAM）获得的地面实况的准确性，并在所进行的实验中比较了其相对于局部SfM和COLMAP的存储使用情况。与地面实况相比，我们提出的方法实现了0.068米的精度，这略低于最先进的方法COLMAP，后者的精度为0.022米。然而，COLMAP所需的数据库大小为400兆字节，而我们的NeRF模型的大小仅为160兆字节。最后，我们进行了消融研究，以评估使用NeRF重建的参考图像的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05134v1" target="_blank">2310.05134v1</a>
                              </td>
                              <td>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</td>
                              <td>Artem Nenashev</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05134v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05134v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04145v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04145v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04145v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用未校准的多摄像头系统从运动中构建结构是一项具有挑战性的任务。本文提出了一种束调整解决方案，该解决方案实现了一个基线约束，即这些相机彼此是静态的。我们假设这些相机安装在移动平台上，未校准，并且粗略同步。为此，我们提出了针对相机具有重叠视图的场景制定的基线约束。该约束被纳入束调整解决方案中，以保持不同相机的相对运动静止。实验使用安装在车辆上的两个并置GoPro相机的视频帧进行，没有系统校准。这两台摄像机被放置在捕捉重叠内容的位置。我们使用所提出的约束进行了束调整，然后生成了3D密集点云。通过将这些密集点云与激光雷达参考数据进行比较来进行评估。我们表明，与传统的束平差相比，我们提出的方法实现了29.38%的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04145v2" target="_blank">2204.04145v2</a>
                              </td>
                              <td>Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</td>
                              <td>Debao Huang</td>
                              <td>2022-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04145v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04145v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择最近的DeDoDe关键点是因为它们具有较高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工拾取的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00783v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Propagating Semantic Labels in Video Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00783v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00783v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割结合了两个子任务：像素级图像掩码的识别和对掩码应用语义标签。最近，引入了所谓的基础模型；在非常大的数据集上训练的通用模型，这些数据集可以专门化并应用于更具体的任务。一个这样的模型，分段任意模型（SAM），执行图像分割。CLIPSeg和MaskRCNN等语义分割系统是在成对片段和语义标签的数据集上进行训练的。但是，手动标记自定义数据非常耗时。这项工作提出了一种对视频中的对象进行分割的方法。一旦在视频帧中找到对象，则可以将该片段传播到未来的帧；从而减少了手动注释的工作量。该方法将SAM与运动结构（SfM）相结合。首先使用SfM将输入到系统的视频重构为3D几何结构。然后使用SAM对视频帧进行分割。然后将SAM识别的片段投影到重建的3D几何体上。在随后的视频帧中，标记的3D几何体被重新投影到新的透视图中，从而减少SAM的调用次数。评估系统性能，包括SAM和SfM组件的贡献。性能通过三个主要指标进行评估：计算时间、带有手动标签的掩码IOU和跟踪丢失次数。结果表明，该系统在视频帧上跟踪对象的计算时间大大提高了人类的性能，但性能较差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00783v1" target="_blank">2310.00783v1</a>
                              </td>
                              <td>Propagating Semantic Labels in Video Data</td>
                              <td>David Balaban</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00783v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00783v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16632v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16632v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we study the problem of minimizing a submodular function $f : 2^V \rightarrow \mathbb{R}$ that is guaranteed to have a $k$-sparse minimizer. We give a deterministic algorithm that computes an additive $\epsilon$-approximate minimizer of such $f$ in $\widetilde{O}(\mathsf{poly}(k) \log(|f|/\epsilon))$ parallel depth using a polynomial number of queries to an evaluation oracle of $f$, where $|f| = \max_{S \subseteq V} |f(S)|$. Further, we give a randomized algorithm that computes an exact minimizer of $f$ with high probability using $\widetilde{O}(|V| \cdot \mathsf{poly}(k))$ queries and polynomial time. When $k = \widetilde{O}(1)$, our algorithms use either nearly-constant parallel depth or a nearly-linear number of evaluation oracle queries. All previous algorithms for this problem either use $\Omega(|V|)$ parallel depth or $\Omega(|V|^2)$ queries.   In contrast to state-of-the-art weakly-polynomial and strongly-polynomial time algorithms for SFM, our algorithms use first-order optimization methods, e.g., mirror descent and follow the regularized leader. We introduce what we call {\em sparse dual certificates}, which encode information on the structure of sparse minimizers, and both our parallel and sequential algorithms provide new algorithmic tools for allowing first-order optimization methods to efficiently compute them. Correspondingly, our algorithm does not invoke fast matrix multiplication or general linear system solvers and in this sense is more combinatorial than previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16632v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了子模函数$f:2^V\rightarrow\mathbb｛R｝$的最小化问题，该子模函数保证具有$k$-稀疏极小值。我们给出了一个确定性算法，该算法使用对$f$的评估预言的多项式查询数来计算$\widetilde｛O｝（\mathsf｛poly｝（k）\log（|f|/\epsilon））$并行深度中的$f$近似极小值，其中$|f|=\max_｛S\substeq V｝|f（S）|$。此外，我们给出了一个随机算法，该算法使用$\widetilde{O}（|V|\cdot\mathsf{poly}（k））$查询和多项式时间以高概率计算$f$的精确极小值。当$k=\widetilde｛O｝（1）$时，我们的算法使用几乎恒定的并行深度或几乎线性数量的评估oracle查询。以前针对此问题的所有算法都使用$\Omega（|V|）$并行深度或$\Omega（|V|^2）$查询。与最先进的SFM弱多项式和强多项式时间算法相比，我们的算法使用一阶优化方法，例如镜像下降和遵循正则化前导。我们介绍了我们所称的稀疏双证书，它对稀疏最小化器结构的信息进行编码，我们的并行和顺序算法都提供了新的算法工具，允许一阶优化方法有效地计算它们。相应地，我们的算法不调用快速矩阵乘法或一般线性系统求解器，并且在这个意义上比以前最先进的方法更具组合性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16632v1" target="_blank">2309.16632v1</a>
                              </td>
                              <td>Sparse Submodular Function Minimization</td>
                              <td>Andrei Graur</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16632v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16632v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13772v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Segmentation from a Moving Monocular Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13772v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13772v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当存在未知的摄像机运动、不同类型的物体运动和复杂的场景结构时，从移动的单目摄像机中识别和分割移动物体是困难的。为了应对这些挑战，我们利用了单目运动分割方法的两个流行分支：基于点轨迹的方法和基于光流的方法，通过在对象级别协同融合这两个高度互补的运动线索。通过这样做，我们能够同时对不同场景结构中的各种复杂物体运动进行建模，这是现有方法无法实现的。我们首先利用对象识别、分割和跟踪方面的最新基础模型，获得视频中每个常见对象的特定对象点轨迹和光流掩模。然后，我们使用极线几何和光流提供的运动信息构建了两个稳健的仿射矩阵，表示整个视频中的成对对象运动仿射。最后，使用共正则化多视图谱聚类来融合两个亲和矩阵，得到最终的聚类。我们的方法在KT3DMoSeg数据集上显示了最先进的性能，该数据集包含复杂的运动和场景结构。能够识别移动物体使我们能够在使用视觉SLAM或SFM时将其移除以用于地图构建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13772v1" target="_blank">2309.13772v1</a>
                              </td>
                              <td>Motion Segmentation from a Moving Monocular Camera</td>
                              <td>Yuxiang Huang</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13772v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13772v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12804v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12804v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12804v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>珊瑚礁是地球上最多样化的生态系统之一，数亿人依赖珊瑚礁。不幸的是，大多数珊瑚礁都受到全球气候变化和当地人为压力的威胁。为了更好地了解珊瑚礁退化背后的动力学，以高空间和时间分辨率进行监测是关键。然而，由于需要大量的体力劳动，量化珊瑚覆盖率和物种丰度的传统监测方法在规模上受到限制。尽管计算机视觉工具已被用于帮助这一过程，特别是用于3D地图绘制的SfM摄影测量和用于图像分割的深度神经网络，但对数据产品的分析造成了瓶颈，有效地限制了其可扩展性。本文提出了一种从自我运动视频映射水下环境的新范式，将使用机器学习来适应水下具有挑战性的条件的3D映射系统与图像语义分割的现代方法相结合。该方法以红海亚喀巴湾北部的珊瑚礁为例，展示了前所未有的高精度3D语义映射，大大降低了所需的人力成本：用廉价的消费级相机在潜水5分钟内获得的100米视频样带可以在5分钟内全自动分析。我们的方法通过向视频样带的全自动分析迈出了一大步，大大扩大了珊瑚礁监测的规模。该方法通过减少劳动力、设备、物流和计算成本，使珊瑚礁横断面民主化。这有助于更有效地为保护政策提供信息。基于运动结构学习的基本计算方法对珊瑚礁以外的水下环境的快速低成本测绘具有广泛的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12804v1" target="_blank">2309.12804v1</a>
                              </td>
                              <td>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</td>
                              <td>Jonathan Sauder</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12804v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12804v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕获时运行在线SfM，新拍摄的动态图像是用相应的姿势和点在线估计的，即，你捕获的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以在在线拍摄的同时实现稳健配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v1" target="_blank">2309.11883v1</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10748v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10748v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10748v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的手-物体交互数据集显示出有限的真实物体可变性，并且依赖于拟合MANO参数模型来获得真实的手形状。为了超越这些限制并推动进一步的研究，我们引入了SHOWMe数据集，该数据集由96个视频组成，用真实和详细的手对象3D纹理网格进行注释。根据最近的工作，我们考虑了一个刚性手对象场景，其中手相对于对象的姿势在整个视频序列中保持不变。这一假设使我们能够将亚毫米精度的地面实况3D扫描注册到SHOWMe中的图像序列中。尽管更简单，但这一假设在所需精度和细节水平很重要的应用中是有意义的，例如，人机协作中的对象移交、对象扫描或操作和接触点分析。重要的是，手对象系统的刚性允许使用由刚性配准步骤和多视图重建（MVR）部分组成的两阶段流水线来处理未知手持对象的基于视频的3D重建。我们仔细评估了这两个阶段的一组非平凡基线，并表明使用SfM工具箱或手部姿态估计器来恢复刚性变换和现成的MVR算法，可以实现有前景的对象不可知的3D手部对象重建。然而，这些方法对最初的相机姿态估计仍然敏感，由于物体上缺乏纹理或手的严重遮挡，这些估计可能不精确，这为重建留下了改进的空间。代码和数据集可在https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10748v1" target="_blank">2309.10748v1</a>
                              </td>
                              <td>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</td>
                              <td>Anilkumar Swamy</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10748v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10748v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10269v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10269v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10269v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>不通航的河流和蓄水池在缓冲社区免受洪水侵袭方面发挥着重要作用，但应急规划者往往没有数据表明它们在淹没周围地区之前可以携带的水量。本文描述了一种实用的方法，即使用未折叠的海洋表面飞行器（USV）收集水深图和浅水河岸的数字表面图，并将其合并为一个统一的体积模型。将泊松曲面重建算法应用于水下表面稀疏声纳深度读数，开发了水线下网格。河岸的密集水线上网格是使用商业运动结构（SfM）包创建的。由于多种原因，合并具有挑战性，其中最重要的是传感器覆盖范围的差距，即USV无法收集声纳深度数据，也无法直观地看到通往河岸的沙滩，因此两个网格可能不会相交。该方法在Hydronalix EMILY USV上进行了演示，该V配备了Humminbird单波束回声测深仪和Teledyne FLIR相机，位于德克萨斯州农工工程扩展服务灾难城市综合体的ESTI湖。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10269v1" target="_blank">2309.10269v1</a>
                              </td>
                              <td>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</td>
                              <td>Jayesh Tripathi</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10269v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10269v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08927v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08927v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08927v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用神经辐射场（NeRF）进行动态重建需要精确的相机姿态。这些通常很难用现有的运动结构（SfM）管道来检索，因为相机和场景内容都可能发生变化。我们提出了DynaMoN，它利用同步定位和映射（SLAM）与运动掩蔽相结合来处理动态场景内容。我们基于SLAM的稳健跟踪模块显著加快了动态NeRF的训练过程，同时提高了合成视图的质量。对TUM RGB-D、BONN RGB-D Dynamic和DyCheck的iPhone数据集这三个真实世界的数据集进行了广泛的实验验证，显示了DynaMoN在相机姿态估计和新颖视图合成方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08927v1" target="_blank">2309.08927v1</a>
                              </td>
                              <td>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</td>
                              <td>Mert Asim Karaoglu</td>
                              <td>2023-09-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08927v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08927v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04643v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parallel Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04643v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query complexity $n^{O(M)}$ and the second method has depth $\widetilde{O}(n^{1/3} M^{2/3})$ and query complexity $O(\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex $\ell_2$-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing $\ell_\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04643v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了子模函数最小化（SFM）的并行复杂性。我们提供了一对方法，可以获得两种新的查询与深度的权衡——在整数值介于$-M$和$M$之间的$n$元素子集上定义的子模函数。第一种方法的深度为$2$，查询复杂度为$n^｛O（M）｝$，第二种方法的厚度为$\widetilde｛O｝（n^｛1/3｝M^｛2/3｝）$，查询复杂性为$O（\mathrm｛poly｝（n，M））$。尽管有一系列关于改进SFM的并行下界的工作，但在我们的工作之前，并行SFM的唯一已知算法要么遵循序列SFM的更通用方法，要么遵循凸$\ell_2$-Lipschitz函数的高度并行最小化。有趣的是，为了获得我们的第二个结果，我们提供了在超立方体上最小化$\ell_\infty$-Lipschitz函数的第一个高度并行算法，该算法获得了接近最优的深度，以获得恒定的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04643v1" target="_blank">2309.04643v1</a>
                              </td>
                              <td>Parallel Submodular Function Minimization</td>
                              <td>Deeparnab Chakrabarty</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04643v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04643v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry (VO) and SLAM have been using multi-view geometry via local structure from motion for decades. These methods have a slight disadvantage in challenging scenarios such as low-texture images, dynamic scenarios, etc. Meanwhile, use of deep neural networks to extract high level features is ubiquitous in computer vision. For VO, we can use these deep networks to extract depth and pose estimates using these high level features. The visual odometry task then can be modeled as an image generation task where the pose estimation is the by-product. This can also be achieved in a self-supervised manner, thereby eliminating the data (supervised) intensive nature of training deep neural networks. Although some works tried the similar approach [1], the depth and pose estimation in the previous works are vague sometimes resulting in accumulation of error (drift) along the trajectory. The goal of this work is to tackle these limitations of past approaches and to develop a method that can provide better depths and pose estimates. To address this, a couple of approaches are explored: 1) Modeling: Using optical flow and recurrent neural networks (RNN) in order to exploit spatio-temporal correlations which can provide more information to estimate depth. 2) Loss function: Generative adversarial network (GAN) [2] is deployed to improve the depth estimation (and thereby pose too), as shown in Figure 1. This additional loss term improves the realism in generated images and reduces artifacts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，视觉里程计（VO）和SLAM一直通过运动的局部结构使用多视图几何。这些方法在低纹理图像、动态场景等具有挑战性的场景中稍有不足。同时，使用深度神经网络提取高级特征在计算机视觉中无处不在。对于VO，我们可以使用这些深度网络来提取使用这些高级特征的深度和姿态估计。视觉里程计任务然后可以被建模为图像生成任务，其中姿态估计是副产品。这也可以以自监督的方式实现，从而消除训练深度神经网络的数据（监督）密集性质。尽管一些工作尝试了类似的方法[1]，但先前工作中的深度和姿态估计是模糊的，有时会导致沿轨迹的误差（漂移）累积。这项工作的目标是解决过去方法的这些局限性，并开发一种可以提供更好深度和姿态估计的方法。为了解决这一问题，我们探索了几种方法：1）建模：使用光流和递归神经网络（RNN）来利用时空相关性，这可以提供更多信息来估计深度。2） 损失函数：部署生成对抗性网络（GAN）[2]以改进深度估计（从而也提高姿态），如图1所示。这个额外的损失项提高了生成图像的真实性并减少了伪影。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04147v1" target="_blank">2309.04147v1</a>
                              </td>
                              <td>Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</td>
                              <td>Akankshya Kar</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Doppelgangers: Learning to Disambiguate Images of Similar Structures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑的视觉消歧任务是确定一对视觉相似的图像是否描绘了相同或不同的3D表面（例如，对称建筑的相同或相反侧）。两张图像观察到不同但在视觉上相似的3D表面，这对人类来说可能很难区分，也可能导致3D重建算法产生错误的结果。我们提出了一种基于学习的视觉消歧方法，将其表述为图像对的二元分类任务。为此，我们为这个问题引入了一个新的数据集，即Doppelgangers，它包括具有基本事实标签的相似结构的图像对。我们还设计了一种网络架构，将局部关键点和匹配的空间分布作为输入，从而能够更好地对局部和全局线索进行推理。我们的评估表明，我们的方法可以在困难的情况下区分虚幻的匹配，并可以集成到SfM管道中，以产生正确的、消除歧义的3D重建。有关我们的代码、数据集和更多结果，请参阅我们的项目页面：http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02420v1" target="_blank">2309.02420v1</a>
                              </td>
                              <td>Doppelgangers: Learning to Disambiguate Images of Similar Structures</td>
                              <td>Ruojin Cai</td>
                              <td>2023-09-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点通常过于稀疏，我们导出了一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v2" target="_blank">2308.08479v2</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00526v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00526v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details with limited generalization. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structures from motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. The self-cost volume implicitly captures the intrinsic geometry of the scene within a single frame. Each individual slice of the volume signifies the relative distances between points and objects within a latent space. Ultimately, this volume is compressed to the depth map via a novel decoding approach. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and $4.5\%$ error reduction from the previous best. In addition, our approach showcases reduced training complexity, computational efficiency, improved generalization, and the ability to recover fine-grained scene details. Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can surpass existing supervised methods by significant margins (AbsRel = $0.043$, $14\%$ error reduction). self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code and the pre-trained weights will be publicly available. Code is available at \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00526v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，自监督单目深度估计在自动驾驶和机器人技术中得到了广泛应用。然而，现有的解决方案主要寻求从即时视觉特征估计深度，并且难以在有限的泛化能力下恢复细粒度的场景细节。在本文中，我们介绍了SQLdepth，这是一种可以有效地从运动中学习细粒度场景结构的新方法。在SQLdepth中，我们提出了一种新的自查询层（SQL）来构建自成本体积并从中推断深度，而不是从特征图中推断深度。自成本体积隐含地捕捉单个帧内场景的固有几何体。体积的每个单独切片表示潜在空间内的点和对象之间的相对距离。最终，通过一种新颖的解码方法将该体积压缩到深度图中。在KITTI和Cityscapes上的实验结果表明，我们的方法获得了显著的最先进的性能（在KITTI上AbsRel=0.082$，在具有改进的地面实况的KITTI上0.052$，在Cityscape上0.106$），与以前的最佳方法相比，实现了9.9\%$、5.5\%$和4.5\%$的误差降低。此外，我们的方法展示了降低的训练复杂性、计算效率、改进的泛化能力以及恢复细粒度场景细节的能力。此外，自监督预训练和度量微调的SQLdepth可以显著超过现有的监督方法（AbsRel=0.043$，误差减少$14\%$）。SQL中面向自匹配的相对距离查询提高了SQLdepth的鲁棒性和零样本泛化能力。代码和预先训练的重量将公开。代码位于\ href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00526v1" target="_blank">2309.00526v1</a>
                              </td>
                              <td>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</td>
                              <td>Youhong Wang</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00526v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00526v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00487v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Object at a Time: Accurate and Robust Structure From Motion for Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00487v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles. Project page: https://oxidification.com/p/one-object-at-a-time/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00487v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注视机器人可以立即、准确、稳健地感知到被注视物体的距离和周围物体的相对位置。我们展示了注视，即在移动时看着一个物体的行为，是如何利用三维空间几何中的规律来获得这些信息的。这些规律引入了旋转-平移耦合，这种耦合在结构中并不常用。为了验证，我们使用了一个带有RGB相机的Franka Emika机器人。我们a）发现，在15厘米的距离上，距离估计的误差小于5毫米，b）展示了在具有挑战性的场景下如何使用相对位置来寻找障碍物。我们将准确的距离估计和障碍物信息结合到反应机器人行为中，该行为能够拾取未知大小的物体，同时受到不可预见的障碍物的阻碍。项目页面：https://oxidification.com/p/one-object-at-a-time/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00487v3" target="_blank">2208.00487v3</a>
                              </td>
                              <td>One Object at a Time: Accurate and Robust Structure From Motion for Robots</td>
                              <td>Aravind Battaje</td>
                              <td>2022-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00487v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00487v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00385v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Voxel 3D Reconstruction Using a Monocular Event Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00385v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for solving dense 3D reconstruction using only a single event camera. To the best of our knowledge, our work is the first attempt in this regard. Our preliminary results demonstrate that the proposed method can produce visually distinguishable dense 3D reconstructions directly without requiring pipelines like those used by existing methods. Additionally, we have created a synthetic dataset with $39,739$ object scans using an event camera simulator. This dataset will help accelerate other relevant research in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00385v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>事件摄像机是受生物系统启发，专门捕捉亮度变化的传感器。与传统的基于帧的相机相比，这些新兴相机具有许多优势，包括高动态范围、高帧率和极低功耗。由于这些优势，事件摄像机越来越多地应用于各个领域，如帧插值、语义分割、里程计和SLAM。然而，它们在VR应用的3D重建中的应用还没有得到充分的探索。该领域以前的方法主要集中在通过深度图估计进行三维重建。产生密集3D重建的方法通常需要多个相机，而利用单个事件相机的方法只能产生半密集的结果。可以产生密集3D重建的其他单相机方法依赖于创建管道，该管道结合了上述方法或其他现有的运动结构（SfM）或多视图立体（MVS）方法。在本文中，我们提出了一种仅使用单个事件相机来解决密集三维重建的新方法。据我们所知，我们的工作是这方面的第一次尝试。我们的初步结果表明，所提出的方法可以直接产生视觉上可区分的密集三维重建，而不需要像现有方法那样使用管道。此外，我们还使用事件相机模拟器创建了一个合成数据集，其中包含39739$的对象扫描。该数据集将有助于加速该领域的其他相关研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00385v1" target="_blank">2309.00385v1</a>
                              </td>
                              <td>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</td>
                              <td>Haodong Chen</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00385v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00385v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10902v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CamP: Camera Preconditioning for Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10902v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10902v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）可以被优化以获得物体和大规模场景的高保真3D场景重建。然而，NeRF需要精确的相机参数作为输入——不准确的相机参数会导致渲染模糊。通常使用运动结构（SfM）方法作为NeRF的预处理步骤来估计外部和内部相机参数，但这些技术很少产生完美的估计。因此，先前的工作已经提出与NeRF一起联合优化相机参数，但这些方法在具有挑战性的设置中容易出现局部最小值。在这项工作中，我们分析了不同的相机参数化如何影响这个联合优化问题，并观察到标准参数化相对于小扰动在大小上表现出很大的差异，这可能导致病态优化问题。我们建议使用代理问题来计算白化变换，该变换消除了相机参数之间的相关性并归一化了它们的效果，并且我们建议在联合优化期间使用该变换作为相机参数的预处理器。我们的预处理相机优化显著提高了Mip-NeRF 360数据集场景的重建质量：与不针对Zip-NeRF等相机进行优化的最先进的NeRF方法相比，我们降低了67%的错误率（RMSE），与使用SCNeRF相机参数化的最先进联合优化方法相比，降低了29%。我们的方法易于实现，不会显著增加运行时间，可以应用于各种相机参数化，并且可以直接集成到其他类似NeRF的模型中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10902v2" target="_blank">2308.10902v2</a>
                              </td>
                              <td>CamP: Camera Preconditioning for Neural Radiance Fields</td>
                              <td>Keunhong Park</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10902v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10902v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿势和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v1" target="_blank">2308.15984v1</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_13903v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Disjoint Pose and Shape for 3D Face Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_13903v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing methods for 3D face reconstruction from a few casually captured images employ deep learning based models along with a 3D Morphable Model(3DMM) as face geometry prior. Structure From Motion(SFM), followed by Multi-View Stereo (MVS), on the other hand, uses dozens of high-resolution images to reconstruct accurate 3D faces.However, it produces noisy and stretched-out results with only two views available. In this paper, taking inspiration from both these methods, we propose an end-to-end pipeline that disjointly solves for pose and shape to make the optimization stable and accurate. We use a face shape prior to estimate face pose and use stereo matching followed by a 3DMM to solve for the shape. The proposed method achieves end-to-end topological consistency, enables iterative face pose refinement procedure, and show remarkable improvement on both quantitative and qualitative results over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_13903v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的从一些随意捕捉的图像重建3D人脸的方法采用基于深度学习的模型以及3D变形模型（3DMM）作为人脸几何先验。另一方面，“运动结构”（SFM）和“多视图立体”（MVS）使用数十幅高分辨率图像来重建精确的3D人脸。然而，在只有两个视图可用的情况下，它会产生嘈杂和拉伸的结果。在本文中，从这两种方法中获得灵感，我们提出了一种端到端的流水线，该流水线对姿态和形状进行不相交求解，以使优化稳定准确。我们在估计面部姿势之前使用面部形状，并使用立体匹配，然后使用3DMM来求解形状。所提出的方法实现了端到端的拓扑一致性，实现了迭代人脸姿态精化过程，并在定量和定性结果上都比现有的最先进的方法有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.13903v1" target="_blank">2308.13903v1</a>
                              </td>
                              <td>Disjoint Pose and Shape for 3D Face Reconstruction</td>
                              <td>Raja Kumar</td>
                              <td>2023-08-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_13903v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.13903v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10003v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10003v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recovering the shape and appearance of real-world objects from natural 2D images is a long-standing and challenging inverse rendering problem. In this paper, we introduce a novel hybrid differentiable rendering method to efficiently reconstruct the 3D geometry and reflectance of a scene from multi-view images captured by conventional hand-held cameras. Our method follows an analysis-by-synthesis approach and consists of two phases. In the initialization phase, we use traditional SfM and MVS methods to reconstruct a virtual scene roughly matching the real scene. Then in the optimization phase, we adopt a hybrid approach to refine the geometry and reflectance, where the geometry is first optimized using an approximate differentiable rendering method, and the reflectance is optimized afterward using a physically-based differentiable rendering method. Our hybrid approach combines the efficiency of approximate methods with the high-quality results of physically-based methods. Extensive experiments on synthetic and real data demonstrate that our method can produce reconstructions with similar or higher quality than state-of-the-art methods while being more efficient.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10003v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自然2D图像中恢复真实世界对象的形状和外观是一个长期存在且具有挑战性的反向渲染问题。在本文中，我们介绍了一种新的混合可微分渲染方法，以从传统手持相机拍摄的多视图图像中有效地重建场景的3D几何结构和反射率。我们的方法采用综合分析法，由两个阶段组成。在初始化阶段，我们使用传统的SfM和MVS方法来重建与真实场景大致匹配的虚拟场景。然后在优化阶段，我们采用混合方法来细化几何和反射率，其中首先使用近似可微分渲染方法优化几何，然后使用基于物理的可微分渲染法优化反射率。我们的混合方法将近似方法的效率与基于物理的方法的高质量结果相结合。对合成数据和真实数据的大量实验表明，我们的方法可以产生与最先进方法相似或更高质量的重建，同时更有效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10003v1" target="_blank">2308.10003v1</a>
                              </td>
                              <td>Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</td>
                              <td>Xiangyang Zhu</td>
                              <td>2023-08-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10003v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10003v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v3" target="_blank">2306.15667v3</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10705v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10705v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the previous 3D human pose estimation work relied on the powerful memory capability of the network to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10705v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以前的大多数3D人体姿态估计工作都依赖于网络强大的存储能力来从训练数据中获得合适的2D-3D映射。很少有研究人员对人体在运动中的姿势变形进行建模。在本文中，我们提出了一种新的人体姿态变形建模方法，并设计了一种基于扩散的运动先验。受运动中非刚性结构领域的启发，我们将重建运动中的三维人体骨骼的任务分为三维参考骨骼的估计和逐帧骨骼变形。使用混合时空NRSfMformer从2D观测序列中同时估计3D参考骨架和每个帧的骨架变形，然后将它们相加以获得每个帧的姿态。随后，使用基于扩散模型的损失项来确保管道学习正确的先验运动知识。最后，我们在主流数据集上评估了我们提出的方法，并获得了优于现有技术的优越结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10705v1" target="_blank">2308.10705v1</a>
                              </td>
                              <td>Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</td>
                              <td>Haorui Ji</td>
                              <td>2023-08-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10705v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10705v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v4" target="_blank">2210.05020v4</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01246v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可以通过以下网站的web界面访问：https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v2" target="_blank">2308.01246v2</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体来说，我们首先提出了一个空间对齐模块（SAM）来将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v2" target="_blank">2305.06794v2</a>
                              </td>
                              <td>MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Large-scale AUV-based Visual Seafloor Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by the increasing number of marine data science applications, there is a growing interest in surveying and exploring the vast, uncharted terrain of the deep sea with robotic platforms. Despite impressive results achieved by many on-land visual mapping algorithms in the past decades, transferring these methods from land to the deep sea remains a challenge due to harsh environmental conditions. Typically, deep-sea exploration involves the use of autonomous underwater vehicles (AUVs) equipped with high-resolution cameras and artificial illumination systems. However, images obtained in this manner often suffer from heterogeneous illumination and quality degradation due to attenuation and scattering, on top of refraction of light rays. All of this together often lets on-land SLAM approaches fail underwater or makes Structure-from-Motion approaches drift or omit difficult images, resulting in gaps, jumps or weakly registered areas. In this work, we present a system that incorporates recent developments in underwater imaging and visual mapping to facilitate automated robotic 3D reconstruction of hectares of seafloor. Our approach is efficient in that it detects and reconsiders difficult, weakly registered areas, to avoid omitting images and to make better use of limited dive time; on the other hand it is computationally efficient; leveraging a hybrid approach combining benefits from SLAM and Structure-from-Motion that runs much faster than incremental reconstructions while achieving at least on-par performance. The proposed system has been extensively tested and evaluated during several research cruises, demonstrating its robustness and practicality in real-world conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在越来越多的海洋数据科学应用的推动下，人们对使用机器人平台测量和探索广阔、未知的深海地形越来越感兴趣。尽管在过去几十年中，许多陆地视觉地图算法取得了令人印象深刻的成果，但由于恶劣的环境条件，将这些方法从陆地转移到深海仍然是一个挑战。通常，深海探测涉及使用配备高分辨率相机和人工照明系统的自动水下航行器。然而，以这种方式获得的图像除了光线的折射之外，还经常由于衰减和散射而遭受不均匀照明和质量下降。所有这些加在一起通常会使陆上SLAM方法在水下失败，或者使“运动结构”方法漂移或忽略困难的图像，从而导致间隙、跳跃或弱配准区域。在这项工作中，我们提出了一个系统，该系统结合了水下成像和视觉地图的最新发展，以促进机器人对公顷海底的自动3D重建。我们的方法是有效的，因为它检测并重新考虑困难的、弱配准的区域，以避免遗漏图像，并更好地利用有限的潜水时间；另一方面，它在计算上是高效的；利用结合SLAM和Structure from Motion的优点的混合方法，该方法比增量重建运行得快得多，同时至少实现了同等性能。所提出的系统在几次研究巡航中进行了广泛的测试和评估，证明了其在现实世界条件下的稳健性和实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06147v1" target="_blank">2308.06147v1</a>
                              </td>
                              <td>Efficient Large-scale AUV-based Visual Seafloor Mapping</td>
                              <td>Mengkun She</td>
                              <td>2023-08-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06147v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10544v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10544v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10544v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-Motion is a technology used to obtain scene structure through image collection, which is a fundamental problem in computer vision. For unordered Internet images, SfM is very slow due to the lack of prior knowledge about image overlap. For sequential images, knowing the large overlap between adjacent frames, SfM can adopt a variety of acceleration strategies, which are only applicable to sequential data. To further improve the reconstruction efficiency and break the gap of strategies between these two kinds of data, this paper presents an efficient covisibility-based incremental SfM. Different from previous methods, we exploit covisibility and registration dependency to describe the image connection which is suitable to any kind of data. Based on this general image connection, we propose a unified framework to efficiently reconstruct sequential images, unordered images, and the mixture of these two. Experiments on the unordered images and mixed data verify the effectiveness of the proposed method, which is three times faster than the state of the art on feature matching, and an order of magnitude faster on reconstruction without sacrificing the accuracy. The source code is publicly available at https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10544v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是一种通过图像采集获得场景结构的技术，是计算机视觉中的一个基本问题。对于无序的互联网图像，由于缺乏图像重叠的先验知识，SfM非常慢。对于序列图像，由于知道相邻帧之间有很大的重叠，SfM可以采用各种加速策略，这些策略仅适用于序列数据。为了进一步提高重建效率，打破这两种数据之间策略的差距，本文提出了一种有效的基于共视性的增量SfM。与以往的方法不同，我们利用共视性和配准依赖性来描述适用于任何类型数据的图像连接。基于这种通用的图像连接，我们提出了一个统一的框架来有效地重建序列图像、无序图像以及这两者的混合图像。在无序图像和混合数据上的实验验证了所提出方法的有效性，该方法在特征匹配方面比现有技术快三倍，在不牺牲精度的情况下重建速度快一个数量级。源代码可在https://github.com/openxrlab/xrsfm</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10544v2" target="_blank">2302.10544v2</a>
                              </td>
                              <td>EC-SfM: Efficient Covisibility-based Structure-from-Motion for Both Sequential and Unordered Images</td>
                              <td>Zhichao Ye</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10544v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10544v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02670v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02670v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02670v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial initialization can be classified into joint and disjoint approaches. Joint approaches tackle both the visual and the inertial parameters together by aligning observations from feature-bearing points based on IMU integration then use a closed-form solution with visual and acceleration observations to find initial velocity and gravity. In contrast, disjoint approaches independently solve the Structure from Motion (SFM) problem and determine inertial parameters from up-to-scale camera poses obtained from pure monocular SLAM. However, previous disjoint methods have limitations, like assuming negligible acceleration bias impact or accurate rotation estimation by pure monocular SLAM. To address these issues, we propose EDI, a novel approach for fast, accurate, and robust visual-inertial initialization. Our method incorporates an Error-state Kalman Filter (ESKF) to estimate gyroscope bias and correct rotation estimates from monocular SLAM, overcoming dependence on pure monocular SLAM for rotation estimation. To estimate the scale factor without prior information, we offer a closed-form solution for initial velocity, scale, gravity, and acceleration bias estimation. To address gravity and acceleration bias coupling, we introduce weights in the linear least-squares equations, ensuring acceleration bias observability and handling outliers. Extensive evaluation on the EuRoC dataset shows that our method achieves an average scale error of 5.8% in less than 3 seconds, outperforming other state-of-the-art disjoint visual-inertial initialization approaches, even in challenging environments and with artificial noise corruption.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02670v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性初始化可以分为联合方法和不相交方法。联合方法通过基于IMU积分对齐特征承载点的观测结果，将视觉和惯性参数结合在一起，然后使用视觉和加速度观测的闭合形式解来找到初始速度和重力。相反，不相交的方法独立地解决了运动结构（SFM）问题，并根据从纯单目SLAM获得的高比例相机姿态确定惯性参数。然而，以前的不相交方法有局限性，比如假设加速度偏差影响可以忽略不计，或者通过纯单目SLAM进行精确的旋转估计。为了解决这些问题，我们提出了EDI，这是一种快速、准确和稳健的视觉惯性初始化的新方法。我们的方法结合了误差状态卡尔曼滤波器（ESKF）来估计陀螺仪偏差，并校正单目SLAM的旋转估计，克服了对纯单目SLAM旋转估计的依赖。为了在没有先验信息的情况下估计比例因子，我们为初始速度、比例、重力和加速度偏差估计提供了一个闭合形式的解决方案。为了解决重力和加速度偏差的耦合问题，我们在线性最小二乘方程中引入了权重，以确保加速度偏差的可观察性并处理异常值。对EuRoC数据集的广泛评估表明，我们的方法在不到3秒内实现了5.8%的平均尺度误差，即使在具有挑战性的环境和人工噪声破坏的情况下，也优于其他最先进的不相交视觉惯性初始化方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02670v1" target="_blank">2308.02670v1</a>
                              </td>
                              <td>EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</td>
                              <td>Weihan Wang</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02670v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02670v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01125v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01125v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01125v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate the ability of our method to perform StereoVO under low visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01125v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳健的特征匹配构成了大多数视觉同步定位和映射（vSLAM）、视觉里程计、3D重建和运动结构（SfM）算法的支柱。然而，从纹理差的场景中恢复特征匹配是一个重大挑战，并且仍然是一个开放的研究领域。在本文中，我们提出了一种基于点和线特征的立体视觉Odometry（StereoVO）技术，该技术使用了一种新的基于注意力图神经网络的特征匹配机制，即使在雾、霾、雨和雪等恶劣天气条件以及夜间照明和眩光等动态照明条件下也能表现良好。我们在多个真实和合成数据集上进行了实验，以验证我们的方法通过稳健的点和线匹配在低能见度天气和照明条件下执行StereoVO的能力。结果表明，与最先进的线匹配算法相比，我们的方法实现了更多的线特征匹配，当与点特征匹配相补充时，线匹配算法在恶劣天气和动态照明条件下始终表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01125v1" target="_blank">2308.01125v1</a>
                              </td>
                              <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network</td>
                              <td>Shenbagaraj Kannapiran</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01125v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01125v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法大多是针对场景的，或者仅限于小场景，因此很难扩展到真实的数据集。在本文中，我们提出了一种新的范式，其中单个通用SCR模型被训练一次，然后部署到新的测试场景中，而不考虑其规模，也不需要进一步的微调。对于给定的查询图像，它从现成的图像检索技术和运动数据库的结构中收集输入：具有稀疏逐点2D-3D注释的相关数据库图像列表。该模型基于转换器架构，并且可以采用可变数量的图像和稀疏的2D-3D注释作为输入。它在几个不同的数据集上进行了训练，在视觉定位方面，它在几个基准测试（包括特定场景的模型）上显著优于其他场景回归方法。特别是，我们在剑桥本地化基准上设定了一个新的技术水平，甚至优于基于特征匹配的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v2" target="_blank">2307.11702v2</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15055v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15055v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15055v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15055v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了PointOdyssey，一个大规模的合成数据集和数据生成框架，用于长期细粒度跟踪算法的训练和评估。我们的目标是通过强调自然运动的长视频来推进最先进的技术。为了实现自然主义的目标，我们使用真实世界的运动捕捉数据制作可变形角色的动画，我们构建3D场景以匹配运动捕捉环境，我们使用通过真实视频上的运动结构挖掘的轨迹来渲染相机视点。我们通过随机化角色外观、运动剖面、材质、照明、3D资产和大气效果来创造组合多样性。我们的数据集目前包括104个视频，平均2000帧长，与之前的工作相比，对应注释多了几个数量级。我们表明，现有的方法可以在我们的数据集中从头开始训练，并且优于已发布的变体。最后，我们介绍了对PIP点跟踪方法的修改，极大地拓宽了其时间感受野，这提高了其在PointOdyssey和两个真实世界基准上的性能。我们的数据和代码可在以下网址公开获取：https://pointodyssey.com</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15055v1" target="_blank">2307.15055v1</a>
                              </td>
                              <td>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</td>
                              <td>Yang Zheng</td>
                              <td>2023-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15055v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15055v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助的小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在一个具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v2" target="_blank">2304.07250v2</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09981v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lazy Visual Localization via Motion Averaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09981v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09981v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉（再）定位对于计算机视觉和机器人的各种应用至关重要。其目标是基于一组摆姿势的数据库图像，估计每个查询图像的6个自由度（DoF）相机姿势。目前，所有领先的解决方案都是基于结构的，它们要么从数据库中显式地构建具有运动结构的3D度量图，要么用场景坐标回归模型隐式地编码3D信息。相反，在不重建3D场景的情况下进行视觉定位提供了明显的好处。它通过减少数据库预处理时间、释放存储需求、不受不完美重建的影响等方式使部署更加方便。在本技术报告中，我们证明了在不从数据库重建场景的情况下实现高定位精度是可能的。实现这一点的关键在于对数据库查询对进行定制的运动平均。实验表明，我们的视觉定位方案LazyLoc与最先进的基于结构的方法相比，具有相当的性能。此外，我们还展示了LazyLoc的多功能性，它可以很容易地扩展到处理复杂的配置，如多查询协同定位和相机钻机。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09981v1" target="_blank">2307.09981v1</a>
                              </td>
                              <td>Lazy Visual Localization via Motion Averaging</td>
                              <td>Siyan Dong</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09981v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09981v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07524v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Causality to Functions with Structural Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07524v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07524v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果关系的精确定义目前是哲学和统计学中一个悬而未决的问题。我们认为因果关系应该被定义为（在数学中）将原因映射到效果的函数。基于结构函数模型，我们提出了因果关系的简化定义。使用delta压缩和对比前向推理，SFM可以产生与我们的直觉相匹配的因果话语，如“X导致Y”和“X是Y的原因”。我们编译了一个因果场景的数据集，并在所有场景中使用SFM。SFM与概率论是相容的，但不可简化为概率论。我们还将SFM与其他因果关系理论进行了比较，并将SFM应用于自由意志、因果解释和精神因果关系等下游问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07524v1" target="_blank">2307.07524v1</a>
                              </td>
                              <td>Reducing Causality to Functions with Structural Models</td>
                              <td>Tianyi Miao</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07524v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07524v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑无人机图像和局部特征的冗余性，在线训练单个码本，避免了其他数据集训练码本的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一个新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM的预测精度提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高出约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在在可变形环境中进行视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'国内/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2311_04205v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04205v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04205v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04205v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04205v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>误解不仅出现在人际交往中，也出现在人与大型语言模型之间。这种差异会使LLM以意想不到的方式解释看似毫不含糊的问题，从而产生错误的回答。尽管人们普遍认为，问题等提示的质量会显著影响LLM提供的回答的质量，但制定LLM能够更好理解的问题的系统方法仍然不成熟。在本文中，我们提出了一种名为“重新表述和回应”（RaR）的方法，该方法允许LLM重新表述和扩展人类提出的问题，并在单个提示中提供回应。这种方法是一种简单而有效的提高性能的提示方法。我们还介绍了RaR的两步变体，其中重新表述的LLM首先重新表述问题，然后将原始问题和重新表述的问题一起传递给不同的响应LLM。这有助于有效利用一个LLM与另一个LLM生成的改写问题。我们的实验表明，我们的方法显著提高了不同模型在各种任务中的性能。我们进一步在理论和实证上对RaR和流行的思想链方法进行了全面的比较。我们表明，RaR与CoT是互补的，可以与CoT结合以实现更好的性能。我们的工作不仅有助于有效提高LLM的性能，而且有助于公平评估LLM的能力。数据和代码可在https://github.com/uclaml/Rephrase-and-Respond.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04205v1" target="_blank">2311.04205v1</a>
                              </td>
                              <td>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</td>
                              <td>Yihe Deng</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04205v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04205v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04199v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04199v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04199v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04199v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Multimodal Models (LMMs) have demonstrated impressive performance across various vision and language tasks, yet their potential applications in recommendation tasks with visual assistance remain unexplored. To bridge this gap, we present a preliminary case study investigating the recommendation capabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a series of qualitative test samples spanning multiple domains and employ these samples to assess the quality of GPT-4V's responses within recommendation scenarios. Evaluation results on these test samples prove that GPT-4V has remarkable zero-shot recommendation abilities across diverse domains, thanks to its robust visual-text comprehension capabilities and extensive general knowledge. However, we have also identified some limitations in using GPT-4V for recommendations, including a tendency to provide similar responses when given similar inputs. This report concludes with an in-depth discussion of the challenges and research opportunities associated with utilizing GPT-4V in recommendation scenarios. Our objective is to explore the potential of extending LMMs from vision and language tasks to recommendation tasks. We hope to inspire further research into next-generation multimodal generative recommendation models, which can enhance user experiences by offering greater diversity and interactivity. All images and prompts used in this report will be accessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04199v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型多模态模型（LMM）在各种视觉和语言任务中表现出了令人印象深刻的性能，但其在视觉辅助推荐任务中的潜在应用仍未得到探索。为了弥补这一差距，我们提出了一个初步的案例研究，调查GPT-4V（ison）的推荐能力，这是OpenAI最近发布的LMM。我们构建了一系列跨越多个领域的定性测试样本，并使用这些样本来评估GPT-4V在推荐场景中的响应质量。对这些测试样本的评估结果证明，GPT-4V凭借其强大的可视化文本理解能力和广泛的通用知识，在不同领域具有显著的零样本推荐能力。然而，我们也发现了使用GPT-4V进行建议的一些局限性，包括在提供类似输入时提供类似响应的趋势。本报告最后深入讨论了在推荐场景中使用GPT-4V的挑战和研究机会。我们的目标是探索将LMM从视觉和语言任务扩展到推荐任务的潜力。我们希望激发对下一代多模式生成推荐模型的进一步研究，该模型可以通过提供更大的多样性和交互性来增强用户体验。此报告中使用的所有图像和提示都可以访问https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04199v1" target="_blank">2311.04199v1</a>
                              </td>
                              <td>Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study</td>
                              <td>Peilin Zhou</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04199v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04199v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_09478v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_09478v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_09478v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_09478v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_09478v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>作为各种语言相关应用程序的通用接口，大型语言模型已经显示出其非凡的功能。基于此，我们的目标是建立一个统一的界面来完成许多视觉语言任务，包括图像描述、视觉问答和视觉基础等。挑战在于使用单一模型通过简单的多模式指令有效地执行不同的视觉语言任务。为了实现这一目标，我们引入了MiniGPT-v2，这是一个可以被视为统一接口的模型，用于更好地处理各种视觉语言任务。我们建议在训练模型时为不同的任务使用唯一标识符。这些标识符使我们的模型能够轻松地更好地区分每个任务指令，并提高每个任务的模型学习效率。经过三阶段训练，实验结果表明，与其他视觉语言多面手模型相比，MiniGPT-v2在许多视觉问答和视觉基础基准上都取得了很强的性能。我们的型号和代码可在https://minigpt-v2.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.09478v3" target="_blank">2310.09478v3</a>
                              </td>
                              <td>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</td>
                              <td>Jun Chen</td>
                              <td>2023-10-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_09478v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.09478v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04177v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04177v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04177v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04177v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04177v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）很聪明，但很健忘。最近对现代LLM的研究（例如，（Bubeck et al.，2023））表明，它们能够执行通常需要人类水平智力的惊人任务。然而，与人类不同的是，冷冻LLM不会随着时间的推移而改善；他们既没有获得新的知识，也没有从成功或失败中吸取教训。提高LLM智能的一些方法包括基于解决问题的性能对模型进行微调（Zelikman等人，2022），以及构建更大、更复杂的模型（Bubeck等人，2023）。然而，这些方法的缺点是需要大量的数据和计算资源来重新训练现有的模型。在本文中，我们探索了使用检索增强生成，也称为RAG（Lewis et al.，2021）来提高解决问题的性能。我们提出了ARM-RAG（用于检索的辅助基本原理存储器增强生成），这是一种在不产生高训练成本的情况下从成功中学习的系统。我们证明了推理链的存储和随后的检索对小学数学问题的表现有积极影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04177v1" target="_blank">2311.04177v1</a>
                              </td>
                              <td>Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation</td>
                              <td>Eric Melz</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04177v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04177v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04166v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Perturbed examples reveal invariances shared by language models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04166v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04166v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04166v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>An explosion of work in language is leading to ever-increasing numbers of available natural language processing models, with little understanding of how new models compare to better-understood models. One major reason for this difficulty is saturating benchmark datasets, which may not reflect well differences in model performance in the wild. In this work, we propose a novel framework for comparing two natural language processing models by revealing their shared invariance to interpretable input perturbations that are designed to target a specific linguistic capability (e.g., Synonym-Invariance, Typo-Invariance). Via experiments on models from within the same and across different architecture families, this framework offers a number of insights about how changes in models (e.g., distillation, increase in size, amount of pre-training) affect multiple well-defined linguistic capabilities. Furthermore, we also demonstrate how our framework can enable evaluation of the invariances shared between models that are available as commercial black-box APIs (e.g., InstructGPT family) and models that are relatively better understood (e.g., GPT-2). Across several experiments, we observe that large language models share many of the invariances encoded by models of various sizes, whereas the invariances encoded by large language models are only shared by other large models. Possessing a wide variety of invariances may be a key reason for the recent successes of large language models, and our framework can shed light on the types of invariances that are retained by or emerge in new models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04166v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言工作的激增导致越来越多的可用自然语言处理模型，人们对新模型与更好理解的模型相比如何了解甚少。造成这种困难的一个主要原因是基准数据集饱和，这可能无法很好地反映野外模型性能的差异。在这项工作中，我们提出了一个新的框架来比较两种自然语言处理模型，通过揭示它们对可解释的输入扰动的共享不变性，这些输入扰动旨在针对特定的语言能力（例如，同义词不变性、类型不变性）。通过对同一体系结构家族内和不同体系结构家族中的模型进行实验，该框架提供了关于模型变化（例如，提取、大小增加、预训练量）如何影响多种定义明确的语言能力的许多见解。此外，我们还展示了我们的框架如何能够评估作为商业黑盒API（例如，InstructGPT家族）可用的模型和相对更好理解的模型（例如，GPT-2）之间共享的不变量。在几个实验中，我们观察到大型语言模型共享由各种大小的模型编码的许多不变量，而由大型语言模型编码的不变量仅由其他大型模型共享。拥有各种各样的不变量可能是大型语言模型最近取得成功的一个关键原因，我们的框架可以揭示新模型保留或出现的不变量类型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04166v1" target="_blank">2311.04166v1</a>
                              </td>
                              <td>Perturbed examples reveal invariances shared by language models</td>
                              <td>Ruchit Rawal</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04166v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04166v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19102v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19102v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19102v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19102v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.   To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to $7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8 quantization, while maintaining the same latency target.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19102v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在内容生成、智能聊天机器人和情感分析等应用中，对大型语言模型（LLM）的需求不断增长，这给LLM服务提供商带来了相当大的挑战。为了有效地使用GPU资源并提高吞吐量，批处理多个请求已成为一种流行的模式；为了进一步加快批处理，LLM量化技术减少了内存消耗并提高了计算能力。然而，流行的量化方案（例如，8位权重激活量化）不能完全利用现代GPU的能力，例如4位整数运算符，从而导致次优性能。为了最大限度地提高LLM的服务吞吐量，我们引入了Atom，这是一种低比特量化方法，可以在可忽略精度损失的情况下实现高吞吐量改进。Atom通过使用低位运算符显著提高了服务吞吐量，并通过低位量化显著降低了内存消耗。它通过应用一种新的混合精度和细粒度量化过程来获得高精度。我们在服务上下文中评估Atom的4位权重激活量化设置。Atom的端到端吞吐量与FP16相比提高了7.73美元，与INT8量化相比提高了2.53美元，同时保持了相同的延迟目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19102v2" target="_blank">2310.19102v2</a>
                              </td>
                              <td>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</td>
                              <td>Yilong Zhao</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19102v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19102v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04155v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04155v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04155v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04155v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them, that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods mostly focus on further training them. However, the extra training of LLMs are usually expensive in terms of GPU compute; worse still, LLMs of interest are oftentimes not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective -- Black-Box Prompt Optimization (BPO) -- to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO is model-agnostic and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22\% increase in the win rate against its original version, and 10\% for GPT-4. Importantly, the \model-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining \model with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04155v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种应用中都取得了令人印象深刻的成功。然而，这些模型往往与人类意图不太一致，这就需要对其进行额外的处理，即对齐问题。为了使LLM更好地遵循用户指令，现有的对齐方法大多侧重于对其进行进一步的训练。然而，LLM的额外训练在GPU计算方面通常是昂贵的；更糟糕的是，感兴趣的LLM通常无法用于用户要求的培训，例如GPT。在这项工作中，我们采用了一种不同的视角——黑盒提示优化（BPO）——来执行对齐。其思想是优化用户提示以适应LLM的输入理解，从而在不更新LLM参数的情况下最好地实现用户的意图。BPO是模型不可知的，经验结果表明，与原始版本相比，与BPO对齐的ChatGPT的胜率提高了22%，GPT-4的胜率增加了10%。重要的是，与模型对齐的LLM可以优于由PPO和DPO对齐的相同模型，并且当将模型与PPO或DPO组合时，它还带来了额外的性能增益。代码和数据集发布于https://github.com/thu-coai/BPO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04155v1" target="_blank">2311.04155v1</a>
                              </td>
                              <td>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</td>
                              <td>Jiale Cheng</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04155v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04155v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04139v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Modelling Sentiment Analysis: LLMs and data augmentation techniques</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04139v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04139v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04139v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper provides different approaches for a binary sentiment classification on a small training dataset. LLMs that provided state-of-the-art results in sentiment analysis and similar domains are being used, such as BERT, RoBERTa and XLNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04139v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提供了在小型训练数据集上进行二元情感分类的不同方法。LLM在情感分析和类似领域提供了最先进的结果，如BERT、RoBERTa和XLNet。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04139v1" target="_blank">2311.04139v1</a>
                              </td>
                              <td>Modelling Sentiment Analysis: LLMs and data augmentation techniques</td>
                              <td>Guillem Senabre Prades</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04139v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04139v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11235v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial-Language Attention Policies for Efficient Robot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11235v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11235v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11235v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11235v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在语言引导的操作方面取得了长足的进步，但现有的工作一直局限于桌面设置。桌面允许完美和一致的相机角度，这些特性在移动操作中是不适用的。涉及在环境中移动的任务计划必须对以自我为中心的观点以及把握平面和角度的变化保持稳健。另一个挑战是确保这一切都是真的，同时仍然能够从有限的数据中有效地学习技能。我们提出了空间语言注意策略（SLAP）作为一种解决方案。SLAP使用三维标记作为输入表示来训练单个多任务、语言条件的动作预测策略。我们的方法显示，在现实世界中，使用单个模型的八个任务的成功率为80%，当引入看不见的杂乱和看不见物体配置时，即使每个任务只有少数例子，成功率也为47.5%。这意味着比之前的工作提高了30%（考虑到看不见的干扰物和配置，提高了20%）。我们看到在移动操作设置方面比基线提高了4倍。此外，我们还展示了SLAP的健壮性如何使我们能够使用大型语言模型从开放词汇指令中执行任务计划，以进行多步移动操作。有关视频，请访问网站：https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11235v3" target="_blank">2304.11235v3</a>
                              </td>
                              <td>Spatial-Language Attention Policies for Efficient Robot Learning</td>
                              <td>Priyam Parashar</td>
                              <td>2023-04-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11235v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11235v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_12425v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Automated Repair of Declarative Software Specifications in the Era of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_12425v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_12425v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_12425v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to improve the correctness and completeness of Alloy declarative specifications through automatic repairs. We analyze the results produced by ChatGPT and compare them with those of leading automatic Alloy repair methods. Our study revealed that while ChatGPT falls short in comparison to existing techniques, it was able to successfully repair bugs that no other technique could address. Our analysis also identified errors in ChatGPT's generated repairs, including improper operator usage, type errors, higher-order logic misuse, and relational arity mismatches. Additionally, we observed instances of hallucinations in ChatGPT-generated repairs and inconsistency in its results. Our study provides valuable insights for software practitioners, researchers, and tool builders considering ChatGPT for declarative specification repairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_12425v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>声明性软件规范语言的日益普及，加上它们在调试方面的固有困难，突显了对适用于此类语言的有效和自动化修复技术的需求。研究人员最近探索了各种自动修复声明性软件规范的方法，如基于模板的修复、反馈驱动的迭代修复和有界穷举方法。大型语言模型的最新发展为声明性规范的自动修复提供了新的机会。在这项研究中，我们评估了利用OpenAI的ChatGPT修复用Alloy声明性语言编写的软件规范的有效性。与命令式语言不同，Alloy中的规范不执行，而是转换为逻辑公式，并使用后端约束求解器进行评估，以识别规范实例和断言的反例。我们的评估重点是ChatGPT通过自动修复来提高Alloy声明性规范的正确性和完整性的能力。我们分析了ChatGPT产生的结果，并将其与领先的自动合金修复方法进行了比较。我们的研究表明，虽然ChatGPT与现有技术相比有所不足，但它能够成功修复其他技术无法解决的缺陷。我们的分析还发现了ChatGPT生成的修复中的错误，包括运算符使用不当、类型错误、高阶逻辑误用和关系arity不匹配。此外，我们在ChatGPT中观察到幻觉产生的修复及其结果的不一致性。我们的研究为考虑ChatGPT进行声明性规范修复的软件从业者、研究人员和工具构建者提供了有价值的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.12425v2" target="_blank">2310.12425v2</a>
                              </td>
                              <td>Automated Repair of Declarative Software Specifications in the Era of Large Language Models</td>
                              <td>Md Rashedul Hasan</td>
                              <td>2023-10-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_12425v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.12425v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04124v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unveiling Safety Vulnerabilities of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04124v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04124v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04124v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern. This paper introduces a unique dataset containing adversarial examples in the form of questions, which we call AttaQ, designed to provoke such harmful or inappropriate responses. We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it. Additionally, we introduce a novel automatic approach for identifying and naming vulnerable semantic regions - input semantic areas for which the model is likely to produce harmful outputs. This is achieved through the application of specialized clustering techniques that consider both the semantic similarity of the input attacks and the harmfulness of the model's responses. Automatically identifying vulnerable semantic regions enhances the evaluation of model weaknesses, facilitating targeted improvements to its safety mechanisms and overall reliability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04124v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型越来越普遍，它们可能做出的有害或不恰当的反应令人担忧。本文介绍了一个独特的数据集，其中包含以问题形式出现的对抗性例子，我们称之为AttaQ，旨在引发这种有害或不恰当的反应。我们通过分析各种模型在受到攻击时的脆弱性来评估数据集的有效性。此外，我们引入了一种新的自动方法来识别和命名脆弱语义区域，即模型可能产生有害输出的输入语义区域。这是通过应用专门的聚类技术来实现的，该技术考虑了输入攻击的语义相似性和模型响应的危害性。自动识别易受攻击的语义区域可以增强对模型弱点的评估，有助于有针对性地改进其安全机制和整体可靠性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04124v1" target="_blank">2311.04124v1</a>
                              </td>
                              <td>Unveiling Safety Vulnerabilities of Large Language Models</td>
                              <td>George Kour</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04124v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04124v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07870v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models as Superpositions of Cultural Perspectives</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07870v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07870v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07870v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07870v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）经常被错误地认为具有个性或一套价值观。我们认为，LLM可以被视为具有不同价值观和个性特征的视角的叠加。LLM表现出依赖于情境的价值观和人格特征，这些价值观和性格特征会根据诱导的视角而变化（而人类则倾向于在不同情境下拥有更连贯的价值观或人格特征）。我们引入了视角可控性的概念，它是指模型采用不同价值观和个性特征的视角的可供性。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究表现出的价值观和性格特征如何从不同的角度变化。通过定性实验，我们表明，当LLM在提示中（隐式或显式）隐含时，LLM表达不同的值，即使这些值没有明显隐含，LLM也表达不同的价值（证明了它们的上下文依赖性）。然后，我们进行了定量实验，以研究不同模型（GPT-4、GPT-3.5、OpenAssistant、StableVicuna、StableLM）的可控性、各种诱导视角方法的有效性以及模型驾驶性能的平滑性。最后，我们研究了我们工作的更广泛意义，并概述了各种相关的科学问题。项目网站位于https://sites.google.com/view/llm-superpositions。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07870v3" target="_blank">2307.07870v3</a>
                              </td>
                              <td>Large Language Models as Superpositions of Cultural Perspectives</td>
                              <td>Grgur Kovač</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07870v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07870v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04076v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Do LLMs exhibit human-like response biases? A case study in survey design</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04076v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04076v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04076v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording -- but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of ``prompts'' have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior. These inconsistencies tend to be more prominent in models that have been instruction fine-tuned. Furthermore, even if a model shows a significant change in the same direction as humans, we find that perturbations that are not meant to elicit significant changes in humans may also result in a similar change, suggesting that such a result could be partially due to other spurious correlations. These results highlight the potential pitfalls of using LLMs to substitute humans in parts of the annotation pipeline, and further underscore the importance of finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04076v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）的能力越来越强，在需要主观标签的现实世界任务中，如在调查和民意调查中，使用LLM作为人类代理的可能性越来越令人兴奋。LLM被广泛引用的一个障碍是它们对即时措辞的敏感性——但有趣的是，人类也以反应偏差的形式对指令变化表现出敏感性。因此，我们认为，如果LLM将被用来近似人类的意见，那么有必要调查LLM在多大程度上也反映了人类的反应偏见（如果有的话）。在这项工作中，我们使用调查设计作为案例研究，对“提示”单词排列引起的人类反应偏差进行了广泛研究。根据先前在社会心理学方面的工作，我们设计了一个数据集，并提出了一个框架来评估LLM在调查问卷中是否表现出类似人类的反应偏差。我们对九个模型的综合评估表明，流行的开放和商业LLM通常不能反映类人行为。这些不一致性往往在经过指令微调的模型中更加突出。此外，即使一个模型显示出与人类相同方向的显著变化，我们发现，不旨在引发人类显著变化的扰动也可能导致类似的变化，这表明这种结果可能部分是由于其他虚假的相关性。这些结果突出了在注释管道的某些部分使用LLM代替人类的潜在陷阱，并进一步强调了模型行为的细粒度特征的重要性。我们的代码、数据集和收集的样本可在https://github.com/lindiatjuatja/BiasMonkey</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04076v1" target="_blank">2311.04076v1</a>
                              </td>
                              <td>Do LLMs exhibit human-like response biases? A case study in survey design</td>
                              <td>Lindia Tjuatja</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04076v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04076v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04072v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04072v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04072v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04072v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04072v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与人类偏好的一致性是大型语言模型（LLM）所期望的特性。目前，主要的对齐方法是基于来自人类反馈的强化学习（RLHF）。尽管RLHF是有效的，但它的实施和训练是复杂的，因此最近的研究探索了如何开发基于监督微调（SFT）的替代对准方法。SFT的一个主要局限性是它本质上是模仿学习，不能完全理解预期的行为是什么。为了解决这个问题，我们提出了一种改进的对齐方法，称为FIGA。与先前的方法不同，我们引入了细粒度（即令牌或短语级别）的质量信号，这些信号是通过对比好的和坏的响应而获得的。我们的做法作出了两大贡献。首先，我们策划了一个精细的比对数据集，将初始响应和相应的修正响应配对。其次，我们设计了一种新的损失函数，可以利用细粒度的质量信号来指导LLM的学习以进行对齐。通过比较一些有竞争力的基线，大量实验证明了我们的方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04072v1" target="_blank">2311.04072v1</a>
                              </td>
                              <td>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</td>
                              <td>Geyang Guo</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04072v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04072v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01305v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01305v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01305v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01305v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing post-training quantization methods for large models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01305v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种任务中表现出优异的性能，但它们会带来巨大的计算和存储成本。量化这些模型是缓解这一问题的有效方法。然而，现有的方法难以在模型精度和硬件效率之间取得平衡。这就是我们引入AWEQ的地方，AWEQ是一种不需要额外训练开销的后期训练方法。AWEQ在超低比特量化和8比特加权和激活（W8A8）量化方面都表现出色。有一种观察结果表明，权重量化比激活量化更不具有挑战性。AWEQ使用信道均衡将激活量化的难度转移到权重，从而在两者的量化难度之间实现平衡，从而最大化性能。我们进一步完善了均衡方法，以减轻量化偏差，确保模型的稳健性。在LLaMA和OPT等流行模型上进行的大量实验表明，对于大型模型，AWEQ优于所有现有的训练后量化方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01305v2" target="_blank">2311.01305v2</a>
                              </td>
                              <td>AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models</td>
                              <td>Baisong Li</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01305v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01305v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04046v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04046v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04046v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04046v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many capable large language models (LLMs) are developed via self-supervised pre-training followed by a reinforcement-learning fine-tuning phase, often based on human or AI feedback. During this stage, models may be guided by their inductive biases to rely on simpler features which may be easier to extract, at a cost to robustness and generalisation. We investigate whether principles governing inductive biases in the supervised fine-tuning of LLMs also apply when the fine-tuning process uses reinforcement learning. Following Lovering et al (2021), we test two hypotheses: that features more $\textit{extractable}$ after pre-training are more likely to be utilised by the final policy, and that the evidence for/against a feature predicts whether it will be utilised. Through controlled experiments on synthetic and natural language tasks, we find statistically significant correlations which constitute strong evidence for these hypotheses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04046v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多有能力的大型语言模型（LLM）是通过自我监督的预训练开发的，然后是强化学习微调阶段，通常基于人类或人工智能的反馈。在这个阶段，模型可能会受到其归纳偏差的引导，依赖于更简单的特征，这些特征可能更容易提取，但代价是鲁棒性和通用性。我们研究了当微调过程使用强化学习时，LLM的监督微调中控制归纳偏差的原则是否也适用。根据Lovering等人（2021），我们检验了两个假设：预训练后更多的$\textit{extractable}$特征更有可能被最终政策使用，支持/反对一个特征的证据可以预测它是否会被使用。通过对合成和自然语言任务的对照实验，我们发现了统计学上显著的相关性，这些相关性构成了这些假设的有力证据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04046v1" target="_blank">2311.04046v1</a>
                              </td>
                              <td>Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features</td>
                              <td>Diogo Cruz</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04046v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04046v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07697v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07697v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07697v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07697v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07697v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在各种任务中都取得了显著的成功，但它们经常与幻觉问题作斗争，尤其是在需要深入负责的推理的场景中。这些问题可以通过在LLM推理中引入外部知识图（KG）来部分解决。在本文中，我们提出了一种新的LLM-KG集成范式“$\hbox｛LLM｝\otimes\hbox{KG｝$”，该范式将LLM视为一个代理，以交互方式探索KG上的相关实体和关系，并基于检索到的知识进行推理。我们通过引入一种称为图上思考（ToG）的新方法来进一步实现这一范式，在该方法中，LLM代理对KG迭代执行波束搜索，发现最有前途的推理路径，并返回最可能的推理结果。我们使用大量精心设计的实验来检验和说明ToG的以下优势：1）与LLM相比，ToG具有更好的深度推理能力；2） ToG通过利用LLM推理和专家反馈，具有知识可追溯性和知识可纠正性；3） ToG为不同的LLM、KGs和提示策略提供了一个灵活的即插即用框架，无需任何额外的培训成本；4） 在某些场景中，具有小LLM模型的ToG的性能可以超过诸如GPT-4的大LLM，并且这降低了LLM部署和应用的成本。作为一种计算成本较低、通用性较好的无训练方法，ToG在9个数据集中的6个数据集中实现了总体SOTA，而以前的大多数SOTA都依赖于额外的训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07697v4" target="_blank">2307.07697v4</a>
                              </td>
                              <td>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph</td>
                              <td>Jiashuo Sun</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07697v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07697v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20246v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20246v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20246v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20246v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20246v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的研究主要集中在开发强大的语言学习模型（LLM），用于单语语言中的数学推理，很少探索在多语言环境中保持功效。为了弥补这一差距，本文率先探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指令数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。基于收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLM，命名为MathOctopus，显著优于传统的开源LLM，并在少数镜头场景中表现出优于ChatGPT的优势。值得注意的是，MathOctopus-13B在MGSM测试集上的准确率达到47.6%，超过了ChatGPT 46.3%。除了显著的结果外，我们还从广泛的实验中发现了几个关键的观察结果和见解：（1）当将拒绝采样策略扩展到多语言环境时，它对模型性能有效，尽管有限。（2） 在多种语言中使用并行语料库进行数学监督微调（SFT）不仅显著提高了模型的多语言性能，而且提高了其单语性能。这表明，制作多语言语料库可以被视为提高特定语言中模型性能的重要策略，尤其是在数学推理任务中。例如，MathOctopus-7B在GSM8K测试集上的英语训练率从42.2%提高到50.8%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20246v3" target="_blank">2310.20246v3</a>
                              </td>
                              <td>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</td>
                              <td>Nuo Chen</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20246v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20246v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_03427v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_03427v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_03427v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_03427v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_03427v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着自然语言处理的最新进展，大型语言模型（LLM）已成为各种现实世界应用程序的强大工具。尽管LLM的能力很强，但其内在的生成能力可能不足以处理复杂的任务，而复杂的任务需要任务规划和外部工具的使用相结合。在本文中，我们首先提出了一个为基于LLM的人工智能代理量身定制的结构化框架，并讨论了解决复杂问题所需的关键能力。在这个框架内，我们设计了两种不同类型的代理（即一步代理和顺序代理）来执行推理过程。随后，我们使用各种LLM实例化框架，并评估它们在典型任务上的任务规划和工具使用（TPTU）能力。通过强调关键发现和挑战，我们的目标是为研究人员和从业者提供有用的资源，以在他们的人工智能应用中利用LLM的力量。我们的研究强调了这些模型的巨大潜力，同时也确定了需要更多调查和改进的领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.03427v3" target="_blank">2308.03427v3</a>
                              </td>
                              <td>TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage</td>
                              <td>Jingqing Ruan</td>
                              <td>2023-08-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_03427v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.03427v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01767v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01767v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01767v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01767v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01767v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近对大型语言模型（LLM）的评估集中在测试其用于基本自然语言任务的零样本/少搜索功能以及将指令转换为工具API的能力。然而，在复杂的多模态环境中，利用复杂工具完成多转弯、多模态指令的LLM的评估尚未得到研究。为了解决这一差距，我们引入了PowerPoint任务完成（PPTC）基准，以评估LLM根据用户说明创建和编辑PPT文件的能力。它包含279个多回合会话，涵盖不同的主题和数百条涉及多模式操作的指令。我们还提出了PPTX-Match评估系统，该系统基于预测文件而不是标签API序列来评估LLM是否完成指令，因此它支持各种LLM生成的API序列。我们测量了3个封闭LLM和6个开源LLM。结果表明，GPT-4在单回合对话测试中的准确率为75.1%，优于其他LLM，但在完成整个会话方面面临挑战，会话准确率仅为6%。我们在基准测试中发现了三个主要的错误原因：多回合会话中的错误积累、长PPT模板处理和多模态感知。这些对未来的LLM和代理系统提出了巨大的挑战。我们在\url发布PPTC的数据、代码和评估系统{https://github.com/gydpku/PPTC}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01767v2" target="_blank">2311.01767v2</a>
                              </td>
                              <td>PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</td>
                              <td>Yiduo Guo</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01767v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01767v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13709v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Undesirable biases in NLP: Addressing challenges of measurement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13709v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13709v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13709v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As Large Language Models and Natural Language Processing (NLP) technology rapidly develop and spread into daily life, it becomes crucial to anticipate how their use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases, from generating derogatory stereotypes to producing disparate outcomes for different social groups. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems and it is often unclear what they actually measure. In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the \emph{construct validity} and the \emph{reliability} of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide NLP practitioners with methodological tools for designing better bias measures, and to inspire them more generally to explore tools from psychometrics when working on bias measurement tools.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13709v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型和自然语言处理（NLP）技术的迅速发展并渗透到日常生活中，预测它们的使用会对人们造成怎样的伤害变得至关重要。近年来备受关注的一个问题是，这项技术表现出了有害的偏见，从产生贬损的刻板印象到为不同的社会群体产生不同的结果。尽管在评估和减轻这些偏差方面投入了大量精力，但我们测量NLP模型偏差的方法存在严重问题，而且通常不清楚它们实际测量的是什么。在本文中，我们提供了一种跨学科的方法，通过采用心理测量学的视角来讨论NLP模型偏差的问题，心理测量学是一个专门测量无法直接观察到的偏差等概念的领域。特别是，我们将探讨心理测量学的两个核心概念，即测量工具的结构有效性和可靠性，并讨论如何在测量模型偏差的背景下应用它们。我们的目标是为NLP从业者提供设计更好的偏见测量的方法工具，并激励他们在使用偏见测量工具时更广泛地探索心理测量工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13709v3" target="_blank">2211.13709v3</a>
                              </td>
                              <td>Undesirable biases in NLP: Addressing challenges of measurement</td>
                              <td>Oskar van der Wal</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13709v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13709v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03839v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aspects of human memory and Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03839v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03839v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03839v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are huge artificial neural networks which primarily serve to generate text, but also provide a very sophisticated probabilistic model of language use. Since generating a semantically consistent text requires a form of effective memory, we investigate the memory properties of LLMs and find surprising similarities with key characteristics of human memory. This result strongly suggests that the biological features of human memory leave an imprint on the way that we structure our textual narratives.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03839v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）是一种巨大的人工神经网络，主要用于生成文本，但也提供了一个非常复杂的语言使用概率模型。由于生成语义一致的文本需要一种有效的记忆形式，我们研究了LLM的记忆特性，发现它与人类记忆的关键特征惊人地相似。这一结果有力地表明，人类记忆的生物学特征在我们构建文本叙事的方式上留下了印记。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03839v1" target="_blank">2311.03839v1</a>
                              </td>
                              <td>Aspects of human memory and Large Language Models</td>
                              <td>Romuald A. Janik</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03839v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03839v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03837v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OLaLa: Ontology Matching with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03837v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03837v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03837v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03837v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本体（更普遍地说：知识图）匹配是一项具有挑战性的任务，自然语言中的信息是需要处理的最重要的信号之一。随着大型语言模型的兴起，有可能以更好的方式将这些知识整合到匹配管道中。仍然需要做出许多决定，例如，如何生成对模型有用的提示，如何在提示中公式化KG中的信息，选择哪种大型语言模型，如何向模型提供现有的对应关系，如何生成候选者等。在本文中，我们提出了一个原型，通过将多个开放的大型语言模型的零样本和少搜索提示应用于本体对齐评估倡议（OAEI）的不同任务来探索这些问题。我们表明，只有少数几个例子和精心设计的提示，就有可能获得与使用更大部分基本事实的监督匹配系统相当的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03837v1" target="_blank">2311.03837v1</a>
                              </td>
                              <td>OLaLa: Ontology Matching with Large Language Models</td>
                              <td>Sven Hertling</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03837v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03837v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03353v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03353v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03353v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03353v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Theory of Mind (ToM) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. Here, we leverage dynamic epistemic logic to isolate a particular component of ToM and to generate controlled problems. We also introduce new verbalization techniques to express these problems in English natural language. Our findings indicate that some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement. Our code and datasets are publicly available (https://huggingface.co/datasets/sileod/mindgames , https://github.com/sileod/llm-theory-of-mind )</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03353v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>心理理论是智力的重要组成部分，但其评估仍然是激烈争论的主题。先前的研究将人类ToM评估应用于自然语言处理模型，使用人工创建的标准化测试或基于规则的模板。然而，这些方法主要侧重于简单化的推理，需要进一步验证。在这里，我们利用动态认识逻辑来隔离ToM的特定组件，并生成受控问题。我们还引入了新的动词化技术来表达英语自然语言中的这些问题。我们的研究结果表明，一些语言模型的缩放（从70M到6B和从350M到174B）并不总是产生比随机机会更好的结果。虽然GPT-4展示了卓越的认知推理能力，但仍有改进的空间。我们的代码和数据集是公开的(https://huggingface.co/datasets/sileod/mindgames，https://github.com/sileod/llm-theory-of-mind）</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03353v2" target="_blank">2305.03353v2</a>
                              </td>
                              <td>MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic</td>
                              <td>Damien Sileo</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03353v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03353v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03812v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Conversations in Galician: a Large Language Model for an Underrepresented Language</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03812v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03812v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03812v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent proliferation of Large Conversation Language Models has highlighted the economic significance of widespread access to this type of AI technologies in the current information age. Nevertheless, prevailing models have primarily been trained on corpora consisting of documents written in popular languages. The dearth of such cutting-edge tools for low-resource languages further exacerbates their underrepresentation in the current economic landscape, thereby impacting their native speakers. This paper introduces two novel resources designed to enhance Natural Language Processing (NLP) for the Galician language. We present a Galician adaptation of the Alpaca dataset, comprising 52,000 instructions and demonstrations. This dataset proves invaluable for enhancing language models by fine-tuning them to more accurately adhere to provided instructions. Additionally, as a demonstration of the dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician, a language not originally supported by the model, by following the Alpaca format. This work contributes to the research on multilingual models tailored for low-resource settings, a crucial endeavor in ensuring the inclusion of all linguistic communities in the development of Large Language Models. Another noteworthy aspect of this research is the exploration of how knowledge of a closely related language, in this case, Portuguese, can assist in generating coherent text when training resources are scarce. Both the Galician Alpaca dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we have made the source code available to facilitate replication of this experiment and encourage further advancements for underrepresented languages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03812v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近大型会话语言模型的激增凸显了在当前信息时代广泛使用这类人工智能技术的经济意义。然而，主流模型主要是在语料库上训练的，语料库由用流行语言编写的文档组成。缺乏这种用于低资源语言的尖端工具，进一步加剧了它们在当前经济格局中的代表性不足，从而影响了母语为英语的人。本文介绍了两种旨在增强加利西亚语自然语言处理（NLP）的新资源。我们展示了对Alpaca数据集的加利西亚改编，包括52000条指令和演示。事实证明，该数据集对于通过微调语言模型以更准确地遵循所提供的指令来增强语言模型是非常宝贵的。此外，作为数据集实用性的演示，我们对LLaMA-7B进行了微调，通过遵循Alpaca格式，以加利西亚语（该模型最初不支持的语言）进行理解和响应。这项工作有助于研究为低资源环境量身定制的多语言模型，这是确保所有语言社区都参与大型语言模型开发的关键努力。这项研究的另一个值得注意的方面是探索在培训资源稀缺的情况下，一门密切相关的语言（在本例中为葡萄牙语）的知识如何有助于生成连贯的文本。Galician Alpaca数据集和Cabuxa-7B都可以在我们的Huggingface Hub上公开访问，我们已经提供了源代码，以促进该实验的复制，并鼓励对代表性不足的语言进行进一步的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03812v1" target="_blank">2311.03812v1</a>
                              </td>
                              <td>Conversations in Galician: a Large Language Model for an Underrepresented Language</td>
                              <td>Eliseo Bao</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03812v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03812v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03799v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03799v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03799v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03799v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $<human, action, object>$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \url{https://github.com/Caoyichao/UniHOI}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03799v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人机交互（HOI）检测旨在理解人与物体之间的复杂关系，预测$<Human，action，object>$三元组，并作为许多计算机视觉任务的基础。然而，现实世界中人机交互的复杂性和多样性给注释和识别带来了重大挑战，尤其是在开放世界背景下识别交互时。本研究通过使用视觉语言（VL）基础模型和大型语言模型（LLM）来探索开放世界环境中的通用交互识别。所提出的方法被称为\emph｛\textbf｛UniHOI｝｝。我们对视觉HOI检测器固有的三个层次特征进行了深入分析，并提出了一种针对VL基础模型的高级关系提取方法，我们称之为基于HO提示的学习。我们的设计包括HO提示引导解码器（HOPD），有助于将基础模型中的高级关系表示与图像中的各种HO对相关联。此外，我们利用LLM（\emph｛即｝GPT）进行交互解释，为复杂的HOI生成更丰富的语言理解。对于开放类别交互识别，我们的方法支持两种输入类型之一：交互短语或解释句。我们高效的架构设计和学习方法有效地释放了VL基础模型和LLM的潜力，使UniHOI能够在监督和零样本设置下以巨大的优势超越所有现有方法。代码和预先训练的权重可在以下网址获得：\url{https://github.com/Caoyichao/UniHOI}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03799v1" target="_blank">2311.03799v1</a>
                              </td>
                              <td>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</td>
                              <td>Yichao Cao</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03799v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03799v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03783v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03783v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03783v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03783v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly. Our project can be found at https://sites.google.com/view/manipmob-mmkg</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03783v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>嵌入式人工智能是人工智能和机器人领域最受欢迎的研究之一，它可以有效地提高为人类服务的真实世界代理（即机器人）的智能。场景知识对于代理人在多变的开放世界中理解周围环境并做出正确决策至关重要。目前，具体任务的知识库是缺失的，大多数现有工作使用通用知识库或预先训练的模型来增强智能体的智能。对于传统的知识库来说，它是稀疏的，在数据收集方面的容量和成本不足。对于预先训练的模型，它们面临知识的不确定性和难以维护的问题。为了克服场景知识的挑战，我们提出了一种结合传统知识工程和大型语言模型的场景驱动多模式知识图（scene MMKG）构建方法。介绍了一种统一的场景知识注入框架用于知识表示。为了评估我们提出的方法的优势，我们实例化了Scene MMKG，考虑到典型的室内机器人功能（操纵和移动），命名为ManipMob MMKG。特征比较表明，我们实例化的ManipMob MMKG在数据收集效率和知识质量方面具有广泛的优势。在典型具体任务上的实验结果表明，使用我们实例化的ManipMob MMKG的知识增强方法可以在不复杂地重新设计模型结构的情况下显著提高性能。我们的项目可以在https://sites.google.com/view/manipmob-mmkg</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03783v1" target="_blank">2311.03783v1</a>
                              </td>
                              <td>Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</td>
                              <td>Song Yaoxian</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03783v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03783v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03778v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03778v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03778v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03778v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative large language models(LLMs) are proficient in solving general problems but often struggle to handle domain-specific tasks. This is because most of domain-specific tasks, such as personalized recommendation, rely on task-related information for optimal performance. Current methods attempt to supplement task-related information to LLMs by designing appropriate prompts or employing supervised fine-tuning techniques. Nevertheless, these methods encounter the certain issue that information such as community behavior pattern in RS domain is challenging to express in natural language, which limits the capability of LLMs to surpass state-of-the-art domain-specific models. On the other hand, domain-specific models for personalized recommendation which mainly rely on user interactions are susceptible to data sparsity due to their limited common knowledge capabilities. To address these issues, we proposes a method to bridge the information gap between the domain-specific models and the general large language models. Specifically, we propose an information sharing module which serves as an information storage mechanism and also acts as a bridge for collaborative training between the LLMs and domain-specific models. By doing so, we can improve the performance of LLM-based recommendation with the help of user behavior pattern information mined by domain-specific models. On the other hand, the recommendation performance of domain-specific models can also be improved with the help of common knowledge learned by LLMs. Experimental results on three real-world datasets have demonstrated the effectiveness of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03778v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成型大型语言模型（LLM）精通于解决一般问题，但往往难以处理特定领域的任务。这是因为大多数特定于领域的任务，如个性化推荐，都依赖于与任务相关的信息来获得最佳性能。当前的方法试图通过设计适当的提示或采用监督微调技术来向LLM补充与任务相关的信息。然而，这些方法遇到了这样一个问题，即RS领域中的社区行为模式等信息难以用自然语言表达，这限制了LLM超越最先进的领域特定模型的能力。另一方面，主要依赖于用户交互的个性化推荐的特定领域模型由于其有限的公共知识能力而容易受到数据稀疏性的影响。为了解决这些问题，我们提出了一种方法来弥合特定领域模型和一般大型语言模型之间的信息差距。具体而言，我们提出了一个信息共享模块，该模块充当信息存储机制，也充当LLM和特定领域模型之间协作训练的桥梁。通过这样做，我们可以借助特定领域模型挖掘的用户行为模式信息来提高基于LLM的推荐的性能。另一方面，特定领域模型的推荐性能也可以在LLM学习的公共知识的帮助下得到提高。在三个真实世界数据集上的实验结果证明了该方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03778v1" target="_blank">2311.03778v1</a>
                              </td>
                              <td>Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation</td>
                              <td>Wenxuan Zhang</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03778v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03778v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03764v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neuro-GPT: Developing A Foundation Model for EEG</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03764v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03764v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03764v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To handle the scarcity and heterogeneity of electroencephalography (EEG) data in Brain-Computer Interface (BCI) tasks, and to harness the vast public data, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale public EEG dataset, using a self-supervised task which learns how to reconstruct the masked chunk in EEG. We then fine-tune the foundation model on a Motor Imagery Classification task where only 9 subjects are available. Experiments demonstrated that applying foundation model can significantly improve classification performance compared to the model trained from scratch, which provides evidence for the advanced generalizability of foundation model and the ability to address the challenges of data scarcity and heterogeneity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03764v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了解决脑机接口（BCI）任务中脑电图（EEG）数据的稀缺性和异质性，并利用大量的公共数据，我们提出了Neuro-GPT，这是一个由EEG编码器和GPT模型组成的基础模型。基础模型在大规模的公共EEG数据集上进行预训练，使用自监督任务学习如何重建EEG中的掩蔽块。然后，我们在只有9个受试者可用的运动图像分类任务中对基础模型进行微调。实验表明，与从头开始训练的模型相比，应用基础模型可以显著提高分类性能，这为基础模型的高级可推广性以及应对数据稀缺和异构挑战的能力提供了证据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03764v1" target="_blank">2311.03764v1</a>
                              </td>
                              <td>Neuro-GPT: Developing A Foundation Model for EEG</td>
                              <td>Wenhui Cui</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03764v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03764v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03285v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">S-LoRA: Serving Thousands of Concurrent LoRA Adapters</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03285v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03285v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03285v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03285v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在大型语言模型的部署中，通常采用“先预训练后微调”的范式。低秩自适应（LoRA）是一种参数有效的微调方法，通常用于使基本模型适应大量任务，从而产生从一个基本模型导出的大量LoRA适配器。我们观察到，这种范式为服务过程中的批量推理提供了重要的机会。为了利用这些机会，我们介绍了S-LoRA，这是一个为许多LoRA适配器的可扩展服务而设计的系统。S-LoRA将所有适配器存储在主内存中，并将当前运行的查询所使用的适配器提取到GPU内存中。为了有效地使用GPU内存并减少碎片，S-LoRA提出了统一寻呼。统一分页使用统一内存池来管理具有不同列组的动态适配器权重和具有不同序列长度的KV缓存张量。此外，S-LoRA采用了一种新颖的张量并行策略和高度优化的自定义CUDA内核，用于LoRA计算的异构批处理。总之，这些功能使S-LoRA能够在单个GPU或多个GPU上以较小的开销为数千个LoRA适配器提供服务。与HuggingFace PEFT和vLLM（最初支持LoRA服务）等最先进的库相比，S-LoRA可以将吞吐量提高4倍，并将服务的适配器数量增加几个数量级。因此，S-LoRA实现了许多特定任务微调模型的可扩展服务，并提供了大规模定制微调服务的潜力。代码可在https://github.com/S-LoRA/S-LoRA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03285v2" target="_blank">2311.03285v2</a>
                              </td>
                              <td>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</td>
                              <td>Ying Sheng</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03285v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03285v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03758v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Model based Long-tail Query Rewriting in Taobao Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03758v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03758v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03758v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Query rewriting serves as an important technique to bridge semantic gaps inherent in the semantic matching process. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of \textit{``\nothing''} caused by semantic gap. In this paper, we present \textbf{\method}, a comprehensive framework that \textbf{B}ridges the s\textbf{E}mantic gap for long-tail \textbf{QUE}ries. \method comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. Specifically, we first construct a rewriting dataset based on rejection sampling, and mix it with multiple auxiliary tasks data to fine tune our large language model (LLM) in a supervised fashion during the first stage. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, which would be fed into Taobao offline system to simulate the retrieval process and obtain the partial order. Leveraging the partial order of candidate rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in enhancing retrieval performance. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (\#Trans) and unique visitor (UV) for long-tail queries. \method has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03758v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在电子商务搜索领域，语义匹配的重要性怎么强调都不为过，因为它直接影响用户体验和公司收入。查询重写是弥合语义匹配过程中固有的语义差距的一项重要技术。然而，现有的查询重写方法往往难以有效地优化长尾查询，并缓解由语义间隙引起的\textit｛`\nothing｝现象。在本文中，我们提出了\textbf｛\method｝，一个综合框架{B}ridgess\textbf{E}mantic长尾间隙\textbf{QUE}ries.该方法包括三个阶段：多指令监督微调（SFT）、离线反馈和目标对准。具体来说，我们首先构建了一个基于拒绝采样的重写数据集，并将其与多个辅助任务数据混合，以在第一阶段以监督的方式微调我们的大型语言模型（LLM）。随后，利用训练有素的LLM，我们使用波束搜索生成多个候选重写，这些重写将被输入淘宝离线系统，以模拟检索过程并获得部分订单。利用候选重写的偏序，我们引入了一种对比学习方法来突出重写之间的区别，并使模型与淘宝网上目标保持一致。离线实验证明了该方法在提高检索性能方面的有效性。在线A/B测试表明，我们的方法可以显著提高长尾查询的商品总量（GMV）、交易数量（\#Trans）和唯一访客（UV）。\自2023年10月以来，该方法已在中国最受欢迎的在线购物平台之一淘宝上部署。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03758v1" target="_blank">2311.03758v1</a>
                              </td>
                              <td>Large Language Model based Long-tail Query Rewriting in Taobao Search</td>
                              <td>Wenjun Peng</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03758v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03758v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03755v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multilingual Mathematical Autoformalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03755v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03755v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03755v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autoformalization is the task of translating natural language materials into machine-verifiable formalisations. Progress in autoformalization research is hindered by the lack of a sizeable dataset consisting of informal-formal pairs expressing the same essence. Existing methods tend to circumvent this challenge by manually curating small corpora or using few-shot learning with large language models. But these methods suffer from data scarcity and formal language acquisition difficulty. In this work, we create $\texttt{MMA}$, a large, flexible, multilingual, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones. Experiments show that language models fine-tuned on $\texttt{MMA}$ produce $16-18\%$ of statements acceptable with minimal corrections on the $\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with the base model. We demonstrate that fine-tuning on multilingual formal data results in more capable autoformalization models even when deployed on monolingual tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03755v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动形式化是将自然语言材料翻译成机器可验证的形式化的任务。由于缺乏一个由表达相同本质的非正式形式对组成的大型数据集，自动规范化研究的进展受到阻碍。现有的方法倾向于通过手动管理小语料库或使用大语言模型的少量镜头学习来规避这一挑战。但这些方法存在数据匮乏和形式语言习得困难的问题。在这项工作中，我们创建了$\texttt｛MMA｝$，这是一个大型、灵活、多语言、多领域的非正式形式对数据集，通过使用语言模型进行反向翻译，即从正式数学语句翻译成相应的非正式数学语句。实验表明，在$\texttt｛MMA｝$上微调的语言模型在$\txttt｛miniF2F｝$和$\textt｛ProofNet｝$基准测试上产生了16-18\%%$的可接受语句，而在基本模型上则是$0\%%$。我们证明，即使部署在单语任务上，对多语言形式数据的微调也会产生更强大的自动规范化模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03755v1" target="_blank">2311.03755v1</a>
                              </td>
                              <td>Multilingual Mathematical Autoformalization</td>
                              <td>Albert Q. Jiang</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03755v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03755v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03754v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Which is better? Exploring Prompting Strategy For LLM-based Metrics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03754v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03754v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03754v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03754v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文将DSBA提交给提示大型语言模型作为可解释度量共享任务，其中系统被提交到两个轨道：小型和大型摘要轨道。随着GPT-4等先进的大型语言模型（LLM）的出现，评估自然语言生成（NLG）的质量变得越来越重要。传统的基于相似性的度量，如BLEU和ROUGE，已被证明与人类评估不一致，不适合于开放式生成任务。为了解决这个问题，我们探索了基于LLM的度量的潜在能力，特别是利用开源LLM。在本研究中，从提示策略、分数聚合和可解释性三个方面系统地分析了各种提示和提示技术。我们的研究重点是制定有效的提示模板，确定NLG质量分数的粒度，并评估上下文示例对基于LLM的评估的影响。此外，对三种聚合策略进行了比较，以确定聚合NLG质量分数的最可靠方法。为了检验可解释性，我们设计了一种策略，为分数生成理由，并分析开源LLM产生的解释的特征。大量的实验提供了关于开源LLM评估能力的见解，并提出了有效的提示策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03754v1" target="_blank">2311.03754v1</a>
                              </td>
                              <td>Which is better? Exploring Prompting Strategy For LLM-based Metrics</td>
                              <td>Joonghoon Kim</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03754v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03754v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03748v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03748v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03748v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03748v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03748v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>统一序列标记以广义的序列到序列格式表达不同的序列标记问题，如命名实体识别、关系提取、语义角色标记等，为最大限度地利用大型语言模型知识进行结构化预测提供了机会。不幸的是，这需要将它们格式化为基础预训练语言模型（PLM）未知的专用增强格式，从而需要对目标格式进行微调。这大大限制了它在数据有限的设置中的有用性，在这些设置中，微调大型模型无法正确地推广到目标格式。为了应对这一挑战并有效利用PLM知识，我们提出了FISH-DIP，这是一种样本感知的动态稀疏微调策略，在微调过程中，根据高度回归示例的反馈，选择性地关注一小部分参数。通过利用稀疏性的动态性，我们的方法减轻了学习良好的样本的影响，并优先考虑表现不佳的实例以改进泛化。在序列标记的五项任务中，我们证明FISH-DIP可以在低资源设置下平稳地优化模型，根据目标评估设置，在完全微调的基础上提供高达40%的性能改进。此外，与上下文学习和其他参数高效微调方法相比，FISH-DIP的性能相当或更好，尤其是在资源极低的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03748v1" target="_blank">2311.03748v1</a>
                              </td>
                              <td>Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning</td>
                              <td>Sarkar Snigdha Sarathi Das</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03748v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03748v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03739v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Large Language Models for Automated Proof Synthesis in Rust</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03739v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03739v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03739v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Formal verification can provably guarantee the correctness of critical system software, but the high proof burden has long hindered its wide adoption. Recently, Large Language Models (LLMs) have shown success in code analysis and synthesis. In this paper, we present a combination of LLMs and static analysis to synthesize invariants, assertions, and other proof structures for a Rust-based formal verification framework called Verus. In a few-shot setting, LLMs demonstrate impressive logical ability in generating postconditions and loop invariants, especially when analyzing short code snippets. However, LLMs lack the ability to retain and propagate context information, a strength of traditional static analysis. Based on these observations, we developed a prototype based on OpenAI's GPT-4 model. Our prototype decomposes the verification task into multiple smaller ones, iteratively queries GPT-4, and combines its output with lightweight static analysis. We evaluated the prototype with a developer in the automation loop on 20 vector-manipulating programs. The results demonstrate that it significantly reduces human effort in writing entry-level proof code.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03739v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式化验证可以证明地保证关键系统软件的正确性，但高昂的证明负担长期阻碍了其广泛采用。最近，大型语言模型（LLM）在代码分析和合成方面取得了成功。在本文中，我们提出了LLM和静态分析的组合，以合成名为Verus的基于Rust的形式验证框架的不变量、断言和其他证明结构。在少数镜头设置中，LLM在生成后条件和循环不变量方面表现出了令人印象深刻的逻辑能力，尤其是在分析短代码片段时。然而，LLM缺乏保留和传播上下文信息的能力，这是传统静态分析的优势。基于这些观察，我们开发了一个基于OpenAI的GPT-4模型的原型。我们的原型将验证任务分解为多个较小的任务，迭代查询GPT-4，并将其输出与轻量级静态分析相结合。我们在20个矢量操作程序的自动化循环中与开发人员一起评估了原型。结果表明，它显著减少了编写入门级验证代码的人力成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03739v1" target="_blank">2311.03739v1</a>
                              </td>
                              <td>Leveraging Large Language Models for Automated Proof Synthesis in Rust</td>
                              <td>Jianan Yao</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03739v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03739v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18581v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18581v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18581v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18581v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have achieved remarkable performance across a wide variety of natural language tasks; however, their large size makes their inference slow and computationally expensive. Focusing on this problem, we propose to instruction tune LLMs with additional explicit losses from the intermediate layers (LITE) and show that it enables these layers to acquire 'good' generation ability without affecting the generation ability of the final layer. We perform 'dynamic confidence-based early exiting' at token level from the intermediate layers which improves the efficiency of text generation without compromising the quality of the generation. We conduct comprehensive experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and holistically evaluate on four different human-instruction test sets. We show that dynamic early exiting achieves consistent and considerable inference computation cost improvements (37.86% for 7B and 46.35% for 13B model) while maintaining the generation quality of the responses. We further conduct a thorough analysis of the results over several important aspects, such as comparing the semantic similarity of the outputs and dissecting the efficiency improvements by comparing the number of tokens generated in the output. In summary, our work contributes to improving the efficiency of LLM inference while maintaining the generation quality, a crucial step en route to enabling their widespread adoption.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18581v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种各样的自然语言任务中都取得了显著的性能；然而，它们的大尺寸使得它们的推理速度慢并且计算成本高。针对这个问题，我们建议在中间层（LITE）有额外显式损失的情况下对LLM进行指令调整，并表明它使这些层能够在不影响最终层的生成能力的情况下获得“良好”的生成能力。我们从中间层在令牌级别执行“基于动态置信度的早期退出”，这在不影响生成质量的情况下提高了文本生成的效率。我们在Alpaca数据集上通过指令调整LLaMA-2模型进行了全面的实验，并在四个不同的人类指令测试集上进行了全面评估。我们表明，动态提前退出在保持响应的生成质量的同时，实现了一致且可观的推理计算成本改进（7B模型为37.86%，13B模型为46.35%）。我们进一步从几个重要方面对结果进行了深入分析，例如比较输出的语义相似性，并通过比较输出中生成的令牌数量来剖析效率的提高。总之，我们的工作有助于提高LLM推理的效率，同时保持生成质量，这是实现LLM广泛采用的关键一步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18581v2" target="_blank">2310.18581v2</a>
                              </td>
                              <td>Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE</td>
                              <td>Neeraj Varshney</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18581v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18581v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03734v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03734v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03734v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03734v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model's capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multi-hop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03734v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>包括大型语言模型（LLM）在内的神经模型在多跳问题回答方面取得了优异的性能。为了从LLM中获得推理能力，最近的工作提出使用思想链（CoT）机制来生成推理链和答案，这增强了模型进行多跳推理的能力。然而，仍然存在一些挑战：例如与不准确的推理、幻觉和缺乏可解释性作斗争。另一方面，信息提取（IE）识别基于文本的实体、关系和事件。提取的结构化信息可以很容易地由人类和机器进行解释（Grishman，2019）。在这项工作中，我们研究了构建和利用提取的语义结构（图）进行多跳问答，特别是推理过程。实证结果和人工评估表明，我们的框架：在两个基准数据集上生成了更忠实的推理链，并显著提高了QA性能。此外，与生成的推理链和基于显著性的解释相比，提取的结构本身自然地提供了人类偏好的有根据的解释。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03734v1" target="_blank">2311.03734v1</a>
                              </td>
                              <td>Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning</td>
                              <td>Ruosen Li</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03734v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03734v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03731v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey of Large Language Models Attribution</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03731v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03731v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03731v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). This paper presents a comprehensive review of the attribution mechanisms employed by these systems, particularly large language models. Though attribution or citation improve the factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The aim of this survey is to provide valuable insights for researchers, aiding in the refinement of attribution methodologies to enhance the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; hence, we maintain a repository to keep track of ongoing studies at https://github.com/HITsz-TMG/awesome-llm-attributions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03731v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开放域生成系统在会话人工智能领域（例如，生成搜索引擎）获得了极大的关注。本文全面回顾了这些系统所采用的归因机制，特别是大型语言模型。尽管归因或引用提高了事实性和可验证性，但模糊的知识库、固有的偏见和过度归因的缺点等问题可能会阻碍这些系统的有效性。这项调查的目的是为研究人员提供有价值的见解，有助于改进归因方法，以提高开放领域生成系统生成的响应的可靠性和准确性。我们认为，这一领域仍处于早期阶段；因此，我们在https://github.com/HITsz-TMG/awesome-llm-attributions.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03731v1" target="_blank">2311.03731v1</a>
                              </td>
                              <td>A Survey of Large Language Models Attribution</td>
                              <td>Dongfang Li</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03731v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03731v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06827v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Teaching Language Models to Hallucinate Less with Synthetic Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06827v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06827v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06827v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06827v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）经常对抽象摘要任务产生幻觉，如基于文档的问答、会议摘要和临床报告生成，即使所有必要的信息都包含在上下文中。然而，优化LLM以减少这些任务中的幻觉是具有挑战性的，因为在每个优化步骤中都很难有效地评估幻觉。在这项工作中，我们表明，减少合成任务的幻觉也可以减少现实世界下游任务的幻觉。我们的方法SynTra首先设计了一个合成任务，在这个任务中幻觉很容易引发和测量。接下来，它通过对合成任务进行前缀调整来优化LLM的系统消息，并最终将系统消息传输到现实的、难以优化的任务中。在三个现实的抽象摘要任务中，SynTra仅使用一个用于监督的合成检索任务来减少两个13B参数LLM的幻觉。我们还发现，优化系统消息而不是模型权重可能是至关重要的；在合成任务中对整个模型进行微调可能会违反直觉地增加幻觉。总的来说，SynTra证明了使用合成数据的额外灵活性可以帮助减少实践中不期望的行为。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06827v3" target="_blank">2310.06827v3</a>
                              </td>
                              <td>Teaching Language Models to Hallucinate Less with Synthetic Tasks</td>
                              <td>Erik Jones</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06827v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06827v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14303v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">QTSumm: Query-Focused Summarization over Tabular Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14303v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14303v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14303v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14303v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人们主要查阅表格进行数据分析或回答特定问题。文本生成系统可以根据用户的信息需求提供准确的表格摘要，有助于更有效地访问相关数据。受此启发，我们定义了一个新的以查询为中心的表格摘要任务，其中文本生成模型必须对给定的表格执行类似人类的推理和分析，以生成定制的摘要。我们为这项任务引入了一个名为QTSumm的新基准，它包含7111个人工注释的查询摘要对，覆盖了2934个表，涵盖了不同的主题。我们研究了QTSumm上的一组强大的基线，包括文本生成、表到文本生成和大型语言模型。实验结果和手动分析表明，这项新任务在表到文本的生成方面对未来的研究提出了重大挑战。此外，我们提出了一种新的方法ReFactor，从表格数据中检索和推理过查询的相关信息，以生成几个自然语言事实。实验结果表明，ReFactor可以通过将生成的事实连接到模型输入来改善基线。我们的数据和代码可在https://github.com/yale-nlp/QTSumm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14303v2" target="_blank">2305.14303v2</a>
                              </td>
                              <td>QTSumm: Query-Focused Summarization over Tabular Data</td>
                              <td>Yilun Zhao</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14303v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14303v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03716v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03716v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03716v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03716v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in text-to-image generation have revolutionized numerous fields, including art and cinema, by automating the generation of high-quality, context-aware images and video. However, the utility of these technologies is often limited by the inadequacy of text prompts in guiding the generator to produce artistically coherent and subject-relevant images. In this paper, We describe the techniques that can be used to make Large Language Models (LLMs) act as Art Directors that enhance image and video generation. We describe our unified system for this called "LaDi". We explore how LaDi integrates multiple techniques for augmenting the capabilities of text-to-image generators (T2Is) and text-to-video generators (T2Vs), with a focus on constrained decoding, intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques are being used today in apps and platforms developed by Plai Labs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03716v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像生成的最新进展通过自动化生成高质量、上下文感知的图像和视频，彻底改变了包括艺术和电影在内的许多领域。然而，这些技术的实用性往往受到文本提示在引导生成器生成艺术连贯和主题相关图像方面的不足的限制。在本文中，我们描述了可用于使大型语言模型（LLM）充当艺术指导的技术，以增强图像和视频生成。我们将我们的统一系统描述为“LaDi”。我们探索了LaDi如何集成多种技术来增强文本到图像生成器（T2I）和文本到视频生成器（T2V）的功能，重点是约束解码、智能提示、微调和检索。LaDi和这些技术目前正在Plai实验室开发的应用程序和平台中使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03716v1" target="_blank">2311.03716v1</a>
                              </td>
                              <td>LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators</td>
                              <td>Allen Roush</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03716v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03716v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14132v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting Language Model Attacks with Perplexity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14132v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14132v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14132v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A novel hack involving Large Language Models (LLMs) has emerged, exploiting adversarial suffixes to deceive models into generating perilous responses. Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content. By evaluating the perplexity of queries with adversarial suffixes using an open-source LLM (GPT-2), we found that they have exceedingly high perplexity values. As we explored a broad range of regular (non-adversarial) prompt varieties, we concluded that false positives are a significant challenge for plain perplexity filtering. A Light-GBM trained on perplexity and token length resolved the false positives and correctly detected most adversarial attacks in the test set.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14132v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一种涉及大型语言模型（LLM）的新型黑客攻击已经出现，它利用对抗性后缀欺骗模型生成危险的响应。这种越狱可以诱使LLM向恶意用户提供复杂的指令，以制造爆炸物、策划银行抢劫或为创建攻击性内容提供便利。通过使用开源LLM（GPT-2）评估带有对抗性后缀的查询的困惑，我们发现它们具有极高的困惑值。当我们探索了广泛的规则（非对抗性）提示变体时，我们得出结论，误报是对普通困惑过滤的一个重大挑战。在困惑和令牌长度上训练的Light GBM解决了误报，并正确地检测到了测试集中的大多数对抗性攻击。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14132v3" target="_blank">2308.14132v3</a>
                              </td>
                              <td>Detecting Language Model Attacks with Perplexity</td>
                              <td>Gabriel Alon</td>
                              <td>2023-08-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14132v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14132v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20587v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20587v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20587v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20587v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is $\href{https://lamo2023.github.io}{\text{this https URL}}$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20587v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>离线强化学习（RL）旨在利用预先收集的数据集找到接近最优的策略。在现实世界的场景中，数据收集可能成本高昂且风险巨大；因此，当域内数据有限时，离线RL变得特别具有挑战性。鉴于大型语言模型（LLM）的最新进展及其少量的学习能力，本文介绍了用于$\textbf｛Mo｝$控制的$\textbf｛La｝$语言模型，这是一个基于决策转换器的通用框架，用于有效地将预训练的语言模型（LM）用于离线RL。我们的框架强调了四个关键组成部分：（1）用顺序预训练的LMs初始化决策转换器，（2）与全权重微调相比，采用LoRA微调方法，有效地将来自LMs的预训练知识和领域内知识相结合，（3）使用非线性MLP变换而不是线性投影来生成嵌入，以及（4）在微调期间整合辅助语言预测损失，以稳定LMs并保持其对语言的原始能力。经验结果表明，$\textbf｛LaMo｝$在稀疏奖励任务中实现了最先进的性能，并缩小了基于价值的离线RL方法与密集奖励任务中的决策转换器之间的差距。特别是，我们的方法在数据样本有限的情况下表现出了卓越的性能。我们的项目网站是$\href{https://lamo2023.github.io}｛\text｛this https URL｝｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20587v3" target="_blank">2310.20587v3</a>
                              </td>
                              <td>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</td>
                              <td>Ruizhe Shi</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20587v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20587v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03687v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03687v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03687v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03687v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have seen great advance in both academia and industry, and their popularity results in numerous open-source frameworks and techniques in accelerating LLM pre-training, fine-tuning, and inference. Training and deploying LLMs are expensive as it requires considerable computing resources and memory, hence many efficient approaches have been developed for improving system pipelines as well as operators. However, the runtime performance can vary significantly across hardware and software stacks, which makes it difficult to choose the best configuration. In this work, we aim to benchmark the performance from both macro and micro perspectives. First, we benchmark the end-to-end performance of pre-training, fine-tuning, and serving LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and 70B) on three 8-GPU platforms with and without individual optimization techniques, including ZeRO, quantization, recomputation, FlashAttention. Then, we dive deeper to provide a detailed runtime analysis of the sub-modules, including computing and communication operators in LLMs. For end users, our benchmark and findings help better understand different optimization techniques, training and inference frameworks, together with hardware platforms in choosing configurations for deploying LLMs. For researchers, our in-depth module-wise analyses discover potential opportunities for future work to further optimize the runtime performance of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03687v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在学术界和工业界都取得了巨大的进步，它们的流行导致了许多开源框架和技术加速了LLM的预训练、微调和推理。培训和部署LLM是昂贵的，因为它需要大量的计算资源和内存，因此已经开发了许多有效的方法来改进系统管道和操作员。然而，运行时性能可能因硬件和软件堆栈而异，这使得选择最佳配置变得困难。在这项工作中，我们的目标是从宏观和微观两个角度来衡量绩效。首先，我们对不同大小的预训练、微调和服务LLM的端到端性能进行了基准测试，即在三个8-GPU平台上使用和不使用单独的优化技术（包括ZeRO、量化、重新计算、FlashAttention）的7亿、13亿和700亿参数（7B、13B和70B）。然后，我们深入研究，提供子模块的详细运行时分析，包括LLM中的计算和通信操作员。对于最终用户，我们的基准测试和发现有助于更好地理解不同的优化技术、训练和推理框架，以及在选择部署LLM的配置时使用的硬件平台。对于研究人员来说，我们深入的模块分析发现了未来进一步优化LLM运行时性能的潜在机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03687v1" target="_blank">2311.03687v1</a>
                              </td>
                              <td>Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models</td>
                              <td>Longteng Zhang</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03687v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03687v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04657v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04657v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04657v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04657v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04657v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了BeaverTails数据集，旨在促进对大型语言模型（LLM）中的安全对齐的研究。该数据集独特地分离了问答对的有用性和无害性注释，从而为这些关键属性提供了不同的视角。我们总共收集了333963对问答（QA）和361903对专家比较数据的安全元标签，用于有用性和无害性指标。我们进一步展示了BeaverTails在内容调节和人类反馈强化学习（RLHF）中的应用，强调了其在LLM中实用安全措施的潜力。我们相信，该数据集为社区提供了重要资源，有助于LLM的安全开发和部署。我们的项目页面位于以下URL：https://sites.google.com/view/pku-beavertails.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04657v3" target="_blank">2307.04657v3</a>
                              </td>
                              <td>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</td>
                              <td>Jiaming Ji</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04657v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04657v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_00333v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Competence-Based Analysis of Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_00333v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_00333v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_00333v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the recent success of large, pretrained neural language models (LLMs) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. To better understand such behavior and motivate the design of more robust LLMs, we provide a causal formulation of linguistic competence in the context of LLMs and propose a general framework to study and measure LLM competence. Our framework, CALM (Competence-based Analysis of Language Models), establishes the first quantitative measure of LLM competence, which we study by damaging models' internal representations of various linguistic properties in the course of performing various tasks using causal probing and evaluating models' alignment under these interventions with a given causal model. We also develop a novel approach for performing causal probing interventions using gradient-based adversarial attacks, which can target a broader range of properties and representations than existing techniques. We carry out a case study of CALM using these interventions to analyze BERT and RoBERTa's competence across a variety of lexical inference tasks, showing that the CALM framework and competence metric can be valuable tools for explaining and predicting their behavior across these tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_00333v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管最近预训练的大型神经语言模型（LLM）在各种提示任务上取得了成功，但这些模型对输入或应用程序上下文的微小变化可能非常脆弱。为了更好地理解这种行为并激励设计更稳健的LLM，我们提供了LLM背景下语言能力的因果公式，并提出了一个研究和衡量LLM能力的通用框架。我们的框架CALM（基于能力的语言模型分析）建立了LLM能力的第一个定量测量，我们通过在执行各种任务的过程中破坏模型对各种语言属性的内部表征来研究，使用因果探究和评估模型在这些干预下与给定因果模型的一致性。我们还开发了一种使用基于梯度的对抗性攻击执行因果探测干预的新方法，该方法可以针对比现有技术更广泛的属性和表示。我们对CALM进行了案例研究，使用这些干预来分析BERT和RoBERTa在各种词汇推理任务中的能力，表明CALM框架和能力度量可以成为解释和预测他们在这些任务中行为的有价值的工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.00333v3" target="_blank">2303.00333v3</a>
                              </td>
                              <td>Competence-Based Analysis of Language Models</td>
                              <td>Adam Davies</td>
                              <td>2023-03-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_00333v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.00333v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03287v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03287v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03287v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03287v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While GPT-4V(ision) impressively models both visual and textual information simultaneously, it's hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03287v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然GPT-4V（vision）令人印象深刻地同时模拟了视觉和文本信息，但它的幻觉行为尚未得到系统评估。为了弥补这一差距，我们引入了一个新的基准，即视觉语言模型中的偏见和干扰挑战（Bingo）。该基准旨在评估和阐明视觉语言模型中两种常见的幻觉类型：偏见和干扰。在这里，偏差指的是模型产生某些类型反应的幻觉的倾向，可能是由于其训练数据的不平衡。干扰属于GPT-4V（vision）的判断可能因文本提示的措辞或输入图像的呈现方式而中断的情况。我们发现了一个显著的区域偏见，即与其他国家的图像或包含其他语言文本的图像相比，GPT-4V（vision）更善于用英语书写来解释西方图像或图像。此外，GPT-4V（vision）容易受到引导性问题的影响，并且在一起解释多个图像时经常会感到困惑。流行的缓解方法，如自我纠正和思维链推理，在解决这些挑战方面并不有效。我们还发现LLaVA和Bard存在类似的偏见和干扰漏洞。我们的研究结果描述了GPT-4V（vision）和最先进的视觉语言模型中的幻觉挑战，并强调了对新解决方案的需求。Bingo基准可在https://github.com/gzcch/Bingo.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03287v2" target="_blank">2311.03287v2</a>
                              </td>
                              <td>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</td>
                              <td>Chenhang Cui</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03287v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03287v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03658v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Linear Representation Hypothesis and the Geometry of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03658v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03658v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03658v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03658v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>非正式地说，“线性表示假设”是指高级概念在某些表示空间中被线性表示为方向。在这篇论文中，我们讨论了两个密切相关的问题：“线性表示”实际上意味着什么？那么，我们如何理解表示空间中的几何概念（例如，余弦相似性或投影）？为了回答这些问题，我们使用反事实语言给出了“线性表示”的两种形式化，一种在输出（单词）表示空间，另一种在输入（句子）空间。然后，我们分别证明了这些与线性探测和模型操纵的联系。为了理解几何概念，我们使用形式化来识别一个特定的（非欧几里得）内积，该内积在我们精确的意义上尊重语言结构。使用这个因果内积，我们展示了如何统一线性表示的所有概念。特别地，这允许使用反事实对来构造探针和操纵矢量。LLaMA-2的实验证明了概念的线性表示的存在，与解释和控制的联系，以及内积选择的基本作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03658v1" target="_blank">2311.03658v1</a>
                              </td>
                              <td>The Linear Representation Hypothesis and the Geometry of Large Language Models</td>
                              <td>Kiho Park</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03658v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03658v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2304_11235v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial-Language Attention Policies for Efficient Robot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11235v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11235v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11235v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11235v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在语言引导的操作方面取得了长足的进步，但现有的工作一直局限于桌面设置。桌面允许完美和一致的相机角度，这些特性在移动操作中是不适用的。涉及在环境中移动的任务计划必须对以自我为中心的观点以及把握平面和角度的变化保持稳健。另一个挑战是确保这一切都是真的，同时仍然能够从有限的数据中有效地学习技能。我们提出了空间语言注意策略（SLAP）作为一种解决方案。SLAP使用三维标记作为输入表示来训练单个多任务、语言条件的动作预测策略。我们的方法显示，在现实世界中，使用单个模型的八个任务的成功率为80%，当引入看不见的杂乱和看不见物体配置时，即使每个任务只有少数例子，成功率也为47.5%。这意味着比之前的工作提高了30%（考虑到看不见的干扰物和配置，提高了20%）。我们看到在移动操作设置方面比基线提高了4倍。此外，我们还展示了SLAP的健壮性如何使我们能够使用大型语言模型从开放词汇指令中执行任务计划，以进行多步移动操作。有关视频，请访问网站：https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11235v3" target="_blank">2304.11235v3</a>
                              </td>
                              <td>Spatial-Language Attention Policies for Efficient Robot Learning</td>
                              <td>Priyam Parashar</td>
                              <td>2023-04-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11235v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11235v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10149v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10149v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10149v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10149v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study introduces the concept of "structural beauty" as an objective computational approach for evaluating the aesthetic appeal of images. Through the utilization of the Segment anything model (SAM), we propose a method that leverages recursive segmentation to extract finer-grained substructures. Additionally, by reconstructing the hierarchical structure, we obtain a more accurate representation of substructure quantity and hierarchy. This approach reproduces and extends our previous research, allowing for the simultaneous assessment of Livingness in full-color images without the need for grayscale conversion or separate computations for foreground and background Livingness. Furthermore, the application of our method to the Scenic or Not dataset, a repository of subjective scenic ratings, demonstrates a high degree of consistency with subjective ratings in the 0-6 score range. This underscores that structural beauty is not solely a subjective perception, but a quantifiable attribute accessible through objective computation. Through our case studies, we have arrived at three significant conclusions. 1) our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings. 2) we observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception. 3) our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10149v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究引入了“结构美”的概念，作为评估图像美感的客观计算方法。通过利用分段任意模型（SAM），我们提出了一种利用递归分段来提取细粒度子结构的方法。此外，通过重构层次结构，我们获得了更准确的子结构数量和层次的表示。这种方法再现并扩展了我们以前的研究，允许在不需要灰度转换或单独计算前景和背景Livingness的情况下同时评估全色图像中的Livingness。此外，将我们的方法应用于风景与否数据集，即主观风景评级的存储库，表明了与0-6分范围内的主观评级的高度一致性。这强调了结构美不仅仅是一种主观感知，而是一种可通过客观计算获得的可量化属性。通过案例研究，我们得出了三个重要结论。1） 我们的方法展示了准确分割有意义物体的能力，包括树木、建筑和窗户，以及绘画中的抽象子结构。2） 我们观察到图像的清晰度会影响我们的计算结果；更清晰的图像往往产生更高的Livingness分数。然而，对于同样模糊的图像，Livingness并没有表现出显著的减少，与人类的视觉感知一致。3） 我们的方法从根本上不同于使用卷积神经网络（CNNs）来预测图像分数的方法。我们的方法不仅提供了计算结果，还提供了透明度和可解释性，将其定位为可解释人工智能（XAI）领域的一条新途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10149v2" target="_blank">2310.10149v2</a>
                              </td>
                              <td>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</td>
                              <td>Yao Qianxiang</td>
                              <td>2023-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10149v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10149v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01442v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01442v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01442v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01442v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation, model inputs, model targets, time series per model, and computational budget.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01442v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型，特别是变形金刚，在包括时间序列预测在内的各个领域都取得了令人印象深刻的成果。虽然现有的时间序列文献主要关注模型架构修改和数据扩充技术，但本文探索了时间序列的深度学习模型的训练模式；如何训练模型，而不管其体系结构如何。我们进行了广泛的实验，以研究在公共时间序列数据集上训练的几个Transformer模型中深度双下降的发生。我们证明了历元深度双下降，并且可以使用更多的历元来恢复过拟合。利用这些发现，在测试的72个基准中，我们在近70%的时间序列预测中获得了最先进的结果。这表明文献中的许多模型可能具有尚未开发的潜力。此外，我们还介绍了一种分类法，用于对训练模式修改进行分类，包括数据扩充、模型输入、模型目标、每个模型的时间序列和计算预算。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01442v2" target="_blank">2311.01442v2</a>
                              </td>
                              <td>Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</td>
                              <td>Valentino Assandri</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01442v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01442v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03583v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03583v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03583v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03583v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work studies a central extremal graph theory problem inspired by a 1975 conjecture of Erd\H{o}s, which aims to find graphs with a given size (number of nodes) that maximize the number of edges without having 3- or 4-cycles. We formulate this problem as a sequential decision-making problem and compare AlphaZero, a neural network-guided tree search, with tabu search, a heuristic local search method. Using either method, by introducing a curriculum -- jump-starting the search for larger graphs using good graphs found at smaller sizes -- we improve the state-of-the-art lower bounds for several sizes. We also propose a flexible graph-generation environment and a permutation-invariant network architecture for learning to search in the space of graphs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03583v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了一个受1975年Erd\H猜想启发的中心极值图论问题{o}s，旨在找到具有给定大小（节点数）的图，该图在不具有3-或4-循环的情况下最大化边数。我们将这个问题表述为一个序列决策问题，并将神经网络引导的树搜索AlphaZero与启发式局部搜索方法禁忌搜索进行比较。使用任何一种方法，通过引入课程——使用在较小尺寸中找到的好图开始搜索较大的图——我们改进了几种尺寸的最先进的下限。我们还提出了一个灵活的图生成环境和一个用于学习在图空间中搜索的置换不变网络架构。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03583v1" target="_blank">2311.03583v1</a>
                              </td>
                              <td>Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search</td>
                              <td>Abbas Mehrabian</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03583v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03583v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03053v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masking Hyperspectral Imaging Data with Pretrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03053v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03053v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与潜在噪声和未知光谱特性相关联的不期望的背景区域的存在降低了高光谱数据处理的性能。掩盖不需要的区域是解决这个问题的关键。仅处理感兴趣的区域在计算成本、所需内存和总体性能方面产生了显著的改进。所提出的处理管道包括两个基本部分：感兴趣区域掩模生成，然后仅在新掩模的高光谱立方体上应用高光谱数据处理技术。我们工作的新颖之处在于用于初步图像分割的方法。我们使用Segment Anything Model（SAM）来提取数据集中的所有对象，然后使用零样本Grounding Dino对象检测器来细化片段，然后执行交集和排除过滤步骤，而无需进行微调或重新训练。为了说明掩蔽过程的有效性，所提出的方法被部署在三个具有挑战性的应用场景中，这些场景需要精确的掩蔽；碎塑料特性、岩芯扫描和垃圾监测。提供了所提出的掩蔽方法对三个应用的数值评估以及所使用的超参数。该方法的脚本将在https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03053v1" target="_blank">2311.03053v1</a>
                              </td>
                              <td>Masking Hyperspectral Imaging Data with Pretrained Models</td>
                              <td>Elias Arbash</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03053v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03053v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01989v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01989v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01989v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01989v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, large-scale pre-trained models such as Segment-Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable success and revolutionized the field of computer vision. These foundation vision models effectively capture knowledge from a large-scale broad data with their vast model parameters, enabling them to perform zero-shot segmentation on previously unseen data without additional training. While they showcase competence in 2D tasks, their potential for enhancing 3D scene understanding remains relatively unexplored. To this end, we present a novel framework that adapts various foundational models for the 3D point cloud segmentation task. Our approach involves making initial predictions of 2D semantic masks using different large vision models. We then project these mask predictions from various frames of RGB-D video sequences into 3D space. To generate robust 3D semantic pseudo labels, we introduce a semantic label fusion strategy that effectively combines all the results via voting. We examine diverse scenarios, like zero-shot learning and limited guidance from sparse 2D point labels, to assess the pros and cons of different vision foundation models. Our approach is experimented on ScanNet dataset for 3D indoor scenes, and the results demonstrate the effectiveness of adopting general 2D foundation models on solving 3D point cloud segmentation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01989v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大规模的预训练模型，如分段任意模型（SAM）和对比语言图像预训练（CLIP），已经取得了显著的成功，并彻底改变了计算机视觉领域。这些基础视觉模型通过其庞大的模型参数有效地从大规模的广泛数据中获取知识，使它们能够在无需额外训练的情况下对以前看不见的数据执行零样本分割。虽然他们展示了在2D任务中的能力，但他们在增强3D场景理解方面的潜力仍然相对未被探索。为此，我们提出了一种新的框架，该框架适用于3D点云分割任务的各种基础模型。我们的方法包括使用不同的大视觉模型对2D语义掩码进行初始预测。然后，我们将这些来自RGB-D视频序列的不同帧的掩码预测投影到3D空间中。为了生成健壮的3D语义伪标签，我们引入了一种语义标签融合策略，该策略通过投票有效地组合了所有结果。我们研究了不同的场景，如零样本学习和稀疏2D点标签的有限指导，以评估不同视觉基础模型的优缺点。我们的方法在三维室内场景的ScanNet数据集上进行了实验，结果证明了采用通用二维基础模型解决三维点云分割任务的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01989v2" target="_blank">2311.01989v2</a>
                              </td>
                              <td>Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation</td>
                              <td>Shichao Dong</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01989v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01989v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11441v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11441v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11441v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11441v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11441v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉提示方法——标记集（SoM），以释放大型多模态模型（LMM）的视觉基础能力，如GPT-4V。如图6所示，1（右），我们使用现成的交互式分割模型，如SEEM/SAM，将图像划分为不同粒度级别的区域，并用一组标记覆盖这些区域，例如字母数字、掩码、框。使用标记的图像作为输入，GPT-4V可以回答需要视觉基础的问题。我们进行了一项全面的实证研究，以验证SoM在各种细粒度视觉和多模式任务上的有效性。例如，我们的实验表明，具有SoM的GPT-4V在零样本设置中优于RefCOCOg上最先进的完全微调的引用表达式理解和分割模型。SoM提示代码公开于：https://github.com/microsoft/SoM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11441v2" target="_blank">2310.11441v2</a>
                              </td>
                              <td>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</td>
                              <td>Jianwei Yang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11441v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11441v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_02386v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Approximating CKY with Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_02386v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_02386v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_02386v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a sentence's parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under \textit{random} PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being a subgradient of a partition function variant with respect to the chart.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_02386v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了转换器模型近似CKY算法的能力，使用它们来直接预测句子的解析，从而避免CKY算法对句子长度的三次依赖。我们发现，在标准选区解析基准测试中，这种方法比使用CKY的类似解析器实现了有竞争力或更好的性能，同时速度更快。我们还评估了这种方法在\textit｛random｝PCFGs下解析的可行性。在这里，我们发现性能随着语法变得更加模糊而下降，这表明转换器没有完全捕获CKY计算。然而，我们也发现，加入额外的归纳偏差是有帮助的，我们提出了一种新的方法，在预测解析时利用图表表示的梯度，类似于CKY算法是划分函数变体相对于图表的次梯度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.02386v2" target="_blank">2305.02386v2</a>
                              </td>
                              <td>Approximating CKY with Transformers</td>
                              <td>Ghazal Khalighinejad</td>
                              <td>2023-05-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_02386v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.02386v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02396v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Precise Robotic Needle-Threading with Tactile Perception and Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02396v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02396v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02396v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work presents a novel tactile perception-based method, named T-NT, for performing the needle-threading task, an application of deformable linear object (DLO) manipulation. This task is divided into two main stages: Tail-end Finding and Tail-end Insertion. In the first stage, the agent traces the contour of the thread twice using vision-based tactile sensors mounted on the gripper fingers. The two-run tracing is to locate the tail-end of the thread.   In the second stage, it employs a tactile-guided reinforcement learning (RL) model to drive the robot to insert the thread into the target needle eyelet. The RL model is trained in a Unity-based simulated environment. The simulation environment supports tactile rendering which can produce realistic tactile images and thread modeling. During insertion, the position of the poke point and the center of the eyelet are obtained through a pre-trained segmentation model, Grounded-SAM, which predicts the masks for both the needle eye and thread imprints. These positions are then fed into the reinforcement learning model, aiding in a smoother transition to real-world applications. Extensive experiments on real robots are conducted to demonstrate the efficacy of our method. More experiments and videos can be found in the supplementary materials and on the website: https://sites.google.com/view/tac-needlethreading.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02396v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作提出了一种新的基于触觉感知的方法，称为T-NT，用于执行穿针任务，这是可变形线性对象（DLO）操作的应用。此任务分为两个主要阶段：尾端查找和尾端插入。在第一阶段，代理使用安装在抓取器手指上的基于视觉的触觉传感器两次跟踪线的轮廓。两次运行跟踪是为了定位线程的尾部。在第二阶段，它采用触觉引导强化学习（RL）模型来驱动机器人将线插入目标针眼。RL模型在基于Unity的模拟环境中进行训练。仿真环境支持触觉渲染，可以生成逼真的触觉图像和线程建模。在插入过程中，通过预训练的分割模型Grounded SAM获得戳点的位置和孔眼的中心，该模型预测针眼和线印记的掩模。然后，这些位置被输入到强化学习模型中，有助于更平稳地过渡到现实世界的应用程序。在真实的机器人上进行了大量的实验来证明我们的方法的有效性。更多的实验和视频可以在补充材料和网站上找到：https://sites.google.com/view/tac-needlethreading.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02396v1" target="_blank">2311.02396v1</a>
                              </td>
                              <td>Precise Robotic Needle-Threading with Tactile Perception and Reinforcement Learning</td>
                              <td>Zhenjun Yu</td>
                              <td>2023-11-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02396v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02396v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05803v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05803v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05803v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05803v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Weakly supervised semantic segmentation (WSSS) aims to bypass the need for laborious pixel-level annotation by using only image-level annotation. Most existing methods rely on Class Activation Maps (CAM) to derive pixel-level pseudo-labels and use them to train a fully supervised semantic segmentation model. Although these pseudo-labels are class-aware, indicating the coarse regions for particular classes, they are not object-aware and fail to delineate accurate object boundaries. To address this, we introduce a simple yet effective method harnessing the Segment Anything Model (SAM), a class-agnostic foundation model capable of producing fine-grained instance masks of objects, parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM masks, resulting in high-quality pseudo-labels that are both class-aware and object-aware. Our approach is highly versatile and can be easily integrated into existing WSSS methods without any modification. Despite its simplicity, our approach shows consistent gain over the state-of-the-art WSSS methods on both PASCAL VOC and MS-COCO datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05803v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督语义分割（WSSS）旨在通过仅使用图像级注释来绕过对费力的像素级注释的需要。大多数现有的方法都依赖于类激活映射（CAM）来导出像素级伪标签，并使用它们来训练完全监督的语义分割模型。尽管这些伪标签是类感知的，指示特定类的粗略区域，但它们不是对象感知的，并且不能描绘准确的对象边界。为了解决这个问题，我们引入了一种简单而有效的方法，利用Segment Anything Model（SAM），这是一个类不可知的基础模型，能够生成对象、部件和子部件的细粒度实例掩码。我们使用CAM伪标签作为线索来选择和组合SAM掩码，从而产生高质量的伪标签，这些伪标签具有类意识和对象意识。我们的方法用途广泛，可以很容易地集成到现有的WSSS方法中，而无需任何修改。尽管我们的方法很简单，但在PASCAL VOC和MS-COCO数据集上，与最先进的WSSS方法相比，我们的方法显示出了一致的增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05803v4" target="_blank">2305.05803v4</a>
                              </td>
                              <td>Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation</td>
                              <td>Tianle Chen</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05803v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05803v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_16466v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation with Meta-Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_16466v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_16466v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_16466v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While the Segment Anything Model (SAM) excels in semantic segmentation for general-purpose images, its performance significantly deteriorates when applied to medical images, primarily attributable to insufficient representation of medical images in its training dataset. Nonetheless, gathering comprehensive datasets and training models that are universally applicable is particularly challenging due to the long-tail problem common in medical images. To address this gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework for few-shot medical image segmentation. Our innovation lies in the design of three key modules: 1) An online fast gradient descent optimizer, further optimized by a meta-learner, which ensures swift and robust adaptation to new tasks. 2) A Self-Sampling module designed to provide well-aligned visual prompts for improved attention allocation; and 3) A robust attention-based decoder specifically designed for medical few-shot learning to capture relationship between different slices. Extensive experiments on a popular abdominal CT dataset and an MRI dataset demonstrate that the proposed method achieves significant improvements over state-of-the-art methods in few-shot segmentation, with an average improvements of 10.21% and 1.80% in terms of DSC, respectively. In conclusion, we present a novel approach for rapid online adaptation in interactive image segmentation, adapting to a new organ in just 0.83 minutes. Code is publicly available on GitHub upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_16466v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然Segment Anything Model（SAM）在通用图像的语义分割方面表现出色，但当应用于医学图像时，其性能显著恶化，这主要归因于其训练数据集中医学图像的表示不足。尽管如此，由于医学图像中常见的长尾问题，收集普遍适用的综合数据集和训练模型尤其具有挑战性。为了解决这一差距，我们提出了一种用于少镜头医学图像分割的自采样元SAM（SSM-SAM）框架。我们的创新在于设计了三个关键模块：1）在线快速梯度下降优化器，由元学习器进一步优化，确保快速、稳健地适应新任务。2） 一个自采样模块，旨在提供对齐的视觉提示，以改进注意力分配；和3）一种鲁棒的基于注意力的解码器，专门为医学少镜头学习而设计，以捕捉不同切片之间的关系。在流行的腹部CT数据集和MRI数据集上进行的大量实验表明，与最先进的方法相比，所提出的方法在少镜头分割方面实现了显著的改进，DSC的平均改进分别为10.21%和1.80%。总之，我们提出了一种在交互式图像分割中快速在线自适应的新方法，仅需0.83分钟即可适应新器官。代码在接受后可在GitHub上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.16466v3" target="_blank">2308.16466v3</a>
                              </td>
                              <td>Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation with Meta-Learning</td>
                              <td>Yiming Zhang</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_16466v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.16466v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01373v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Recognize Any Regions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01373v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01373v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01373v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module. Through extensive experiments in the context of open-world object recognition, our RegionSpot demonstrates significant performance improvements over prior alternatives, while also providing substantial computational savings. For instance, training our model with 3 million data in a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean average precision (mAP), with an even larger margin by 14.8 % for more challenging and rare categories.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01373v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>理解无约束图像中单个区域或补丁的语义，例如在开放世界对象检测中，是计算机视觉中一项关键但具有挑战性的任务。在像CLIP这样强大的图像级视觉语言（ViL）基础模型的成功基础上，最近的努力试图通过从头开始用大量的区域标签对训练对比模型，或者将检测模型的输出与区域建议的图像级表示对齐，来利用它们的能力。尽管取得了显著进展，但这些方法仍受到计算密集型训练需求、数据噪声易感性和上下文信息不足的困扰。为了解决这些局限性，我们探索了现成基础模型的协同潜力，利用它们各自在本地化和语义方面的优势。我们介绍了一种新的、通用的、高效的区域识别架构，名为RegionSpot，旨在将来自定位基础模型（例如SAM）的位置感知定位知识与从ViL模型（例如CLIP）提取的语义信息相集成。为了充分利用预训练的知识，同时最大限度地减少训练开销，我们冻结了两个基础模型，将优化工作仅集中在基于注意力的轻量级知识集成模块上。通过在开放世界对象识别的背景下进行广泛的实验，我们的RegionSpot展示了与先前的替代方案相比的显著性能改进，同时也提供了大量的计算节约。例如，使用8个V100 GPU在一天内用300万个数据训练我们的模型。我们的模型在平均精度（mAP）方面优于GLIP 6.5%，在更具挑战性和罕见的类别中，差距更大，达到14.8%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01373v1" target="_blank">2311.01373v1</a>
                              </td>
                              <td>Recognize Any Regions</td>
                              <td>Haosen Yang</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01373v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01373v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06229v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving word mover's distance by leveraging self-attention matrix</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06229v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06229v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06229v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Measuring the semantic similarity between two sentences is still an important task. The word mover's distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with significant overlaps of similar words, even if they are semantically very different. Here, we attempt to improve WMD by incorporating the sentence structure represented by BERT's self-attention matrix (SAM). The proposed method is based on the Fused Gromov-Wasserstein distance, which simultaneously considers the similarity of the word embedding and the SAM for calculating the optimal transport between two sentences. Experiments demonstrate the proposed method enhances WMD and its variants in paraphrase identification with near-equivalent performance in semantic textual similarity. Our code is available at \url{https://github.com/ymgw55/WSMD}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06229v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>衡量两个句子之间的语义相似性仍然是一项重要的任务。单词移动器距离（WMD）通过单词嵌入集合之间的最佳对齐来计算相似性。然而，大规模杀伤性武器不利用语序，这使得区分相似单词有显著重叠的句子变得很有挑战性，即使它们在语义上非常不同。在这里，我们试图通过引入以BERT的自注意矩阵（SAM）为代表的句子结构来改进WMD。所提出的方法基于Fused-Gromov-Wasserstein距离，该距离同时考虑了单词嵌入和SAM的相似性，以计算两句之间的最佳传输。实验表明，该方法增强了WMD及其变体的转述识别能力，在语义-文本相似性方面具有近似等效的性能。我们的代码位于\url{https://github.com/ymgw55/WSMD}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06229v2" target="_blank">2211.06229v2</a>
                              </td>
                              <td>Improving word mover's distance by leveraging self-attention matrix</td>
                              <td>Hiroaki Yamagiwa</td>
                              <td>2022-11-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06229v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06229v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01011v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01011v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01011v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01011v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01011v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在现实世界的应用程序中越来越多地被使用，但它们仍然容易受到提示注入攻击：恶意的第三方提示会破坏系统设计者的意图。为了帮助研究人员研究这个问题，我们提供了一个数据集，其中包括超过126000次即时注射攻击和46000次针对即时注射的基于提示的“防御”，所有这些都是由名为Tensor Trust的在线游戏的玩家创建的。据我们所知，这是目前用于LLM教学的最大的人工生成对抗性示例数据集。我们数据集中的攻击有很多易于解释的结构，并揭示了LLM的弱点。我们还使用数据集来创建抵抗两种类型的提示注入的基准，我们称之为提示提取和提示劫持。我们的基准测试结果表明，Tensor Trust数据集中的许多模型都容易受到攻击策略的影响。此外，我们还表明，数据集中的一些攻击策略可以推广到已部署的基于LLM的应用程序，尽管它们对游戏的约束非常不同。我们在发布所有数据和源代码https://tensortrust.ai/paper</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01011v1" target="_blank">2311.01011v1</a>
                              </td>
                              <td>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</td>
                              <td>Sam Toyer</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01011v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01011v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01004v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01004v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01004v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01004v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01004v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着多模态和大型语言模型的发展，基于深度学习的医学图像字幕技术有可能提供有价值的诊断建议。然而，当前的通用文本和图像预训练模型在描述医学图像中的复杂细节时并不能产生令人满意的结果。在本文中，我们提出了一种新的医学图像字幕方法，该方法以分段任意模型（SAM）为指导，通过通用和详细特征提取实现增强编码。此外，我们的方法采用了一种独特的预训练策略和混合语义学习，以同时捕捉医学图像中的整体信息和更精细的细节。我们证明了这种方法的有效性，因为它在生成医学图像描述的各种评估指标上优于预先训练的BLIP2模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01004v1" target="_blank">2311.01004v1</a>
                              </td>
                              <td>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning</td>
                              <td>Gaoang Wang</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01004v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01004v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16698v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Any Shadow: Segment Anything for Video Shadow Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16698v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16698v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16698v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) has achieved great success in the field of natural image segmentation. Nevertheless, SAM tends to consider shadows as background and therefore does not perform segmentation on them. In this paper, we propose ShadowSAM, a simple yet effective framework for fine-tuning SAM to detect shadows. Besides, by combining it with long short-term attention mechanism, we extend its capability for efficient video shadow detection. Specifically, we first fine-tune SAM on ViSha training dataset by utilizing the bounding boxes obtained from the ground truth shadow mask. Then during the inference stage, we simulate user interaction by providing bounding boxes to detect a specific frame (e.g., the first frame). Subsequently, using the detected shadow mask as a prior, we employ a long short-term network to learn spatial correlations between distant frames and temporal consistency between adjacent frames, thereby achieving precise shadow information propagation across video frames. Extensive experimental results demonstrate the effectiveness of our method, with notable margin over the state-of-the-art approaches in terms of MAE and IoU metrics. Moreover, our method exhibits accelerated inference speed compared to previous video shadow detection approaches, validating the effectiveness and efficiency of our method. The source code is now publicly available at https://github.com/harrytea/Detect-AnyShadow.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16698v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在自然图像分割领域取得了巨大的成功。然而，SAM倾向于将阴影视为背景，因此不对其执行分割。在本文中，我们提出了ShadowSAM，这是一个简单而有效的框架，用于微调SAM以检测阴影。此外，通过将其与长短期注意力机制相结合，我们扩展了其高效视频阴影检测的能力。具体来说，我们首先通过利用从地面实况阴影掩模获得的边界框，在ViSha训练数据集上微调SAM。然后在推理阶段，我们通过提供边界框来检测特定帧（例如，第一帧）来模拟用户交互。随后，使用检测到的阴影掩模作为先验，我们使用长短期网络来学习远处帧之间的空间相关性和相邻帧之间的时间一致性，从而实现阴影信息在视频帧之间的精确传播。大量的实验结果证明了我们方法的有效性，在MAE和IoU指标方面比最先进的方法有显著的优势。此外，与以前的视频阴影检测方法相比，我们的方法显示出更快的推理速度，验证了我们方法的有效性和效率。源代码现已在https://github.com/harrytea/Detect-AnyShadow.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16698v2" target="_blank">2305.16698v2</a>
                              </td>
                              <td>Detect Any Shadow: Segment Anything for Video Shadow Detection</td>
                              <td>Yonghui Wang</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16698v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16698v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04178v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimal Transport Model Distributional Robustness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04178v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04178v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04178v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in the data space. In this work, we explore an optimal transport-based distributional robustness framework in model spaces. Specifically, we examine a model distribution within a Wasserstein ball centered on a given model distribution that maximizes the loss. We have developed theories that enable us to learn the optimal robust center model distribution. Interestingly, our developed theories allow us to flexibly incorporate the concept of sharpness awareness into training, whether it's a single model, ensemble models, or Bayesian Neural Networks, by considering specific forms of the center model distribution. These forms include a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrate that Sharpness-Aware Minimization (SAM) is a specific case of our framework when using a Dirac delta distribution over a single model, while our framework can be seen as a probabilistic extension of SAM. To validate the effectiveness of our framework in the aforementioned settings, we conducted extensive experiments, and the results reveal remarkable improvements compared to the baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04178v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布鲁棒性是一个很有前途的框架，用于训练深度学习模型，这些模型不太容易受到对抗性示例和数据分布变化的影响。以前的工作主要集中在利用数据空间中的分布鲁棒性。在这项工作中，我们在模型空间中探索了一个基于最优传输的分布式鲁棒性框架。具体来说，我们检查了Wasserstein球内的模型分布，该模型分布以最大化损失的给定模型分布为中心。我们已经开发了一些理论，使我们能够学习最优鲁棒中心模型分布。有趣的是，我们开发的理论使我们能够通过考虑中心模型分布的特定形式，将清晰度意识的概念灵活地纳入训练中，无论是单个模型、集成模型还是贝叶斯神经网络。这些形式包括单个模型上的狄拉克-德尔塔分布、多个模型上的均匀分布和通用贝叶斯神经网络。此外，我们证明，当在单个模型上使用Dirac delta分布时，Sharpness Aware Minimization（SAM）是我们框架的一个特定情况，而我们的框架可以被视为SAM的概率扩展。为了验证我们的框架在上述设置中的有效性，我们进行了广泛的实验，结果显示与基线相比有显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04178v2" target="_blank">2306.04178v2</a>
                              </td>
                              <td>Optimal Transport Model Distributional Robustness</td>
                              <td>Van-Anh Nguyen</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04178v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04178v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16623v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16623v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16623v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16623v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segmentation is an essential step for remote sensing image processing. This study aims to advance the application of the Segment Anything Model (SAM), an innovative image segmentation model by Meta AI, in the field of remote sensing image analysis. SAM is known for its exceptional generalization capabilities and zero-shot learning, making it a promising approach to processing aerial and orbital images from diverse geographical contexts. Our exploration involved testing SAM across multi-scale datasets using various input prompts, such as bounding boxes, individual points, and text descriptors. To enhance the model's performance, we implemented a novel automated technique that combines a text-prompt-derived general example with one-shot training. This adjustment resulted in an improvement in accuracy, underscoring SAM's potential for deployment in remote sensing imagery and reducing the need for manual annotation. Despite the limitations encountered with lower spatial resolution images, SAM exhibits promising adaptability to remote sensing data analysis. We recommend future research to enhance the model's proficiency through integration with supplementary fine-tuning techniques and other networks. Furthermore, we provide the open-source code of our modifications on online repositories, encouraging further and broader adaptations of SAM to the remote sensing domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16623v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分割是遥感图像处理的重要步骤。本研究旨在推进Meta AI创新的图像分割模型Segment Anything Model（SAM）在遥感图像分析领域的应用。SAM以其卓越的泛化能力和零样本学习而闻名，这使其成为处理不同地理背景下的航空和轨道图像的一种很有前途的方法。我们的探索涉及使用各种输入提示（如边界框、单个点和文本描述符）在多尺度数据集上测试SAM。为了提高模型的性能，我们实现了一种新的自动化技术，该技术将文本提示派生的一般示例与一次性训练相结合。这一调整提高了精度，突出了SAM在遥感图像中部署的潜力，并减少了手动注释的需要。尽管空间分辨率较低的图像存在局限性，SAM在遥感数据分析方面表现出了良好的适应性。我们建议未来的研究通过与补充微调技术和其他网络的集成来提高模型的熟练度。此外，我们在在线存储库上提供了我们修改的开源代码，鼓励SAM进一步更广泛地适应遥感领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16623v2" target="_blank">2306.16623v2</a>
                              </td>
                              <td>The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot</td>
                              <td>Lucas Prado Osco</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16623v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16623v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00134v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Joint Depth Prediction and Semantic Segmentation with Multi-View SAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00134v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00134v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00134v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task approaches to joint depth and segmentation prediction are well-studied for monocular images. Yet, predictions from a single-view are inherently limited, while multiple views are available in many robotics applications. On the other end of the spectrum, video-based and full 3D methods require numerous frames to perform reconstruction and segmentation. With this work we propose a Multi-View Stereo (MVS) technique for depth prediction that benefits from rich semantic features of the Segment Anything Model (SAM). This enhanced depth prediction, in turn, serves as a prompt to our Transformer-based semantic segmentation decoder. We report the mutual benefit that both tasks enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our approach consistently outperforms single-task MVS and segmentation models, along with multi-task monocular methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00134v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>针对单目图像，对联合深度和分割预测的多任务方法进行了深入研究。然而，来自单个视图的预测本质上是有限的，而在许多机器人应用中可以使用多个视图。另一方面，基于视频的全3D方法需要大量的帧来执行重建和分割。通过这项工作，我们提出了一种用于深度预测的多视图立体（MVS）技术，该技术得益于分段任意模型（SAM）丰富的语义特征。这种增强的深度预测反过来又作为我们基于Transformer的语义分割解码器的提示。我们在ScanNet数据集的定量和定性研究中报告了这两项任务的互利性。我们的方法始终优于单任务MVS和分割模型，以及多任务单目方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00134v1" target="_blank">2311.00134v1</a>
                              </td>
                              <td>Joint Depth Prediction and Semantic Segmentation with Multi-View SAM</td>
                              <td>Mykhailo Shvets</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00134v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00134v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01187v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMAug: Point Prompt Augmentation for Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01187v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01187v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01187v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potential of visual prompt augmentation for computer vision. Codes of SAMAug are available at github.com/yhydhx/SAMAug</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01187v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种用于分段任意模型（SAM）的新的视觉点增强方法SAMAug，该方法提高了交互式图像分割的性能。SAMAug生成增强点提示，以向SAM提供有关用户意图的更多信息。从初始点提示开始，SAM生成初始掩码，然后将其输入到我们提出的SAMAug中，以生成增强点提醒。通过结合这些额外的点，SAM可以基于增强点提示和初始提示生成增强分割掩码，从而提高分割性能。我们使用四种不同的点增强策略进行了评估：随机采样、基于最大差熵的采样、最大距离和显著性。在COCO、Fundus、COVID QUEx和ISIC2018数据集上的实验结果表明，SAMAug可以提高SAM的分割结果，尤其是使用最大距离和显著性。SAMAug展示了计算机视觉视觉提示增强的潜力。SAMAug的代码可在github.com/yhydhx/SAMAug上获得</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01187v2" target="_blank">2307.01187v2</a>
                              </td>
                              <td>SAMAug: Point Prompt Augmentation for Segment Anything Model</td>
                              <td>Haixing Dai</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01187v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01187v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20120v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20120v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20120v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20120v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this report, we present our approach to the EPIC-KITCHENS VISOR Hand Object Segmentation Challenge, which focuses on the estimation of the relation between the hands and the objects given a single frame as input. The EPIC-KITCHENS VISOR dataset provides pixel-wise annotations and serves as a benchmark for hand and active object segmentation in egocentric video. Our approach combines the baseline method, i.e., Point-based Rendering (PointRend) and the Segment Anything Model (SAM), aiming to enhance the accuracy of hand and object segmentation outcomes, while also minimizing instances of missed detection. We leverage accurate hand segmentation maps obtained from the baseline method to extract more precise hand and in-contact object segments. We utilize the class-agnostic segmentation provided by SAM and apply specific hand-crafted constraints to enhance the results. In cases where the baseline model misses the detection of hands or objects, we re-train an object detector on the training set to enhance the detection accuracy. The detected hand and in-contact object bounding boxes are then used as prompts to extract their respective segments from the output of SAM. By effectively combining the strengths of existing methods and applying our refinements, our submission achieved the 1st place in terms of evaluation criteria in the VISOR HOS Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20120v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本报告中，我们介绍了EPIC-KITCHENS VISOR手部对象分割挑战的方法，该方法侧重于在给定单个帧作为输入的情况下估计手部和对象之间的关系。EPIC-KITCHENS VISOR数据集提供像素注释，并作为以自我为中心的视频中手部和活动对象分割的基准。我们的方法结合了基线方法，即基于点的渲染（PointRend）和分段任意模型（SAM），旨在提高手和对象分割结果的准确性，同时最大限度地减少遗漏检测的情况。我们利用从基线方法获得的精确手部分割图来提取更精确的手部和接触对象分割。我们利用SAM提供的类不可知分割，并应用特定的手工约束来增强结果。在基线模型未检测到手或物体的情况下，我们在训练集上重新训练物体检测器，以提高检测精度。然后，检测到的手和接触对象边界框被用作提示，从SAM的输出中提取它们各自的片段。通过有效地结合现有方法的优势并应用我们的改进，我们的提交在VISOR HOS挑战赛中获得了评估标准的第一名。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20120v1" target="_blank">2310.20120v1</a>
                              </td>
                              <td>Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023</td>
                              <td>Fen Fang</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20120v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20120v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12488v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sharpness-Aware Minimization and the Edge of Stability</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12488v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12488v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12488v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value. The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12488v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的实验表明，当用步长$\eta$训练具有梯度下降（GD）的神经网络时，损失的Hessian算子范数通常会增长，直到它大约达到$2/\eta$，之后它会在这个值周围波动。基于损失的局部二次近似，量$2/\eta$被称为“稳定性边缘”。我们进行了类似的计算，以获得清晰度感知最小化（SAM）的“稳定性边缘”，这是GD的一种变体，已被证明可以提高其泛化能力。与GD的情况不同，得到的SAM边缘取决于梯度的范数。使用三个深度学习训练任务，我们从经验上看到，SAM在该分析确定的稳定性边缘运行。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12488v4" target="_blank">2309.12488v4</a>
                              </td>
                              <td>Sharpness-Aware Minimization and the Edge of Stability</td>
                              <td>Philip M. Long</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12488v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12488v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03899v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Label-free Scene Understanding by Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03899v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03899v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03899v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4\% and 33.5\% mIoU on ScanNet, improving 4.7\% and 7.9\%, respectively. For nuImages and nuScenes datasets, the performance is 22.1\% and 26.8\% with improvements of 3.5\% and 6.0\%, respectively. Code is available. (https://github.com/runnanchen/Label-Free-Scene-Understanding).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03899v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型，如对比视觉语言预训练（CLIP）和分割任何内容（SAM），在图像分类和分割任务上表现出令人印象深刻的零样本性能。然而，将CLIP和SAM结合起来进行无标签场景理解还有待探索。在本文中，我们研究了视觉基础模型在使网络能够在没有标记数据的情况下理解2D和3D世界方面的潜力。主要的挑战在于在极噪声伪标签下有效地监督网络，这些伪标签由CLIP生成，并在从2D域到3D域的传播过程中进一步加剧。为了应对这些挑战，我们提出了一种新的跨模态噪声监督（CNS）方法，该方法利用CLIP和SAM的优势来同时监督2D和3D网络。特别地，我们引入了一种预测一致性正则化来共同训练2D和3D网络，然后使用SAM的鲁棒特征表示进一步增强网络的潜在空间一致性。在不同的室内和室外数据集上进行的实验证明了我们的方法在理解2D和3D开放环境方面的卓越性能。我们的2D和3D网络在ScanNet上实现了28.4%和33.5%mIoU的无标签语义分割，分别提高了4.7%和7.9%。对于nuImages和nuScenes数据集，性能分别为22.1%和26.8%，改进幅度分别为3.5%和6.0%。代码可用。(https://github.com/runnanchen/Label-Free-Scene-Understanding)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03899v2" target="_blank">2306.03899v2</a>
                              </td>
                              <td>Towards Label-free Scene Understanding by Vision Foundation Models</td>
                              <td>Runnan Chen</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03899v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03899v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19522v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Natural Domain Foundation Models Useful for Medical Image Classification?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19522v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19522v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19522v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 in particular, consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19522v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习领域正朝着使用通用基础模型的方向发展，这些模型可以很容易地适应不同的任务。虽然这种范式转变已经成为自然语言处理领域的常见做法，但计算机视觉的进展却较慢。在本文中，我们试图通过研究各种最先进的基础模型对医学图像分类任务的可转移性来解决这个问题。具体而言，我们评估了五个基础模型的性能，即SAM、SEEM、DINOv2、BLIP和OpenCLIP，它们跨越了四个成熟的医学成像数据集。我们探索了不同的训练环境，以充分利用这些模型的潜力。我们的研究结果喜忧参半。尤其是DINOv2，始终优于ImageNet预训练的标准实践。然而，其他基础模型未能始终超过这一既定基线，这表明它们在医学图像分类任务中的可移植性存在局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19522v1" target="_blank">2310.19522v1</a>
                              </td>
                              <td>Are Natural Domain Foundation Models Useful for Medical Image Classification?</td>
                              <td>Joana Palés Huix</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19522v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19522v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19257v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19257v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19257v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实例检测（InsDet）是机器人和计算机视觉中一个长期存在的问题，旨在检测杂乱场景中的对象实例（由一些视觉实例预定义）。尽管它具有实际意义，但它的进步被对象检测所掩盖，对象检测旨在检测属于某些预定义类的对象。一个主要原因是，按照今天的标准，当前的InsDet数据集规模太小。例如，流行的InsDet数据集GMU（2016年发布）只有23个实例，远远少于2014年发布的著名对象检测数据集COCO（80个类）。我们有动力引入一个新的InsDet数据集和协议。首先，我们为InsDet定义了一个逼真的设置：训练数据包括多视图实例捕获，以及不同的场景图像，允许通过粘贴带有自由框注释的实例图像来合成训练图像。其次，我们发布了一个真实世界的数据库，其中包含100个对象实例的多视图捕获和高分辨率（6k x 8k）测试图像。第三，我们在数据集上广泛研究了InsDet的基线方法，分析了它们的性能，并提出了未来的工作建议。令人惊讶的是，使用现成的类不可知分割模型（Segment Anything model，SAM）和自监督特征表示DINOv2表现最好，比重新利用对象检测器的端到端训练的InsDet模型（例如FasterRCNN和RetinaNet）更好地实现了>10 AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19257v1" target="_blank">2310.19257v1</a>
                              </td>
                              <td>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</td>
                              <td>Qianqian Shen</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19257v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19257v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15161v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-Med3D</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15161v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15161v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15161v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although the Segment Anything Model (SAM) has demonstrated impressive performance in 2D natural image segmentation, its application to 3D volumetric medical images reveals significant shortcomings, namely suboptimal performance and unstable prediction, necessitating an excessive number of prompt points to attain the desired outcomes. These issues can hardly be addressed by fine-tuning SAM on medical data because the original 2D structure of SAM neglects 3D spatial information. In this paper, we introduce SAM-Med3D, the most comprehensive study to modify SAM for 3D medical images. Our approach is characterized by its comprehensiveness in two primary aspects: firstly, by comprehensively reformulating SAM to a thorough 3D architecture trained on a comprehensively processed large-scale volumetric medical dataset; and secondly, by providing a comprehensive evaluation of its performance. Specifically, we train SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3D excels at capturing 3D spatial information, exhibiting competitive performance with significantly fewer prompt points than the top-performing fine-tuned SAM in the medical domain. We then evaluate its capabilities across 15 datasets and analyze it from multiple perspectives, including anatomical structures, modalities, targets, and generalization abilities. Our approach, compared with SAM, showcases pronouncedly enhanced efficiency and broad segmentation capabilities for 3D volumetric medical images. Our code is released at https://github.com/uni-medical/SAM-Med3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15161v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管Segment Anything Model（SAM）在2D自然图像分割中表现出了令人印象深刻的性能，但其在3D体积医学图像中的应用揭示了显著的缺点，即性能次优和预测不稳定，需要过多的提示点才能获得所需的结果。这些问题很难通过在医学数据上微调SAM来解决，因为SAM的原始2D结构忽略了3D空间信息。在本文中，我们介绍了SAM-Med3D，这是针对3D医学图像修改SAM的最全面的研究。我们的方法的特点是其在两个主要方面的全面性：首先，通过将SAM全面重新制定为在全面处理的大规模体积医学数据集上训练的彻底的3D架构；第二，对其业绩进行全面评价。具体来说，我们用超过131K个3D掩模和247个类别来训练SAM-Med3D。我们的SAM-Med3D擅长捕捉3D空间信息，与医疗领域表现最好的微调SAM相比，其提示点明显更少，表现出有竞争力的性能。然后，我们在15个数据集中评估其能力，并从多个角度对其进行分析，包括解剖结构、模态、目标和泛化能力。与SAM相比，我们的方法显著提高了3D体积医学图像的效率和广泛的分割能力。我们的代码发布于https://github.com/uni-medical/SAM-Med3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15161v2" target="_blank">2310.15161v2</a>
                              </td>
                              <td>SAM-Med3D</td>
                              <td>Haoyu Wang</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15161v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15161v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14736v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上评估分类时，SAMCLR的表现至少与SimCLR相当，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v2" target="_blank">2310.14736v2</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16292v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sharpness-Aware Minimization Leads to Low-Rank Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16292v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16292v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16292v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank reduction mechanism can be more complex, especially for deep networks with pre-activation skip connections and self-attention layers. We make our code available at https://github.com/tml-epfl/sam-low-rank-features.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16292v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>清晰度感知最小化（SAM）是最近提出的一种方法，它可以最小化神经网络训练损失的清晰度。虽然它的泛化改进是众所周知的，也是主要的动机，但我们发现了SAM的另一个有趣的影响：在神经网络的不同层发生的特征秩的降低。我们表明，这种低秩效应发生得非常广泛：对于不同的架构，如全连接网络、卷积网络、视觉转换器，以及对于不同的目标，如回归、分类、语言图像对比训练。为了更好地理解这一现象，我们从机理上理解了低秩特征是如何在简单的两层网络中出现的。我们观察到，大量激活被SAM完全修剪，这直接有助于秩的降低。我们从理论上证实了这种影响，并检验了它也可能发生在深度网络中，尽管整体的降秩机制可能更复杂，尤其是对于具有预激活跳过连接和自注意层的深度网络。我们的代码可在https://github.com/tml-epfl/sam-low-rank-features.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16292v2" target="_blank">2305.16292v2</a>
                              </td>
                              <td>Sharpness-Aware Minimization Leads to Low-Rank Features</td>
                              <td>Maksym Andriushchenko</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16292v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16292v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18709v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Audio-Visual Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18709v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18709v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18709v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a new multi-modal task, namely audio-visual instance segmentation (AVIS), in which the goal is to identify, segment, and track individual sounding object instances in audible videos, simultaneously. To our knowledge, it is the first time that instance segmentation has been extended into the audio-visual domain. To better facilitate this research, we construct the first audio-visual instance segmentation benchmark (AVISeg). Specifically, AVISeg consists of 1,258 videos with an average duration of 62.6 seconds from YouTube and public audio-visual datasets, where 117 videos have been annotated by using an interactive semi-automatic labeling tool based on the Segment Anything Model (SAM). In addition, we present a simple baseline model for the AVIS task. Our new model introduces an audio branch and a cross-modal fusion module to Mask2Former to locate all sounding objects. Finally, we evaluate the proposed method using two backbones on AVISeg. We believe that AVIS will inspire the community towards a more comprehensive multi-modal understanding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18709v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的多模态任务，即视听实例分割（AVIS），其目标是同时识别、分割和跟踪音频视频中的单个发声对象实例。据我们所知，这是第一次将实例分割扩展到视听领域。为了更好地促进这项研究，我们构建了第一个视听实例分割基准（AVISeg）。具体而言，AVISeg由来自YouTube和公共视听数据集的1258个视频组成，平均持续时间为62.6秒，其中117个视频通过使用基于Segment Anything Model（SAM）的交互式半自动标记工具进行了注释。此外，我们还为AVIS任务提供了一个简单的基线模型。我们的新模型在Mask2Former中引入了一个音频分支和一个跨模态融合模块，以定位所有探测对象。最后，我们在AVISeg上使用两个主干对所提出的方法进行了评估。我们相信AVIS将激励社会对多模式的理解更加全面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18709v1" target="_blank">2310.18709v1</a>
                              </td>
                              <td>Audio-Visual Instance Segmentation</td>
                              <td>Ruohao Guo</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18709v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18709v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18660v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Models for Generalist Geospatial Artificial Intelligence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18660v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18660v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18660v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18660v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在开发适应性强、可重复使用的人工智能模型方面取得的重大进展预计将对地球科学和遥感产生重大影响。基础模型通过自我监督在大型未标记数据集上进行预训练，然后使用小型标记数据集对各种下游任务进行微调。本文介绍了一个一流的框架，用于在广泛的地理空间数据上对基础模型进行有效的预训练和微调。我们利用这个框架创建了Prithvi，这是一个基于转换器的地理空间基础模型，在来自协调陆地卫星哨兵2号（HLS）数据集的超过1TB的多光谱卫星图像上进行了预训练。我们的研究证明了我们的框架在成功地将Prithvi微调到一系列地球观测任务方面的有效性，这些任务是以前在基础模型上的工作所没有解决的，包括多时相云隙插补、洪水测绘、野火疤痕分割和多时相作物分割。我们的实验表明，与利用随机初始化的权重相比，预训练的模型加速了微调过程。此外，预训练的Prithvi与最先进的相比也很好，例如，在结构相似性指数方面，在多时相云插补方面比条件GAN模型高出5pp（或5.7%）。最后，由于地球观测领域标记数据的可用性有限，我们逐渐减少了可用于改进模型的标记数据的数量，以评估数据效率，并证明可以在不影响模型准确性的情况下大幅减少数据。经过预训练的1亿参数模型和相应的微调工作流程已通过Hugging Face公开发布，作为对全球地球科学界的开源贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18660v1" target="_blank">2310.18660v1</a>
                              </td>
                              <td>Foundation Models for Generalist Geospatial Artificial Intelligence</td>
                              <td>Johannes Jakubik</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18660v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18660v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2311_04212v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Instance Matting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04212v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04212v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04212v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting~(VIM), that is, estimating alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin. The project is open-sourced at https://github.com/SHI-Labs/VIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04212v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>传统的视频遮片为视频帧中出现的所有实例输出一个阿尔法遮片，从而不区分各个实例。虽然视频实例分割提供了时间一致的实例掩码，但对于遮片应用来说，结果是不令人满意的，特别是由于应用了二值化。为了弥补这一不足，我们提出了视频实例Matting~（VIM），即在视频序列的每一帧估计每个实例的阿尔法矩阵。为了解决这个具有挑战性的问题，我们提出了MSG-VIM，一种掩码序列引导的视频实例Matting神经网络，作为VIM的一种新的基线模型。MSG-VIM利用口罩增强的混合功能，使预测对不准确和不一致的口罩指导具有鲁棒性。它结合了时间掩模和时间特征引导，以提高阿尔法遮片预测的时间一致性。此外，我们为VIM构建了一个新的基准，称为VIM50，它包括50个视频剪辑，其中多个人类实例作为前景对象。为了评估VIM任务的性能，我们引入了一种合适的度量，称为视频实例感知Matting质量~（VIMQ）。我们提出的模型MSG-VIM在VIM50基准上建立了一个强大的基线，并在很大程度上优于现有方法。该项目开源于https://github.com/SHI-Labs/VIM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04212v1" target="_blank">2311.04212v1</a>
                              </td>
                              <td>Video Instance Matting</td>
                              <td>Jiachen Li</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04212v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04212v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04193v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Selective Visual Representations Improve Convergence and Generalization for Embodied AI</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04193v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04193v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04193v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code and pretrained models are available at our project website: https://embodied-codebook.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04193v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具体的人工智能模型通常使用现成的视觉骨干，如CLIP，对其视觉观察进行编码。尽管这种通用表示编码了关于场景的丰富的句法和语义信息，但这些信息中的大部分通常与手头的特定任务无关。这在学习过程中引入了噪音，并分散了主体对任务相关视觉线索的注意力。受人类选择性注意力的启发，即人们根据自己的经验、知识和手头的任务过滤感知的过程，我们引入了一种参数有效的方法来过滤具体人工智能的视觉刺激。我们的方法使用一个小的可学习码本模块引入了任务条件下的瓶颈。该码本被联合训练以优化任务奖励，并充当视觉观察上的任务条件选择性滤波器。我们的实验展示了ProcTHOR、ArchitecTHOR、RoboTHOR、AI2iTHOR和ManipulaTHOR这5个基准点在目标导航和物体位移方面的最先进性能。当适应于诸如Habitat之类的其他模拟环境时，由码本产生的滤波表示也能够更好地泛化并更快地收敛。我们的定性分析表明，代理更有效地探索他们的环境，并且他们的表示保留了与任务相关的信息，如目标对象识别，而忽略了关于其他对象的多余信息。代码和预训练模型可在我们的项目网站上获得：https://embodied-codebook.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04193v1" target="_blank">2311.04193v1</a>
                              </td>
                              <td>Selective Visual Representations Improve Convergence and Generalization for Embodied AI</td>
                              <td>Ainaz Eftekhar</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04193v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04193v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04066v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can CLIP Help Sound Source Localization?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04066v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04066v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04066v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained image-text models demonstrate remarkable versatility across diverse tasks, benefiting from their robust representational capabilities and effective multimodal alignment. We extend the application of these models, specifically CLIP, to the domain of sound source localization. Unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04066v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预训练的图像-文本模型在不同的任务中表现出显著的通用性，得益于其强大的表示能力和有效的多模式对齐。我们将这些模型，特别是CLIP的应用扩展到声源定位领域。与传统方法不同，我们使用预先训练的CLIP模型，没有明确的文本输入，仅依赖于视听对应。为此，我们引入了一个框架，将音频信号转换为与CLIP的文本编码器兼容的令牌，从而产生音频驱动的嵌入。通过直接使用这些嵌入，我们的方法为所提供的音频生成基于音频的掩码，从突出显示的区域提取基于音频的图像特征，并使用视听对应目标将它们与音频驱动的嵌入对齐。我们的研究结果表明，利用预先训练的图像-文本模型，我们的模型能够为探测对象生成更完整、更紧凑的定位图。大量实验表明，我们的方法显著优于最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04066v1" target="_blank">2311.04066v1</a>
                              </td>
                              <td>Can CLIP Help Sound Source Localization?</td>
                              <td>Sooyoung Park</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04066v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04066v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03943v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP Guided Image-perceptive Prompt Learning for Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03943v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03943v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03943v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we delve into the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, We initially learn image-perceptive prompts to distinguish between original and target images using CLIP model, in the meanwhile, we introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUT as enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03943v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像增强是计算机视觉和图像处理领域的一个重要研究领域。近年来，已经开发了许多基于学习的图像增强方法，其中查找表（LUT）已被证明是一种有效的工具。在本文中，我们深入研究了对比语言图像预训练（CLIP）引导的提示学习的潜力，提出了一种简单的图像增强结构CLIP-LUT。我们发现，CLIP的先验知识可以有效地识别退化图像的质量，从而提供可靠的指导。具体来说，我们最初使用CLIP模型学习图像感知提示来区分原始图像和目标图像，同时，我们引入了一个非常简单的网络，通过结合一个简单的基线来预测三个不同LUT的权重作为增强网络。获得的提示用于像损失函数一样引导增强网络，提高模型的性能。我们证明，通过简单地将简单的方法与CLIP相结合，我们可以获得令人满意的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03943v1" target="_blank">2311.03943v1</a>
                              </td>
                              <td>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement</td>
                              <td>Zinuo Li</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03943v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03943v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03774v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03774v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03774v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03774v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner. With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency. Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6\% on eight image classification datasets with higher inference speed. Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks. Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03774v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比视觉语言预训练，即CLIP，在感知开放世界的视觉概念方面表现出显著的潜力，从而实现有效的零样本图像识别。然而，基于CLIP的少数镜头学习方法通常需要对少数镜头样本的参数进行离线微调，从而导致推理时间更长，并在某些领域存在过度拟合的风险。为了应对这些挑战，我们提出了Meta Adapter，这是一种轻量级的残差风格适配器，以在线方式完善由少量镜头样本引导的CLIP功能。通过少量的训练样本，我们的方法可以实现有效的少镜头学习能力，并在没有额外微调的情况下推广到看不见的数据或任务，实现有竞争力的性能和高效率。在没有铃声和口哨声的情况下，我们的方法在八个具有更高推理速度的图像分类数据集上比最先进的在线少镜头学习方法平均高3.6%。此外，我们的模型简单灵活，是一个直接适用于下游任务的即插即用模块。在没有进一步微调的情况下，Meta Adapter在开放词汇表对象检测和分割任务中获得了显著的性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03774v1" target="_blank">2311.03774v1</a>
                              </td>
                              <td>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</td>
                              <td>Cheng Cheng</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03774v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03774v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_03897v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Geodesic Multi-Modal Mixup for Robust Fine-Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_03897v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_03897v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_03897v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on hard negatives as well as original negatives and positives with contrastive loss. Based on the theoretical analysis about hardness guarantee and limiting behavior, we justify the use of our method. Extensive experiments on retrieval, calibration, few- or zero-shot classification (under distribution shift), embedding arithmetic, and image captioning further show that our method provides transferable representations, enabling robust model adaptation on diverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_03897v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预训练的多模态模型，如CLIP，提供了可转移的嵌入，并在各种应用中显示出有希望的结果。然而，对学习的多模态嵌入的分析相对未被探索，并且可以提高嵌入的可转移性。在这项工作中，我们观察到CLIP为两种不同的模态保持分离的嵌入子空间，然后我们通过一致性对齐的透镜对其进行研究，以测量学习表示的质量。无论是从理论上还是从经验上，我们都表明，即使在微调之后，CLIP仍然保持着较差的一致性和对准性。这种缺乏对齐和一致性的情况可能会限制嵌入的可转移性和稳健性。为此，我们设计了一种新的用于鲁棒表示的微调方法，该方法具有更好的对齐性和一致性。首先，我们提出了一种大地测量多模混合方法，该方法混合图像和文本的嵌入，以在超球面上生成硬负样本。然后，我们在硬负、原始负和具有对比损失的正上微调模型。基于对硬度保证和极限行为的理论分析，我们证明了我们的方法的正确性。在检索、校准、少量或零样本分类（分布偏移下）、嵌入算法和图像字幕方面的大量实验进一步表明，我们的方法提供了可转移的表示，能够在不同的任务上实现稳健的模型适配。代码：https://github.com/changdaeoh/multimodal-mixup</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.03897v4" target="_blank">2203.03897v4</a>
                              </td>
                              <td>Geodesic Multi-Modal Mixup for Robust Fine-Tuning</td>
                              <td>Changdae Oh</td>
                              <td>2022-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_03897v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.03897v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03485v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03485v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03485v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03485v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel method for learning reward functions for robotic motions by harnessing the power of a CLIP-based model. Traditional reward function design often hinges on manual feature engineering, which can struggle to generalize across an array of tasks. Our approach circumvents this challenge by capitalizing on CLIP's capability to process both state features and image inputs effectively. Given a pair of consecutive observations, our model excels in identifying the motion executed between them. We showcase results spanning various robotic activities, such as directing a gripper to a designated target and adjusting the position of a cube. Through experimental evaluations, we underline the proficiency of our method in precisely deducing motion and its promise to enhance reinforcement learning training in the realm of robotics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03485v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的方法，通过利用基于CLIP的模型的力量来学习机器人运动的奖励函数。传统的奖励函数设计通常依赖于手动功能工程，而手动功能工程很难在一系列任务中进行概括。我们的方法通过利用CLIP有效处理状态特征和图像输入的能力来规避这一挑战。给定两个连续的观测值，我们的模型在识别它们之间执行的运动方面表现出色。我们展示了各种机器人活动的结果，例如将夹具引导到指定目标和调整立方体的位置。通过实验评估，我们强调了我们的方法在精确推导运动方面的熟练程度，以及它在机器人领域加强强化学习训练的前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03485v1" target="_blank">2311.03485v1</a>
                              </td>
                              <td>CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations</td>
                              <td>Xuzhe Dang</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03485v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03485v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03223v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CarbonFish -- A Bistable Underactuated Compliant Fish Robot capable of High Frequency Undulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03223v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03223v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03223v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Hair Clip Mechanism HCM represents an innovative in plane prestressed bistable mechanism, as delineated in our preceding studies, devised to augment the functional prowess of soft robotics. When juxtaposed with conventional soft and compliant robotic systems, HCMs exhibit pronounced rigidity, augmented mobility, reproducible repeatability, and an effective design and fabrication paradigm. In this research, we investigate the feasibility of utilizing carbon fiber reinforced plastic CFRP as the foundational material for an HCM based fish robot, herein referred to as CarbonFish. Our objective centers on realizing high frequency undulatory motion, thereby laying the groundwork for accelerated aquatic locomotion in subsequent models. We proffer an exhaustive design and fabrication schema underpinned by mathematical principles. Preliminary evaluations of our single actuated CarbonFish have evidenced an undulation frequency approaching 10 Hz, suggesting its potential to outperform other biologically inspired aquatic entities as well as real fish.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03223v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>发夹机构HCM代表了一种创新的平面内预应力双稳态机构，如我们之前的研究所述，旨在增强软机器人的功能能力。当与传统柔软柔顺的机器人系统并置时，HCM表现出显著的刚性、增强的机动性、可重复性以及有效的设计和制造模式。在这项研究中，我们研究了利用碳纤维增强塑料CFRP作为基于HCM的鱼类机器人（以下简称CarbonFish）的基础材料的可行性。我们的目标是实现高频波动运动，从而为后续模型中的加速水生运动奠定基础。我们提供了以数学原理为基础的详尽的设计和制造方案。对我们的单驱动CarbonFish的初步评估表明，其波动频率接近10 Hz，这表明它有潜力超越其他受生物启发的水生实体以及真正的鱼类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03223v1" target="_blank">2311.03223v1</a>
                              </td>
                              <td>CarbonFish -- A Bistable Underactuated Compliant Fish Robot capable of High Frequency Undulation</td>
                              <td>Zechen Xiong</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03223v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03223v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_08348v7_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast Swimming Robots Based on Elastic Instability</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_08348v7_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_08348v7_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_08348v7_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Inspired by the snap-through action of a steel hairclip, we propose a design method for in-plane prestressed mechanisms that exhibit biomimetic morphing and high locomotion performance. Compliant bistable flapping mechanisms are fabricated using this method and are mounted on our untethered soft robotic fish. Using this mechanism, we achieve life-like undulation with a Strouhal number (1) of St = 0.28 and a velocity of 2.03 body lengths per second (43.6 cm/s), a three-fold improvement over past compliant fish robots. A tethered pneumatic version indicates that this mechanism is compatible with soft actuators. We study the mechanism both computationally and experimentally and suggest that elastic instability may offer a path to overcome the speed challenge of soft and compliant robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_08348v7_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>受钢制发夹的搭扣动作的启发，我们提出了一种平面内预应力机构的设计方法，该机构具有仿生变形和高运动性能。柔顺的双稳态扑动机构是用这种方法制造的，并安装在我们的无约束软机器鱼上。使用这种机制，我们实现了类似生命的波动，斯特劳哈尔数（1）为St=0.28，速度为每秒2.03体长（43.6厘米/秒），比过去的柔顺鱼类机器人提高了三倍。系留式气动版本表明该机构与软致动器兼容。我们通过计算和实验研究了该机制，并认为弹性不稳定性可能为克服柔性柔顺机器人的速度挑战提供了一条途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.08348v7" target="_blank">2207.08348v7</a>
                              </td>
                              <td>Fast Swimming Robots Based on Elastic Instability</td>
                              <td>Zechen Xiong</td>
                              <td>2022-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_08348v7_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.08348v7" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03212v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Designing a Hair-Clip Inspired Bistable Mechanism for Soft Fish Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03212v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03212v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03212v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Hair clip mechanism (HCM) is an in-plane prestressed bistable mechanism proposed in our previous research [1]~[5] to enhance the functionality of soft robotics. HCMs have several advantages, such as high rigidity, high mobility, good repeatability, and design and fabrication simplicity, compared to existing soft and compliant robotics. Using our experience with fish robots, this work delves into designing a novel HCM robotic propulsion system made from PETG plastic, carbon fiber-reinforced plastic (CFRP), and steel. Detailed derivation and verification of the HCM theory are given, and the influence of key parameters like dimensions, material types, and servo motor specifications are summarized. The designing algorithm offers insight into HCM robotics. It enables us to search for suitable components, operate robots at a desired frequency, and achieve high-frequency and high-speed undulatory swimming for fish robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03212v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>发夹机构（HCM）是我们在先前的研究[1]~[5]中提出的一种平面内预应力双稳态机构，用于增强软机器人的功能。与现有的柔性柔性机器人相比，HCM具有几个优点，如高刚性、高移动性、良好的可重复性以及设计和制造的简单性。利用我们在鱼类机器人方面的经验，这项工作深入设计了一种由PETG塑料、碳纤维增强塑料（CFRP）和钢制成的新型HCM机器人推进系统。对HCM理论进行了详细的推导和验证，总结了尺寸、材料类型和伺服电机规格等关键参数的影响。该设计算法提供了对HCM机器人的深入了解。它使我们能够搜索合适的组件，以所需的频率操作机器人，并实现鱼类机器人的高频、高速波动游泳。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03212v1" target="_blank">2311.03212v1</a>
                              </td>
                              <td>Designing a Hair-Clip Inspired Bistable Mechanism for Soft Fish Robots</td>
                              <td>Zechen Xiong</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03212v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03212v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07760v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PRE: Vision-Language Prompt Learning with Reparameterization Encoder</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07760v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07760v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07760v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to unseen classes while maintaining the capacity to learn Base classes. Instead of directly optimizing the prompts, PRE employs a prompt encoder to reparameterize the input prompt embeddings, enhancing the exploration of task-specific knowledge from few-shot samples. Experiments and extensive ablation studies on 8 benchmarks demonstrate that our approach is an efficient method for prompt learning. Specifically, PRE achieves a notable enhancement of 5.60% in average accuracy on New classes and 3% in Harmonic mean compared to CoOp in the 16-shot setting, all achieved within a good training time.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07760v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型预先训练的视觉语言模型（如CLIP）在零样本向下游任务的可转移性方面表现出了巨大的潜力。然而，为了获得最佳性能，手动选择提示对于提高下游图像分布和文本类描述之间的一致性是必要的。这种手动提示工程是在实践中部署此类模型的主要挑战，因为它需要领域专业知识，而且非常耗时。为了避免非琐碎的提示工程，最近的工作上下文优化（CoOp）使用可学习的文本标记将提示学习的概念引入到视觉领域。虽然CoOp可以比手动提示实现实质性的改进，但其习得的上下文更难推广到同一数据集中更广泛的看不见的类。在这项工作中，我们提出了带重新参数化编码器的提示学习（PRE），这是一种简单有效的方法，可以增强可学习提示对看不见的类的泛化能力，同时保持学习基类的能力。PRE没有直接优化提示，而是使用提示编码器对输入提示嵌入进行重新参数化，从而增强了对少数镜头样本中特定任务知识的探索。对8个基准点的实验和广泛的消融研究表明，我们的方法是一种快速学习的有效方法。具体而言，与16杆设置中的CoOp相比，PRE在新类上的平均准确率和谐波平均值分别显著提高了5.60%和3%，所有这些都是在良好的训练时间内实现的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07760v2" target="_blank">2309.07760v2</a>
                              </td>
                              <td>PRE: Vision-Language Prompt Learning with Reparameterization Encoder</td>
                              <td>Anh Pham Thi Minh</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07760v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07760v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20700v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20700v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20700v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20700v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips ("shot-level") depicting a single scene. To deliver a coherent long video ("story-level"), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos. Project page: https://vchitect.github.io/SEINE-project/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20700v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，视频生成已经取得了实质性的进展，并取得了现实的结果。尽管如此，现有的人工智能生成的视频通常是描绘单个场景的非常短的片段（“镜头级别”）。为了提供连贯的长视频（“故事级别”），需要在不同的剪辑之间具有创造性的过渡和预测效果。本文提出了一个短到长视频扩散模型SEINE，该模型侧重于生成转换和预测。目标是生成高质量的长视频，在场景和不同长度的镜头级视频之间进行平滑和创造性的转换。具体来说，我们提出了一个随机掩码视频扩散模型，以基于文本描述自动生成转换。通过提供不同场景的图像作为输入，结合基于文本的控制，我们的模型生成了确保连贯性和视觉质量的过渡视频。此外，该模型可以很容易地扩展到各种任务，例如图像到视频动画和自回归视频预测。为了对这一新的生成任务进行全面评估，我们提出了三个平稳和创造性过渡的评估标准：时间一致性、语义相似性和视频文本语义对齐。大量的实验验证了我们的方法相对于现有的生成转换和预测方法的有效性，从而能够创建故事级的长视频。项目页面：https://vchitect.github.io/SEINE-project/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20700v2" target="_blank">2310.20700v2</a>
                              </td>
                              <td>SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction</td>
                              <td>Xinyuan Chen</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20700v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20700v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08138v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08138v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08138v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08138v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user's demand as the task instruction and prompts the agent to find the object matches the specified demand. DDN aims to relax the stringent conditions of VON by focusing on fulfilling the user's demand rather than relying solely on predefined object categories or names. We propose a method first acquire textual attribute features of objects by extracting common knowledge from a large language model. These textual attribute features are subsequently aligned with visual attribute features using Contrastive Language-Image Pre-training (CLIP). By incorporating the visual attribute features as prior knowledge, we enhance the navigation process. Experiments on AI2Thor with the ProcThor dataset demonstrate the visual attribute features improve the agent's navigation performance and outperform the baseline methods commonly used in VON.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08138v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉对象导航（VON）的任务涉及代理在给定场景中定位特定对象的能力。为了成功完成VON任务，必须满足两个基本条件：1）用户必须知道所需对象的名称；以及2）用户指定的对象必须实际存在于场景内。为了满足这些条件，模拟器可以将预定义的对象名称和位置合并到场景的元数据中。然而，在现实世界中，确保这些条件始终得到满足往往是一项挑战。处于陌生环境中的人可能不知道场景中存在哪些对象，或者他们可能错误地指定了一个实际上不存在的对象。尽管如此，尽管存在这些挑战，人类可能仍然对一个物体有需求，而场景中的其他物体可能会以同等的方式满足这一需求。因此，我们提出了需求驱动导航（DDN），它利用用户的需求作为任务指令，并提示代理查找与指定需求匹配的对象。DDN旨在通过专注于满足用户的需求而不是仅仅依赖于预定义的对象类别或名称来放宽VON的严格条件。我们提出了一种方法，首先通过从大型语言模型中提取公共知识来获取对象的文本属性特征。随后使用对比语言图像预训练（CLIP）将这些文本属性特征与视觉属性特征对齐。通过将视觉属性特征作为先验知识，我们增强了导航过程。使用ProcThor数据集在AI2Thor上进行的实验表明，视觉属性特征提高了代理的导航性能，并优于VON中常用的基线方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08138v3" target="_blank">2309.08138v3</a>
                              </td>
                              <td>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</td>
                              <td>Hongcheng Wang</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08138v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08138v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01989v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01989v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01989v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01989v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, large-scale pre-trained models such as Segment-Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable success and revolutionized the field of computer vision. These foundation vision models effectively capture knowledge from a large-scale broad data with their vast model parameters, enabling them to perform zero-shot segmentation on previously unseen data without additional training. While they showcase competence in 2D tasks, their potential for enhancing 3D scene understanding remains relatively unexplored. To this end, we present a novel framework that adapts various foundational models for the 3D point cloud segmentation task. Our approach involves making initial predictions of 2D semantic masks using different large vision models. We then project these mask predictions from various frames of RGB-D video sequences into 3D space. To generate robust 3D semantic pseudo labels, we introduce a semantic label fusion strategy that effectively combines all the results via voting. We examine diverse scenarios, like zero-shot learning and limited guidance from sparse 2D point labels, to assess the pros and cons of different vision foundation models. Our approach is experimented on ScanNet dataset for 3D indoor scenes, and the results demonstrate the effectiveness of adopting general 2D foundation models on solving 3D point cloud segmentation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01989v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大规模的预训练模型，如分段任意模型（SAM）和对比语言图像预训练（CLIP），已经取得了显著的成功，并彻底改变了计算机视觉领域。这些基础视觉模型通过其庞大的模型参数有效地从大规模的广泛数据中获取知识，使它们能够在无需额外训练的情况下对以前看不见的数据执行零样本分割。虽然他们展示了在2D任务中的能力，但他们在增强3D场景理解方面的潜力仍然相对未被探索。为此，我们提出了一种新的框架，该框架适用于3D点云分割任务的各种基础模型。我们的方法包括使用不同的大视觉模型对2D语义掩码进行初始预测。然后，我们将这些来自RGB-D视频序列的不同帧的掩码预测投影到3D空间中。为了生成健壮的3D语义伪标签，我们引入了一种语义标签融合策略，该策略通过投票有效地组合了所有结果。我们研究了不同的场景，如零样本学习和稀疏2D点标签的有限指导，以评估不同视觉基础模型的优缺点。我们的方法在三维室内场景的ScanNet数据集上进行了实验，结果证明了采用通用二维基础模型解决三维点云分割任务的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01989v2" target="_blank">2311.01989v2</a>
                              </td>
                              <td>Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation</td>
                              <td>Shichao Dong</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01989v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01989v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01539v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01539v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01539v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01539v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset is publicly available at https://open-dataset.tusen.ai/TSTTC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01539v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>接触时间（TTC）估计是评估碰撞风险的关键任务，广泛用于各种驾驶员辅助和自动驾驶系统。过去的几十年见证了相关理论和算法的发展。流行的基于学习的方法需要在现实世界场景中使用大规模的TTC数据集。在这项工作中，我们在驾驶场景中提出了一个大规模的面向对象的TTC数据集，用于促进单目相机的TTC估计。为了收集有价值的样本，并使不同TTC值的数据相对平衡，我们通过数千小时的驾驶数据，选择了超过200K个具有预设数据分布的序列。为了增加小TTC案例的数量，我们还使用最新的神经渲染方法生成剪辑。此外，我们提供了几个简单而有效的TTC估计基线，并在所提出的数据集上对其进行了广泛评估，以证明其有效性。建议的数据集可在https://open-dataset.tusen.ai/TSTTC.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01539v3" target="_blank">2309.01539v3</a>
                              </td>
                              <td>TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios</td>
                              <td>Yuheng Shi</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01539v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01539v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_17425v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data Filtering Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_17425v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_17425v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_17425v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art CLIP models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 84.4% zero-shot transfer accuracy on ImageNet, out-performing models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_17425v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型训练集已成为机器学习的基石，也是语言建模和多模式学习最新进展的基础。虽然预训练的数据管理通常仍然是临时的，但一种常见的模式是首先从Web收集大量数据，然后通过各种启发式方法将该候选池筛选到实际的训练集。在这项工作中，我们研究了为过滤大型未分级数据集的第二步学习数据过滤网络（DFN）的问题。我们的关键发现是，用于过滤的网络的质量与其在下游任务上的性能不同：例如，在ImageNet上表现良好的模型可能比在少量高质量数据上训练的ImageNet精度较低的模型产生更差的训练集。基于我们的见解，我们构建了新的数据过滤网络，以引入最先进的图像文本数据集。具体而言，我们性能最好的数据集DFN-5B使我们能够为其计算预算训练最先进的CLIP模型：除了对各种任务的其他改进外，在我们的数据集上训练的ViT-H在ImageNet上实现了84.4%的零样本传输准确率，在其他数据集（如LAION-2B、DataComp-1B或OpenAI的WIT）上训练的性能较差的模型。为了促进数据集设计的进一步研究，我们还发布了一个新的20亿示例数据集DFN-2B，并表明仅使用公开可用的数据就可以从头开始训练高性能数据过滤网络。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.17425v3" target="_blank">2309.17425v3</a>
                              </td>
                              <td>Data Filtering Networks</td>
                              <td>Alex Fang</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_17425v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.17425v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_03480v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Guided Exploration for Zero-Shot Object Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_03480v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_03480v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_03480v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to a uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via \textit{real-world} experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_03480v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了LGX（Language-guided Exploration），这是一种用于Language-Driven零样本对象目标导航（L-ZSON）的新算法，其中所包含的代理在以前看不见的环境中导航到唯一描述的目标对象。我们的方法通过利用大型语言模型（LLM）的常识性推理能力来做出顺序导航决策，从而使用大型语言模型来完成这项任务。同时，我们使用预先训练的视觉语言基础模型进行广义目标物体检测。我们在RoboTHOR上实现了最先进的零样本物体导航结果，成功率（SR）比OWL-ViT CLIP on Wheels（OWL-CoW）的当前基线提高了27%以上。此外，我们研究了LLM在机器人导航中的应用，并分析了影响模型输出的各种提示策略。最后，我们通过\textit｛真实世界｝实验展示了我们的方法的优势，这些实验表明LGX在检测和导航到视觉上独特的对象方面具有卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.03480v2" target="_blank">2303.03480v2</a>
                              </td>
                              <td>Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Guided Exploration for Zero-Shot Object Navigation</td>
                              <td>Vishnu Sashank Dorbala</td>
                              <td>2023-03-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_03480v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.03480v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_16604v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bi-directional Training for Composed Image Retrieval via Text Prompt Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_16604v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_16604v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_16604v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as described by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures with minimum changes, which improves the performance of the model. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datasets show that our novel approach achieves improved performance over a baseline BLIP-based model that itself already achieves competitive performance. Our code is released at https://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_16604v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>合成图像检索基于由参考图像和描述期望变化的修改文本组成的多模态用户查询来搜索目标图像。解决这一具有挑战性的任务的现有方法学习从（参考图像、修改文本）对到图像嵌入的映射，然后与大型图像语料库进行匹配。一个尚未探索的领域是反向，这提出了一个问题，当按照文本描述进行修改时，什么样的参考图像会产生给定的目标图像？在这项工作中，我们提出了一种双向训练方案，该方案利用了这种反向查询，并可以应用于现有的组合图像检索架构，同时具有最小的变化，从而提高了模型的性能。为了对双向查询进行编码，我们为指定查询方向的修改文本预先准备了一个可学习的令牌，然后微调文本嵌入模块的参数。我们不对网络架构进行其他更改。在两个标准数据集上的实验表明，我们的新方法比基于基线BLIP的模型实现了更好的性能，该模型本身已经实现了有竞争力的性能。我们的代码发布于https://github.com/Cuberick-Orion/Bi-Blip4CIR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.16604v2" target="_blank">2303.16604v2</a>
                              </td>
                              <td>Bi-directional Training for Composed Image Retrieval via Text Prompt Learning</td>
                              <td>Zheyuan Liu</td>
                              <td>2023-03-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_16604v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.16604v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02612v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02612v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02612v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02612v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding capabilities, making it possible to handle certain tasks through the Visual Question Answering (VQA) paradigm. This paper explores the potential of VQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and is the first to conduct qualitative and quantitative evaluations on the popular MVTec AD and VisA datasets. Considering that this task requires both image-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three components: 1) Granular Region Division, 2) Prompt Designing, 3) Text2Segmentation for easy quantitative evaluation, and have made some different attempts for comparative analysis. The results show that GPT-4V can achieve certain results in the zero-shot AD task through a VQA paradigm, such as achieving image-level 77.1/88.0 and pixel-level 68.0/76.6 AU-ROCs on MVTec AD and VisA datasets, respectively. However, its performance still has a certain gap compared to the state-of-the-art zero-shot method, e.g., WinCLIP ann CLIP-AD, and further research is needed. This study provides a baseline reference for the research of VQA-oriented LMM in the zero-shot AD task, and we also post several possible future works. Code is available at \url{https://github.com/zhangzjn/GPT-4V-AD}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02612v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型多模式模型（LMM）GPT-4V（vision）赋予GPT-4视觉基础能力，使其能够通过视觉问答（VQA）范式处理某些任务。本文探索了面向VQA的GPT-4V在最近流行的视觉异常检测（AD）中的潜力，并首次对流行的MVTec AD和VisA数据集进行定性和定量评估。考虑到这项任务需要图像/像素级别的评估，所提出的GPT-4V-AD框架包含三个组成部分：1）颗粒区域划分，2）提示设计，3）文本2分段以便于定量评估，并对比较分析进行了一些不同的尝试。结果表明，GPT-4V可以通过VQA范式在零样本AD任务中实现一定的结果，例如在MVTec AD和VisA数据集上分别实现图像级77.1/88.0和像素级68.0/76.6 AU-ROC。然而，与最先进的零样本方法（如WinCLIP ann CLIP-AD）相比，其性能仍有一定差距，需要进一步研究。本研究为面向VQA的LMM在零样本AD任务中的研究提供了基线参考，并提出了几项可能的未来工作。代码位于\url{https://github.com/zhangzjn/GPT-4V-AD}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02612v1" target="_blank">2311.02612v1</a>
                              </td>
                              <td>Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection</td>
                              <td>Jiangning Zhang</td>
                              <td>2023-11-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02612v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02612v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_12636v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_12636v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_12636v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_12636v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language pre-training models (VLP) are vulnerable, especially to multimodal adversarial samples, which can be crafted by adding imperceptible perturbations on both original images and texts. However, under the black-box setting, there have been no works to explore the transferability of multimodal adversarial attacks against the VLP models. In this work, we take CLIP as the surrogate model and propose a gradient-based multimodal attack method to generate transferable adversarial samples against the VLP models. By applying the gradient to optimize the adversarial images and adversarial texts simultaneously, our method can better search for and attack the vulnerable images and text information pairs. To improve the transferability of the attack, we utilize contrastive learning including image-text contrastive learning and intra-modal contrastive learning to have a more generalized understanding of the underlying data distribution and mitigate the overfitting of the surrogate model so that the generated multimodal adversarial samples have a higher transferability for VLP models. Extensive experiments validate the effectiveness of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_12636v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练模型（VLP）很容易受到攻击，尤其是多模式对抗性样本，这可以通过在原始图像和文本上添加难以察觉的扰动来制作。然而，在黑盒设置下，还没有研究针对VLP模型的多模式对抗性攻击的可转移性的工作。在这项工作中，我们以CLIP作为代理模型，并提出了一种基于梯度的多模式攻击方法来生成针对VLP模型的可转移对抗性样本。通过应用梯度同时优化对抗性图像和对抗性文本，我们的方法可以更好地搜索和攻击易受攻击的图像和文本信息对。为了提高攻击的可转移性，我们利用包括图像-文本对比学习和模态内对比学习在内的对比学习，对底层数据分布有更广泛的理解，并减轻代理模型的过拟合，以便生成的多模态对抗性样本对VLP模型具有更高的可转移能力。大量实验验证了该方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.12636v2" target="_blank">2308.12636v2</a>
                              </td>
                              <td>Exploring Transferability of Multimodal Adversarial Samples for Vision-Language Pre-training Models with Contrastive Learning</td>
                              <td>Youze Wang</td>
                              <td>2023-08-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_12636v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.12636v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02536v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02536v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02536v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02536v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding-based vision and language models have been successfully applied to low-level vision tasks, aiming to precisely locate objects referred in captions. The effectiveness of grounding representation learning heavily relies on the scale of the training dataset. Despite being a useful data enrichment strategy, data augmentation has received minimal attention in existing vision and language tasks as augmentation for image-caption pairs is non-trivial. In this study, we propose a robust phrase grounding model trained with text-conditioned and text-unconditioned data augmentations. Specifically, we apply text-conditioned color jittering and horizontal flipping to ensure semantic consistency between images and captions. To guarantee image-caption correspondence in the training samples, we modify the captions according to pre-defined keywords when applying horizontal flipping. Additionally, inspired by recent masked signal reconstruction, we propose to use pixel-level masking as a novel form of data augmentation. While we demonstrate our data augmentation method with MDETR framework, the proposed approach is applicable to common grounding-based vision and language tasks with other frameworks. Finally, we show that image encoder pretrained on large-scale image and language datasets (such as CLIP) can further improve the results. Through extensive experiments on three commonly applied datasets: Flickr30k, referring expressions and GQA, our method demonstrates advanced performance over the state-of-the-arts with various metrics. Code can be found in https://github.com/amzn/augment-the-pairs-wacv2024.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02536v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于基础的视觉和语言模型已成功应用于低级视觉任务，旨在精确定位字幕中提及的对象。基础表示学习的有效性在很大程度上取决于训练数据集的规模。尽管数据扩充是一种有用的数据扩充策略，但在现有的视觉和语言任务中，数据扩充却很少受到关注，因为图像字幕对的扩充并非易事。在这项研究中，我们提出了一个用文本条件和文本非条件数据增强训练的鲁棒短语基础模型。具体来说，我们应用文本条件下的颜色抖动和水平翻转来确保图像和字幕之间的语义一致性。为了保证训练样本中的图像字幕对应性，我们在应用水平翻转时根据预定义的关键字修改字幕。此外，受最近掩蔽信号重建的启发，我们建议使用像素级掩蔽作为一种新的数据增强形式。虽然我们用MDETR框架演示了我们的数据扩充方法，但所提出的方法适用于其他框架的基于公共基础的视觉和语言任务。最后，我们证明了在大规模图像和语言数据集（如CLIP）上预训练的图像编码器可以进一步改进结果。通过在三个常用数据集上进行广泛的实验：Flickr30k、引用表达式和GQA，我们的方法在各种指标上展示了优于现有技术的先进性能。代码可在中找到https://github.com/amzn/augment-the-pairs-wacv2024.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02536v1" target="_blank">2311.02536v1</a>
                              </td>
                              <td>Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models</td>
                              <td>Jingru Yi</td>
                              <td>2023-11-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02536v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02536v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00729v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00729v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00729v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00729v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationship between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00729v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>时间动作检测（TAD）涉及未修剪视频中动作实例的定位和分类。虽然标准TAD遵循在大型训练数据上使用封闭集设置的完全监督学习，但最近的零样本TAD方法通过利用大型对比可视化语言（ViL）预训练模型展示了有前景的开放集设置。然而，现有的零样本TAD方法在如何正确构建定位和分类这两个相互依赖的任务之间的强关系以及使ViL模型适应视频理解方面存在局限性。在这项工作中，我们介绍了ZEETAD，具有两个模块：双重定位和零样本提案分类。前者是一个基于Transformer的模块，它检测动作事件，同时选择性地收集关键的语义嵌入以供以后识别。后一个模块是基于CLIP的模块，它根据每个时间单元的文本和帧输入生成语义嵌入。此外，我们通过使用轻量级适配器最小限度地更新冻结的CLIP编码器，增强了对看不见类的判别能力。在THUMOS14和ActivityNet-1.3数据集上进行的大量实验证明了我们的方法在零样本TAD中的卓越性能，以及从ViL模型到看不见的动作类别的有效知识转移。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00729v2" target="_blank">2311.00729v2</a>
                              </td>
                              <td>ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection</td>
                              <td>Thinh Phan</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00729v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00729v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08675v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improved baselines for vision-language pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08675v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08675v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08675v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive learning has emerged as an efficient framework to learn multimodal representations. CLIP, a seminal work in this area, achieved impressive results by training on paired image-text data using the contrastive loss. Recent work claims improvements over CLIP using additional non-contrastive losses inspired from self-supervised learning. However, it is sometimes hard to disentangle the contribution of these additional losses from other implementation details, e.g., data augmentation or regularization techniques, used to train the model. To shed light on this matter, in this paper, we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning. In particular, we use the loss functions that were proven successful for visual self-supervised learning to align image and text modalities. We find that these baselines outperform a basic implementation of CLIP. However, when a stronger training recipe is employed, the advantage disappears. Indeed, we find that a simple CLIP baseline can also be improved substantially, up to a 25% relative improvement on downstream zero-shot tasks, by using well-known training techniques that are popular in other subfields. Moreover, we discover that it is enough to apply image and text augmentations to make up for most of the improvement attained by prior works. With our improved training recipe for CLIP, we obtain state-of-the-art performance on four standard datasets, and consistently outperform prior work (up to +4% on the largest dataset), while being substantially simpler. The code is available at https://github.com/facebookresearch/clip-rocket</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08675v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比学习已经成为学习多模态表征的有效框架。CLIP是该领域的一项开创性工作，通过使用对比损失对成对的图像文本数据进行训练，取得了令人印象深刻的结果。最近的工作声称，在自我监督学习的启发下，使用了额外的非对比损失来改进CLIP。然而，有时很难将这些额外损失的贡献与其他实现细节区分开来，例如用于训练模型的数据增强或正则化技术。为了阐明这一点，在本文中，我们首先提出、实现和评估了通过将对比学习与自我监督学习的最新进展相结合而获得的几个基线。特别是，我们使用在视觉自监督学习中被证明是成功的损失函数来对齐图像和文本模式。我们发现这些基线优于CLIP的基本实现。然而，当采用更强的训练配方时，优势就会消失。事实上，我们发现，通过使用在其他子领域流行的众所周知的训练技术，简单的CLIP基线也可以得到显著改善，在下游零样本任务上的相对改善高达25%。此外，我们发现应用图像和文本增强来弥补先前工作所获得的大部分改进是足够的。通过改进CLIP的训练配方，我们在四个标准数据集上获得了最先进的性能，并始终优于先前的工作（在最大的数据集上高达+4%），同时大大简化。代码可在https://github.com/facebookresearch/clip-rocket</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08675v2" target="_blank">2305.08675v2</a>
                              </td>
                              <td>Improved baselines for vision-language pre-training</td>
                              <td>Enrico Fini</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08675v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08675v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02236v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Fine-Tuning of Vision-Language Models for Domain Generalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02236v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02236v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02236v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transfer learning enables the sharing of common knowledge among models for a variety of downstream tasks, but traditional methods suffer in limited training data settings and produce narrow models incapable of effectively generalizing under distribution shifts. Foundation models have recently demonstrated impressive zero-shot inference capabilities and robustness under distribution shifts. However, zero-shot evaluation for these models has been predominantly confined to benchmarks with simple distribution shifts, limiting our understanding of their effectiveness under the more realistic shifts found in practice. Moreover, common fine-tuning methods for these models have yet to be evaluated against vision models in few-shot scenarios where training data is limited. To address these gaps, we present a new recipe for few-shot fine-tuning of the popular vision-language foundation model CLIP and evaluate its performance on challenging benchmark datasets with realistic distribution shifts from the WILDS collection. Our experimentation demonstrates that, while zero-shot CLIP fails to match performance of trained vision models on more complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only counterparts in terms of in-distribution and out-of-distribution accuracy at all levels of training data availability. This provides a strong incentive for adoption of foundation models within few-shot learning applications operating with real-world data. Code is available at https://github.com/mit-ll/robust-vision-language-finetuning</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02236v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>迁移学习能够在各种下游任务的模型之间共享共同知识，但传统方法在有限的训练数据设置中受到影响，并产生无法在分布变化下有效推广的狭窄模型。基础模型最近展示了令人印象深刻的零样本推理能力和分布变化下的鲁棒性。然而，这些模型的零样本评估主要局限于具有简单分布变化的基准，这限制了我们在实践中发现的更现实的变化下对其有效性的理解。此外，在训练数据有限的少数镜头场景中，这些模型的常见微调方法尚未与视觉模型进行评估。为了解决这些差距，我们提出了一种新的方法来对流行的视觉语言基础模型CLIP进行几次微调，并评估其在具有挑战性的基准数据集上的性能，这些数据集的分布与WILDS集合的实际分布不同。我们的实验表明，虽然零样本CLIP无法在更复杂的基准上匹配训练的视觉模型的性能，但在所有级别的训练数据可用性下，很少的热CLIP微调在分布内和分布外精度方面都优于无视觉的同行。这为在使用真实世界数据的少数镜头学习应用程序中采用基础模型提供了强有力的激励。代码可在https://github.com/mit-ll/robust-vision-language-finetuning</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02236v1" target="_blank">2311.02236v1</a>
                              </td>
                              <td>Robust Fine-Tuning of Vision-Language Models for Domain Generalization</td>
                              <td>Kevin Vogt-Lowell</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02236v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02236v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_04873v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_04873v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_04873v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_04873v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current methods for few-shot action recognition mainly fall into the metric learning framework following ProtoNet, which demonstrates the importance of prototypes. Although they achieve relatively good performance, the effect of multimodal information is ignored, e.g. label texts. In this work, we propose a novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic information of label texts as multimodal information to enhance prototypes. A CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain features with good multimodal initialization. Then in the visual flow, visual prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module for example. In the text flow, a semantic-enhanced (SE) module and an inflating operation are used to obtain text prototypes. The final multimodal prototypes are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we define a PRototype SImilarity DiffErence (PRIDE) to evaluate the quality of prototypes, which is used to verify our improvement on the prototype level and effectiveness of MORN. We conduct extensive experiments on four popular datasets, and MORN achieves state-of-the-art results on HMDB51, UCF101, Kinetics and SSv2. When plugging PRIDE into the training stage, the performance can be further improved.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_04873v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前的小镜头动作识别方法主要属于ProtoNet之后的度量学习框架，这表明了原型的重要性。尽管它们实现了相对较好的性能，但多模式信息的影响被忽略了，例如标签文本。在这项工作中，我们提出了一种新的多模态原型增强网络（MORN），该网络使用标签文本的语义信息作为多模态信息来增强原型。引入了CLIP视觉编码器和冻结CLIP文本编码器，以获得具有良好多模式初始化的特征。然后在视觉流中，例如，通过时态关系交叉变换器（TRX）模块来计算视觉原型。在文本流中，使用语义增强（SE）模块和膨胀操作来获得文本原型。然后通过多模式原型增强（MPE）模块来计算最终的多模式原型。此外，我们定义了一个原型相似性差异（PRIDE）来评估原型的质量，用于验证我们对MORN原型水平和有效性的改进。我们在四个流行的数据集上进行了广泛的实验，MORN在HMDB51、UCF101、Kinetics和SSv2上取得了最先进的结果。将PRIDE融入训练阶段，可以进一步提高成绩。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.04873v2" target="_blank">2212.04873v2</a>
                              </td>
                              <td>Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition</td>
                              <td>Xinzhe Ni</td>
                              <td>2022-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_04873v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.04873v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20343v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Multi-modal Encoders for Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20343v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20343v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20343v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid growth of online multimedia services, such as e-commerce platforms, has necessitated the development of personalised recommendation approaches that can encode diverse content about each item. Indeed, modern multi-modal recommender systems exploit diverse features obtained from raw images and item descriptions to enhance the recommendation performance. However, the existing multi-modal recommenders primarily depend on the features extracted individually from different media through pre-trained modality-specific encoders, and exhibit only shallow alignments between different modalities - limiting these systems' ability to capture the underlying relationships between the modalities. In this paper, we investigate the usage of large multi-modal encoders within the specific context of recommender systems, as these have previously demonstrated state-of-the-art effectiveness when ranking items across various domains. Specifically, we tailor two state-of-the-art multi-modal encoders (CLIP and VLMo) for recommendation tasks using a range of strategies, including the exploration of pre-trained and fine-tuned encoders, as well as the assessment of the end-to-end training of these encoders. We demonstrate that pre-trained large multi-modal encoders can generate more aligned and effective user/item representations compared to existing modality-specific encoders across three multi-modal recommendation datasets. Furthermore, we show that fine-tuning these large multi-modal encoders with recommendation datasets leads to an enhanced recommendation performance. In terms of different training paradigms, our experiments highlight the essential role of the end-to-end training of large multi-modal encoders in multi-modal recommendation systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20343v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着电子商务平台等在线多媒体服务的快速发展，有必要开发个性化的推荐方法，对每件商品的不同内容进行编码。事实上，现代多模式推荐系统利用从原始图像和项目描述中获得的各种特征来提高推荐性能。然而，现有的多模态推荐器主要依赖于通过预先训练的模态特定编码器从不同媒体中单独提取的特征，并且在不同模态之间只表现出浅的对齐，这限制了这些系统捕捉模态之间潜在关系的能力。在本文中，我们研究了大型多模式编码器在推荐系统的特定环境中的使用，因为这些编码器在对不同领域的项目进行排名时已经证明了最先进的有效性。具体而言，我们使用一系列策略为推荐任务定制了两个最先进的多模式编码器（CLIP和VLMo），包括探索预训练和微调编码器，以及评估这些编码器的端到端训练。我们证明，在三个多模态推荐数据集上，与现有的模态特定编码器相比，预先训练的大型多模态编码器可以生成更一致、更有效的用户/项目表示。此外，我们还表明，用推荐数据集微调这些大型多模态编码器可以提高推荐性能。就不同的训练范式而言，我们的实验强调了大型多模式编码器的端到端训练在多模式推荐系统中的重要作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20343v2" target="_blank">2310.20343v2</a>
                              </td>
                              <td>Large Multi-modal Encoders for Recommendation</td>
                              <td>Zixuan Yi</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20343v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20343v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14926v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reference-based Restoration of Digitized Analog Videotapes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14926v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14926v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14926v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14926v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，模拟磁带一直是主要的视频数据存储设备。存储在模拟录像带上的视频表现出由磁带老化和读取器设备故障引起的独特退化模式，这与在胶片和数字视频恢复任务中观察到的不同。在这项工作中，我们提出了一种基于参考的数字化模拟录像带PE（TAPE）重建方法。我们利用CLIP进行零样本伪影检测，通过描述不同伪影的文本提示来识别每个视频中最干净的帧。然后，我们选择与输入帧最相似的干净帧，并将它们用作参考。我们设计了一个基于转换器的Swin-UNet网络，该网络通过我们的多参考空间特征融合（MRSFF）块利用相邻帧和参考帧。MRSFF块依赖于交叉注意力和注意力池来利用每个参考帧中最有用的部分。为了解决现实世界视频中缺乏基本事实的问题，我们创建了一个视频合成数据集，展示了与模拟录像带中常见的伪像非常相似的伪像。定量和定性实验都表明，与其他最先进的方法相比，我们的方法是有效的。代码、模型和合成数据集可在https://github.com/miccunifi/TAPE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14926v2" target="_blank">2310.14926v2</a>
                              </td>
                              <td>Reference-based Restoration of Digitized Analog Videotapes</td>
                              <td>Lorenzo Agnolucci</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14926v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14926v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18961v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18961v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18961v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18961v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18961v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本异常检测（ZSAD）需要使用辅助数据训练的检测模型来检测异常，而无需在目标数据集中使用任何训练样本。当由于各种问题（例如数据隐私）而无法访问训练数据时，这是一项至关重要的任务，但由于模型需要推广到不同领域的异常，其中前景对象、异常区域和背景特征（如不同产品/器官上的缺陷/肿瘤）的出现可能会有很大差异，因此这项任务具有挑战性。最近，大型预先训练的视觉语言模型（VLM），如CLIP，在包括异常检测在内的各种视觉任务中表现出强大的零样本识别能力。然而，它们的ZSAD性能较弱，因为VLM更专注于对前景对象的类语义建模，而不是对图像中的异常/正常性建模。在本文中，我们介绍了一种新的方法，即AnomalyCLIP，以使CLIP适应不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习对象不可知的文本提示，无论图像的前景对象如何，都可以捕捉图像中的一般正常和异常。这使得我们的模型能够关注异常图像区域，而不是对象语义，从而能够对不同类型的对象进行广义正态和异常识别。在17个真实世界异常检测数据集上进行的大规模实验表明，AnomalyCLIP在来自各种缺陷检测和医学成像领域的具有高度不同类别语义的数据集中实现了检测和分割异常的优异零样本性能。代码将在https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18961v2" target="_blank">2310.18961v2</a>
                              </td>
                              <td>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</td>
                              <td>Qihang Zhou</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18961v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18961v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03689v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COLA: A Benchmark for Compositional Text-to-image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03689v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03689v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03689v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Compositional reasoning is a hallmark of human visual intelligence. Yet, despite the size of large vision-language models, they struggle to represent simple compositions by combining objects with their attributes. To measure this lack of compositional capability, we design Cola, a text-to-image retrieval benchmark to Compose Objects Localized with Attributes. To solve Cola, a model must retrieve images with the correct configuration of attributes and objects and avoid choosing a distractor image with the same objects and attributes but in the wrong configuration. Cola contains about 1.2k composed queries of 168 objects and 197 attributes on around 30K images. Our human evaluation finds that Cola is 83.33% accurate, similar to contemporary compositionality benchmarks. Using Cola as a testbed, we explore empirical modeling designs to adapt pre-trained vision-language models to reason compositionally. We explore 6 adaptation strategies on 2 seminal vision-language models, using compositionality-centric test benchmarks - Cola and CREPE. We find the optimal adaptation strategy is to train a multi-modal attention layer that jointly attends over the frozen pre-trained image and language features. Surprisingly, training multimodal layers on CLIP performs better than tuning a larger FLAVA model with already pre-trained multimodal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA to comparable levels, suggesting that training multimodal layers using contrastive attribute-object data is key, as opposed to using them pre-trained. Lastly, we show that Cola is harder than a closely related contemporary benchmark, CREPE, since simpler fine-tuning strategies without multimodal layers suffice on CREPE but not on Cola. However, we still see a significant gap between our best adaptation and human accuracy, suggesting considerable room for further research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03689v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>合成推理是人类视觉智能的标志。然而，尽管大型视觉语言模型很大，但它们很难通过将对象与其属性相结合来表示简单的组成。为了衡量这种合成能力的缺乏，我们设计了Cola，这是一个文本到图像的检索基准，用于合成用属性本地化的对象。为了解决可乐问题，模型必须检索具有正确属性和对象配置的图像，并避免选择具有相同对象和属性但配置错误的干扰图像。Cola在大约3万张图像上包含168个对象和197个属性的约1.2万个组合查询。我们的人类评估发现，Cola的准确率为83.33%，类似于当代的合成基准。使用Cola作为试验台，我们探索了经验建模设计，以使预先训练的视觉语言模型适应合成推理。我们使用以合成为中心的测试基准——Cola和CREPE，在两个开创性的视觉语言模型上探索了6种适应策略。我们发现，最佳的适应策略是训练一个多模态注意力层，该层联合处理冻结的预训练图像和语言特征。令人惊讶的是，在CLIP上训练多模式层比用已经预先训练的多模式层调整更大的FLAVA模型表现得更好。此外，我们的自适应策略将CLIP和FLAVA提高到了可比的水平，这表明使用对比属性对象数据训练多模式层是关键，而不是使用预先训练的多模式层。最后，我们表明，Cola比密切相关的当代基准CREPE更难，因为没有多模式层的更简单的微调策略在CREPE上就足够了，但在Cola上就不够了。然而，我们仍然看到我们的最佳适应能力和人类的准确性之间存在巨大差距，这表明我们还有很大的研究空间。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03689v3" target="_blank">2305.03689v3</a>
                              </td>
                              <td>COLA: A Benchmark for Compositional Text-to-image Retrieval</td>
                              <td>Arijit Ray</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03689v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03689v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models: A Baseline Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transfer capabilities in a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first observe that, unlike fine-tuning from ImageNet pre-trained models, as previous methods do, fine-tuning from foundation models yields significantly poorer results, sometimes even worse than training from scratch. While freezing the backbones, we demonstrate that although the foundation models greatly improve the performance of the baseline method that trains the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. Based on these findings, we introduce \textit{CLIP distillation}, a parameter-free method specifically designed to distill target knowledge from CLIP models. The core of our \textit{CLIP distillation} lies in a self-calibration technique for automatic temperature scaling, a feature that significantly enhances the baseline's out-class detection capability. Although simple, our method outperforms previous approaches in most benchmark tasks, excelling in evaluation metrics including H-score/H$^3$-score and the newly proposed universal classification rate (UCR) metric. We hope that our investigation and the proposed simple framework can serve as a strong baseline to facilitate future studies in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务中显示出了令人印象深刻的学习和迁移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先观察到，与之前的方法一样，从ImageNet预训练模型进行微调不同，从基础模型进行微调会产生明显较差的结果，有时甚至比从头开始训练更差。在冻结主干的同时，我们证明，尽管基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。基于这些发现，我们介绍了\textit｛CLIP蒸馏｝，这是一种专门设计用于从CLIP模型中提取目标知识的无参数方法。我们的\textit｛CLIP蒸馏｝的核心在于自动温度标度的自校准技术，这一功能显著增强了基线的超一流检测能力。尽管简单，但我们的方法在大多数基准任务中都优于以前的方法，在评估指标方面表现出色，包括H-core/H$^3$-得分和新提出的通用分类率（UCR）指标。我们希望，我们的调查和拟议的简单框架能够作为一个强有力的基线，促进该领域未来的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v2" target="_blank">2305.11092v2</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models: A Baseline Study</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16397v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Diffusion Models Vision-And-Language Reasoners?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16397v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16397v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16397v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5. Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16397v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本条件图像生成模型最近显示出使用去噪扩散过程在质量上取得了巨大成功。然而，与判别视觉和语言模型不同，对这些基于扩散的生成模型进行高级现象（如合成性）的自动细粒度定量评估是一项不平凡的任务。为此，我们进行了两项创新。首先，我们使用一种称为DiffusionITM的新方法，为任何图像-文本匹配（ITM）任务转换基于扩散的模型（在我们的情况下，为稳定扩散）。其次，我们介绍了生成判别评估基准（GDBench），包括7项复杂的视觉和语言任务、偏差评估和详细分析。我们发现Stable Diffusion+DiffusionITM在许多任务上具有竞争力，在CLEVR和Winoground等合成任务上优于CLIP。我们通过对MS-COCO进行微调，同时保留生成能力，通过转移设置进一步提高了其组成性能。我们还测量了扩散模型中的定型偏差，发现稳定扩散2.1在很大程度上比稳定扩散1.5的偏差更小。总的来说，我们的结果指向了一个令人兴奋的方向，使判别和生成模型评估更加接近。我们将很快发布代码和基准测试设置。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16397v3" target="_blank">2305.16397v3</a>
                              </td>
                              <td>Are Diffusion Models Vision-And-Language Reasoners?</td>
                              <td>Benno Krojer</td>
                              <td>2023-05-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16397v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16397v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_09270v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_09270v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_09270v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_09270v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Owing to the power of vision-language foundation models, e.g., CLIP, the area of image synthesis has seen recent important advances. Particularly, for style transfer, CLIP enables transferring more general and abstract styles without collecting the style images in advance, as the style can be efficiently described with natural language, and the result is optimized by minimizing the CLIP similarity between the text description and the stylized image. However, directly using CLIP to guide style transfer leads to undesirable artifacts (mainly written words and unrelated visual entities) spread over the image. In this paper, we propose SpectralCLIP, which is based on a spectral representation of the CLIP embedding sequence, where most of the common artifacts occupy specific frequencies. By masking the band including these frequencies, we can condition the generation process to adhere to the target style properties (e.g., color, texture, paint stroke, etc.) while excluding the generation of larger-scale structures corresponding to the artifacts. Experimental results show that SpectralCLIP prevents the generation of artifacts effectively in quantitative and qualitative terms, without impairing the stylisation quality. We also apply SpectralCLIP to text-conditioned image generation and show that it prevents written words in the generated images. Our code is available at https://github.com/zipengxuc/SpectralCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_09270v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于视觉语言基础模型（例如CLIP）的力量，图像合成领域最近取得了重要进展。特别地，对于风格转移，CLIP能够在不预先收集风格图像的情况下转移更一般和抽象的风格，因为可以用自然语言有效地描述风格，并且通过最小化文本描述和风格化图像之间的CLIP相似性来优化结果。然而，直接使用CLIP来引导风格转移会导致图像上出现不希望有的伪影（主要是书面文字和不相关的视觉实体）。在本文中，我们提出了SpectralCLIP，它基于CLIP嵌入序列的频谱表示，其中大多数常见伪影占据特定频率。通过掩蔽包括这些频率的频带，我们可以调节生成过程以遵守目标样式属性（例如，颜色、纹理、绘制笔划等），同时排除与伪影相对应的更大规模结构的生成。实验结果表明，SpectralCLIP在定量和定性方面有效地防止了伪影的产生，而不会损害风格化质量。我们还将SpectralCLIP应用于文本条件图像生成，并表明它可以防止生成的图像中出现书面单词。我们的代码可在https://github.com/zipengxuc/SpectralCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.09270v3" target="_blank">2303.09270v3</a>
                              </td>
                              <td>SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective</td>
                              <td>Zipeng Xu</td>
                              <td>2023-03-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_09270v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.09270v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01459v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01459v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01459v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01459v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains -- distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top- 1 accuracy beyond existing prompt-learning techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01459v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的视觉语言模型的零样本推广很有前景，这导致了它们在许多下游任务中使用即时学习。先前的工作已经表明，使用熵最小化来调整测试时间提示，以适应看不见的领域的文本提示。虽然有效，但这忽略了性能下降到看不见的领域的关键原因——分布转移。在这项工作中，我们通过使用提示调优将分布外（OOD）测试样本统计信息与源数据的统计信息对齐，明确地处理了这个问题。我们使用单个测试样本在测试时通过最小化特征分布偏移来适应多模式提示，以弥补测试域中的差距。根据领域泛化基准进行评估，我们的方法比现有的提示学习技术提高了零样本前1的精度，比基线MaPLe提高了3.08%。在10个数据集的跨数据集泛化中，与现有技术相比，我们的方法在所有数据集中都得到了一致的改进。我们的源代码和模型可在https://jameelhassan.github.io/promptalign.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01459v1" target="_blank">2311.01459v1</a>
                              </td>
                              <td>Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</td>
                              <td>Jameel Hassan</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01459v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01459v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15111v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Image Captioning with CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15111v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15111v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15111v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image captioning, a fundamental task in vision-language understanding, seeks to generate accurate natural language descriptions for provided images. Current image captioning approaches heavily rely on high-quality image-caption pairs, which can be hard to obtain for many domains. To address this, we introduce a self-supervised image captioning method. After learning an initial signal from a small labeled dataset, our method transitions to self-supervised learning on unlabeled data, leveraging the auxiliary task of enhancing the CLIP relevance between images and generated captions. Remarkably, despite utilizing less than 2% of the labeled COCO dataset, our method delivers a performance comparable to state-of-the-art models trained on the complete dataset. Human evaluations further reveal that our method produces captions with greater distinctiveness and informativeness, two attributes inherently challenging to achieve through supervised learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15111v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像字幕是视觉语言理解的一项基本任务，旨在为所提供的图像生成准确的自然语言描述。当前的图像字幕方法在很大程度上依赖于高质量的图像字幕对，这在许多领域都很难获得。为了解决这个问题，我们介绍了一种自监督的图像字幕方法。在从小的标记数据集中学习初始信号后，我们的方法过渡到对未标记数据的自监督学习，利用增强图像和生成的字幕之间的CLIP相关性的辅助任务。值得注意的是，尽管使用了不到2%的标记COCO数据集，但我们的方法提供了与在完整数据集上训练的最先进模型相当的性能。人类评估进一步表明，我们的方法产生的字幕具有更大的独特性和信息性，这两个属性天生具有通过监督学习实现的挑战性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15111v2" target="_blank">2306.15111v2</a>
                              </td>
                              <td>Self-Supervised Image Captioning with CLIP</td>
                              <td>Chuanyang Jin</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15111v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15111v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01373v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Recognize Any Regions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01373v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01373v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01373v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module. Through extensive experiments in the context of open-world object recognition, our RegionSpot demonstrates significant performance improvements over prior alternatives, while also providing substantial computational savings. For instance, training our model with 3 million data in a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean average precision (mAP), with an even larger margin by 14.8 % for more challenging and rare categories.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01373v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>理解无约束图像中单个区域或补丁的语义，例如在开放世界对象检测中，是计算机视觉中一项关键但具有挑战性的任务。在像CLIP这样强大的图像级视觉语言（ViL）基础模型的成功基础上，最近的努力试图通过从头开始用大量的区域标签对训练对比模型，或者将检测模型的输出与区域建议的图像级表示对齐，来利用它们的能力。尽管取得了显著进展，但这些方法仍受到计算密集型训练需求、数据噪声易感性和上下文信息不足的困扰。为了解决这些局限性，我们探索了现成基础模型的协同潜力，利用它们各自在本地化和语义方面的优势。我们介绍了一种新的、通用的、高效的区域识别架构，名为RegionSpot，旨在将来自定位基础模型（例如SAM）的位置感知定位知识与从ViL模型（例如CLIP）提取的语义信息相集成。为了充分利用预训练的知识，同时最大限度地减少训练开销，我们冻结了两个基础模型，将优化工作仅集中在基于注意力的轻量级知识集成模块上。通过在开放世界对象识别的背景下进行广泛的实验，我们的RegionSpot展示了与先前的替代方案相比的显著性能改进，同时也提供了大量的计算节约。例如，使用8个V100 GPU在一天内用300万个数据训练我们的模型。我们的模型在平均精度（mAP）方面优于GLIP 6.5%，在更具挑战性和罕见的类别中，差距更大，达到14.8%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01373v1" target="_blank">2311.01373v1</a>
                              </td>
                              <td>Recognize Any Regions</td>
                              <td>Haosen Yang</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01373v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01373v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12513v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12513v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12513v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12513v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lending new context to previously mixed results regarding the NLU capabilities of multimodal models. We conclude that exposure to images during pretraining affords inherent visual reasoning knowledge that is reflected in language-only tasks that require implicit visual reasoning. Our findings bear importance in the broader context of multimodal learning, providing principled guidelines for the choice of text encoders used in such contexts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12513v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数人使用视觉想象力来理解和推理语言，但BERT等模型使用仅在文本预训练中获得的知识来推理语言。在这项工作中，我们研究了视觉和语言预训练是否可以提高涉及隐式视觉推理的纯文本任务的性能，主要关注零样本探测方法。我们提出了一套视觉语言理解（VLU）任务，用于探索文本编码器模型的视觉推理能力，以及各种非视觉自然语言理解（NLU）任务进行比较。我们还提供了一种新的零样本知识探测方法Stroop探测，用于将CLIP等模型应用于纯文本任务，而不需要预测头，如BERT等模型的掩蔽语言建模头。我们表明，SOTA多模式训练的文本编码器在VLU任务上优于单模式训练的文字编码器，而在NLU任务上表现不佳，这为之前关于多模式模型的NLU能力的混合结果提供了新的背景。我们得出的结论是，在预训练过程中接触图像提供了固有的视觉推理知识，这些知识反映在需要隐含视觉推理的纯语言任务中。我们的研究结果在更广泛的多模式学习背景下具有重要意义，为在这种背景下使用的文本编码器的选择提供了原则性指导。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12513v2" target="_blank">2303.12513v2</a>
                              </td>
                              <td>Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding</td>
                              <td>Morris Alper</td>
                              <td>2023-03-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12513v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12513v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01034v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01034v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01034v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01034v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic modalities. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 and KITTI dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6\% in terms of MARE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01034v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过预训练的视觉语言模型（VLM），如CLIP，在涉及视觉和语言模式整合的一系列任务中表现出了增强的性能。当CLIP用于深度估计任务时，从输入图像中分割出的补丁可以与深度信息的一系列语义描述相结合，以获得相似性结果。然后，通过对与预定义语义描述相对应的深度值（称为深度仓）进行加权和求和来实现深度的粗略估计。零样本方法避开了传统全监督深度估计方法的计算和时间密集性。然而，这种利用固定深度仓的方法可能不能有效地推广，因为来自不同场景的图像可能表现出不同的深度分布。为了应对这一挑战，我们提出了一种基于少量镜头的方法，该方法学习将VLM用于单目深度估计，以平衡训练成本和泛化能力。具体来说，它为不同的场景分配不同的深度仓，这些深度仓可以由模型在推理过程中选择。此外，我们结合了可学习的提示对输入文本进行预处理，将易于人类理解的文本转换为易于建模理解的向量，并进一步提高了性能。由于每个场景只有一张图像用于训练，我们在NYU V2和KITTI数据集上的大量实验结果表明，我们的方法在MARE方面比以前最先进的方法高出10.6\%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01034v1" target="_blank">2311.01034v1</a>
                              </td>
                              <td>Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation</td>
                              <td>Xueting Hu</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01034v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01034v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01032v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Decentralized Generalized Approximate Message-Passing for Tree-Structured Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01032v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01032v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01032v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decentralized generalized approximate message-passing (GAMP) is proposed for compressed sensing from distributed generalized linear measurements in a tree-structured network. Consensus propagation is used to realize average consensus required in GAMP via local communications between adjacent nodes. Decentralized GAMP is applicable to all tree-structured networks that do not necessarily have central nodes connected to all other nodes. State evolution is used to analyze the asymptotic dynamics of decentralized GAMP for zero-mean independent and identically distributed Gaussian sensing matrices. The state evolution recursion for decentralized GAMP is proved to have the same fixed points as that for centralized GAMP when homogeneous measurements with an identical dimension in all nodes are considered. Furthermore, existing long-memory proof strategy is used to prove that the state evolution recursion for decentralized GAMP with the Bayes-optimal denoisers converges to a fixed point. These results imply that the state evolution recursion for decentralized GAMP with the Bayes-optimal denoisers converges to the Bayes-optimal fixed point for the homogeneous measurements when the fixed point is unique. Numerical results for decentralized GAMP are presented in the cases of linear measurements and clipping. As examples of tree-structured networks, a one-dimensional chain and a tree with no central nodes are considered.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01032v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>针对树状网络中分布式广义线性测量的压缩传感问题，提出了分散广义近似消息传递（GAMP）方法。一致性传播用于通过相邻节点之间的本地通信来实现GAMP中所需的平均一致性。去中心化GAMP适用于所有树形结构的网络，这些网络不一定具有连接到所有其他节点的中心节点。利用状态演化分析了零均值独立和同分布高斯传感矩阵的分散GAMP的渐近动力学。当考虑所有节点中具有相同维数的齐次测量时，证明了分散GAMP的状态演化递归与集中GAMP具有相同的不动点。此外，利用现有的长记忆证明策略证明了具有贝叶斯最优去噪器的分散GAMP的状态进化递归收敛到不动点。这些结果表明，当不动点是唯一的时，具有贝叶斯最优去噪器的分散GAMP的状态演化递归收敛于齐次测量的贝叶斯最优不动点。给出了线性测量和削波情况下分散GAMP的数值结果。作为树结构网络的例子，考虑了一维链和没有中心节点的树。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01032v1" target="_blank">2311.01032v1</a>
                              </td>
                              <td>Decentralized Generalized Approximate Message-Passing for Tree-Structured Networks</td>
                              <td>Keigo Takeuchi</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01032v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01032v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12145v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Feature Distillation for Zero-shot Annotation Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12145v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12145v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12145v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new setting for detecting unseen objects called Zero-shot Annotation object Detection (ZAD). It expands the zero-shot object detection setting by allowing the novel objects to exist in the training images and restricts the additional information the detector uses to novel category names. Recently, to detect unseen objects, large-scale vision-language models (e.g., CLIP) are leveraged by different methods. The distillation-based methods have good overall performance but suffer from a long training schedule caused by two factors. First, existing work creates distillation regions biased to the base categories, which limits the distillation of novel category information. Second, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space. To solve these problems, we propose Efficient feature distillation for Zero-shot Annotation object Detection (EZAD). Firstly, EZAD adapts the CLIP's feature space to the target detection domain by re-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation proposals with potential novel category names to avoid the distillation being overly biased toward the base categories. Finally, EZAD takes advantage of semantic meaning for regression to further improve the model performance. As a result, EZAD outperforms the previous distillation-based methods in COCO by 4% with a much shorter training schedule and achieves a 3% improvement on the LVIS dataset. Our code is available at https://github.com/dragonlzm/EZAD</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12145v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的检测不可见对象的设置，称为零样本注释对象检测（ZAD）。它通过允许新对象存在于训练图像中来扩展零样本对象检测设置，并将检测器使用的附加信息限制为新类别名称。最近，为了检测看不见的物体，不同的方法利用了大规模视觉语言模型（例如CLIP）。基于蒸馏的方法具有良好的整体性能，但由于两个因素导致训练日程过长。首先，现有的工作创建了偏向于基本类别的蒸馏区域，这限制了新类别信息的蒸馏。其次，直接使用CLIP的原始特征进行提取忽略了CLIP的训练数据和检测数据集之间的域间隙，这使得难以学习从图像区域到视觉语言特征空间的映射。为了解决这些问题，我们提出了用于零样本注释对象检测（EZAD）的有效特征提取。首先，EZAD通过对CLIP进行重新归一化，使CLIP的特征空间适应目标检测域；其次，EZAD使用CLIP生成具有潜在新颖类别名称的蒸馏建议，以避免蒸馏过度偏向基本类别。最后，EZAD利用语义进行回归，进一步提高了模型的性能。因此，EZAD在COCO中以更短的训练计划比以前的基于蒸馏的方法好4%，并在LVIS数据集上实现了3%的改进。我们的代码可在https://github.com/dragonlzm/EZAD</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12145v4" target="_blank">2303.12145v4</a>
                              </td>
                              <td>Efficient Feature Distillation for Zero-shot Annotation Object Detection</td>
                              <td>Zhuoming Liu</td>
                              <td>2023-03-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12145v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12145v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03103v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dual Prompt Tuning for Domain-Aware Federated Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03103v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03103v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03103v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% average accuracy over six domains in the DomainNet dataset, which improves the original CLIP by a large margin of 14.8%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03103v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>联合学习是一种分布式机器学习范式，允许多个客户端使用其本地数据协同训练共享模型。尽管如此，由于客户端之间普遍存在的域转换，传统的联合学习算法往往难以很好地推广。在这项工作中，我们考虑了一个具有挑战性但现实的联合学习场景，其中每个客户端的训练数据来自不同的域。我们通过利用即时学习技术来应对领域转换的挑战，并提出了一种新的方法，称为联合双提示调整（Fed-DPT）。具体而言，Fed DPT采用预先训练的视觉语言模型，然后应用视觉和文本提示调整，以促进对去中心化数据的领域自适应。Fed-DPT的大量实验证明了它在领域感知联合学习中的显著有效性。使用预先训练的CLIP模型（ViT-Base作为图像编码器），所提出的Fed-DPT在DomainNet数据集中的六个域上达到了68.4%的平均精度，这将原始CLIP提高了14.8%的大幅度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03103v3" target="_blank">2310.03103v3</a>
                              </td>
                              <td>Dual Prompt Tuning for Domain-Aware Federated Learning</td>
                              <td>Guoyizhe Wei</td>
                              <td>2023-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03103v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03103v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01859v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NLLB-CLIP -- train performant multilingual image retrieval model on a budget</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01859v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01859v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01859v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Today, the exponential rise of large models developed by academic and industrial institutions with the help of massive computing resources raises the question of whether someone without access to such resources can make a valuable scientific contribution. To explore this, we tried to solve the challenging task of multilingual image retrieval having a limited budget of $1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from the NLLB model. To train the model, we used an automatically created dataset of 106,246 good-quality images with captions in 201 languages derived from the LAION COCO dataset. We trained multiple models using image and text encoders of various sizes and kept different parts of the model frozen during the training. We thoroughly analyzed the trained models using existing evaluation datasets and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is comparable in quality to state-of-the-art models and significantly outperforms them on low-resource languages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01859v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>如今，学术和工业机构在大量计算资源的帮助下开发的大型模型呈指数级增长，这引发了一个问题，即无法获得这些资源的人是否能够做出宝贵的科学贡献。为了探索这一点，我们试图解决预算有限的1000美元的多语言图像检索这一具有挑战性的任务。因此，我们提出了NLLB-CLIP-CLIP模型，该模型具有来自NLLB模型的文本编码器。为了训练模型，我们使用了一个自动创建的数据集，该数据集由106246张高质量图像组成，并从LAION COCO数据集派生出201种语言的字幕。我们使用不同大小的图像和文本编码器训练了多个模型，并在训练过程中冻结了模型的不同部分。我们使用现有的评估数据集和新创建的XTD200和Flickr30k-200数据集对训练后的模型进行了彻底分析。我们表明，NLLB-CLIP在质量上与最先进的模型相当，并且在低资源语言上显著优于它们。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01859v3" target="_blank">2309.01859v3</a>
                              </td>
                              <td>NLLB-CLIP -- train performant multilingual image retrieval model on a budget</td>
                              <td>Alexander Visheratin</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01859v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01859v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00750v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are These the Same Apple? Comparing Images Based on Object Intrinsics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00750v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00750v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00750v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00750v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类视觉系统可以在不同的外在因素（如照明、物体姿态和背景）下毫不费力地识别物体，但当前的计算机视觉系统经常难以应对这些变化。理解和改进人工视觉系统的一个重要步骤是纯粹基于定义对象身份的内在对象属性来测量图像相似性。这个问题在计算机视觉文献中被研究为重新识别，尽管主要局限于特定的对象类别，如人和汽车。我们建议将其扩展到一般的对象类别，探索一种基于对象本质的图像相似性度量。为了对这些测量进行基准测试，我们收集了在不同外部因素（如照明、姿势和成像条件）下的18000美元的普通配对对象图像（CUTE）数据集。虽然现有的方法（如LPIPS和CLIP分数）不能很好地测量对象的本质，但我们发现，将从对比自监督学习中学习到的深度特征与前景滤波相结合是一种简单而有效的近似相似性的方法。我们对预先训练的特征和前景提取方法进行了广泛的调查，以得出一个强有力的基线，在当前方法中，该基线可以最好地测量以对象为中心的图像的内在相似性。最后，我们证明了我们的方法可以帮助下游应用，例如作为人类受试者的模拟物和改进可推广的重新识别。请访问我们的项目网站https://s-tian.github.io/projects/cute/用于数据的可视化和我们度量的演示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00750v1" target="_blank">2311.00750v1</a>
                              </td>
                              <td>Are These the Same Apple? Comparing Images Based on Object Intrinsics</td>
                              <td>Klemen Kotar</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00750v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00750v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00613v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Controllable Music Production with Diffusion Models and Guidance Gradients</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00613v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00613v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00613v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00613v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们展示了如何使用扩散模型的条件生成来处理44.1kHz立体声音频音乐制作中的各种现实任务，并提供采样时间指导。我们考虑的场景包括音乐音频的延续、修复和再生，在两个不同的音乐曲目之间创建平滑的过渡，以及将所需的风格特征转移到现有的音频剪辑中。我们通过在一个简单的框架中应用采样时的指导来实现这一点，该框架支持重建和分类损失，或两者的任何组合。这种方法确保生成的音频可以匹配其周围的上下文，或者符合相对于任何合适的预训练分类器或嵌入模型指定的类分布或潜在表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00613v1" target="_blank">2311.00613v1</a>
                              </td>
                              <td>Controllable Music Production with Diffusion Models and Guidance Gradients</td>
                              <td>Mark Levy</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00613v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00613v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_14240v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boon: A Neural Search Engine for Cross-Modal Information Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_14240v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_14240v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_14240v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-Semantic Embedding (VSE) networks can help search engines better understand the meaning behind visual content and associate it with relevant textual information, leading to more accurate search results. VSE networks can be used in cross-modal search engines to embed image and textual descriptions in a shared space, enabling image-to-text and text-to-image retrieval tasks. However, the full potential of VSE networks for search engines has yet to be fully explored. This paper presents Boon, a novel cross-modal search engine that combines two state-of-the-art networks: the GPT-3.5-turbo large language model, and the VSE network VITR (VIsion Transformers with Relation-focused learning) to enhance the engine's capabilities in extracting and reasoning with regional relationships in images. VITR employs encoders from CLIP that were trained with 400 million image-description pairs and it was fine-turned on the RefCOCOg dataset. Boon's neural-based components serve as its main functionalities: 1) a 'cross-modal search engine' that enables end-users to perform image-to-text and text-to-image retrieval. 2) a 'multi-lingual conversational AI' component that enables the end-user to converse about one or more images selected by the end-user. Such a feature makes the search engine accessible to a wide audience, including those with visual impairments. 3) Boon is multi-lingual and can take queries and handle conversations about images in multiple languages. Boon was implemented using the Django and PyTorch frameworks. The interface and capabilities of the Boon search engine are demonstrated using the RefCOCOg dataset, and the engine's ability to search for multimedia through the web is facilitated by Google's API.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_14240v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语义嵌入（VSE）网络可以帮助搜索引擎更好地理解视觉内容背后的含义，并将其与相关文本信息相关联，从而获得更准确的搜索结果。VSE网络可以用于跨模态搜索引擎，在共享空间中嵌入图像和文本描述，实现图像到文本和文本到图像的检索任务。然而，VSE网络在搜索引擎中的全部潜力还有待充分探索。本文介绍了Boon，一种新型的跨模态搜索引擎，它结合了两个最先进的网络：GPT-3.5-turbo大型语言模型和VSE网络VITR（具有关系集中学习的VIsion Transformers），以增强引擎提取和推理图像中区域关系的能力。VITR采用了CLIP的编码器，这些编码器经过4亿个图像描述对的训练，并在RefCOCOg数据集上进行了微调。Boon基于神经的组件是其主要功能：1）“跨模态搜索引擎”，使最终用户能够执行图像到文本和文本到图像的检索。2） “多语言对话AI”组件，使最终用户能够就最终用户选择的一个或多个图像进行对话。这样的功能使搜索引擎能够被广泛的受众访问，包括那些有视觉障碍的人。3） Boon会说多种语言，可以用多种语言处理有关图像的查询和对话。Boon是使用Django和PyTorch框架实现的。Boon搜索引擎的界面和功能通过RefCOCOOg数据集进行了演示，谷歌的API促进了该引擎通过网络搜索多媒体的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.14240v2" target="_blank">2307.14240v2</a>
                              </td>
                              <td>Boon: A Neural Search Engine for Cross-Modal Information Retrieval</td>
                              <td>Yan Gong</td>
                              <td>2023-07-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_14240v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.14240v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00453v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00453v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00453v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00453v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper considers zero-shot Anomaly Detection (AD), a valuable yet under-studied task, which performs AD without any reference images of the test objects. Specifically, we employ a language-guided strategy and propose a simple-yet-effective architecture CLIP-AD, leveraging the superior zero-shot classification capabilities of the large vision-language model CLIP. A natural idea for anomaly segmentation is to directly calculate the similarity between text/image features, but we observe opposite predictions and irrelevant highlights in the results. Inspired by the phenomena, we introduce a Staged Dual-Path model (SDP) that effectively uses features from various levels and applies architecture and feature surgery to address these issues. Furthermore, delving beyond surface phenomena, we identify the problem arising from misalignment of text/image features in the joint embedding space. Thus, we introduce a fine-tuning strategy by adding linear layers and construct an extended model SDP+, further enhancing the performance. Abundant experiments demonstrate the effectiveness of our approach, e.g., on VisA, SDP outperforms SOTA by +1.0/+1.2 in classification/segmentation F1 scores, while SDP+ achieves +1.9/+11.7 improvements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00453v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文考虑了零样本异常检测（AD），这是一项有价值但研究不足的任务，它在没有任何测试对象参考图像的情况下执行AD。具体而言，我们采用了一种以语言为导向的策略，并提出了一种简单有效的架构CLIP-AD，利用了大型视觉语言模型CLIP优越的零样本分类能力。异常分割的一个自然想法是直接计算文本/图像特征之间的相似性，但我们在结果中观察到相反的预测和不相关的亮点。受这些现象的启发，我们引入了一个分阶段双路径模型（SDP），该模型有效地使用了来自不同级别的特征，并应用架构和特征运算来解决这些问题。此外，在表面现象之外，我们发现了联合嵌入空间中文本/图像特征错位引起的问题。因此，我们引入了一种通过添加线性层的微调策略，并构建了一个扩展模型SDP+，进一步提高了性能。大量的实验证明了我们的方法的有效性，例如，在VisA上，SDP在分类/分割F1分数上比SOTA高+1.0/+1.2，而SDP+实现了+1.9/+11.7的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00453v1" target="_blank">2311.00453v1</a>
                              </td>
                              <td>CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection</td>
                              <td>Xuhai Chen</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00453v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00453v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00278v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00278v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00278v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00278v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Few-shot object detection, which focuses on detecting novel objects with few labels, is an emerging challenge in the community. Recent studies show that adapting a pre-trained model or modified loss function can improve performance. In this paper, we explore leveraging the power of Contrastive Language-Image Pre-training (CLIP) and hard negative classification loss in low data setting. Specifically, we propose Re-scoring using Image-language Similarity for Few-shot object detection (RISF) which extends Faster R-CNN by introducing Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss (BNRL). The former adapts CLIP, which performs zero-shot classification, to re-score the classification scores of a detector using image-class similarities, the latter is modified classification loss considering the punishment for fake backgrounds as well as confusing categories on a generalized few-shot object detection dataset. Extensive experiments on MS-COCO and PASCAL VOC show that the proposed RISF substantially outperforms the state-of-the-art approaches. The code will be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00278v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>少镜头物体检测专注于检测带有少量标签的新物体，是社区中一个新兴的挑战。最近的研究表明，调整预先训练的模型或修改的损失函数可以提高性能。在本文中，我们探索了在低数据设置中利用对比语言图像预训练（CLIP）和硬负分类损失的能力。具体而言，我们提出了使用图像语言相似度进行少镜头物体检测的重新评分（RISF），该方法通过引入使用CLIP（CM-CLIP）和背景负重标度损失（BNRL）的校准模块来扩展Faster R-CNN。前者采用执行零样本分类的CLIP，利用图像类相似性对检测器的分类得分进行重新评分，后者考虑到对虚假背景的惩罚以及在广义少热点对象检测数据集上混淆类别，对分类损失进行修改。对MS-COCO和PASCAL VOC的大量实验表明，所提出的RISF大大优于最先进的方法。该代码将可用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00278v1" target="_blank">2311.00278v1</a>
                              </td>
                              <td>Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection</td>
                              <td>Min Jae Jung</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00278v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00278v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2311_03570v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cal-DETR: Calibrated Detection Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03570v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03570v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管深度神经网络在一些计算机视觉任务中表现出了令人印象深刻的预测性能，但它容易做出过于自信的预测。这限制了DNN在许多安全关键应用中的采用和更广泛的利用。然而，最近一直在努力校准DNN，几乎所有的努力都集中在分类任务上。令人惊讶的是，很少有人关注校准现代基于DNN的目标检测器，尤其是检测变压器，它们最近表现出了良好的检测性能，并在许多决策系统中具有影响力。在这项工作中，我们通过提出一种用于校准检测变压器（Cal-DETR）的机制来解决这个问题，特别是用于可变形DETR、UP-DETR和DINO。我们追求列车时刻校准路线，并做出以下贡献。首先，我们提出了一种简单而有效的方法来量化基于变换器的对象检测器的不确定性。其次，我们开发了一种不确定性引导的logit调制机制，该机制利用不确定性来调制类logit。第三，我们开发了一种logit混合方法，该方法作为具有检测特定损耗的正则化子，也是对不确定性引导的logit调制技术的补充，以进一步提高校准性能。最后，我们在三个域内和四个域外场景中进行了广泛的实验。结果证实了Cal DETR在校准域内和域外检测方面的有效性，同时保持甚至提高了检测性能。我们的代码库和预先训练的模型可以访问\url{https://github.com/akhtarvision/cal-detr}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03570v1" target="_blank">2311.03570v1</a>
                              </td>
                              <td>Cal-DETR: Calibrated Detection Transformer</td>
                              <td>Muhammad Akhtar Munir</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03570v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03570v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg,MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation.Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV17 object detection on UAVDT, and video instance segmentation on DAVIS 2017.We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https://github.com/ISL-CV/FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，称为特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值偏移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为骨干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，在实例分割中实现了40.8（+2.3）%AP，42.1%AP。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的UAV17对象检测和DAVIS 2017上的视频实例分割。我们通过展示可视化和各种消融研究来更好地了解FLSL的成功。源代码位于https://github.com/ISL-CV/FLSL.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v4" target="_blank">2306.06203v4</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03053v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masking Hyperspectral Imaging Data with Pretrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03053v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03053v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与潜在噪声和未知光谱特性相关联的不期望的背景区域的存在降低了高光谱数据处理的性能。掩盖不需要的区域是解决这个问题的关键。仅处理感兴趣的区域在计算成本、所需内存和总体性能方面产生了显著的改进。所提出的处理管道包括两个基本部分：感兴趣区域掩模生成，然后仅在新掩模的高光谱立方体上应用高光谱数据处理技术。我们工作的新颖之处在于用于初步图像分割的方法。我们使用Segment Anything Model（SAM）来提取数据集中的所有对象，然后使用零样本Grounding Dino对象检测器来细化片段，然后执行交集和排除过滤步骤，而无需进行微调或重新训练。为了说明掩蔽过程的有效性，所提出的方法被部署在三个具有挑战性的应用场景中，这些场景需要精确的掩蔽；碎塑料特性、岩芯扫描和垃圾监测。提供了所提出的掩蔽方法对三个应用的数值评估以及所使用的超参数。该方法的脚本将在https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03053v1" target="_blank">2311.03053v1</a>
                              </td>
                              <td>Masking Hyperspectral Imaging Data with Pretrained Models</td>
                              <td>Elias Arbash</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03053v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03053v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01584v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01584v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tax incentives and fiscal bonuses have had a significant impact on the Italian economy over the past decade. In particular, the "Superbonus 110" tax relief in 2020, offering a generous 110% deduction for expenses related to energy efficiency improvements and seismic risk reduction in buildings, has played a pivotal role. However, the surge in construction activities has also brought about an unfortunate increase in fraudulent activities. To address this challenge, our research introduces a practical system for monitoring and managing the entire process of the Superbonus 110 tax credit, from its initiation to redemption. This system leverages artificial intelligence and blockchain technology to streamline tax credit management and incorporates controllers based on a Decentralised Autonomous Organisation architecture, bolstered by a Multi-agent System. The outcome of our work is a system capable of establishing a tokenomics framework that caters to the needs and functionalities of both investors and operators. Moreover, it features a robust control system to prevent inadvertent errors like double spending, overspending, and deceitful practices such as false claims of completed work. The collaborative approach between the Decentralised Autonomous Organisation and the Multi-agent System enhances trust and security levels among participants in a competitive environment where potential fraudsters might attempt to exploit the system. It also enables comprehensive tracking and monitoring of the entire Superbonus process. In the realm of engineering, our project represents an innovative fusion of blockchain technology and Multi-agent Systems, advancing the application of artificial intelligence. This integration guarantees the validation, recording, and execution of transactions with a remarkable level of trust and transparency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01584v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>过去十年，税收优惠和财政奖金对意大利经济产生了重大影响。特别是，2020年的“超级奖金110”税收减免发挥了关键作用，为提高建筑能效和降低地震风险的相关费用提供了110%的慷慨减免。然而，建筑活动的激增也带来了欺诈活动的不幸增加。为了应对这一挑战，我们的研究引入了一个实用的系统，用于监控和管理超级奖金110税收抵免的整个过程，从启动到赎回。该系统利用人工智能和区块链技术简化税收抵免管理，并结合了基于分散自治组织架构的控制器，由多代理系统支持。我们的工作成果是建立一个能够建立一个符合投资者和运营商需求和功能的标记基因组学框架的系统。此外，它还具有一个强大的控制系统，以防止意外的错误，如重复支出、超支和欺诈行为，如对已完成工作的虚假声明。分散自治组织和多代理系统之间的合作方法提高了参与者之间的信任和安全水平，在竞争环境中，潜在的欺诈者可能会试图利用该系统。它还可以全面跟踪和监控整个超级奖金流程。在工程领域，我们的项目代表了区块链技术和多智能体系统的创新融合，推动了人工智能的应用。这种集成保证了交易的验证、记录和执行具有显著的信任和透明度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01584v2" target="_blank">2311.01584v2</a>
                              </td>
                              <td>Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01584v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01584v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多对比学习（CL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。尽管在改进文本前任务、架构或鲁棒性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为，到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，该策略旨在扩展随机视图生成，以在CL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向传递，3）对抗性地选择产生最差损失的对，以及4）使用所选对运行后向传递。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。HVS只需要300个历元的预训练，就可以与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度放缓，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估精度持续提高0.4%至1.9%，在多种CL方法（如DINO、SimSiam和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v2" target="_blank">2310.03940v2</a>
                              </td>
                              <td>Hard View Selection for Contrastive Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models: A Baseline Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transfer capabilities in a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first observe that, unlike fine-tuning from ImageNet pre-trained models, as previous methods do, fine-tuning from foundation models yields significantly poorer results, sometimes even worse than training from scratch. While freezing the backbones, we demonstrate that although the foundation models greatly improve the performance of the baseline method that trains the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. Based on these findings, we introduce \textit{CLIP distillation}, a parameter-free method specifically designed to distill target knowledge from CLIP models. The core of our \textit{CLIP distillation} lies in a self-calibration technique for automatic temperature scaling, a feature that significantly enhances the baseline's out-class detection capability. Although simple, our method outperforms previous approaches in most benchmark tasks, excelling in evaluation metrics including H-score/H$^3$-score and the newly proposed universal classification rate (UCR) metric. We hope that our investigation and the proposed simple framework can serve as a strong baseline to facilitate future studies in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务中显示出了令人印象深刻的学习和迁移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先观察到，与之前的方法一样，从ImageNet预训练模型进行微调不同，从基础模型进行微调会产生明显较差的结果，有时甚至比从头开始训练更差。在冻结主干的同时，我们证明，尽管基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。基于这些发现，我们介绍了\textit｛CLIP蒸馏｝，这是一种专门设计用于从CLIP模型中提取目标知识的无参数方法。我们的\textit｛CLIP蒸馏｝的核心在于自动温度标度的自校准技术，这一功能显著增强了基线的超一流检测能力。尽管简单，但我们的方法在大多数基准任务中都优于以前的方法，在评估指标方面表现出色，包括H-core/H$^3$-得分和新提出的通用分类率（UCR）指标。我们希望，我们的调查和拟议的简单框架能够作为一个强有力的基线，促进该领域未来的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v2" target="_blank">2305.11092v2</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models: A Baseline Study</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08854v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rank-DETR for High Quality Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08854v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design that prioritizes predictions of more accurate localization accuracy during ranking to boost the AP under high IoU thresholds. We apply our method to improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong COCO object detection results when using different backbones such as ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08854v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代检测变换器（DETR）使用一组对象查询来预测边界框的列表，根据它们的分类置信度得分对它们进行排序，并选择排名靠前的预测作为给定输入图像的最终检测结果。高性能的对象检测器需要边界框预测的精确排序。对于基于DETR的检测器，由于分类分数和定位精度之间的不对准，排名靠前的边界框的定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向秩的设计，结合称为秩DETR，介绍了一种简单且高性能的基于DETR的对象检测器。我们的主要贡献包括：（i）一种面向秩的架构设计，它可以提示阳性预测并抑制阴性预测，以确保更低的假阳性率；以及（ii）一种基于秩的损失函数和匹配成本设计，它在排序过程中优先考虑更准确定位精度的预测，以在高IoU阈值下提高AP。我们将我们的方法应用于改进最近的SOTA方法（例如，H-DETR和DINO-DETR），并在使用不同骨干（如ResNet-$50$、Swin-T和Swin-L）时报告了强大的COCO对象检测结果，证明了我们方法的有效性。代码位于\url{https://github.com/LeapLabTHU/Rank-DETR}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08854v3" target="_blank">2310.08854v3</a>
                              </td>
                              <td>Rank-DETR for High Quality Object Detection</td>
                              <td>Yifan Pu</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08854v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08854v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01646v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01646v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new competitive baseline on the more challenging benchmarks SemiAves, SemiCUB, SemiFungi and Semi-iNat.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01646v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了SemiGPC，这是一种基于高斯过程的分布感知标签细化策略，其中模型的预测来自标签的后验分布。与CoMatch和SimMatch等其他基于缓冲区的半监督方法不同，我们的SemiGPC包括一个归一化项，该项可以解决全局数据分布的不平衡问题，同时保持局部敏感性。这种显式控制允许SemiGPC对确认偏差更具鲁棒性，尤其是在类不平衡的情况下。我们发现，当与不同的半监督方法（如FixMatch、ReMixMatch、SimMatch和FreeMatch）以及不同的预训练策略（包括MSN和Dino）配对时，SemiGPC可以提高性能。我们还表明，在标准CIFAR10-LT/CIFAR100-LT上，SemiGPC在不同程度的类不平衡下，尤其是在低数据状态下，实现了最先进的结果。在更具挑战性的基准SemiAves、SemiCUB、SemiFungi和Semi-iNat上，与新的竞争基线相比，使用SemiGPC还导致平均准确率增加约2%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01646v1" target="_blank">2311.01646v1</a>
                              </td>
                              <td>SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</td>
                              <td>Abdelhak Lemkhenter</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01646v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01646v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10726v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Few-Shot Panoptic Segmentation With Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10726v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10726v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前最先进的全景分割方法需要大量的带注释的训练数据，这既困难又昂贵，对其广泛采用构成了重大挑战。与此同时，视觉表征学习的最新突破引发了范式的转变，导致了可以用完全未标记的图像训练的大型基础模型的出现。在这项工作中，我们建议利用这种任务不可知的图像特征，通过呈现具有近0个标签的分割全景信息（SPINO）来实现少镜头全景分割。详细地说，我们的方法将DINOv2主干与轻量级网络头相结合，用于语义分割和边界估计。我们表明，尽管我们的方法仅用10张注释图像进行训练，但它预测了可用于任何现有全景分割方法的高质量伪标签。值得注意的是，我们证明，与完全监督的基线相比，SPINO在使用不到0.3%的基本事实标签的情况下取得了有竞争力的结果，为利用基础模型学习复杂的视觉识别任务铺平了道路。为了说明其普遍适用性，我们进一步将SPINO部署在室外和室内环境的真实世界机器人视觉系统上。为了促进未来的研究，我们在http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10726v2" target="_blank">2309.10726v2</a>
                              </td>
                              <td>Few-Shot Panoptic Segmentation With Foundation Models</td>
                              <td>Markus Käppeler</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10726v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10726v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00230v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00230v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00230v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00230v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00230v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉位置识别（VPR）技术来确定公开可用图像的地理位置是现实世界VPR应用的一个紧迫问题。尽管目前大多数VPR方法在理想条件下都能获得良好的结果，但它们在复杂环境中的性能通常不令人满意，这些环境的特点是光照变化、季节变化和移动物体造成的遮挡。在本研究中，我们利用DINOv2模型作为骨干网络进行修剪和微调，以提取稳健的图像特征。我们提出了一种新的VPR架构，称为DINO Mix，它将基础视觉模型与特征聚合相结合。该架构依赖于基础视觉模型强大的图像特征提取功能。我们使用基于MLP Mixer的混合模块来聚合图像特征，从而产生全局鲁棒和可推广的描述符，从而实现高精度的VPR。我们通过实验证明，所提出的DINO-Mix架构显著优于当前最先进的（SOTA）方法。在具有光照变化、季节变化和遮挡的测试集（Tokyo24/7，Nordland，SF-XL-Testv1）中，我们提出的DINO Mix架构分别实现了91.75%、80.18%和82%的Top-1准确率。与SOTA方法相比，我们的体系结构的平均准确度提高了5.14%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00230v1" target="_blank">2311.00230v1</a>
                              </td>
                              <td>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</td>
                              <td>Gaoshuang Huang</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00230v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00230v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了语义多样的图像数据集的各种生成模型，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过对生成模型进行迄今为止最大规模的评估实验，测量了人类对生成样本的图像真实感的感知，并发现没有任何现有指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性、稀有性和记忆性的17个现代指标相比，我们发现，人类判断的扩散模型的最先进的感知真实性并没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，目前的指标并不能正确地检测记忆：文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和一个模块化库，以计算9个不同编码器的17个通用度量https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v2" target="_blank">2306.04675v2</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19522v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Natural Domain Foundation Models Useful for Medical Image Classification?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19522v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19522v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19522v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 in particular, consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19522v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习领域正朝着使用通用基础模型的方向发展，这些模型可以很容易地适应不同的任务。虽然这种范式转变已经成为自然语言处理领域的常见做法，但计算机视觉的进展却较慢。在本文中，我们试图通过研究各种最先进的基础模型对医学图像分类任务的可转移性来解决这个问题。具体而言，我们评估了五个基础模型的性能，即SAM、SEEM、DINOv2、BLIP和OpenCLIP，它们跨越了四个成熟的医学成像数据集。我们探索了不同的训练环境，以充分利用这些模型的潜力。我们的研究结果喜忧参半。尤其是DINOv2，始终优于ImageNet预训练的标准实践。然而，其他基础模型未能始终超过这一既定基线，这表明它们在医学图像分类任务中的可移植性存在局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19522v1" target="_blank">2310.19522v1</a>
                              </td>
                              <td>Are Natural Domain Foundation Models Useful for Medical Image Classification?</td>
                              <td>Joana Palés Huix</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19522v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19522v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19257v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19257v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19257v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实例检测（InsDet）是机器人和计算机视觉中一个长期存在的问题，旨在检测杂乱场景中的对象实例（由一些视觉实例预定义）。尽管它具有实际意义，但它的进步被对象检测所掩盖，对象检测旨在检测属于某些预定义类的对象。一个主要原因是，按照今天的标准，当前的InsDet数据集规模太小。例如，流行的InsDet数据集GMU（2016年发布）只有23个实例，远远少于2014年发布的著名对象检测数据集COCO（80个类）。我们有动力引入一个新的InsDet数据集和协议。首先，我们为InsDet定义了一个逼真的设置：训练数据包括多视图实例捕获，以及不同的场景图像，允许通过粘贴带有自由框注释的实例图像来合成训练图像。其次，我们发布了一个真实世界的数据库，其中包含100个对象实例的多视图捕获和高分辨率（6k x 8k）测试图像。第三，我们在数据集上广泛研究了InsDet的基线方法，分析了它们的性能，并提出了未来的工作建议。令人惊讶的是，使用现成的类不可知分割模型（Segment Anything model，SAM）和自监督特征表示DINOv2表现最好，比重新利用对象检测器的端到端训练的InsDet模型（例如FasterRCNN和RetinaNet）更好地实现了>10 AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19257v1" target="_blank">2310.19257v1</a>
                              </td>
                              <td>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</td>
                              <td>Qianqian Shen</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19257v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19257v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14736v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上评估分类时，SAMCLR的表现至少与SimCLR相当，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v2" target="_blank">2310.14736v2</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18642v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One-shot Localization and Segmentation of Medical Images with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18642v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in Vision Transformers (ViT) and Stable Diffusion (SD) models with their ability to capture rich semantic features of the image have been used for image correspondence tasks on natural images. In this paper, we examine the ability of a variety of pre-trained ViT (DINO, DINOv2, SAM, CLIP) and SD models, trained exclusively on natural images, for solving the correspondence problems on medical images. While many works have made a case for in-domain training, we show that the models trained on natural images can offer good performance on medical images across different modalities (CT,MR,Ultrasound) sourced from various manufacturers, over multiple anatomical regions (brain, thorax, abdomen, extremities), and on wide variety of tasks. Further, we leverage the correspondence with respect to a template image to prompt a Segment Anything (SAM) model to arrive at single shot segmentation, achieving dice range of 62%-90% across tasks, using just one image as reference. We also show that our single-shot method outperforms the recently proposed few-shot segmentation method - UniverSeg (Dice range 47%-80%) on most of the semantic segmentation tasks(six out of seven) across medical imaging modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18642v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉变换器（ViT）和稳定扩散（SD）模型的最新进展及其捕捉图像丰富语义特征的能力已被用于自然图像上的图像对应任务。在本文中，我们检验了各种预先训练的ViT（DINO、DINOv2、SAM、CLIP）和SD模型（仅在自然图像上训练）解决医学图像上的对应问题的能力。虽然许多工作已经为域内训练提供了理由，但我们表明，在自然图像上训练的模型可以在来自不同制造商的不同模态（CT、MR、超声）、多个解剖区域（大脑、胸部、腹部、四肢）和各种任务的医学图像上提供良好的性能。此外，我们利用与模板图像的对应关系来提示Segment Anything（SAM）模型实现单次分割，仅使用一张图像作为参考，即可在任务中实现62%-90%的骰子范围。我们还表明，在医学成像模式的大多数语义分割任务（七分之六）上，我们的单镜头方法优于最近提出的少镜头分割方法UniverSeg（Dice范围47%-80%）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18642v1" target="_blank">2310.18642v1</a>
                              </td>
                              <td>One-shot Localization and Segmentation of Medical Images with Foundation Models</td>
                              <td>Deepa Anand</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18642v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18642v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18251v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Self-Supervised Approach to Land Cover Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18251v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Land use/land cover change (LULC) maps are integral resources in earth science and agricultural research. Due to the nature of such maps, the creation of LULC maps is often constrained by the time and human resources necessary to accurately annotate satellite imagery and remote sensing data. While computer vision models that perform semantic segmentation to create detailed labels from such data are not uncommon, litle research has been done on self-supervised and unsupervised approaches to labelling LULC maps without the use of ground-truth masks. Here, we demonstrate a self-supervised method of land cover segmentation that has no need for high-quality ground truth labels. The proposed deep learning employs a frozen pre-trained ViT backbone transferred from DINO in a STEGO architecture and is fine-tuned using a custom dataset consisting of very high resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning, an accuracy of roughly 52% was observed across 5 samples, signifying the feasibility of self-supervised models for the automated labelling of VHR LULC maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18251v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>土地利用/土地覆盖变化图是地球科学和农业研究中不可或缺的资源。由于这类地图的性质，LULC地图的绘制往往受到准确注释卫星图像和遥感数据所需的时间和人力资源的限制。虽然执行语义分割以从这些数据中创建详细标签的计算机视觉模型并不罕见，但很少有人对在不使用地面实况掩码的情况下标记LULC地图的自监督和无监督方法进行研究。在这里，我们展示了一种自监督的土地覆盖分割方法，该方法不需要高质量的地面实况标签。所提出的深度学习采用了从STEGO架构中的DINO转移来的冻结预训练ViT骨干，并使用由超高分辨率（VHR）卫星图像组成的自定义数据集进行了微调。在仅仅10个时期的微调之后，在5个样本中观察到大约52%的准确率，这表明自监督模型用于VHR-LULC图的自动标记的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18251v1" target="_blank">2310.18251v1</a>
                              </td>
                              <td>A Self-Supervised Approach to Land Cover Segmentation</td>
                              <td>Charles Moore</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18251v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18251v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation, conditional density estimation, and density estimation with missing data tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间建立良好联系的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。它们在各种密度估计、条件密度估计和具有缺失数据任务的密度估计中的效用得到了说明。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v2" target="_blank">2305.13552v2</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08825v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08825v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08825v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs. Code will be made available at https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08825v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型（MLLMs）通过引入视觉感知接口，在扩展大语言模型的能力方面取得了重大进展。尽管出现了令人兴奋的应用程序和各种指令调整数据，但现有的方法往往依赖于CLIP或其变体作为视觉分支，而只是从深层提取特征。然而，这些方法缺乏对MLLM中的视觉编码器的全面分析。在本文中，我们对MLLMs中不同视觉编码器的有效性进行了广泛的研究。我们的研究结果表明，CLIP的浅层特征为细粒度任务（如接地和区域理解）提供了特殊的优势。令人惊讶的是，未经文本图像对齐预训练的仅视觉模型DINO，作为MLLMs中的视觉分支，表现出了良好的性能。通过简单地为其配备MLP层进行对齐，DINO在细粒度相关感知任务中超越了CLIP。基于这些观察结果，我们提出了一种简单而有效的特征合并策略，称为COMM，该策略将CLIP和DINO与多级特征合并相结合，以增强MLLM的视觉能力。我们通过在各种基准上进行综合实验来评估COMM，包括图像字幕、视觉问答、视觉基础和对象幻觉。实验结果表明，与现有方法相比，COMM的性能优越，显示了其在MLLM中增强的视觉能力。代码将在https://github.com/YuchenLiu98/COMM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08825v2" target="_blank">2310.08825v2</a>
                              </td>
                              <td>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</td>
                              <td>Dongsheng Jiang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08825v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08825v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10912v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10912v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10912v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10912v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. At the heart of IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompting techniques. IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10912v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，计算机视觉领域发生了范式转变，反映了大型语言模型在自然语言处理领域的变革性影响。本文深入探讨了开放世界分割的探索，提出了一种称为图像提示分割（IPSeg）的新方法，该方法利用了视觉基础模型的力量。IPSeg的核心是无训练范式的原则，它利用了图像提示技术。IPSeg利用包含主观视觉概念的单个图像作为灵活的提示来查询视觉基础模型，如DINOv2和Stable Diffusion。我们的方法为提示图像和输入图像提取鲁棒特征，然后通过一个新颖的特征交互模块将输入表示与提示表示匹配，以生成突出显示输入图像中目标对象的点提示。生成的点提示被进一步用于引导Segment Anything Model对输入图像中的目标对象进行分割。所提出的方法通过消除对详尽培训课程的需求而脱颖而出，从而提供了一个更高效和可扩展的解决方案。在COCO、PASCAL VOC和其他数据集上的实验证明了IPSeg使用直观的图像提示进行灵活的开放世界分割的有效性。这项工作开创了通过图像中传达的视觉概念来挖掘开放世界理解的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10912v1" target="_blank">2310.10912v1</a>
                              </td>
                              <td>Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</td>
                              <td>Lv Tang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10912v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10912v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08873v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08873v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08873v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08873v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework's effectiveness and adaptability to diverse environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08873v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文利用大型语言和视觉语言模型提出了一种交互式导航框架，使机器人能够在有可穿越障碍物的环境中导航。我们利用大型语言模型（GPT-3.5）和开放集视觉语言模型（Grounding DINO）来创建一个动作感知成本图，以在不进行微调的情况下执行有效的路径规划。使用大型模型，我们可以实现一个端到端的系统，从“你能穿过窗帘给我送药吗？”这样的文本说明，到具有动作感知属性的边界框（例如窗帘）。它们可以用于将激光雷达点云划分为两部分：可遍历部分和不可遍历部分，然后构建一个行动感知成本图，以生成可行的路径。预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，允许在交互式导航任务中快速部署。我们选择使用多个可遍历对象，如窗帘和草，通过指示机器人遍历它们来进行验证。此外，还测试了在医疗场景中穿过窗帘的情况。所有实验结果都证明了所提出的框架的有效性和对不同环境的适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08873v1" target="_blank">2310.08873v1</a>
                              </td>
                              <td>Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</td>
                              <td>Zhen Zhang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08873v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08873v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_07033v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_07033v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_07033v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_07033v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and DINO algorithms. We evaluated performance on six clinically relevant tasks from three anatomic sites and two institutions: breast cancer detection, inflammatory bowel disease detection, breast cancer estrogen receptor prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer immunotherapy response prediction. Our results demonstrate that pre-training on pathology data is beneficial for downstream performance compared to pre-training on natural images. Additionally, the DINO algorithm achieved better generalization performance across all tasks tested. The presented results signify a phase change in computational pathology research, paving the way into a new era of more performant models based on large-scale, parallel pre-training at the billion-image scale.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_07033v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在自我监督学习方面取得的突破使得能够使用大型未标记数据集来训练视觉基础模型，该模型可以推广到各种下游任务。虽然这种训练范式非常适合注释稀少的医学领域，但医学领域，特别是病理学领域的大规模预训练尚未得到广泛研究。先前在病理学自我监督学习方面的工作利用较小的数据集进行预训练和评估下游性能。该项目的目的是通过在大型临床病理学数据集上进行预训练和评估下游性能，训练最大的学术基础模型，并对最突出的自监督学习算法进行基准测试。我们收集了迄今为止最大的病理学数据集，包括来自42.3万张显微镜载玻片的30多亿张图像。我们比较了使用掩蔽自动编码器（MAE）和DINO算法的视觉变换器模型的预训练。我们评估了来自三个解剖部位和两个机构的六项临床相关任务的表现：乳腺癌症检测、炎症性肠病检测、乳腺癌症雌激素受体预测、肺腺癌EGFR突变预测和癌症免疫疗法反应预测。我们的结果表明，与对自然图像的预训练相比，对病理学数据的预训练有利于下游性能。此外，DINO算法在所有测试任务中都实现了更好的泛化性能。所提出的结果标志着计算病理学研究的一个阶段性变化，为进入一个基于十亿图像规模的大规模并行预训练的更高性能模型的新时代铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.07033v1" target="_blank">2310.07033v1</a>
                              </td>
                              <td>Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images</td>
                              <td>Gabriele Campanella</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_07033v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.07033v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06836v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Does Stable Diffusion Know about the 3D Scene?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06836v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06836v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06836v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in generative models like Stable Diffusion enable the generation of highly photo-realistic images. Our objective in this paper is to probe the diffusion network to determine to what extent it 'understands' different properties of the 3D scene depicted in an image. To this end, we make the following contributions: (i) We introduce a protocol to evaluate whether a network models a number of physical 'properties' of the 3D scene by probing for explicit features that represent these properties. The probes are applied on datasets of real images with annotations for the property. (ii) We apply this protocol to properties covering scene geometry, scene material, support relations, lighting, and view dependent measures. (iii) We find that Stable Diffusion is good at a number of properties including scene geometry, support relations, shadows and depth, but less performant for occlusion. (iv) We also apply the probes to other models trained at large-scale, including DINO and CLIP, and find their performance inferior to that of Stable Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06836v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成模型（如稳定扩散）的最新进展使得能够生成高度照片逼真的图像。我们在本文中的目标是探索扩散网络，以确定它在多大程度上“理解”图像中描绘的3D场景的不同特性。为此，我们做出了以下贡献：（i）我们引入了一种协议，通过探测表示3D场景的显式特征来评估网络是否对这些物理“属性”进行建模。探针应用于带有属性注释的真实图像数据集。（ii）我们将此协议应用于涵盖场景几何、场景材质、支持关系、照明和视图相关度量的属性。（iii）我们发现Stable Diffusion在许多属性上都很好，包括场景几何、支持关系、阴影和深度，但在遮挡方面性能较差。（iv）我们还将探针应用于大规模训练的其他模型，包括DINO和CLIP，发现它们的性能不如稳定扩散。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06836v1" target="_blank">2310.06836v1</a>
                              </td>
                              <td>What Does Stable Diffusion Know about the 3D Scene?</td>
                              <td>Guanqi Zhan</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06836v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06836v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16118v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16118v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16118v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16118v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields - dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonstrate that D$^3$Fields are both generalizable and effective for zero-shot robotic manipulation tasks. In quantitative comparisons with state-of-the-art dense descriptors, such as Dense Object Nets and DINO, D$^3$Fields exhibit significantly better generalization abilities and manipulation accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16118v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景表示一直是机器人操作系统中至关重要的设计选择。理想的表示应该是3D、动态和语义的，以满足不同操作任务的需求。然而，以前的作品往往同时缺乏这三种特性。在这项工作中，我们介绍了D$^3$Fields-动态三维描述符字段。这些字段捕获底层3D环境的动态，并对语义特征和实例掩码进行编码。具体来说，我们将工作空间中的任意3D点投影到多视图2D视觉观察上，并对从基础模型中导出的特征进行插值。由此产生的融合描述符字段允许使用具有不同上下文、样式和实例的2D图像来实现灵活的目标规范。为了评估这些描述域的有效性，我们以零样本的方式将我们的表示应用于广泛的机器人操作任务。通过在现实世界场景和模拟中的广泛评估，我们证明D$^3$Fields对于零样本机器人操纵任务是可推广和有效的。在与最先进的密集描述符（如密集对象网和DINO）的定量比较中，D$^3$Fields表现出明显更好的泛化能力和操作精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16118v2" target="_blank">2309.16118v2</a>
                              </td>
                              <td>D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</td>
                              <td>Yixuan Wang</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16118v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16118v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone, and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch, and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the ground-work for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对该模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练在模型性能上略有改进，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了性能的小幅提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v1" target="_blank">2310.03513v1</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hierarchical Concept Discovery Models: A Concept Pyramid Scheme</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep Learning algorithms have recently gained significant attention due to their impressive performance. However, their high complexity and un-interpretable mode of operation hinders their confident deployment in real-world safety-critical tasks. This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly interpretable decision making process with respect to human understandable concepts, on multiple levels of granularity. To this end, we propose a novel hierarchical concept discovery formulation leveraging: (i) recent advances in image-text models, and (ii) an innovative formulation for multi-level concept selection via data-driven and sparsity inducing Bayesian arguments. Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more granular concept information residing in patch-specific regions of the image scene. As we experimentally show, the proposed construction not only outperforms recent CBM approaches, but also yields a principled framework towards interpetability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习算法最近因其令人印象深刻的性能而受到极大关注。然而，它们的高复杂性和不可解释的操作模式阻碍了它们在现实世界安全关键任务中的自信部署。这项工作的目标是事前可解释性，特别是概念瓶颈模型（CBM）。我们的目标是设计一个框架，允许在多个粒度级别上，就人类可理解的概念而言，进行高度可解释的决策过程。为此，我们提出了一种新的分层概念发现公式，该公式利用：（i）图像-文本模型的最新进展，以及（ii）通过数据驱动和稀疏性诱导的贝叶斯论点进行多级概念选择的创新公式。在这个框架内，概念信息不仅仅依赖于整个图像和一般非结构化概念之间的相似性；相反，我们引入了概念层次的概念，以揭示和利用驻留在图像场景的补丁特定区域中的更细粒度的概念信息。正如我们实验表明的那样，所提出的构造不仅优于最近的CBM方法，而且产生了一个关于互操作性的原则框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02116v1" target="_blank">2310.02116v1</a>
                              </td>
                              <td>Hierarchical Concept Discovery Models: A Concept Pyramid Scheme</td>
                              <td>Konstantinos P. Panousis</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02116v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和未知数据中推广的能力之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v1" target="_blank">2310.02048v1</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_02808v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Effective Self-supervised Pre-training on Low-compute Networks without Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_02808v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_02808v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_02808v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive progress of self-supervised learning (SSL), its applicability to low-compute networks has received limited attention. Reported performance has trailed behind standard supervised pre-training by a large margin, barring self-supervised learning from making an impact on models that are deployed on device. Most prior works attribute this poor performance to the capacity bottleneck of the low-compute networks and opt to bypass the problem through the use of knowledge distillation (KD). In this work, we revisit SSL for efficient neural networks, taking a closer at what are the detrimental factors causing the practical limitations, and whether they are intrinsic to the self-supervised low-compute setting. We find that, contrary to accepted knowledge, there is no intrinsic architectural bottleneck, we diagnose that the performance bottleneck is related to the model complexity vs regularization strength trade-off. In particular, we start by empirically observing that the use of local views can have a dramatic impact on the effectiveness of the SSL methods. This hints at view sampling being one of the performance bottlenecks for SSL on low-capacity networks. We hypothesize that the view sampling strategy for large neural networks, which requires matching views in very diverse spatial scales and contexts, is too demanding for low-capacity architectures. We systematize the design of the view sampling mechanism, leading to a new training methodology that consistently improves the performance across different SSL methods (e.g. MoCo-v2, SwAV, DINO), different low-size networks (e.g. MobileNetV2, ResNet18, ResNet34, ViT-Ti), and different tasks (linear probe, object detection, instance segmentation and semi-supervised learning). Our best models establish a new state-of-the-art for SSL methods on low-compute networks despite not using a KD loss term.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_02808v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管自监督学习（SSL）取得了令人印象深刻的进展，但其在低计算网络中的适用性却受到了有限的关注。报告的性能在很大程度上落后于标准监督的预训练，禁止自我监督学习对部署在设备上的模型产生影响。大多数先前的工作将这种较差的性能归因于低计算网络的容量瓶颈，并选择通过使用知识蒸馏（KD）来绕过该问题。在这项工作中，我们重新审视了高效神经网络的SSL，深入了解了造成实际限制的有害因素是什么，以及它们是否是自监督低计算设置所固有的。我们发现，与公认的知识相反，不存在固有的体系结构瓶颈，我们诊断出性能瓶颈与模型复杂性与正则化强度的权衡有关。特别是，我们从经验上观察到，局部视图的使用会对SSL方法的有效性产生巨大影响。这暗示视图采样是低容量网络上SSL的性能瓶颈之一。我们假设，大型神经网络的视图采样策略需要在非常不同的空间尺度和上下文中匹配视图，对于低容量架构来说要求太高。我们将视图采样机制的设计系统化，从而产生了一种新的训练方法，该方法在不同的SSL方法（例如MoCo-v2、SwAV、DINO）、不同的低规模网络（例如MobileNetV2、ResNet18、ResNet34、ViT-Ti）和不同的任务（线性探测、对象检测、实例分割和半监督学习）中持续提高性能。我们的最佳模型为低计算网络上的SSL方法建立了新的最先进的技术，尽管没有使用KD损失项。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.02808v2" target="_blank">2210.02808v2</a>
                              </td>
                              <td>Effective Self-supervised Pre-training on Low-compute Networks without Distillation</td>
                              <td>Fuwen Tan</td>
                              <td>2022-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_02808v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.02808v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择最近的DeDoDe关键点是因为它们具有较高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工拾取的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏大规模和多样化的3D开放词汇分割数据集来训练健壮和可推广的模型，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v3" target="_blank">2305.14093v3</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14265v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14265v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14265v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14265v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the advances in robotics a large proportion of the of parts handling tasks in the automotive industry's internal logistics are not automated but still performed by humans. A key component to competitively automate these processes is a 6D pose estimation that can handle a large number of different parts, is adaptable to new parts with little manual effort, and is sufficiently accurate and robust with respect to industry requirements. In this context, the question arises as to the current status quo with respect to these measures. To address this we built a representative 6D pose estimation pipeline with state-of-the-art components from economically scalable real to synthetic data generation to pose estimators and evaluated it on automotive parts with regards to a realistic sequencing process. We found that using the data generation approaches, the performance of the trained 6D pose estimators are promising, but do not meet industry requirements. We reveal that the reason for this is the inability of the estimators to provide reliable uncertainties for their poses, rather than the ability of to provide sufficiently accurate poses. In this context we further analyzed how RGB- and RGB-D-based approaches compare against this background and show that they are differently vulnerable to the domain gap induced by synthetic data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14265v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管机器人技术取得了进步，但汽车行业内部物流中很大一部分零件处理任务并不是自动化的，而是由人类完成的。有竞争力地自动化这些过程的一个关键组件是6D姿态估计，它可以处理大量不同的零件，只需很少的手动操作就可以适应新零件，并且相对于行业要求来说足够准确和稳健。在这方面，出现了关于这些措施目前现状的问题。为了解决这一问题，我们建立了一个具有代表性的6D姿态估计管道，其中包括从经济可扩展的真实到合成数据生成到姿态估计器的最先进组件，并根据现实的测序过程在汽车零件上进行了评估。我们发现，使用数据生成方法，经过训练的6D姿态估计器的性能很有希望，但不符合行业要求。我们发现，造成这种情况的原因是估计器无法为其姿态提供可靠的不确定性，而不是提供足够准确姿态的能力。在这种情况下，我们进一步分析了基于RGB和RGB-D的方法在这种背景下的比较，并表明它们不同地容易受到合成数据引起的域间隙的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14265v1" target="_blank">2309.14265v1</a>
                              </td>
                              <td>Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</td>
                              <td>Philipp Quentin</td>
                              <td>2023-09-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14265v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14265v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13578v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13578v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13578v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13578v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Panoptic segmentation in agriculture is an advanced computer vision technique that provides a comprehensive understanding of field composition. It facilitates various tasks such as crop and weed segmentation, plant panoptic segmentation, and leaf instance segmentation, all aimed at addressing challenges in agriculture. Exploring the application of panoptic segmentation in agriculture, the 8th Workshop on Computer Vision in Plant Phenotyping and Agriculture (CVPPA) hosted the challenge of hierarchical panoptic segmentation of crops and weeds using the PhenoBench dataset. To tackle the tasks presented in this competition, we propose an approach that combines the effectiveness of the Segment AnyThing Model (SAM) for instance segmentation with prompt input from object detection models. Specifically, we integrated two notable approaches in object detection, namely DINO and YOLO-v8. Our best-performing model achieved a PQ+ score of 81.33 based on the evaluation metrics of the competition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13578v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业中的全景分割是一种先进的计算机视觉技术，可以全面了解田地的组成。它促进了各种任务，如作物和杂草分割、植物全景分割和叶片实例分割，所有这些都旨在解决农业中的挑战。探索全景分割在农业中的应用，第八届植物表型与农业计算机视觉研讨会（CVPPA）主办了使用PhenoBench数据集对作物和杂草进行分层全景分割的挑战。为了解决此次竞争中提出的任务，我们提出了一种方法，该方法将分段AnyThing模型（SAM）的有效性（例如，分段）与来自对象检测模型的即时输入相结合。具体来说，我们在物体检测中集成了两种著名的方法，即DINO和YOLO-v8。根据比赛的评估指标，我们表现最好的模型获得了81.33的PQ+分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13578v1" target="_blank">2309.13578v1</a>
                              </td>
                              <td>A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition</td>
                              <td>Khoa Dang Nguyen</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13578v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13578v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12969v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Every Thing with Few Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12969v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12969v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12969v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12969v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开集对象检测的目的是检测训练中发现的任意类别之外的任意类别。最近的进步采用了开放词汇范式，利用视觉语言主干来用语言表示类别。在本文中，我们介绍了DE ViT，这是一种开放集对象检测器，它使用仅视觉的DINOv2主干，并通过示例图像而不是语言来学习新的类别。为了提高一般检测能力，我们将多分类任务转换为二进制分类任务，同时绕过每类推理，并提出了一种新的区域传播定位技术。我们使用COCO和LVIS在开放词汇、少镜头和单镜头对象检测基准上评估了DE-ViT。对于COCO，DE ViT比开放词汇SoTA高出6.9 AP50，并在新类别中达到50 AP50。DE ViT在10次发射时以15毫安时的容量超过了少数发射的SoTA，在30次发射时则以7.2毫安时的体积超过了SoTA，而在一次发射时，则以2.8 AP50的容量超过SoTA。对于LVIS，DE ViT比开放词汇表SoTA高2.2掩码AP，达到34.3掩码AP。代码可在https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12969v1" target="_blank">2309.12969v1</a>
                              </td>
                              <td>Detect Every Thing with Few Examples</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12969v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12969v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12925v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12925v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12925v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12925v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Microarchitectural timing side channels have been thoroughly investigated as a security threat in hardware designs featuring shared buffers (e.g., caches) and/or parallelism between attacker and victim task execution. Contradicting common intuitions, recent activities demonstrate, however, that this threat is real also in microcontroller SoCs without such features. In this paper, we describe SoC-wide timing side channels previously neglected by security analysis and present a new formal method to close this gap. In a case study with the RISC-V Pulpissimo SoC platform, our method found a vulnerability to a so far unknown attack variant that allows an attacker to obtain information about a victim's memory access behavior. After implementing a conservative fix, we were able to verify that the SoC is now secure w.r.t. timing side channels.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12925v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在以共享缓冲区（例如缓存）和/或攻击者和受害者任务执行之间的并行性为特征的硬件设计中，微体系结构定时侧通道已被彻底调查为安全威胁。然而，与普遍的直觉相矛盾的是，最近的活动表明，这种威胁在没有这些功能的微控制器SoC中也是真实存在的。在本文中，我们描述了以前被安全分析忽视的SoC宽定时侧信道，并提出了一种新的形式化方法来弥补这一差距。在RISC-V Pulpissimo SoC平台的案例研究中，我们的方法发现了一个迄今未知的攻击变体的漏洞，该漏洞使攻击者能够获得有关受害者内存访问行为的信息。在实现保守修复后，我们能够验证SoC现在是安全的w.r.t.定时侧信道。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12925v1" target="_blank">2309.12925v1</a>
                              </td>
                              <td>A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them</td>
                              <td>Johannes Müller</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12925v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12925v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10972v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10972v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10972v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10972v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10972v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当标记数据稀少时，准确确定图像的显著区域是一项挑战。基于DINO的自监督方法最近利用了由逐片特征捕获的有意义的图像语义来定位前景对象。最近的方法也结合了直观的先验，并在无监督的对象划分方法中证明了其价值。在本文中，我们提出了SEMPART，它在图像的基于DINO的语义图上联合推断粗分和细分。此外，SEMPART使用图驱动的正则化来保留精细边界细节，并成功地将粗略掩码语义提取到精细掩码中。我们显著的目标检测和单目标定位结果表明，SEMPART在没有额外后处理的情况下快速生成高质量的掩模，并受益于粗分支和细分支的协同优化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10972v1" target="_blank">2309.10972v1</a>
                              </td>
                              <td>SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</td>
                              <td>Sriram Ravindran</td>
                              <td>2023-09-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10972v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10972v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07970v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07970v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07970v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07970v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grasping objects by a specific part is often crucial for safety and for executing downstream tasks. Yet, learning-based grasp planners lack this behavior unless they are trained on specific object part data, making it a significant challenge to scale object diversity. Instead, we propose LERF-TOGO, Language Embedded Radiance Fields for Task-Oriented Grasping of Objects, which uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query. To accomplish this, we first reconstruct a LERF of the scene, which distills CLIP embeddings into a multi-scale 3D language field queryable with text. However, LERF has no sense of objectness, meaning its relevancy outputs often return incomplete activations over an object which are insufficient for subsequent part queries. LERF-TOGO mitigates this lack of spatial grouping by extracting a 3D object mask via DINO features and then conditionally querying LERF on this mask to obtain a semantic distribution over the object with which to rank grasps from an off-the-shelf grasp planner. We evaluate LERF-TOGO's ability to grasp task-oriented object parts on 31 different physical objects, and find it selects grasps on the correct part in 81% of all trials and grasps successfully in 69%. See the project website at: lerftogo.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07970v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由特定部件抓住物体通常对安全和执行下游任务至关重要。然而，基于学习的抓取规划者缺乏这种行为，除非他们接受特定对象零件数据的训练，这使得缩放对象多样性成为一个重大挑战。相反，我们提出了LERF-TOGO，面向任务的对象抓取的语言嵌入辐射场，它使用视觉语言模型零样本在给定自然语言查询的对象上输出抓取分布。为了实现这一点，我们首先重建场景的LERF，该LERF将CLIP嵌入提取到可通过文本查询的多尺度3D语言字段中。然而，LERF没有对象性，这意味着它的相关性输出通常会返回对象上不完整的激活，这不足以进行后续的零件查询。LER-TOGO通过经由DINO特征提取3D对象掩码，然后有条件地查询该掩码上的LERF，以获得对象上的语义分布，从而从现成的抓取规划器对抓取进行排序，从而缓解了这种空间分组的缺乏。我们评估了LER-TOGO在31个不同实物上抓取面向任务的物体部分的能力，发现它在81%的试验中选择了正确部分的抓取，在69%的试验中成功抓取。参见项目网站：lerftogo.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07970v2" target="_blank">2309.07970v2</a>
                              </td>
                              <td>Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping</td>
                              <td>Adam Rashid</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07970v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07970v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to 40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们的习得表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作预训练编码器上的正则化术语，以补救低质量表示。与基线相比，使用Q-Score正则化进行微调可以在ImageNet-100上将SSL模型的线性探测精度提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性密切相关，通过Q分数正则化增强这些特征使SSL表示更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v5" target="_blank">2203.01881v5</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>