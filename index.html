<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-12-09</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04031v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Importance of Coordinate Frames in Dynamic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04031v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04031v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04031v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. While expressing object points in a local reference frame may seem intuitive, it may not necessarily lead to the most accurate and robust solutions. This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. To this end, we introduce a front-end agnostic framework using GTSAM that can be used to evaluate various Dynamic SLAM formulations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04031v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数同时定位和地图绘制（SLAM）系统传统上假设了一个静态世界，这与现实世界的场景不一致。为了使机器人能够在动态环境中安全地导航和规划，必须使用能够处理移动物体的表示。动态SLAM是SLAM研究中的一个新兴领域，因为它提高了整个系统的精度，同时提供了对物体运动的额外估计。最新的文献提供了动态SLAM的两种主要公式，表示世界或对象坐标系中的动态对象点。虽然在局部参考系中表达对象点可能看起来很直观，但它可能不一定能得出最准确、最稳健的解决方案。本文对各种动态SLAM公式进行了深入分析，确定了解决该问题的最佳方法。为此，我们引入了一个使用GTSAM的前端不可知框架，该框架可用于评估各种动态SLAM公式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04031v1" target="_blank">2312.04031v1</a>
                              </td>
                              <td>The Importance of Coordinate Frames in Dynamic SLAM</td>
                              <td>Jesse Morris</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04031v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04031v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08769v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08769v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08769v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08769v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper quantifies the performance of visual SLAM that leverages multi-scale fiducial markers (i.e., artificial landmarks that can be detected at a wide range of distances) to show its potential for reliable takeoff and landing navigation in rotorcraft. Prior work has shown that square markers with a black-and-white pattern of grid cells can be used to improve the performance of visual SLAM with color cameras. We extend this prior work to allow nested marker layouts. We evaluate performance during semi-autonomous takeoff and landing operations in a variety of environmental conditions by a DJI Matrice 300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to obtain ground truth pose estimates. Performance measures include absolute trajectory error and the fraction of the number of estimated poses to the total frame. We release all of our results -- our dataset and the code of the implementation of the visual SLAM with fiducial markers -- to the public as open-source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08769v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文量化了视觉SLAM的性能，该视觉SLAM利用多尺度基准标记（即可以在宽距离内检测到的人工地标）来显示其在旋翼飞机中可靠起飞和着陆导航的潜力。先前的工作已经表明，具有黑白网格单元图案的方形标记可以用于提高彩色相机的视觉SLAM的性能。我们扩展了之前的工作，允许嵌套标记布局。我们使用带有两个FLIR Blackfly彩色相机的DJI Matrice 300 RTK旋翼机，使用RTK GNSS获得地面真实姿态估计，评估了在各种环境条件下半自主起飞和着陆操作期间的性能。性能度量包括绝对轨迹误差和估计姿态数量占总帧的比例。我们以开源的形式向公众发布我们的所有结果——我们的数据集和带有基准标记的可视化SLAM的实现代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08769v2" target="_blank">2309.08769v2</a>
                              </td>
                              <td>The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft</td>
                              <td>Jongwon Lee</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08769v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08769v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02684v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02684v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02684v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02684v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Point clouds have shown significant potential in various domains, including Simultaneous Localization and Mapping (SLAM). However, existing approaches either rely on dense point clouds to achieve high localization accuracy or use generalized descriptors to reduce map size. Unfortunately, these two aspects seem to conflict with each other. To address this limitation, we propose a unified architecture, DeepPointMap, achieving excellent preference on both aspects. We utilize neural network to extract highly representative and sparse neural descriptors from point clouds, enabling memory-efficient map representation and accurate multi-scale localization tasks (e.g., odometry and loop-closure). Moreover, we showcase the versatility of our framework by extending it to more challenging multi-agent collaborative SLAM. The promising results obtained in these scenarios further emphasize the effectiveness and potential of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02684v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>点云在包括同步定位和映射（SLAM）在内的各个领域都显示出了巨大的潜力。然而，现有的方法要么依靠密集的点云来实现高定位精度，要么使用广义描述符来减小地图大小。不幸的是，这两个方面似乎相互冲突。为了解决这一限制，我们提出了一个统一的体系结构DeepPointMap，在这两个方面都实现了卓越的偏好。我们利用神经网络从点云中提取具有高度代表性和稀疏性的神经描述符，从而实现高效记忆的地图表示和准确的多尺度定位任务（例如里程计和环路闭合）。此外，我们通过将框架扩展到更具挑战性的多智能体协作SLAM，展示了其多功能性。在这些场景中获得的有希望的结果进一步强调了我们方法的有效性和潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02684v1" target="_blank">2312.02684v1</a>
                              </td>
                              <td>DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors</td>
                              <td>Xiaze Zhang</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02684v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02684v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02599v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor Positioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02599v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02599v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02599v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A Magnetic field Aided Inertial Navigation System (MAINS) for indoor navigation is proposed in this paper. MAINS leverages an array of magnetometers to measure spatial variations in the magnetic field, which are then used to estimate the displacement and orientation changes of the system, thereby aiding the inertial navigation system (INS). Experiments show that MAINS significantly outperforms the stand-alone INS, demonstrating a remarkable two orders of magnitude reduction in position error. Furthermore, when compared to the state-of-the-art magnetic-field-aided navigation approach, the proposed method exhibits slightly improved horizontal position accuracy. On the other hand, it has noticeably larger vertical error on datasets with large magnetic field variations. However, one of the main advantages of MAINS compared to the state-of-the-art is that it enables flexible sensor configurations. The experimental results show that the position error after 2 minutes of navigation in most cases is less than 3 meters when using an array of 30 magnetometers. Thus, the proposed navigation solution has the potential to solve one of the key challenges faced with current magnetic-field simultaneous localization and mapping (SLAM) solutions: the very limited allowable length of the exploration phase during which unvisited areas are mapped.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02599v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于室内导航的磁场辅助惯性导航系统。MAINS利用磁强计阵列来测量磁场的空间变化，然后用于估计系统的位移和方向变化，从而帮助惯性导航系统（INS）。实验表明，MAINS明显优于独立的INS，位置误差显著降低了两个数量级。此外，与最先进的磁场辅助导航方法相比，该方法的水平位置精度略有提高。另一方面，在磁场变化较大的数据集上，它具有明显较大的垂直误差。然而，与最先进的技术相比，MAINS的主要优势之一是它能够实现灵活的传感器配置。实验结果表明，当使用30个磁强计的阵列时，导航2分钟后的位置误差在大多数情况下小于3米。因此，所提出的导航解决方案有可能解决当前磁场同步定位和测绘（SLAM）解决方案面临的一个关键挑战：测绘未访问区域的勘探阶段的允许长度非常有限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02599v1" target="_blank">2312.02599v1</a>
                              </td>
                              <td>MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor Positioning</td>
                              <td>Chuan Huang</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02599v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02599v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/Huang-Chuan/MAINSvsMAGEKF" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/huang-chuan/mainsvsmagekf" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02353v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient 2D Graph SLAM for Sparse Sensing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02353v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02353v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02353v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) plays a vital role in mapping unknown spaces and aiding autonomous navigation. Virtually all state-of-the-art solutions today for 2D SLAM are designed for dense and accurate sensors such as laser range-finders (LiDARs). However, these sensors are not suitable for resource-limited nano robots, which become increasingly capable and ubiquitous nowadays, and these robots tend to mount economical and low-power sensors that can only provide sparse and noisy measurements. This introduces a challenging problem called SLAM with sparse sensing. This work addresses the problem by adopting the form of the state-of-the-art graph-based SLAM pipeline with a novel frontend and an improvement for loop closing in the backend, both of which are designed to work with sparse and uncertain range data. Experiments show that the maps constructed by our algorithm have superior quality compared to prior works on sparse sensing. Furthermore, our method is capable of running in real-time on a modern PC with an average processing time of 1/100th the input interval time.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02353v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位与映射（SLAM）在绘制未知空间和辅助自主导航方面发挥着至关重要的作用。如今，几乎所有最先进的2D SLAM解决方案都是为密集和精确的传感器设计的，如激光测距仪（LiDAR）。然而，这些传感器不适合资源有限的纳米机器人，如今，纳米机器人的能力越来越强，无处不在，而且这些机器人往往安装经济、低功耗的传感器，只能提供稀疏和嘈杂的测量。这引入了一个具有挑战性的问题，称为具有稀疏感测的SLAM。这项工作通过采用最先进的基于图的SLAM流水线的形式来解决这个问题，该流水线具有新颖的前端和对后端闭环的改进，这两种方法都被设计用于处理稀疏和不确定范围的数据。实验表明，与先前的稀疏传感工作相比，我们的算法构建的地图具有更好的质量。此外，我们的方法能够在现代PC上实时运行，平均处理时间为输入间隔时间的1/100。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02353v1" target="_blank">2312.02353v1</a>
                              </td>
                              <td>Efficient 2D Graph SLAM for Sparse Sensing</td>
                              <td>Hanzhi Zhou</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02353v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02353v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/shiftlab-nanodrone/sparse-gslam" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/shiftlab-nanodrone/sparse-gslam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02141v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iMatching: Imperative Correspondence Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02141v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02141v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习特征对应关系是计算机视觉的一项基础任务，对视觉里程计和三维重建等下游应用具有极其重要的意义。尽管最近在数据驱动模型方面取得了进展，但由于缺乏准确的每像素对应标签，特征对应学习仍然受到限制。为了克服这一困难，我们引入了一种新的自监督方案，即命令式学习（IL），用于训练特征对应关系。它可以在没有任何相机姿势或深度标签的情况下，在任意不间断的视频上进行函授学习，预示着自我监督函授学习的新时代。具体来说，我们将对应学习问题公式化为双层优化，将束调整的重投影误差作为模型的监督信号。为了避免大的内存和计算开销，我们利用驻点通过束调整有效地反向传播隐式梯度。通过广泛的实验，我们在包括特征匹配和姿态估计在内的任务上表现出了卓越的性能，在这些任务中，我们获得了比最先进的匹配模型平均30%的精度增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02141v1" target="_blank">2312.02141v1</a>
                              </td>
                              <td>iMatching: Imperative Correspondence Learning</td>
                              <td>Zitong Zhan</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02141v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02141v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02126v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02126v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02126v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02126v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02126v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>密集同时定位和映射（SLAM）对于具体场景的理解至关重要。最近的工作表明，3D高斯可以使用多个姿势的相机进行高质量的场景重建和实时渲染。在这种情况下，我们首次展示了通过3D高斯表示场景可以使用单个非聚焦单目RGB-D相机实现密集SLAM。我们的方法SplaTAM解决了先前基于辐射场的表示的局限性，包括快速渲染和优化、确定区域是否先前已映射的能力，以及通过添加更多高斯来进行结构化地图扩展。我们使用在线跟踪和映射管道，同时对其进行剪裁，以专门使用底层高斯表示和轮廓引导的优化，通过可微分渲染进行优化。大量实验表明，SplaTAM在相机姿态估计、地图构建和新颖视图合成方面实现了高达两倍的最先进性能，证明了其优于现有方法，同时允许实时渲染高分辨率密集3D地图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02126v1" target="_blank">2312.02126v1</a>
                              </td>
                              <td>SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM</td>
                              <td>Nikhil Keetha</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02126v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02126v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We develop accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和测绘（SLAM）是许多应用（如自主导航和勘探）的基本任务。尽管已经发布了许多SLAM数据集，但当前的SLAM解决方案仍难以获得持续和有弹性的性能。一个主要问题是缺乏高质量的数据集，包括不同的全天候条件和评估稳健性的可靠指标。这种限制极大地限制了SLAM技术的可扩展性和可推广性，影响了它们的开发、验证和部署。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在将SLAM推向全天候环境，以追求最稳健的SLAM性能。它包含了多种退化环境，包括30多个不同的场景，如无结构走廊、不同的照明条件和烟雾和灰尘等感知障碍物；多模式传感器，如激光雷达、鱼眼相机、IMU和热像仪；以及多种运动方式，如空中机器人、腿式机器人和轮式机器人。我们为SLAM开发了准确性和稳健性评估跟踪，并引入了新的稳健性度量。进行了全面的研究，揭示了新的观察结果、挑战和未来研究的机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v3" target="_blank">2307.07607v3</a>
                              </td>
                              <td>SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_09168v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_09168v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_09168v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_09168v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>When an agent, person, vehicle or robot is moving through an unknown environment without GNSS signals, online mapping of nonlinear terrains can be used to improve position estimates when the agent returns to a previously mapped area. Mapping algorithms using online Gaussian process (GP) regression are commonly integrated in algorithms for simultaneous localisation and mapping (SLAM). However, GP mapping algorithms have increasing computational demands as the mapped area expands relative to spatial field variations. This is due to the need for estimating an increasing amount of map parameters as the area of the map grows. Contrary to this, we propose a recursive GP mapping estimation algorithm which uses local basis functions in an information filter to achieve spatial scalability. Our proposed approximation employs a global grid of finite support basis functions but restricts computations to a localized subset around each prediction point. As our proposed algorithm is recursive, it can naturally be incorporated into existing algorithms that uses Gaussian process maps for SLAM. Incorporating our proposed algorithm into an extended Kalman filter (EKF) for magnetic field SLAM reduces the overall computational complexity of the algorithm. We show experimentally that our algorithm is faster than existing methods when the mapped area is large and the map is based on many measurements, both for recursive mapping tasks and for magnetic field SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_09168v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当代理、人、车辆或机器人在没有GNSS信号的情况下穿过未知环境时，当代理返回到先前映射的区域时，可以使用非线性地形的在线映射来改进位置估计。使用在线高斯过程（GP）回归的映射算法通常集成在用于同时定位和映射（SLAM）的算法中。然而，随着映射区域相对于空间场变化的扩展，GP映射算法具有越来越高的计算需求。这是由于随着地图面积的增长，需要估计越来越多的地图参数。与此相反，我们提出了一种递归GP映射估计算法，该算法在信息滤波器中使用局部基函数来实现空间可伸缩性。我们提出的近似使用有限支持基函数的全局网格，但将计算限制在每个预测点周围的局部子集。由于我们提出的算法是递归的，因此它可以自然地被纳入使用高斯过程图进行SLAM的现有算法中。将我们提出的算法结合到用于磁场SLAM的扩展卡尔曼滤波器（EKF）中降低了算法的总体计算复杂度。我们通过实验表明，当映射区域很大并且映射基于许多测量时，无论是对于递归映射任务还是对于磁场SLAM，我们的算法都比现有方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.09168v2" target="_blank">2210.09168v2</a>
                              </td>
                              <td>Spatially scalable recursive estimation of Gaussian process terrain maps using local basis functions</td>
                              <td>Frida Marie Viset</td>
                              <td>2022-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_09168v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.09168v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fridaviset/predictiondependentbasisfunctions" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00204v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DNS SLAM: Dense Neural Semantic-Informed SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00204v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00204v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00204v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, coordinate-based neural implicit representations have shown promising results for the task of Simultaneous Localization and Mapping (SLAM). While achieving impressive performance on small synthetic scenes, these methods often suffer from oversmoothed reconstructions, especially for complex real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D semantic SLAM approach featuring a hybrid representation. Relying only on 2D semantic priors, we propose the first semantic neural SLAM method that trains class-wise scene representations while providing stable camera tracking at the same time. Our method integrates multi-view geometry constraints with image-based feature extraction to improve appearance details and to output color, density, and semantic class information, enabling many downstream applications. To further enable real-time tracking, we introduce a lightweight coarse scene representation which is trained in a self-supervised manner in latent space. Our experimental results achieve state-of-the-art performance on both synthetic data and real-world data tracking while maintaining a commendable operational speed on off-the-shelf hardware. Further, our method outputs class-wise decomposed reconstructions with better texture capturing appearance and geometric details.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00204v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于坐标的神经隐式表示在同时定位和映射（SLAM）任务中显示出了很好的结果。虽然这些方法在小的合成场景中获得了令人印象深刻的性能，但它们往往会遭受过度平滑的重建，尤其是在复杂的真实世界场景中。在这项工作中，我们介绍了DNS SLAM，这是一种新的神经RGB-D语义SLAM方法，具有混合表示。仅依靠2D语义先验，我们提出了第一种语义神经SLAM方法，该方法训练类场景表示，同时提供稳定的相机跟踪。我们的方法将多视图几何约束与基于图像的特征提取相结合，以改善外观细节并输出颜色、密度和语义类信息，从而实现许多下游应用。为了进一步实现实时跟踪，我们引入了一种轻量级的粗略场景表示，该表示在潜在空间中以自监督的方式进行训练。我们的实验结果在合成数据和真实世界的数据跟踪方面都实现了最先进的性能，同时在现成的硬件上保持了值得称赞的操作速度。此外，我们的方法输出具有更好的纹理捕捉外观和几何细节的类分解重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00204v1" target="_blank">2312.00204v1</a>
                              </td>
                              <td>DNS SLAM: Dense Neural Semantic-Informed SLAM</td>
                              <td>Kunyi Li</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00204v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00204v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_05504v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_05504v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_05504v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_05504v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Various autonomous applications rely on recognizing specific known landmarks in their environment. For example, Simultaneous Localization And Mapping (SLAM) is an important technique that lays the foundation for many common tasks, such as navigation and long-term object tracking. This entails building a map on the go based on sensory inputs which are prone to accumulating errors. Recognizing landmarks in the environment plays a vital role in correcting these errors and further improving the accuracy of SLAM. The most popular choice of sensors for conducting SLAM today is optical sensors such as cameras or LiDAR sensors. These can use landmarks such as QR codes as a prerequisite. However, such sensors become unreliable in certain conditions, e.g., foggy, dusty, reflective, or glass-rich environments. Sonar has proven to be a viable alternative to manage such situations better. However, acoustic sensors also require a different type of landmark. In this paper, we put forward a method to detect the presence of bio-mimetic acoustic landmarks using support vector machines trained on the frequency bands of the reflecting acoustic echoes using an embedded real-time imaging sonar.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_05504v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>各种自主应用程序依赖于识别其环境中的特定已知地标。例如，同步定位和映射（SLAM）是一种重要的技术，它为许多常见任务（如导航和长期目标跟踪）奠定了基础。这需要根据容易累积错误的感官输入在旅途中构建地图。识别环境中的地标对于纠正这些错误和进一步提高SLAM的准确性起着至关重要的作用。如今，用于进行SLAM的最流行的传感器选择是光学传感器，例如相机或激光雷达传感器。这些可以使用诸如二维码之类的地标作为先决条件。然而，这种传感器在某些条件下变得不可靠，例如雾蒙蒙、多尘、反光或富含玻璃的环境。声纳已经被证明是一种可行的替代方案，可以更好地管理这种情况。然而，声学传感器也需要不同类型的地标。在本文中，我们提出了一种方法，使用嵌入式实时成像声纳在反射声回波的频带上训练的支持向量机来检测生物模拟声学标志的存在。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.05504v3" target="_blank">2308.05504v3</a>
                              </td>
                              <td>Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</td>
                              <td>Maarten de Backer</td>
                              <td>2023-08-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_05504v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.05504v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18189v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-based Visual Inertial Velometer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18189v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18189v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18189v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution. Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion. However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time. One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment. This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data. Therefore, we propose a mapping-free design for event-based visual-inertial state estimation in this paper. Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras. The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit. Experiments on the synthetic dataset demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18189v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于神经形态事件的相机是受生物启发的视觉传感器，具有异步像素和极高的时间分辨率。这些有利的特性使它们成为解决攻击性自我运动下的状态估计任务的绝佳选择。然而，在最先进的基于事件的视觉里程计系统中，当局部地图不能及时更新时，经常会出现相机姿态跟踪的故障。这一特定领域的最大障碍之一是缺乏有效和稳健的数据关联方法，而不会对环境施加任何假设。然而，由于事件数据的运动相关可观察性，这个问题似乎不太可能像在标准视觉中那样得到解决。因此，我们在本文中提出了一种基于事件的视觉惯性状态估计的无映射设计。我们发现，恢复瞬时线速度更符合事件摄像机的差分工作原理，而不是估计事件摄像机的位置。所提出的基于事件的视觉惯性速度计利用了连续时间公式，该公式增量地融合了来自立体事件相机和惯性测量单元的异构测量。在合成数据集上的实验表明，该方法可以以低延迟恢复度量尺度下的瞬时线速度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18189v1" target="_blank">2311.18189v1</a>
                              </td>
                              <td>Event-based Visual Inertial Velometer</td>
                              <td>Xiuyuan Lu</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18189v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00068v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00068v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00068v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00068v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sparse LiDAR point clouds cause severe loss of detail of static structures and reduce the density of static points available for navigation. Reduced density can be detrimental to navigation under several scenarios. We observe that despite high sparsity, in most cases, the global topology of LiDAR outlining the static structures can be inferred. We utilize this property to obtain a backbone skeleton of a static LiDAR scan in the form of a single connected component that is a proxy to its global topology. We utilize the backbone to augment new points along static structures to overcome sparsity. Newly introduced points could correspond to existing static structures or to static points that were earlier obstructed by dynamic objects. To the best of our knowledge, we are the first to use this strategy for sparse LiDAR point clouds. Existing solutions close to our approach fail to identify and preserve the global static LiDAR topology and generate sub-optimal points. We propose GLiDR, a Graph Generative network that is topologically regularized using 0-dimensional Persistent Homology (PH) constraints. This enables GLiDR to introduce newer static points along a topologically consistent global static LiDAR backbone. GLiDR generates precise static points using 32x sparser dynamic scans and performs better than the baselines across three datasets. The newly introduced static points allow GLiDR to outperform LiDAR-based navigation using SLAM in several settings. GLiDR generates a valuable byproduct - an accurate binary segmentation mask of static and dynamic objects that is helpful for navigation planning and safety in constrained environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00068v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稀疏的激光雷达点云会导致静态结构细节的严重损失，并降低可用于导航的静态点的密度。在几种情况下，密度降低可能对导航不利。我们观察到，尽管稀疏性很高，但在大多数情况下，可以推断出概述静态结构的激光雷达的全局拓扑结构。我们利用这一特性以单个连接组件的形式获得静态激光雷达扫描的主干骨架，该组件是其全局拓扑的代理。我们利用主干来沿着静态结构增加新的点，以克服稀疏性。新引入的点可以对应于现有的静态结构或先前被动态对象遮挡的静态点。据我们所知，我们是第一个将这种策略用于稀疏激光雷达点云的人。与我们的方法接近的现有解决方案无法识别和保持全局静态激光雷达拓扑，并生成次优点。我们提出了GLiDR，这是一个使用0维持久同调（PH）约束进行拓扑正则化的图生成网络。这使得GLiDR能够沿着拓扑一致的全球静态激光雷达主干引入较新的静态点。GLiDR使用32倍稀疏的动态扫描生成精确的静态点，并且在三个数据集上的性能优于基线。新引入的静态点允许GLiDR在多种设置中优于使用SLAM的基于激光雷达的导航。GLiDR生成了一个有价值的副产品-静态和动态对象的精确二进制分割掩码，有助于在受限环境中进行导航规划和安全。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00068v1" target="_blank">2312.00068v1</a>
                              </td>
                              <td>GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds</td>
                              <td>Prashant Kumar</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00068v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00068v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17754v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cinematic Behavior Transfer via NeRF-based Differentiable Filming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17754v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17754v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17754v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the evolving landscape of digital media and video production, the precise manipulation and reproduction of visual elements like camera movements and character actions are highly desired. Existing SLAM methods face limitations in dynamic scenes and human pose estimation often focuses on 2D projections, neglecting 3D statuses. To address these issues, we first introduce a reverse filming behavior estimation technique. It optimizes camera trajectories by leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then introduce a cinematic transfer pipeline that is able to transfer various shot types to a new 2D video or a 3D virtual environment. The incorporation of 3D engine workflow enables superior rendering and control abilities, which also achieves a higher rating in the user study.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17754v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在数字媒体和视频制作不断发展的环境中，人们非常需要对相机动作和角色动作等视觉元素进行精确的操纵和再现。现有的SLAM方法在动态场景中面临限制，并且人体姿态估计通常侧重于2D投影，而忽略了3D状态。为了解决这些问题，我们首先介绍了一种反向拍摄行为估计技术。它通过利用NeRF作为可微分渲染器并细化SMPL轨迹来优化相机轨迹。然后，我们介绍了一种电影传输管道，该管道能够将各种镜头类型传输到新的2D视频或3D虚拟环境。3D引擎工作流的结合实现了卓越的渲染和控制能力，这也在用户研究中获得了更高的评价。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17754v1" target="_blank">2311.17754v1</a>
                              </td>
                              <td>Cinematic Behavior Transfer via NeRF-based Differentiable Filming</td>
                              <td>Xuekun Jiang</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17754v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16728v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16728v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16728v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16728v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16728v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经绘制和SLAM系统的集成最近在联合定位和真实感视图重建方面显示出有希望的结果。然而，现有的方法完全依赖于隐式表示，资源匮乏，无法在便携式设备上运行，这偏离了SLAM的初衷。在本文中，我们提出了Photo SLAM，这是一种具有超基元映射的新型SLAM框架。具体来说，我们同时利用显式几何特征进行定位，并学习隐式光度特征来表示观测环境的纹理信息。除了基于几何特征主动加密超基元外，我们还引入了一种基于高斯金字塔的训练方法来逐步学习多层次特征，提高了真实感映射的性能。对单眼、立体和RGB-D数据集的广泛实验证明，我们提出的系统Photo SLAM在在线真实感映射方面显著优于当前最先进的SLAM系统，例如，在Replica数据集中，PSNR高出30%，渲染速度快数百倍。此外，Photo SLAM可以使用Jetson AGX Orin等嵌入式平台以实时速度运行，显示了机器人应用的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16728v1" target="_blank">2311.16728v1</a>
                              </td>
                              <td>Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</td>
                              <td>Huajian Huang</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16728v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16728v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11310v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Twilight SLAM: Navigating Low-Light Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11310v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11310v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11310v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a comparative study of low-light visual SLAM pipelines, specifically focusing on determining an efficient combination of the state-of-the-art low-light image enhancement algorithms with standard and contemporary Simultaneous Localization and Mapping (SLAM) frameworks by evaluating their performance in challenging low-light conditions. In this study, we investigate the performance of several different low-light SLAM pipelines for dark and/or poorly-lit datasets as opposed to just partially dim-lit datasets like other works in the literature. Our study takes an experimental approach to qualitatively and quantitatively compare the chosen combinations of modules to enhance the feature-based visual SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11310v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对微光视觉SLAM管道进行了比较研究，特别是通过评估其在具有挑战性的微光条件下的性能，确定最先进的微光图像增强算法与标准和现代同步定位和映射（SLAM）框架的有效组合。在这项研究中，我们研究了几种不同的弱光SLAM管道在黑暗和/或光线不足的数据集中的性能，而不是像文献中的其他工作一样仅在部分昏暗的数据集上。我们的研究采用实验方法对所选择的模块组合进行定性和定量比较，以增强基于特征的视觉SLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11310v3" target="_blank">2304.11310v3</a>
                              </td>
                              <td>Twilight SLAM: Navigating Low-Light Environments</td>
                              <td>Surya Pratap Singh</td>
                              <td>2023-04-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11310v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11310v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14970v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UWB Radar SLAM: an Anchorless Approach in Vision Denied Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14970v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14970v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14970v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR and cameras are frequently used as sensors for simultaneous localization and mapping (SLAM). However, these sensors are prone to failure under low visibility (e.g. smoke) or places with reflective surfaces (e.g. mirrors). On the other hand, electromagnetic waves exhibit better penetration properties when the wavelength increases, thus are not affected by low visibility. Hence, this paper presents ultra-wideband (UWB) radar as an alternative to the existing sensors. UWB is generally known to be used in anchor-tag SLAM systems. One or more anchors are installed in the environment and the tags are attached to the robots. Although this method performs well under low visibility, modifying the existing infrastructure is not always feasible. UWB has also been used in peer-to-peer ranging collaborative SLAM systems. However, this requires more than a single robot and does not include mapping in the mentioned environment like smoke. Therefore, the presented approach in this paper solely depends on the UWB transceivers mounted on-board. In addition, an extended Kalman filter (EKF) SLAM is used to solve the SLAM problem at the back-end. Experiments were conducted and demonstrated that the proposed UWB-based radar SLAM is able to map natural point landmarks inside an indoor environment while improving robot localization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14970v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>激光雷达和相机经常被用作同时定位和映射（SLAM）的传感器。然而，这些传感器在低能见度（如烟雾）或具有反射表面（如镜子）的地方容易发生故障。另一方面，当波长增加时，电磁波表现出更好的穿透特性，因此不受低能见度的影响。因此，本文提出了超宽带（UWB）雷达作为现有传感器的替代方案。UWB通常已知用于锚标签SLAM系统中。一个或多个锚被安装在环境中，并且标签被附接到机器人上。尽管这种方法在低可见性下表现良好，但修改现有基础设施并不总是可行的。UWB还被用于对等测距协作SLAM系统中。然而，这需要不止一个机器人，并且不包括在上述环境（如烟雾）中进行映射。因此，本文提出的方法仅依赖于安装在机载的UWB收发器。此外，在后端使用扩展卡尔曼滤波器（EKF）SLAM来解决SLAM问题。实验表明，所提出的基于UWB的雷达SLAM能够在改善机器人定位的同时，绘制室内环境中的自然点地标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14970v1" target="_blank">2311.14970v1</a>
                              </td>
                              <td>UWB Radar SLAM: an Anchorless Approach in Vision Denied Indoor Environments</td>
                              <td>H. A. G. C. Premachandra</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14970v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14970v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_16748v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Perception and Semantic Mapping Method for Robot Autonomy in Orchards</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_16748v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_16748v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_16748v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agricultural robots must navigate challenging dynamic and semi-structured environments. Recently, environmental modeling using LiDAR-based SLAM has shown promise in providing highly accurate geometry. However, how this chaotic environmental information can be used to achieve effective robot automation in the agricultural sector remains unexplored. In this study, we propose a novel semantic mapping and navigation framework for achieving robotic autonomy in orchards. It consists of two main components: a semantic processing module and a navigation module. First, we present a novel 3D detection network architecture, 3D-ODN, which can accurately process object instance information from point clouds. Second, we develop a framework to construct the visibility map by incorporating semantic information and terrain analysis. By combining these two critical components, our framework is evaluated in a number of key horticultural production scenarios, including a robotic system for in-situ phenotyping and daily monitoring, and a selective harvesting system in apple orchards. The experimental results show that our method can ensure high accuracy in understanding the environment and enable reliable robot autonomy in agricultural environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_16748v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业机器人必须在充满挑战的动态和半结构化环境中导航。最近，使用基于激光雷达的SLAM的环境建模在提供高度精确的几何形状方面显示出了前景。然而，如何利用这些混乱的环境信息在农业部门实现有效的机器人自动化仍有待探索。在这项研究中，我们提出了一种新的语义映射和导航框架，用于实现果园中的机器人自主性。它由两个主要组件组成：语义处理模块和导航模块。首先，我们提出了一种新的三维检测网络架构3D-ODN，它可以准确地处理点云中的对象实例信息。其次，我们开发了一个框架，通过结合语义信息和地形分析来构建可见性地图。通过结合这两个关键组成部分，我们的框架在许多关键的园艺生产场景中进行了评估，包括用于原位表型和日常监测的机器人系统，以及苹果园的选择性收获系统。实验结果表明，我们的方法可以确保对环境的高精度理解，并使机器人在农业环境中实现可靠的自主性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.16748v3" target="_blank">2308.16748v3</a>
                              </td>
                              <td>A Novel Perception and Semantic Mapping Method for Robot Autonomy in Orchards</td>
                              <td>Yaoqiang Pan</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_16748v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.16748v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00168v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00168v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00168v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00168v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a pose-graph attentional graph neural network, called P-GAT, which compares (key)nodes between sequential and non-sequential sub-graphs for place recognition tasks as opposed to a common frame-to-frame retrieval problem formulation currently implemented in SOTA place recognition methods. P-GAT uses the maximum spatial and temporal information between neighbour cloud descriptors -- generated by an existing encoder -- utilising the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph neural network, P-GAT relates point clouds captured in nearby locations in Euclidean space and their embeddings in feature space. Experimental results on the large-scale publically available datasets demonstrate the effectiveness of our approach in scenes lacking distinct features and when training and testing environments have different distributions (domain adaptation). Further, an exhaustive comparison with the state-of-the-art shows improvements in performance gains. Code is available at https://github.com/csiro-robotics/P-GAT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00168v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种位姿图注意力图神经网络，称为P-GAT，它比较用于位置识别任务的顺序子图和非顺序子图之间的（关键）节点，而不是目前在SOTA位置识别方法中实现的常见的逐帧检索问题公式。P-GAT利用姿态图SLAM的概念，使用由现有编码器生成的相邻云描述符之间的最大空间和时间信息。利用内部和相互注意力以及图神经网络，P-GAT将欧几里得空间中附近位置捕获的点云及其在特征空间中的嵌入联系起来。在大规模公开可用数据集上的实验结果证明了我们的方法在缺乏明显特征的场景中以及在训练和测试环境具有不同分布（领域自适应）时的有效性。此外，与最先进的技术进行详尽的比较表明，性能提升有所提高。代码位于https://github.com/csiro-robotics/p-gat.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00168v3" target="_blank">2309.00168v3</a>
                              </td>
                              <td>Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition</td>
                              <td>Milad Ramezani</td>
                              <td>2023-08-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00168v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00168v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01121v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01121v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01121v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01121v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01121v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几何导航是当今机器人学的一个成熟领域，研究重点正转向更高层次的场景理解，如语义映射。当机器人需要与环境互动时，它必须能够理解周围环境的上下文信息。这项工作的重点是对正在建造或已经建造的地图中的对象进行分类和定位。为了进一步探索这一方向，我们提出了一个框架，该框架可以使用多模式传感器融合方法（结合来自RGB-D相机和激光雷达的RGB和深度数据）在已知环境中自主检测和定位预定义对象。该框架由三个关键元素组成：通过RGB数据了解环境，通过多模式传感器融合估计深度，以及管理伪影（即过滤和稳定测量）。实验表明，所提出的框架可以在不进行后处理的情况下准确检测真实样本环境中98%的物体，而85%和80%的物体分别使用单个RGBD相机或RGB+激光雷达装置进行映射。与单传感器（相机或激光雷达）实验的比较表明，传感器融合使机器人能够准确检测远近障碍物，而在纯视觉或基于激光的方法中，这些障碍物可能会有噪声或不精确。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01121v2" target="_blank">2307.01121v2</a>
                              </td>
                              <td>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</td>
                              <td>Federico Rollo</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01121v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01121v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/federicorollo/artifacts_mapping" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12580v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12580v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12580v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12580v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A swarm of robots has advantages over a single robot, since it can explore larger areas much faster and is more robust to single-point failures. Accurate relative positioning is necessary to successfully carry out a collaborative mission without collisions. When Visual Simultaneous Localization and Mapping (VSLAM) is used to estimate the poses of each robot, inter-agent loop closing is widely applied to reduce the relative positioning errors. This technique can mitigate errors using the feature points commonly observed by different robots. However, it requires significant computing and communication capabilities to detect inter-agent loops, and to process the data transmitted by multiple agents. In this paper, we propose Collaborative SLAM using Visual Odometry and Range measurements (CoVOR-SLAM) to overcome this challenge. In the framework of CoVOR-SLAM, robots only need to exchange pose estimates, covariances (uncertainty) of the estimates, and range measurements between robots. Since CoVOR-SLAM does not require to associate visual features and map points observed by different agents, the computational and communication loads are significantly reduced. The required range measurements can be obtained using pilot signals of the communication system, without requiring complex additional infrastructure. We tested CoVOR-SLAM using real images as well as real ultra-wideband-based ranges obtained with two rovers. In addition, CoVOR-SLAM is evaluated with a larger scale multi-agent setup exploiting public image datasets and ranges generated using a realistic simulation. The results show that CoVOR-SLAM can accurately estimate the robots' poses, requiring much less computational power and communication capabilities than the inter-agent loop closing technique.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12580v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一组机器人比单个机器人有优势，因为它可以更快地探索更大的区域，并且对单点故障更具鲁棒性。准确的相对定位对于在没有碰撞的情况下成功执行协作任务是必要的。当使用视觉同步定位与映射（VSLAM）来估计每个机器人的姿态时，广泛应用代理间闭环来减少相对定位误差。该技术可以利用不同机器人通常观察到的特征点来减轻误差。然而，它需要显著的计算和通信能力来检测代理间环路，并处理多个代理传输的数据。在本文中，我们提出了使用视觉里程计和测距的协作SLAM（CoVOR SLAM）来克服这一挑战。在CoVOR-SLAM的框架中，机器人只需要交换姿态估计、估计的协方差（不确定性）和机器人之间的距离测量。由于CoVOR SLAM不需要将不同代理观测到的视觉特征和地图点相关联，因此显著减少了计算和通信负载。所需的距离测量可以使用通信系统的导频信号来获得，而不需要复杂的附加基础设施。我们使用真实图像以及使用两辆漫游车获得的基于真实超宽带的距离来测试CoVOR SLAM。此外，CoVOR SLAM使用更大规模的多智能体设置进行评估，该设置利用真实模拟生成的公共图像数据集和范围。结果表明，与代理间闭环技术相比，CoVOR-SLAM可以准确地估计机器人的姿态，所需的计算能力和通信能力要小得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12580v1" target="_blank">2311.12580v1</a>
                              </td>
                              <td>CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for Multi-Robot Systems</td>
                              <td>Young-Hee Lee</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12580v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12580v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11700v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11700v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11700v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11700v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11700v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了$\textbf{GS-SLAM}$，它首先在同步定位和映射（SLAM）系统中使用3D高斯表示。它有助于更好地平衡效率和准确性。与最近使用神经隐式表示的SLAM方法相比，我们的方法使用了实时可微分的飞溅渲染管道，大大加快了地图优化和RGB-D重新渲染的速度。具体而言，我们提出了一种自适应扩展策略，该策略添加新的或删除有噪声的3D高斯，以有效地重建新的观测场景几何结构并改进先前观测区域的映射。该策略对于扩展3D高斯表示以重建整个场景而不是在现有方法中合成静态对象至关重要。此外，在姿态跟踪过程中，设计了一种有效的从粗到细的技术来选择可靠的3D高斯表示来优化相机姿态，从而减少了运行时间并实现了稳健的估计。与Replica、TUM-RGBD数据集上现有的最先进的实时方法相比，我们的方法实现了具有竞争力的性能。源代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11700v2" target="_blank">2311.11700v2</a>
                              </td>
                              <td>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</td>
                              <td>Chi Yan</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11700v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11700v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11013v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Implicit Event-RGBD Neural SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11013v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11013v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11013v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11013v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>内隐神经SLAM近年来取得了显著的进展。然而，现有方法在非理想场景中面临着重大挑战，如运动模糊或照明变化，这通常会导致收敛失败、定位漂移和失真映射等问题。为了应对这些挑战，我们提出了$\textbf{EN-SLAM}$，这是第一个事件RGBD隐式神经SLAM框架，它有效地利用了事件数据的高速率和高动态范围优势进行跟踪和映射。具体而言，EN-SLAM提出了一种可微分CRF（相机响应函数）渲染技术，通过共享辐射场生成不同的RGB和事件相机数据，该技术通过学习具有捕获事件和RGBD监督的统一隐式表示进行优化。此外，基于事件的时间差特性，我们提出了一种用于事件联合跟踪和全局束调整的时间聚合优化策略，利用事件的连续差约束，显著提高了跟踪的准确性和鲁棒性。最后，我们构建了模拟数据集$\textbf｛DEV Indoors｝$和真实捕获的数据集$_textbf{DEV Reals｝$，其中包含6个场景、17个具有实际运动模糊和照明变化的序列，用于评估。实验结果表明，在各种具有挑战性的环境中，我们的方法在跟踪ATE和映射ACC方面都优于SOTA方法，实时FPS为$17$FPS。代码和数据集将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11013v2" target="_blank">2311.11013v2</a>
                              </td>
                              <td>Implicit Event-RGBD Neural SLAM</td>
                              <td>Delin Qu</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11013v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11013v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12245v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Accurate Loop Closure Detection in Semantic SLAM with 3D Semantic Covisibility Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12245v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12245v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12245v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop closure is necessary for correcting errors accumulated in simultaneous localization and mapping (SLAM) in unknown environments. However, conventional loop closure methods based on low-level geometric or image features may cause high ambiguity by not distinguishing similar scenarios. Thus, incorrect loop closures can occur. Though semantic 2D image information is considered in some literature to detect loop closures, there is little work that compares 3D scenes as an integral part of a semantic SLAM system. This paper introduces an approach, called SmSLAM+LCD, integrated into a semantic SLAM system to combine high-level 3D semantic information and low-level feature information to conduct accurate loop closure detection and effective drift reduction. The effectiveness of our approach is demonstrated in testing results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12245v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环对于校正未知环境中同时定位和映射（SLAM）中积累的误差是必要的。然而，基于低级几何或图像特征的传统闭环方法可能由于不区分类似场景而导致高度模糊。因此，可能会出现错误的循环闭合。尽管在一些文献中考虑了语义2D图像信息来检测环路闭合，但很少有工作将3D场景作为语义SLAM系统的组成部分进行比较。本文介绍了一种称为SmSLAM+LCD的方法，该方法集成到语义SLAM系统中，将高级3D语义信息和低级特征信息相结合，以进行准确的闭环检测和有效的漂移减少。测试结果证明了我们方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12245v1" target="_blank">2311.12245v1</a>
                              </td>
                              <td>Towards Accurate Loop Closure Detection in Semantic SLAM with 3D Semantic Covisibility Graphs</td>
                              <td>Zhentian Qian</td>
                              <td>2023-11-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12245v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12245v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13236v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Autonomous Search of Semantic Objects in Unknown Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13236v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13236v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13236v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper addresses the problem of enabling a robot to search for a semantic object, i.e., an object with a semantic label, in an unknown and GPS-denied environment. For the robot in the unknown environment to detect and find the target semantic object, it must perform simultaneous localization and mapping (SLAM) at both geometric and semantic levels using its onboard sensors while planning and executing its motion based on the ever-updated SLAM results. In other words, the robot must be able to conduct simultaneous localization, semantic mapping, motion planning, and execution in real-time in the presence of sensing and motion uncertainty. This is an open problem as it combines semantic SLAM based on perception and real-time motion planning and execution under uncertainty. Moreover, the goals of the robot motion change on the fly depending on whether and how the robot can detect the target object. We propose a novel approach to tackle the problem, leveraging semantic SLAM, Bayesian Networks, Markov Decision Process, and Real-Time Dynamic Programming. The results in simulation and real experiments demonstrate the effectiveness and efficiency of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13236v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文解决了使机器人能够在未知和GPS拒绝的环境中搜索语义对象的问题，即具有语义标签的对象。为了使机器人在未知环境中检测和找到目标语义对象，它必须使用其机载传感器在几何和语义层面上同时执行定位和映射（SLAM），同时基于不断更新的SLAM结果规划和执行其运动。换句话说，机器人必须能够在存在传感和运动不确定性的情况下实时进行同步定位、语义映射、运动规划和执行。这是一个悬而未决的问题，因为它将基于感知的语义SLAM与不确定性下的实时运动规划和执行相结合。此外，机器人运动的目标在飞行中会根据机器人是否以及如何检测目标物体而变化。我们提出了一种新的方法来解决这个问题，利用语义SLAM、贝叶斯网络、马尔可夫决策过程和实时动态规划。仿真和实际实验结果证明了该方法的有效性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13236v2" target="_blank">2302.13236v2</a>
                              </td>
                              <td>Autonomous Search of Semantic Objects in Unknown Environments</td>
                              <td>Zhentian Qian</td>
                              <td>2023-02-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13236v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13236v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08142v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08142v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08142v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08142v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel optimization-based Visual-Inertial SLAM system designed for multiple partially overlapped camera systems, named MAVIS. Our framework fully exploits the benefits of wide field-of-view from multi-camera systems, and the metric scale measurements provided by an inertial measurement unit (IMU). We introduce an improved IMU pre-integration formulation based on the exponential function of an automorphism of SE_2(3), which can effectively enhance tracking performance under fast rotational motion and extended integration time. Furthermore, we extend conventional front-end tracking and back-end optimization module designed for monocular or stereo setup towards multi-camera systems, and introduce implementation details that contribute to the performance of our system in challenging scenarios. The practical validity of our approach is supported by our experiments on public datasets. Our MAVIS won the first place in all the vision-IMU tracks (single and multi-session SLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the second place.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08142v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于优化的视觉惯性SLAM系统，该系统设计用于多个部分重叠的相机系统，称为MAVIS。我们的框架充分利用了多摄像头系统的宽视场以及惯性测量单元（IMU）提供的公制尺度测量的优势。我们引入了一种基于SE_2（3）自同构的指数函数的改进IMU预积分公式，该公式可以有效地提高在快速旋转运动和延长积分时间下的跟踪性能。此外，我们将为单目或立体设置而设计的传统前端跟踪和后端优化模块扩展到多摄像头系统，并介绍了在具有挑战性的场景中有助于提高系统性能的实现细节。我们在公共数据集上的实验支持了我们方法的实际有效性。我们的MAVIS在2023年Hilti SLAM挑战赛上以1.7倍于第二名的成绩获得了所有视觉IMU曲目（单节和多节SLAM）的第一名。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08142v3" target="_blank">2309.08142v3</a>
                              </td>
                              <td>MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration</td>
                              <td>Yifu Wang</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08142v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08142v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11260v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Radarize: Large-Scale Radar SLAM for Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11260v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11260v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11260v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Radarize, a self-contained SLAM pipeline for indoor environments that uses only a low-cost commodity single-chip mmWave radar. Our radar-native approach leverages phenomena unique to radio frequencies, such as doppler shift-based odometry, to improve performance. We evaluate our method on a large-scale dataset of 146 trajectories spanning 4 campus buildings, totaling approximately 4680m of travel distance. Our results show that our method outperforms state-of-the-art radar-based approaches by approximately 5x in terms of odometry and 8x in terms of end-to-end SLAM, as measured by absolute trajectory error (ATE), without the need additional sensors such as IMUs or wheel odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11260v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Radarize，这是一种适用于室内环境的独立SLAM管道，仅使用低成本的商品单片毫米波雷达。我们的雷达原生方法利用射频特有的现象，如基于多普勒频移的里程计，来提高性能。我们在跨越4栋校园建筑的146条轨迹的大规模数据集上评估了我们的方法，总行程约4680米。我们的结果表明，在不需要IMU或车轮里程计等额外传感器的情况下，通过绝对轨迹误差（ATE）测量，我们的方法在里程计方面比最先进的基于雷达的方法好大约5倍，在端到端SLAM方面好8倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11260v1" target="_blank">2311.11260v1</a>
                              </td>
                              <td>Radarize: Large-Scale Radar SLAM for Indoor Environments</td>
                              <td>Emerson Sie</td>
                              <td>2023-11-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11260v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11260v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_1710_05502v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Non-iterative SLAM for Warehouse Robots Using Ground Textures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_1710_05502v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_1710_05502v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_1710_05502v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel visual SLAM method for the warehouse robot with a single downward-facing camera using ground textures. Traditional methods resort to feature matching or point registration for pose optimization, which easily suffers from repetitive features and poor texture quality. In this paper, we present a robust kernel cross-correlator for robust image-level registration. Compared with the existing methods that often use iterative solutions, our method, named non-iterative visual SLAM (NI-SLAM), has a closed-form solution with a complexity of $O(n\log n)$. This allows it to run very efficiently, yet still provide better accuracy and robustness than the state-of-the-art methods. In the experiments, we demonstrate that it achieves 78% improvement over the state-of-the-art systems for indoor and outdoor localization. We have successfully tested it on warehouse robots equipped with a single downward camera, showcasing its product-ready superiority in a real operating area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_1710_05502v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于地面纹理的仓库机器人视觉SLAM方法，该方法具有单个面向下的摄像头。传统方法采用特征匹配或点配准进行姿态优化，容易出现特征重复、纹理质量差的问题。在本文中，我们提出了一种用于鲁棒图像级配准的鲁棒核互相关器。与现有的经常使用迭代解的方法相比，我们的方法被命名为非迭代视觉SLAM（NI-SLAM），具有复杂度为$O（n\logn）$的闭式解。这使得它能够非常高效地运行，但仍然比最先进的方法提供更好的准确性和稳健性。在实验中，我们证明它在室内和室外定位方面比最先进的系统提高了78%。我们已经在配备了一个向下摄像头的仓库机器人上成功测试了它，展示了它在实际操作领域的产品准备优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/1710.05502v3" target="_blank">1710.05502v3</a>
                              </td>
                              <td>Non-iterative SLAM for Warehouse Robots Using Ground Textures</td>
                              <td>Kuan Xu</td>
                              <td>2017-10-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_1710_05502v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/1710.05502v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sair-lab/ni-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05735v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05735v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05735v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05735v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate perception of objects in the environment is important for improving the scene understanding capability of SLAM systems. In robotic and augmented reality applications, object maps with semantic and metric information show attractive advantages. In this paper, we present RO-MAP, a novel multi-object mapping pipeline that does not rely on 3D priors. Given only monocular input, we use neural radiance fields to represent objects and couple them with a lightweight object SLAM based on multi-view geometry, to simultaneously localize objects and implicitly learn their dense geometry. We create separate implicit models for each detected object and train them dynamically and in parallel as new observations are added. Experiments on synthetic and real-world datasets demonstrate that our method can generate semantic object map with shape reconstruction, and be competitive with offline methods while achieving real-time performance (25Hz). The code and dataset will be available at: https://github.com/XiaoHan-Git/RO-MAP</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05735v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确感知环境中的物体对于提高SLAM系统的场景理解能力非常重要。在机器人和增强现实应用中，具有语义和度量信息的对象地图显示出诱人的优势。在本文中，我们提出了RO-MAP，这是一种新的不依赖于3D先验的多对象映射管道。仅在给定单目输入的情况下，我们使用神经辐射场来表示对象，并将它们与基于多视图几何的轻量级对象SLAM耦合，以同时定位对象并隐式学习其密集几何。我们为每个检测到的对象创建单独的隐式模型，并在添加新的观测值时对其进行动态并行训练。在合成和真实世界数据集上的实验表明，我们的方法可以通过形状重建生成语义对象图，并在实现实时性能（25Hz）的同时与离线方法具有竞争力。代码和数据集将在以下位置提供：https://github.com/xiaohan-git/ro-map</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05735v2" target="_blank">2304.05735v2</a>
                              </td>
                              <td>RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields</td>
                              <td>Xiao Han</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05735v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaohan-git/ro-map" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11016v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SNI-SLAM: Semantic Neural Implicit SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11016v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11016v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11016v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit representation, that simultaneously performs accurate semantic mapping, high-quality surface reconstruction, and robust camera tracking. In this system, we introduce hierarchical semantic representation to allow multi-level semantic comprehension for top-down structured semantic mapping of the scene. In addition, to fully utilize the correlation between multiple attributes of the environment, we integrate appearance, geometry and semantic features through cross-attention for feature collaboration. This strategy enables a more multifaceted understanding of the environment, thereby allowing SNI-SLAM to remain robust even when single attribute is defective. Then, we design an internal fusion-based decoder to obtain semantic, RGB, Truncated Signed Distance Field (TSDF) values from multi-level features for accurate decoding. Furthermore, we propose a feature loss to update the scene representation at the feature level. Compared with low-level losses such as RGB loss and depth loss, our feature loss is capable of guiding the network optimization on a higher-level. Our SNI-SLAM method demonstrates superior performance over all recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in accurate semantic segmentation and real-time semantic mapping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11016v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了SNI-SLAM，这是一种利用神经隐式表示的语义SLAM系统，它同时执行准确的语义映射、高质量的表面重建和稳健的相机跟踪。在这个系统中，我们引入了分层语义表示，以允许对场景的自上而下的结构化语义映射进行多级语义理解。此外，为了充分利用环境的多个属性之间的相关性，我们通过交叉关注来整合外观、几何和语义特征，以进行特征协作。这种策略能够更全面地了解环境，从而使SNI-SLAM即使在单个属性有缺陷的情况下也能保持稳健。然后，我们设计了一个基于内部融合的解码器，从多层次特征中获得语义、RGB、截断有符号距离场（TSDF）值，用于精确解码。此外，我们提出了一种在特征级别更新场景表示的特征损失。与RGB损失和深度损失等低水平损失相比，我们的特征损失能够在更高水平上指导网络优化。我们的SNI-SLAM方法在Replica和ScanNet数据集上的映射和跟踪精度方面优于最近所有基于NeRF的SLAM方法，同时在准确的语义分割和实时语义映射方面也表现出出色的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11016v1" target="_blank">2311.11016v1</a>
                              </td>
                              <td>SNI-SLAM: Semantic Neural Implicit SLAM</td>
                              <td>Siting Zhu</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11016v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11016v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16490v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active SLAM Utility Function Exploiting Path Entropy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16490v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16490v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16490v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article we present a utility function for Active SLAM (A-SLAM) which utilizes map entropy along with D-Optimality criterion metrices for weighting goal frontier candidates. We propose a utility function for frontier goal selection that exploits the occupancy grid map by utilizing the path entropy and favors unknown map locations for maximum area coverage while maintaining a low localization and mapping uncertainties. We quantify the efficiency of our method using various graph connectivity matrices and map efficiency indexes for an environment exploration task. Using simulation and experimental results against similar approaches we achieve an average of 32% more coverage using publicly available data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16490v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了主动SLAM（a-SLAM）的效用函数，该函数利用映射熵和D-最优准则度量来加权目标前沿候选者。我们提出了一种用于边界目标选择的效用函数，该函数通过利用路径熵来利用占用网格图，并在保持低定位和映射不确定性的同时，支持未知地图位置以实现最大区域覆盖。我们使用环境探索任务的各种图连通性矩阵和地图效率指数来量化我们的方法的效率。通过对类似方法的模拟和实验结果，我们使用公开可用的数据集实现了平均32%的覆盖率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16490v2" target="_blank">2309.16490v2</a>
                              </td>
                              <td>Active SLAM Utility Function Exploiting Path Entropy</td>
                              <td>Muhammad Farhan Ahmed</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16490v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16490v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09525v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09525v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09525v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09525v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09525v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经隐式表示已成为在同时定位和映射（SLAM）中提供密集几何的一种有前途的解决方案。然而，这方面的现有方法在全局一致性和低延迟方面存在不足。本文提出NGEL-SLAM来应对上述挑战。为了确保全局一致性，我们的系统利用了传统的基于特征的跟踪模块，该模块包含循环闭合。此外，我们通过使用多个神经隐式场表示场景来保持全局一致性映射，从而能够快速调整循环闭合。此外，我们的系统允许通过使用基于八叉树的隐式表示来快速收敛。对环路闭合的快速响应和快速收敛相结合，使我们的系统成为一个真正的低延迟系统，实现全局一致性。我们的系统能够渲染高保真RGB-D图像，同时提取密集完整的表面。在合成数据集和真实世界数据集上的实验表明，我们的系统在保持低延迟的同时实现了最先进的跟踪和映射精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09525v1" target="_blank">2311.09525v1</a>
                              </td>
                              <td>NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System</td>
                              <td>Yunxuan Mao</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09525v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09525v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_08013v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CP-SLAM: Collaborative Neural Point-based SLAM System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_08013v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_08013v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_08013v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a collaborative implicit neural simultaneous localization and mapping (SLAM) system with RGB-D image sequences, which consists of complete front-end and back-end modules including odometry, loop detection, sub-map fusion, and global refinement. In order to enable all these modules in a unified framework, we propose a novel neural point based 3D scene representation in which each point maintains a learnable neural feature for scene encoding and is associated with a certain keyframe. Moreover, a distributed-to-centralized learning strategy is proposed for the collaborative implicit SLAM to improve consistency and cooperation. A novel global optimization framework is also proposed to improve the system accuracy like traditional bundle adjustment. Experiments on various datasets demonstrate the superiority of the proposed method in both camera tracking and mapping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_08013v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种具有RGB-D图像序列的协作隐式神经同时定位和映射（SLAM）系统，该系统由完整的前端和后端模块组成，包括里程计、环路检测、子图融合和全局细化。为了在统一的框架中实现所有这些模块，我们提出了一种新的基于神经点的3D场景表示，其中每个点都保持用于场景编码的可学习神经特征，并与某个关键帧相关联。此外，针对协作隐式SLAM，提出了一种分布式到集中式的学习策略，以提高一致性和协作性。还提出了一种新的全局优化框架，以像传统的束平差一样提高系统精度。在各种数据集上的实验证明了该方法在相机跟踪和映射方面的优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.08013v1" target="_blank">2311.08013v1</a>
                              </td>
                              <td>CP-SLAM: Collaborative Neural Point-based SLAM System</td>
                              <td>Jiarui Hu</td>
                              <td>2023-11-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_08013v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.08013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_03062v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_03062v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_03062v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_03062v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of a SLAM algorithm with place recognition technology empowers it with the ability to mitigate accumulated errors and to relocalize itself. However, existing methods for point cloud-based place recognition predominantly rely on the matching of descriptors, which are mostly lidar-centric. These methods suffer from two major drawbacks: first, they cannot perform place recognition when the distance between two point clouds is significant, and second, they can only calculate the rotation angle without considering the offset in the X and Y directions. To overcome these limitations, we propose a novel local descriptor that is constructed around the Main Object. By using a geometric method, we can accurately calculate the relative pose. We have provided a theoretical analysis to demonstrate that this method can overcome the aforementioned limitations. Furthermore, we conducted extensive experiments on KITTI Odometry and KITTI360, which indicate that our proposed method has significant advantages over state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_03062v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SLAM算法与位置识别技术的集成使其能够减少累积的错误并重新定位自己。然而，现有的基于点云的位置识别方法主要依赖于描述符的匹配，这些描述符大多以激光雷达为中心。这些方法有两个主要缺点：第一，当两点云之间的距离很大时，它们无法进行位置识别；第二，它们只能计算旋转角度，而不考虑X和Y方向上的偏移。为了克服这些限制，我们提出了一种新的局部描述符，它是围绕主对象构建的。通过使用几何方法，我们可以精确地计算相对姿态。我们提供了一个理论分析来证明这种方法可以克服上述限制。此外，我们在KITTI Odometry和KITTI360上进行了广泛的实验，这表明我们提出的方法比最先进的方法具有显著的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.03062v3" target="_blank">2206.03062v3</a>
                              </td>
                              <td>Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</td>
                              <td>Haodong Yuan</td>
                              <td>2022-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_03062v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.03062v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06659v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06659v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06659v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06659v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a real-time segmentation and reconstruction system that utilizes RGB-D images to generate accurate and detailed individual 3D models of objects within a captured scene. Leveraging state-of-the-art instance segmentation techniques, the system performs pixel-level segmentation on RGB-D data, effectively separating foreground objects from the background. The segmented objects are then reconstructed into distinct 3D models in a high-performance computation platform. The real-time 3D modelling can be applied across various domains, including augmented/virtual reality, interior design, urban planning, road assistance, security systems, and more. To achieve real-time performance, the paper proposes a method that effectively samples consecutive frames to reduce network load while ensuring reconstruction quality. Additionally, a multi-process SLAM pipeline is adopted for parallel 3D reconstruction, enabling efficient cutting of the clustering objects into individuals. This system employs the industry-leading framework YOLO for instance segmentation. To improve YOLO's performance and accuracy, modifications were made to resolve duplicated or false detection of similar objects, ensuring the reconstructed models align with the targets. Overall, this work establishes a robust real-time system with a significant enhancement for object segmentation and reconstruction in the indoor environment. It can potentially be extended to the outdoor scenario, opening up numerous opportunities for real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06659v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种实时分割和重建系统，该系统利用RGB-D图像来生成捕获场景中对象的精确和详细的单个3D模型。利用最先进的实例分割技术，该系统对RGB-D数据进行像素级分割，有效地将前景对象与背景分离。然后在高性能计算平台中将分割的对象重建为不同的3D模型。实时3D建模可以应用于各个领域，包括增强/虚拟现实、室内设计、城市规划、道路辅助、安全系统等。为了实现实时性，本文提出了一种在保证重建质量的同时，对连续帧进行有效采样以减少网络负载的方法。此外，采用多进程SLAM流水线进行并行三维重建，能够有效地将聚类对象切割成个体。该系统采用行业领先的YOLO框架进行细分。为了提高YOLO的性能和准确性，对其进行了修改，以解决类似物体的重复或错误检测，确保重建的模型与目标对准。总的来说，这项工作建立了一个强大的实时系统，大大增强了室内环境中的对象分割和重建。它有可能扩展到户外场景，为现实世界的应用开辟了许多机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06659v1" target="_blank">2311.06659v1</a>
                              </td>
                              <td>3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</td>
                              <td>Xi Sun</td>
                              <td>2023-11-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06659v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06659v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06149v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Visual Odometry Using Genetic Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06149v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06149v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06149v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our work aims to estimate the camera motion mounted on the head of a mobile robot or a moving object from RGB-D images in a static scene. The problem of motion estimation is transformed into a nonlinear least squares function. Methods for solving such problems are iterative. Various classic methods gave an iterative solution by linearizing this function. We can also use the metaheuristic optimization method to solve this problem and improve results. In this paper, a new algorithm is developed for visual odometry using a sequence of RGB-D images. This algorithm is based on a genetic algorithm. The proposed iterative genetic algorithm searches using particles to estimate the optimal motion and then compares it to the traditional methods. To evaluate our method, we use the root mean square error to compare it with the based energy method and another metaheuristic method. We prove the efficiency of our innovative algorithm on a large set of images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06149v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的工作旨在从静态场景中的RGB-D图像中估计安装在移动机器人或移动物体头部的相机运动。将运动估计问题转化为一个非线性最小二乘函数。解决此类问题的方法是迭代的。各种经典方法通过线性化该函数给出了迭代解。我们还可以使用元启发式优化方法来解决这个问题并提高结果。在本文中，利用RGB-D图像序列开发了一种新的视觉里程计算法。该算法基于遗传算法。所提出的迭代遗传算法使用粒子来搜索最优运动，然后将其与传统方法进行比较。为了评估我们的方法，我们使用均方根误差将其与基于能量的方法和另一种元启发式方法进行比较。我们在大量图像上证明了我们的创新算法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06149v1" target="_blank">2311.06149v1</a>
                              </td>
                              <td>Dense Visual Odometry Using Genetic Algorithm</td>
                              <td>Slimane Djema</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06149v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06149v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05600v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FogROS2-Sky: Optimizing Latency and Cost for Multi-Cloud Robot Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05600v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05600v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05600v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper studies the cost-performance tradeoffs in cloud robotics with heterogeneous cloud service providers, which have complex pricing models and varying application requirements. We present FogROS2-Sky, a cost-efficient open source robotics platform that offloads unmodified ROS2 applications to multiple cloud providers and enables fine-grained cost analysis for ROS2 applications' communication with multiple cloud providers. As each provider offers different options for CPU, GPU, memory, and latency, it can be very difficult for users to decide which to choose. FogROS2-Sky includes an optimization algorithm, which either finds the best available hardware specification that fulfills the user's latency and cost constraints or reports that such a specification does not exist. We use FogROS2-Sky to perform time-cost analysis on three robotics applications: visual SLAM, grasp planning, and motion planning. We are able to sample different hardware setups at nearly half the cost while still create cost and latency functions suitable for the optimizer. We also evaluate the optimizer's efficacy for these applications with the Pareto frontier and show that the optimizer selects efficient hardware configurations to balance cost and latency. Videos and code are available on the website https://sites.google.com/view/fogros2-sky</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05600v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了云机器人与异构云服务提供商的性价比权衡，异构云服务供应商具有复杂的定价模型和不同的应用需求。我们介绍了FogROS2-Sky，这是一个具有成本效益的开源机器人平台，它将未修改的ROS2应用程序卸载到多个云提供商，并能够对ROS2应用与多个云供应商的通信进行细粒度成本分析。由于每个提供商提供不同的CPU、GPU、内存和延迟选项，用户很难决定选择哪一个。FogROS2-Sky包括一个优化算法，该算法要么找到满足用户延迟和成本限制的最佳可用硬件规范，要么报告不存在这样的规范。我们使用FogROS2-Sky对三种机器人应用程序进行时间成本分析：视觉SLAM、抓取规划和运动规划。我们能够以几乎一半的成本对不同的硬件设置进行采样，同时仍然创建适合优化器的成本和延迟函数。我们还用Pareto前沿评估了优化器对这些应用程序的功效，并表明优化器选择了有效的硬件配置来平衡成本和延迟。网站上提供了视频和代码https://sites.google.com/view/fogros2-sky</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05600v1" target="_blank">2311.05600v1</a>
                              </td>
                              <td>FogROS2-Sky: Optimizing Latency and Cost for Multi-Cloud Robot Applications</td>
                              <td>Kaiyuan Chen</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05600v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05600v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02831v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02831v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02831v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02831v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02831v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环作为SLAM的关键组成部分之一，在纠正累积误差方面起着至关重要的作用。传统的基于外观的方法，如单词袋模型，往往受到局部2D特征和训练数据量的限制，使其在现实世界场景中的通用性和鲁棒性较差，导致环路闭合中的漏检测或误报检测。为了解决这些问题，我们首先提出了一种基于多级验证的对象级数据关联方法，该方法可以将当前帧的2D语义特征与地图的3D对象地标相关联。接下来，利用这些关联关系，我们介绍了一种基于二次对象映射拓扑的语义环闭合方法，该方法通过对象的拓扑图来表示场景，并通过比较拓扑图的差异来实现宽视场下的精确环闭合。最后，我们将这两种方法集成到一个完整的对象感知SLAM系统中。定性实验和消融研究证明了所提出的对象级数据关联算法的有效性和稳健性。定量实验表明，我们的语义环闭合方法在精度、召回率和定位精度指标方面优于现有的最先进方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02831v3" target="_blank">2311.02831v3</a>
                              </td>
                              <td>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</td>
                              <td>Zhenzhong Cao</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02831v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02831v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03722v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03722v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03722v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03722v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计和同步定位与映射（SLAM）已被研究为计算机视觉和机器人领域最重要的任务之一，有助于实现自主导航和增强现实系统。在基于特征的里程计/SLAM的情况下，移动的视觉传感器从不同的视点观察一组3D点，通常通过特征跟踪和匹配来建立每个图像中投影的2D点之间的对应关系。然而，由于对应点可能是错误的和有噪声的，可靠的不确定性估计可以提高里程计/SLAM方法的准确性。此外，惯性测量单元用于帮助视觉传感器进行视觉惯性融合。在本文中，我们提出了一种使用惯性制导来估计特征对应的不确定性的方法，该惯性制导对运动模糊、照明变化和遮挡引起的图像退化具有鲁棒性。对引导分布进行建模，以采样可能的对应关系，我们将该分布拟合为基于图像误差的能量函数，产生比传统方法更稳健的不确定性。我们还通过将其纳入最近的一种用于公共数据集的视觉惯性里程计/SLAM算法来证明我们的方法的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03722v1" target="_blank">2311.03722v1</a>
                              </td>
                              <td>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry/SLAM</td>
                              <td>Seongwook Yoon</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03722v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03722v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03484v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03484v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03484v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Semi-autonomous mapping with GPS-guided aerial platforms that fly preplanned missions is already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and mission planning. This paper presents Osprey, an autonomous aerial mapping system with state-of-the-art multi-session mapping capabilities. It enables a non-expert operator to specify a bounded target area that the aerial platform can then map autonomously, over multiple flights if necessary. Field experiments with Osprey demonstrate that this system can achieve greater map coverage of large industrial sites than manual surveys with a pilot-flown aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in separate missions using $112$ minutes of autonomous flight time. True colour maps were created from images captured by Osprey using pointcloud and NeRF reconstruction methods. These maps provide useful data for structural inspection tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03484v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>航空测绘系统对许多测量应用（如工业检查或农业监测）都很重要。使用GPS引导的空中平台执行预先计划的任务的半自主地图已经广泛可用，但完全自主的系统可以显著提高效率。自动绘制复杂的三维结构需要一个执行在线绘制和任务规划的系统。本文介绍了Osprey，一种具有最先进的多会话映射能力的自主航空测绘系统。它使非专家操作员能够指定一个有界目标区域，然后空中平台可以在必要时通过多次飞行自主绘制该区域的地图。Osprey的现场实验表明，与使用飞行员飞行的空中平台或地面激光扫描仪（TLS）进行手动测量相比，该系统可以实现更大的大型工业场地地图覆盖范围。三个地点的总地面覆盖面积为7085美元，最高高度为2700美元，在单独的任务中使用112美元的自主飞行时间绘制了地图。使用点云和NeRF重建方法，从Osprey拍摄的图像中创建了真实的彩色地图。这些地图为结构检查任务提供了有用的数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03484v1" target="_blank">2311.03484v1</a>
                              </td>
                              <td>Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</td>
                              <td>Rowan Border</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03484v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03484v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_02327v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_02327v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_02327v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640*480, 346*260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations. The dataset is available at https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_02327v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用多个传感器增强了复杂的环境感知，并提高了对不同亮度条件和高速运动模式的弹性，实现了精确的定位和映射。本文提出了ECMD，这是一个以事件为中心的多传感器数据集，包含81个序列，覆盖了200多公里的各种具有挑战性的驾驶场景，包括高速运动、重复场景、动态物体等。ECMD提供了来自两组不同分辨率的立体事件相机（640*480、346*260）、立体工业相机、红外相机，一个顶部安装的机械激光雷达，带有两个倾斜的激光雷达、两个消费者级GNSS接收器和一个机载IMU。同时，使用厘米级高精度GNSS-RTK/INS导航系统获得了车辆的地面实况。所有传感器都经过了良好的校准，并在硬件级别上进行了时间同步，同时记录数据。我们还评估了几种最先进的SLAM算法，用于对视觉和激光雷达SLAM进行基准测试，并确定其局限性。数据集位于https://arclab-hku.github.io/ecmd/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.02327v1" target="_blank">2311.02327v1</a>
                              </td>
                              <td>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</td>
                              <td>Peiyu Chen</td>
                              <td>2023-11-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_02327v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.02327v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/arclab-hku/event_based_vo-vio-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18917v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18917v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18917v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18917v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>先前将神经辐射场（NeRF）集成到同步定位和映射（SLAM）框架中的尝试要么依赖于静态场景的假设，要么将动态对象视为异常值。然而，现实世界中的大多数场景都是动态的。在本文中，我们提出了一种时变表示来跟踪和重建动态场景。我们的系统同时维护两个过程，跟踪过程和映射过程。对于跟踪过程，对整个输入图像进行均匀采样，并对RGB图像的训练进行自监督。对于映射过程，我们利用已知遮罩来区分动态对象和静态背景，并对两种类型的区域应用不同的采样策略。两个过程的参数优化由两个阶段组成，第一阶段将时间与3D位置相关联，以将变形场转换为规范场。第二种方法将时间与规范场中的三维位置相关联，以获得颜色和符号距离函数（SDF）。此外，我们还提出了一种新的基于重叠率的关键帧选择策略。我们在两个公开可用的合成数据集上评估了我们的方法，并验证了与当前最先进的动态映射方法相比，我们的方法更有效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18917v2" target="_blank">2310.18917v2</a>
                              </td>
                              <td>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</td>
                              <td>Chengyao Duan</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18917v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18917v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转的激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展我们之前仅专注于全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v1" target="_blank">2311.00928v1</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00276v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00276v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00276v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent decades, the field of robotic mapping has witnessed widespread research and development in LiDAR (Light Detection And Ranging)-based simultaneous localization and mapping (SLAM) techniques. In this paper, we review the state-of-the-art in LiDAR-based SLAM and explore the remaining challenges that still require attention to satisfy the needs of contemporary applications. A distinctive aspect of this study lies in its literature survey, which specifically investigates the application of various types and configurations of LiDAR, setting it apart from prior reviews. Furthermore, several representative comparisons of LiDAR-based SLAM algorithms are presented, which can serve as a point of reference. Finally, the paper concludes with an insightful discussion on the emergence of new frontiers in the domain of LiDAR-based SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00276v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近几十年来，基于激光雷达（光探测和测距）的同时定位和测绘（SLAM）技术在机器人测绘领域得到了广泛的研究和发展。在本文中，我们回顾了基于激光雷达的SLAM的最新技术，并探讨了满足当代应用需求仍需关注的剩余挑战。这项研究的一个独特之处在于其文献调查，该调查专门调查了各种类型和配置的激光雷达的应用，使其与先前的综述不同。此外，还对基于激光雷达的SLAM算法进行了一些有代表性的比较，可供参考。最后，本文对基于激光雷达的SLAM领域新前沿的出现进行了深入的讨论。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00276v1" target="_blank">2311.00276v1</a>
                              </td>
                              <td>LiDAR-based SLAM for robotic mapping: state of the art and new frontiers</td>
                              <td>Xiangdi Yue</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00276v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00276v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_02257v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Perception System for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_02257v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_02257v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent surge in interest in autonomous driving stems from its rapidly developing capacity to enhance safety, efficiency, and convenience. A pivotal aspect of autonomous driving technology is its perceptual systems, where core algorithms have yielded more precise algorithms applicable to autonomous driving, including vision-based Simultaneous Localization and Mapping (SLAMs), object detection, and tracking algorithms. This work introduces a visual-based perception system for autonomous driving that integrates trajectory tracking and prediction of moving objects to prevent collisions, while addressing autonomous driving's localization and mapping requirements. The system leverages motion cues from pedestrians to monitor and forecast their movements and simultaneously maps the environment. This integrated approach resolves camera localization and the tracking of other moving objects in the scene, subsequently generating a sparse map to facilitate vehicle navigation. The performance, efficiency, and resilience of this approach are substantiated through comprehensive evaluations of both simulated and real-world datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_02257v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近人们对自动驾驶的兴趣激增，源于其快速发展的提高安全性、效率和便利性的能力。自动驾驶技术的一个关键方面是其感知系统，其中核心算法产生了适用于自动驾驶的更精确的算法，包括基于视觉的同步定位和映射（SLAM）、物体检测和跟踪算法。这项工作介绍了一种基于视觉的自动驾驶感知系统，该系统集成了运动物体的轨迹跟踪和预测，以防止碰撞，同时满足自动驾驶的定位和映射要求。该系统利用行人的运动提示来监测和预测他们的运动，同时绘制环境地图。这种集成方法解决了摄像机定位和场景中其他移动物体的跟踪问题，随后生成稀疏地图以便于车辆导航。通过对模拟和真实世界数据集的综合评估，证实了这种方法的性能、效率和弹性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.02257v2" target="_blank">2303.02257v2</a>
                              </td>
                              <td>Visual Perception System for Autonomous Driving</td>
                              <td>Qi Zhang</td>
                              <td>2023-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_02257v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.02257v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_05927v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_05927v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_05927v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature extraction and matching are the basic parts of many robotic vision tasks, such as 2D or 3D object detection, recognition, and registration. As known, 2D feature extraction and matching have already been achieved great success. Unfortunately, in the field of 3D, the current methods fail to support the extensive application of 3D LiDAR sensors in robotic vision tasks, due to the poor descriptiveness and inefficiency. To address this limitation, we propose a novel 3D feature representation method: Linear Keypoints representation for 3D LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully considers the characteristics (such as the sparsity, and complexity of scenes) of LiDAR point clouds, and represents the keypoint with its robust neighbor keypoints, which provide strong distinction in the description of the keypoint. The proposed LinK3D has been evaluated on two public datasets (i.e., KITTI, Steven VLP16), and the experimental results show that our method greatly outperforms the state-of-the-art in matching performance. More importantly, LinK3D shows excellent real-time performance, faster than the sensor frame rate at 10 Hz of a typical rotating LiDAR sensor. LinK3D only takes an average of 32 milliseconds to extract features from the point cloud collected by a 64-beam LiDAR, and takes merely about 8 milliseconds to match two LiDAR scans when executed in a notebook with an Intel Core i7 @2.2 GHz processor. Moreover, our method can be widely extended to various 3D vision applications. In this paper, we apply the proposed LinK3D to the LiDAR odometry and place recognition task of LiDAR SLAM. The experimental results show that our method can improve the efficiency and accuracy of LiDAR SLAM system.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_05927v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征提取和匹配是许多机器人视觉任务的基本部分，如2D或3D对象检测、识别和配准。众所周知，二维特征提取和匹配已经取得了巨大的成功。不幸的是，在3D领域，由于描述性差和效率低，目前的方法无法支持3D激光雷达传感器在机器人视觉任务中的广泛应用。为了解决这一限制，我们提出了一种新的3D特征表示方法：3D激光雷达点云的线性关键点表示，称为LinK3D。LinK3D的新颖之处在于，它充分考虑了激光雷达点云的特点（如场景的稀疏性和复杂性），并用其稳健的邻居关键点来表示关键点，这在关键点的描述中提供了很强的区分性。所提出的LinK3D已经在两个公共数据集（即KITTI、Steven VLP16）上进行了评估，实验结果表明，我们的方法在匹配性能上大大优于最先进的方法。更重要的是，LinK3D显示出出色的实时性能，比典型旋转激光雷达传感器在10Hz下的传感器帧速率更快。LinK3D从64束激光雷达收集的点云中提取特征平均只需32毫秒，在使用英特尔酷睿i7@2.2 GHz处理器的笔记本电脑中执行时，匹配两次激光雷达扫描仅需约8毫秒。此外，我们的方法可以广泛扩展到各种三维视觉应用中。在本文中，我们将所提出的LinK3D应用于激光雷达SLAM的测距和位置识别任务。实验结果表明，该方法可以提高激光雷达SLAM系统的效率和精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.05927v2" target="_blank">2206.05927v2</a>
                              </td>
                              <td>LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud</td>
                              <td>Yunge Cui</td>
                              <td>2022-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_05927v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.05927v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04563v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Geometry Grounded Deep Structure From Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04563v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04563v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构（SfM）是计算机视觉界的一个长期问题，其目的是从一组不受约束的2D图像中重建场景的相机姿态和3D结构。经典框架通过检测和匹配关键点、配准图像、三角测量3D点和进行束调整，以增量的方式解决了这个问题。最近的研究工作主要围绕着利用深度学习技术的力量来增强特定元素（例如，关键点匹配），但仍基于原始的、不可微分的管道。相反，我们提出了一种新的深度流水线VGGSfM，其中每个组件都是完全可微的，因此可以以端到端的方式进行训练。为此，我们引入了新的机制和简化。首先，我们在深度2D点跟踪的最新进展的基础上提取可靠的像素精确轨迹，这消除了对成对匹配进行链接的需要。此外，我们根据图像和跟踪特征同时恢复所有相机，而不是逐渐注册相机。最后，我们优化相机，并通过可微分束调整层对3D点进行三角测量。我们在三个流行的数据集上获得了最先进的性能，即CO3D、IMC Phototourism和ETH3D。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04563v1" target="_blank">2312.04563v1</a>
                              </td>
                              <td>Visual Geometry Grounded Deep Structure From Motion</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04563v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得足够好的BA初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v2" target="_blank">2311.17245v2</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/fsgs/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/pf-grt/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v2" target="_blank">2310.03704v2</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v2" target="_blank">2306.09012v2</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/cea-list/monoprob</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04634v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04634v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04634v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一种可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04634v1" target="_blank">2311.04634v1</a>
                              </td>
                              <td>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</td>
                              <td>Linus Franke</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04634v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04634v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lfranke/vet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的照片真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。代码位于：https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从内窥镜视频中生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对自监督鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，并且轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中进行高质量的3D对象重建。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从随意图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们认为NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于单目相机重建的现有技术主要依赖于运动结构（SfM）流水线。然而，这种方法往往会产生缺乏关键尺度信息的重建结果，随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，在地图绘制结果中追求精确的缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节水平。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaobaiiiiii/colmap-pcd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05134v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05134v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05134v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是移动机器人的一项关键任务，研究人员正在不断开发新的方法来提高其效率。在本文中，我们提出了一种使用运动结构（SfM）技术提高视觉定位准确性的新方法。我们强调了全局SfM的局限性，它具有高延迟，以及局部SfM面临的挑战，后者需要大型图像数据库才能进行精确重建。为了解决这些问题，我们建议利用神经辐射场（NeRF），而不是图像数据库，来减少存储所需的空间。我们建议，对先前查询位置周围的参考图像进行采样可以带来进一步的改进。我们根据使用激光雷达和高级激光雷达实时测距和测绘（A-LOAM）获得的地面实况评估了我们提出的方法的准确性，并在所进行的实验中比较了其相对于局部SfM和COLMAP的存储使用情况。与地面实况相比，我们提出的方法实现了0.068米的精度，这略低于最先进的方法COLMAP，后者的精度为0.022米。然而，COLMAP所需的数据库大小为400兆字节，而我们的NeRF模型的大小仅为160兆字节。最后，我们进行了消融研究，以评估使用NeRF重建的参考图像的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05134v1" target="_blank">2310.05134v1</a>
                              </td>
                              <td>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</td>
                              <td>Artem Nenashev</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05134v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04145v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04145v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04145v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用未校准的多摄像机系统从运动中构建结构是一项具有挑战性的任务。本文提出了一种束平差解决方案，该解决方案实现了一个基线约束，即这些相机彼此静止。我们假设这些相机安装在移动平台上，未经校准，并且粗略同步。为此，我们提出了针对相机具有重叠视图的场景制定的基线约束。该约束被纳入束调整解决方案中，以保持不同相机的相对运动静止。实验使用安装在车辆上的两个并置GoPro相机的视频帧进行，无需系统校准。这两台摄像机被放置在拍摄重叠内容的位置。我们使用所提出的约束进行了束调整，然后生成了3D密集点云。通过将这些密集点云与激光雷达参考数据进行比较来进行评估。我们表明，与传统的束平差相比，我们提出的方法实现了29.38%的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04145v2" target="_blank">2204.04145v2</a>
                              </td>
                              <td>Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</td>
                              <td>Debao Huang</td>
                              <td>2022-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04145v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04145v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择了最近的DeDoDe关键点，因为它们具有很高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工挑选的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00783v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Propagating Semantic Labels in Video Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00783v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00783v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割结合了两个子任务：像素级图像掩码的识别和对掩码应用语义标签。最近，引入了所谓的基础模型；在非常大的数据集上训练的通用模型，这些数据集可以专门化并应用于更具体的任务。一个这样的模型，分段任意模型（SAM），执行图像分割。像CLIPSeg和MaskRCNN这样的语义分割系统是在成对片段和语义标签的数据集上训练的。但是，手动标记自定义数据非常耗时。这项工作提出了一种对视频中的对象进行分割的方法。一旦在视频帧中找到对象，则可以将该片段传播到未来的帧；从而减少了手动注释的工作量。该方法将SAM与运动结构（SfM）相结合。首先使用SfM将输入到系统的视频重构为3D几何结构。然后使用SAM对视频帧进行分割。然后将SAM识别的片段投影到重建的3D几何体上。在随后的视频帧中，标记的3D几何体被重新投影到新的透视图中，从而减少SAM的调用次数。评估系统性能，包括SAM和SfM组件的贡献。性能通过三个主要指标进行评估：计算时间、带有手动标签的掩码IOU和跟踪丢失数量。结果表明，该系统在视频帧上跟踪对象的计算时间大大提高了人类的性能，但性能较差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00783v1" target="_blank">2310.00783v1</a>
                              </td>
                              <td>Propagating Semantic Labels in Video Data</td>
                              <td>David Balaban</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00783v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00783v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16632v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16632v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we study the problem of minimizing a submodular function $f : 2^V \rightarrow \mathbb{R}$ that is guaranteed to have a $k$-sparse minimizer. We give a deterministic algorithm that computes an additive $\epsilon$-approximate minimizer of such $f$ in $\widetilde{O}(\mathsf{poly}(k) \log(|f|/\epsilon))$ parallel depth using a polynomial number of queries to an evaluation oracle of $f$, where $|f| = \max_{S \subseteq V} |f(S)|$. Further, we give a randomized algorithm that computes an exact minimizer of $f$ with high probability using $\widetilde{O}(|V| \cdot \mathsf{poly}(k))$ queries and polynomial time. When $k = \widetilde{O}(1)$, our algorithms use either nearly-constant parallel depth or a nearly-linear number of evaluation oracle queries. All previous algorithms for this problem either use $\Omega(|V|)$ parallel depth or $\Omega(|V|^2)$ queries.   In contrast to state-of-the-art weakly-polynomial and strongly-polynomial time algorithms for SFM, our algorithms use first-order optimization methods, e.g., mirror descent and follow the regularized leader. We introduce what we call {\em sparse dual certificates}, which encode information on the structure of sparse minimizers, and both our parallel and sequential algorithms provide new algorithmic tools for allowing first-order optimization methods to efficiently compute them. Correspondingly, our algorithm does not invoke fast matrix multiplication or general linear system solvers and in this sense is more combinatorial than previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16632v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了子模函数$f:2^V\rightarrow\mathbb｛R｝$的最小化问题，该子模函数保证具有$k$-稀疏极小值。我们给出了一个确定性算法，该算法使用对$f$的评估预言的多项式查询数来计算$\widetilde｛O｝（\mathsf｛poly｝（k）\log（|f|/\epsilon））$并行深度中的$f$近似极小值，其中$|f|=\max_｛S\substeq V｝|f（S）|$。此外，我们给出了一个随机算法，该算法使用$\widetilde{O}（|V|\cdot\mathsf{poly}（k））$查询和多项式时间以高概率计算$f$的精确极小值。当$k=\widetilde｛O｝（1）$时，我们的算法使用几乎恒定的并行深度或几乎线性数量的评估oracle查询。以前针对此问题的所有算法都使用$\Omega（|V|）$并行深度或$\Omega（|V|^2）$查询。与最先进的SFM弱多项式和强多项式时间算法相比，我们的算法使用一阶优化方法，例如镜像下降和遵循正则化前导。我们介绍了我们所称的｛\em稀疏双证书｝，它对关于稀疏最小化器结构的信息进行编码，并且我们的并行和顺序算法都提供了新的算法工具，允许一阶优化方法有效地计算它们。相应地，我们的算法不调用快速矩阵乘法或一般线性系统求解器，并且在这个意义上比以前最先进的方法更具组合性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16632v1" target="_blank">2309.16632v1</a>
                              </td>
                              <td>Sparse Submodular Function Minimization</td>
                              <td>Andrei Graur</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16632v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16632v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13772v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Segmentation from a Moving Monocular Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13772v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13772v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当存在未知的摄像机运动、不同类型的物体运动和复杂的场景结构时，从移动的单目摄像机中识别和分割移动物体是困难的。为了应对这些挑战，我们利用了单目运动分割方法的两个流行分支：基于点轨迹的方法和基于光流的方法，通过在对象级别协同融合这两个高度互补的运动线索。通过这样做，我们能够同时对不同场景结构中的各种复杂物体运动进行建模，这是现有方法无法实现的。我们首先利用对象识别、分割和跟踪方面的最新基础模型，获得视频中每个常见对象的特定对象点轨迹和光流掩模。然后，我们使用极线几何和光流提供的运动信息构建了两个稳健的仿射矩阵，表示整个视频中的成对对象运动仿射。最后，使用共正则化多视图谱聚类来融合两个亲和矩阵，得到最终的聚类结果。我们的方法在KT3DMoSeg数据集上显示了最先进的性能，该数据集包含复杂的运动和场景结构。当使用视觉SLAM或SFM时，能够识别移动物体使我们能够将其移除以用于地图构建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13772v1" target="_blank">2309.13772v1</a>
                              </td>
                              <td>Motion Segmentation from a Moving Monocular Camera</td>
                              <td>Yuxiang Huang</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13772v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13772v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12804v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12804v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12804v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>珊瑚礁是地球上最多样化的生态系统之一，数亿人依赖珊瑚礁。不幸的是，大多数珊瑚礁都受到全球气候变化和当地人为压力的威胁。为了更好地了解珊瑚礁退化背后的动力学，以高空间和时间分辨率进行监测是关键。然而，由于需要大量的体力劳动，量化珊瑚覆盖率和物种丰度的传统监测方法在规模上受到限制。尽管计算机视觉工具已被用于帮助这一过程，特别是用于3D地图绘制的SfM摄影测量和用于图像分割的深度神经网络，但数据产品的分析造成了瓶颈，有效地限制了其可扩展性。本文提出了一种从自我运动视频映射水下环境的新范式，将使用机器学习来适应水下具有挑战性的条件的3D映射系统与图像语义分割的现代方法相结合。该方法以红海亚喀巴湾北部的珊瑚礁为例，展示了前所未有的高精度3D语义映射，大大降低了所需的劳动力成本：用廉价的消费级相机在潜水5分钟内获取的100米视频样带可以在5分钟内全自动分析。我们的方法通过向视频样带的全自动分析迈出了一大步，大大扩大了珊瑚礁监测的规模。该方法通过减少劳动力、设备、物流和计算成本，使珊瑚礁断面民主化。这有助于更有效地为保护政策提供信息。基于运动结构学习的基本计算方法对珊瑚礁以外的水下环境的快速低成本测绘具有广泛的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12804v1" target="_blank">2309.12804v1</a>
                              </td>
                              <td>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</td>
                              <td>Jonathan Sauder</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12804v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12804v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕捉的同时运行在线SfM，新拍摄的动态图像是用相应的姿势和点进行在线估计的，即，你捕捉到的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以实现在以在线方式拍摄的同时稳健地配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v1" target="_blank">2309.11883v1</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10748v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10748v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10748v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的手-物体交互数据集显示出有限的真实物体可变性，并依赖于拟合MANO参数模型来获得真实的手形。为了超越这些限制并推动进一步的研究，我们引入了SHOWMe数据集，该数据集由96个视频组成，用真实和详细的手对象3D纹理网格进行注释。根据最近的工作，我们考虑了一个刚性手对象场景，其中手相对于对象的姿势在整个视频序列中保持不变。这一假设使我们能够将亚毫米精度的地面实况3D扫描注册到SHOWMe中的图像序列中。尽管更简单，但这一假设在所需精度和细节水平很重要的应用中是有意义的，例如，人机协作中的对象移交、对象扫描或操作和接触点分析。重要的是，手对象系统的刚性允许使用由刚性配准步骤和多视图重建（MVR）部分组成的两阶段流水线来处理未知手持对象的基于视频的3D重建。我们仔细评估了这两个阶段的一组非平凡基线，并表明使用SfM工具箱或手部姿态估计器来恢复刚性变换和现成的MVR算法，可以实现有前景的对象不可知的3D手部对象重建。然而，这些方法对初始相机姿态估计仍然敏感，由于对象上缺乏纹理或手的严重遮挡，初始相机姿态评估可能不精确，这为重建留下了改进的空间。代码和数据集可在https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10748v1" target="_blank">2309.10748v1</a>
                              </td>
                              <td>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</td>
                              <td>Anilkumar Swamy</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10748v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10748v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10269v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10269v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10269v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>非通航河流和蓄水池在缓冲社区免受洪水侵袭方面发挥着重要作用，但应急规划者往往没有数据表明它们在淹没周围地区之前可以携带的水量。本文描述了一种实用的方法，即使用未折叠的海洋表面飞行器（USV）收集水深图和浅水堤的数字表面图，并将其合并为一个统一的体积模型。将泊松曲面重建算法应用于水下表面的稀疏声纳深度读数，开发了水线下网格。河岸的密集水线上网格是使用商业运动结构（SfM）包创建的。由于许多原因，合并是具有挑战性的，最重要的是传感器覆盖范围的差距，即USV无法收集声纳深度数据或视觉上看到通向堤岸的沙滩，因此两个网格可能不会相交。该方法在Hydronalix EMILY USV上进行了演示，该V带有Humminbird单波束回声测深仪和Teledyne FLIR相机，位于德克萨斯州农工工程扩展服务灾难城市综合体的ESTI湖。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10269v1" target="_blank">2309.10269v1</a>
                              </td>
                              <td>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</td>
                              <td>Jayesh Tripathi</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10269v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10269v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08927v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08927v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08927v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用神经辐射场（NeRF）进行动态重建需要精确的相机姿态。这些通常很难用现有的运动结构（SfM）管道来检索，因为相机和场景内容都可能发生变化。我们提出了DynaMoN，它利用同步定位和映射（SLAM）与运动掩蔽相结合来处理动态场景内容。我们基于SLAM的稳健跟踪模块显著加快了动态NeRF的训练过程，同时提高了合成视图的质量。对TUM RGB-D、BONN RGB-D Dynamic和DyCheck的iPhone数据集这三个真实世界的数据集进行了广泛的实验验证，显示了DynaMoN在相机姿态估计和新颖视图合成方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08927v1" target="_blank">2309.08927v1</a>
                              </td>
                              <td>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</td>
                              <td>Mert Asim Karaoglu</td>
                              <td>2023-09-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08927v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08927v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04643v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parallel Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04643v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04643v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query complexity $n^{O(M)}$ and the second method has depth $\widetilde{O}(n^{1/3} M^{2/3})$ and query complexity $O(\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex $\ell_2$-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing $\ell_\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04643v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑了子模函数最小化（SFM）的并行复杂性。我们提供了一对方法，以获得两种新的查询与深度的权衡——在整数值在$-M$和$M$之间的$n$元素子集上定义的子模函数。第一种方法的深度为$2$，查询复杂度为$n^｛O（M）｝$，第二种方法的厚度为$\widetilde｛O｝（n^｛1/3｝M^｛2/3｝）$，查询复杂性为$O（\mathrm｛poly｝（n，M））$。尽管有一系列关于改进的并行SFM下界的工作，但在我们的工作之前，并行SFM的唯一已知算法要么遵循序列SFM的更通用方法，要么遵循凸$\ell_2$-Lipschitz函数的高度并行最小化。有趣的是，为了获得我们的第二个结果，我们提供了在超立方体上最小化$\ell_\infty$-Lipschitz函数的第一个高度并行算法，该算法获得了接近最优的深度，以获得恒定的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04643v1" target="_blank">2309.04643v1</a>
                              </td>
                              <td>Parallel Submodular Function Minimization</td>
                              <td>Deeparnab Chakrabarty</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04643v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04643v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_04147v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_04147v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_04147v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry (VO) and SLAM have been using multi-view geometry via local structure from motion for decades. These methods have a slight disadvantage in challenging scenarios such as low-texture images, dynamic scenarios, etc. Meanwhile, use of deep neural networks to extract high level features is ubiquitous in computer vision. For VO, we can use these deep networks to extract depth and pose estimates using these high level features. The visual odometry task then can be modeled as an image generation task where the pose estimation is the by-product. This can also be achieved in a self-supervised manner, thereby eliminating the data (supervised) intensive nature of training deep neural networks. Although some works tried the similar approach [1], the depth and pose estimation in the previous works are vague sometimes resulting in accumulation of error (drift) along the trajectory. The goal of this work is to tackle these limitations of past approaches and to develop a method that can provide better depths and pose estimates. To address this, a couple of approaches are explored: 1) Modeling: Using optical flow and recurrent neural networks (RNN) in order to exploit spatio-temporal correlations which can provide more information to estimate depth. 2) Loss function: Generative adversarial network (GAN) [2] is deployed to improve the depth estimation (and thereby pose too), as shown in Figure 1. This additional loss term improves the realism in generated images and reduces artifacts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_04147v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几十年来，视觉里程计（VO）和SLAM一直通过运动的局部结构使用多视图几何。这些方法在低纹理图像、动态场景等具有挑战性的场景中稍有不足。同时，使用深度神经网络提取高级特征在计算机视觉中无处不在。对于VO，我们可以使用这些深度网络来提取使用这些高级特征的深度和姿态估计。视觉里程测量任务然后可以被建模为图像生成任务，其中姿态估计是副产品。这也可以以自监督的方式实现，从而消除训练深度神经网络的数据（监督）密集性质。尽管一些工作尝试了类似的方法[1]，但先前工作中的深度和姿态估计是模糊的，有时会导致沿轨迹的误差（漂移）累积。这项工作的目标是解决过去方法的这些局限性，并开发一种可以提供更好深度和姿态估计的方法。为了解决这一问题，我们探索了几种方法：1）建模：使用光流和递归神经网络（RNN）来利用时空相关性，这可以提供更多的信息来估计深度。2） 损失函数：如图1所示，部署生成对抗性网络（GAN）[2]来改进深度估计（从而也提高姿态）。这个额外的损失项提高了生成图像的真实性并减少了伪影。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.04147v1" target="_blank">2309.04147v1</a>
                              </td>
                              <td>Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry</td>
                              <td>Akankshya Kar</td>
                              <td>2023-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_04147v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.04147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Doppelgangers: Learning to Disambiguate Images of Similar Structures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑视觉消歧任务，即确定一对视觉相似的图像是否描绘了相同或不同的3D表面（例如，对称建筑的相同或相反侧）。两幅图像观察到不同但在视觉上相似的3D表面的伪图像匹配，对人类来说可能很难区分，也可能导致3D重建算法产生错误的结果。我们提出了一种基于学习的视觉消歧方法，将其表述为图像对的二元分类任务。为此，我们为这个问题引入了一个新的数据集，即Doppelgangers，它包括具有基本事实标签的相似结构的图像对。我们还设计了一种网络架构，该架构将局部关键点和匹配的空间分布作为输入，从而能够更好地对局部和全局线索进行推理。我们的评估表明，我们的方法可以在困难的情况下区分虚幻的匹配，并可以集成到SfM管道中，以产生正确的、消除歧义的3D重建。有关我们的代码、数据集和更多结果，请参阅我们的项目页面：http://doppelgangers-3d.github.io/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02420v1" target="_blank">2309.02420v1</a>
                              </td>
                              <td>Doppelgangers: Learning to Disambiguate Images of Similar Structures</td>
                              <td>Ruojin Cai</td>
                              <td>2023-09-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02420v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/RuojinCai/Doppelgangers" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/parskatt/dedode</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v2" target="_blank">2308.08479v2</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00526v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00526v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00526v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details with limited generalization. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structures from motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. The self-cost volume implicitly captures the intrinsic geometry of the scene within a single frame. Each individual slice of the volume signifies the relative distances between points and objects within a latent space. Ultimately, this volume is compressed to the depth map via a novel decoding approach. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and $4.5\%$ error reduction from the previous best. In addition, our approach showcases reduced training complexity, computational efficiency, improved generalization, and the ability to recover fine-grained scene details. Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can surpass existing supervised methods by significant margins (AbsRel = $0.043$, $14\%$ error reduction). self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code and the pre-trained weights will be publicly available. Code is available at \href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00526v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，自监督单目深度估计在自动驾驶和机器人技术中得到了广泛应用。然而，现有的解决方案主要寻求从即时视觉特征估计深度，并且难以在有限的泛化能力下恢复细粒度的场景细节。在本文中，我们介绍了SQLdepth，这是一种可以有效地从运动中学习细粒度场景结构的新方法。在SQLdepth中，我们提出了一种新颖的自查询层（SQL）来构建自成本体积并从中推断深度，而不是从特征图中推断深度。自成本体积隐含地捕捉单个帧内场景的固有几何体。体积的每个单独切片表示潜在空间内的点和对象之间的相对距离。最终，通过一种新颖的解码方法将该体积压缩到深度图中。在KITTI和Cityscapes上的实验结果表明，我们的方法获得了显著的最先进的性能（AbsRel在KITTI上为0.082$，在具有改进的地面实况的KITTI上为0.052$，而在Cityscape上为0.106$），比以前的最佳方法实现了9.9\%$、5.5\%$和4.5\%$的误差减少。此外，我们的方法展示了降低的训练复杂性、计算效率、改进的泛化能力以及恢复细粒度场景细节的能力。此外，自监督预训练和度量微调的SQLdepth可以以显著的优势超过现有的监督方法（AbsRel=0.043$，误差减少$14\%$）。SQL中面向自匹配的相对距离查询提高了SQLdepth的鲁棒性和零样本泛化能力。代码和预先训练的重量将公开。代码位于\ href{https://github.com/hisfog/sqldepth-impl}{https://github.com/hisfog/sqldepth-impl}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00526v1" target="_blank">2309.00526v1</a>
                              </td>
                              <td>SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation</td>
                              <td>Youhong Wang</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00526v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00526v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_00487v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Object at a Time: Accurate and Robust Structure From Motion for Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_00487v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_00487v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles. Project page: https://oxidification.com/p/one-object-at-a-time/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_00487v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注视机器人可以立即、准确、稳健地感知到被注视物体的距离和周围物体的相对位置。我们展示了固定，即在移动时看着一个物体的行为，是如何利用三维空间几何中的规律来获得这些信息的。这些规律引入了旋转-平移耦合，这在结构运动中并不常见。为了验证，我们使用了带有RGB相机的Franka Emika机器人。我们a）发现，在15厘米的距离上，距离估计的误差小于5毫米，b）展示了在具有挑战性的场景下如何使用相对位置来寻找障碍物。我们将准确的距离估计和障碍物信息结合到反应机器人行为中，该行为能够拾取未知大小的物体，同时受到不可预见的障碍物的阻碍。项目页面：https://oxidification.com/p/one-object-at-a-time/。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.00487v3" target="_blank">2208.00487v3</a>
                              </td>
                              <td>One Object at a Time: Accurate and Robust Structure From Motion for Robots</td>
                              <td>Aravind Battaje</td>
                              <td>2022-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_00487v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.00487v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_00385v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Voxel 3D Reconstruction Using a Monocular Event Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_00385v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_00385v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view Stereo (MVS) methods. In this paper, we propose a novel approach for solving dense 3D reconstruction using only a single event camera. To the best of our knowledge, our work is the first attempt in this regard. Our preliminary results demonstrate that the proposed method can produce visually distinguishable dense 3D reconstructions directly without requiring pipelines like those used by existing methods. Additionally, we have created a synthetic dataset with $39,739$ object scans using an event camera simulator. This dataset will help accelerate other relevant research in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_00385v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>事件摄像机是受生物系统启发，专门捕捉亮度变化的传感器。与传统的基于帧的相机相比，这些新兴相机具有许多优势，包括高动态范围、高帧速率和极低功耗。由于这些优势，事件摄像机越来越多地应用于各个领域，如帧插值、语义分割、里程计和SLAM。然而，它们在VR应用的3D重建中的应用还没有得到充分的探索。该领域以前的方法主要集中在通过深度图估计进行三维重建。产生密集3D重建的方法通常需要多个相机，而利用单个事件相机的方法只能产生半密集的结果。可以产生密集3D重建的其他单相机方法依赖于创建管道，该管道结合了上述方法或其他现有的运动结构（SfM）或多视图立体（MVS）方法。在本文中，我们提出了一种仅使用单个事件相机来解决密集三维重建的新方法。据我们所知，我们的工作是这方面的第一次尝试。我们的初步结果表明，所提出的方法可以直接产生视觉上可区分的密集三维重建，而不需要像现有方法那样使用管道。此外，我们还使用事件相机模拟器创建了一个合成数据集，其中包含39739美元的对象扫描。该数据集将有助于加速该领域的其他相关研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.00385v1" target="_blank">2309.00385v1</a>
                              </td>
                              <td>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</td>
                              <td>Haodong Chen</td>
                              <td>2023-09-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_00385v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.00385v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10902v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CamP: Camera Preconditioning for Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10902v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10902v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10902v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可以优化神经辐射场（NeRF）以获得物体和大规模场景的高保真3D场景重建。然而，NeRF需要精确的相机参数作为输入——不准确的相机参数会导致渲染模糊。通常使用运动结构（SfM）方法作为NeRF的预处理步骤来估计外部和内部相机参数，但这些技术很少产生完美的估计。因此，先前的工作已经提出与NeRF一起联合优化相机参数，但这些方法在具有挑战性的设置中容易出现局部最小值。在这项工作中，我们分析了不同的相机参数化如何影响这个联合优化问题，并观察到标准参数化相对于小扰动在大小上表现出很大的差异，这可能导致病态优化问题。我们建议使用代理问题来计算白化变换，该变换消除了相机参数之间的相关性并归一化了它们的效果，并且我们建议在联合优化期间使用该变换作为相机参数的预处理器。我们的预处理相机优化显著提高了Mip-NeRF 360数据集场景的重建质量：与不优化Zip-NeRF等相机的最先进NeRF方法相比，我们将错误率（RMSE）降低了67%，与使用SCNeRF相机参数化的最先进联合优化方法相比，降低了29%。我们的方法易于实现，不会显著增加运行时间，可以应用于各种相机参数化，并且可以直接集成到其他类似NeRF的模型中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10902v2" target="_blank">2308.10902v2</a>
                              </td>
                              <td>CamP: Camera Preconditioning for Neural Radiance Fields</td>
                              <td>Keunhong Park</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10902v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10902v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_13903v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Disjoint Pose and Shape for 3D Face Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_13903v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_13903v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing methods for 3D face reconstruction from a few casually captured images employ deep learning based models along with a 3D Morphable Model(3DMM) as face geometry prior. Structure From Motion(SFM), followed by Multi-View Stereo (MVS), on the other hand, uses dozens of high-resolution images to reconstruct accurate 3D faces.However, it produces noisy and stretched-out results with only two views available. In this paper, taking inspiration from both these methods, we propose an end-to-end pipeline that disjointly solves for pose and shape to make the optimization stable and accurate. We use a face shape prior to estimate face pose and use stereo matching followed by a 3DMM to solve for the shape. The proposed method achieves end-to-end topological consistency, enables iterative face pose refinement procedure, and show remarkable improvement on both quantitative and qualitative results over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_13903v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的从一些随意捕捉的图像进行3D人脸重建的方法使用基于深度学习的模型以及3D变形模型（3DMM）作为人脸几何先验。另一方面，“运动结构”（SFM）和“多视图立体”（MVS）使用数十幅高分辨率图像来重建精确的3D人脸。然而，在只有两个视图可用的情况下，它会产生嘈杂和拉伸的结果。在本文中，我们从这两种方法中获得灵感，提出了一种端到端的流水线，该流水线对姿态和形状进行不相交求解，以使优化稳定准确。我们在估计人脸姿态之前使用人脸形状，并使用立体匹配和3DMM来求解形状。所提出的方法实现了端到端的拓扑一致性，实现了迭代人脸姿态精化过程，并在定量和定性结果上都比现有的最先进的方法有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.13903v1" target="_blank">2308.13903v1</a>
                              </td>
                              <td>Disjoint Pose and Shape for 3D Face Reconstruction</td>
                              <td>Raja Kumar</td>
                              <td>2023-08-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_13903v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.13903v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/raja-kumar/disjoint_pose_shape_for_3D_face" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10003v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10003v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10003v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recovering the shape and appearance of real-world objects from natural 2D images is a long-standing and challenging inverse rendering problem. In this paper, we introduce a novel hybrid differentiable rendering method to efficiently reconstruct the 3D geometry and reflectance of a scene from multi-view images captured by conventional hand-held cameras. Our method follows an analysis-by-synthesis approach and consists of two phases. In the initialization phase, we use traditional SfM and MVS methods to reconstruct a virtual scene roughly matching the real scene. Then in the optimization phase, we adopt a hybrid approach to refine the geometry and reflectance, where the geometry is first optimized using an approximate differentiable rendering method, and the reflectance is optimized afterward using a physically-based differentiable rendering method. Our hybrid approach combines the efficiency of approximate methods with the high-quality results of physically-based methods. Extensive experiments on synthetic and real data demonstrate that our method can produce reconstructions with similar or higher quality than state-of-the-art methods while being more efficient.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10003v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自然2D图像中恢复真实世界对象的形状和外观是一个长期存在且具有挑战性的反向渲染问题。在本文中，我们介绍了一种新的混合可微绘制方法，以从传统手持相机拍摄的多视图图像中有效地重建场景的3D几何结构和反射率。我们的方法遵循综合分析法，由两个阶段组成。在初始化阶段，我们使用传统的SfM和MVS方法来重建与真实场景大致匹配的虚拟场景。然后在优化阶段，我们采用混合方法来细化几何体和反射率，其中首先使用近似可微渲染方法优化几何体，然后使用基于物理的可微渲染法优化反射率。我们的混合方法将近似方法的效率与基于物理的方法的高质量结果相结合。对合成数据和真实数据的大量实验表明，我们的方法可以产生与最先进方法相似或更高质量的重建，同时更高效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10003v1" target="_blank">2308.10003v1</a>
                              </td>
                              <td>Efficient Multi-View Inverse Rendering Using a Hybrid Differentiable Rendering Method</td>
                              <td>Xiangyang Zhu</td>
                              <td>2023-08-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10003v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10003v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/HsiangYangChu/DRBIR" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion显著优于经典的SfM管道和学习的方法。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v3" target="_blank">2306.15667v3</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/PoseDiffusion" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_10705v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_10705v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_10705v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the previous 3D human pose estimation work relied on the powerful memory capability of the network to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_10705v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以前的大多数3D人体姿态估计工作都依赖于网络强大的存储能力来从训练数据中获得合适的2D-3D映射。很少有工作研究人体运动中姿势变形的建模。在本文中，我们提出了一种新的人体姿态变形建模方法，并设计了一种基于扩散的运动先验。受运动中非刚性结构领域的启发，我们将重建运动中的三维人体骨骼的任务分为三维参考骨骼的估计和逐帧骨骼变形。使用混合时空NRSfMformer从2D观测序列中同时估计3D参考骨架和每个帧的骨架变形，然后将它们相加以获得每个帧的姿态。随后，使用基于扩散模型的损失项来确保管道学习正确的先验运动知识。最后，我们在主流数据集上评估了我们提出的方法，并获得了优于现有技术的优越结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.10705v1" target="_blank">2308.10705v1</a>
                              </td>
                              <td>Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</td>
                              <td>Haorui Ji</td>
                              <td>2023-08-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_10705v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.10705v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的拉普拉斯算子之间的理论关系。我们利用这些结果来设计一个协作求解器，在该求解器中，机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与精度之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们还证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v4" target="_blank">2210.05020v4</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01246v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01246v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01246v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01246v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文化遗产（CH）遗址的数字保护对于保护它们免受自然灾害或人类活动的破坏至关重要。由于计算机视觉和摄影测量的进步，创建CH遗址的3D模型已成为一种流行的数字保存方法。然而，这一过程耗时、昂贵，通常需要专门的设备和专业知识，这对资源有限的发展中国家构成了挑战。此外，缺乏开放的3D模型存储库阻碍了研究和公众对其遗产的参与。为了解决这些问题，我们提出了Tirtha，一个用于众包CH网站图像并创建其3D模型的网络平台。Tirtha采用了最先进的运动结构（SfM）和多视图立体（MVS）技术。它是模块化的、可扩展的和具有成本效益的，允许随着摄影测量的发展而结合新技术。Tirtha可通过web界面访问，网址为https://tirtha.niser.ac.in并且可以在内部部署或在云环境中部署。在我们的案例研究中，我们通过使用众包图像创建印度奥迪沙寺庙的3D模型来证明该管道的有效性。这些模型可在Tirtha网站上查看、交互和下载。我们的工作旨在为计算机视觉、遗产保护和相关领域的研究提供众包图像和3D重建的数据集。总的来说，Tirtha是朝着数字保护民主化迈出的一步，主要是在资源有限的发展中国家。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01246v2" target="_blank">2308.01246v2</a>
                              </td>
                              <td>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</td>
                              <td>Jyotirmaya Shivottam</td>
                              <td>2023-08-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01246v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01246v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/smlab-niser/tirtha-public" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04556v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models for Mathematicians</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04556v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04556v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04556v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LMMs to change how mathematicians work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04556v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像ChatGPT这样的大型语言模型（LLM）因其通用语言理解，特别是其生成高质量文本或计算机代码的能力而受到极大的兴趣。对于许多职业来说，LLM是一种宝贵的工具，可以加快和提高工作质量。在这篇文章中，我们讨论了它们在多大程度上可以帮助专业数学家。我们首先提供所有现代语言模型中使用的转换器模型的数学描述。基于最近的研究，我们概述了最佳实践和潜在问题，并报告了语言模型的数学能力。最后，我们揭示了LMM改变数学家工作方式的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04556v1" target="_blank">2312.04556v1</a>
                              </td>
                              <td>Large Language Models for Mathematicians</td>
                              <td>Simon Frieder</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04556v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04556v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04554v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improved Visual Grounding through Self-Consistent Explanations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04554v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04554v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04554v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --"grounding"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04554v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过训练以将图像与文本匹配的视觉和语言模型可以与视觉解释方法相结合，以指向图像中特定对象的位置。我们的工作表明，通过微调以实现自洽的视觉解释，可以进一步提高这些模型的定位能力。我们提出了一种使用大型语言模型用转述扩充现有文本图像数据集的策略，以及SelfEQ，这是一种在视觉解释图上的弱监督策略，用于鼓励自一致性的转述。具体来说，对于输入的文本短语，我们试图生成一个转述并微调模型，使短语和转述映射到图像中的同一区域。我们假设，这既扩展了模型能够处理的词汇表，又提高了基于梯度的视觉解释方法（例如GradCAM）突出显示的对象位置的质量。我们展示了SelfEQ在Flickr30k、ReferIt和RefCOCO+上的性能优于强基线方法和之前的几项工作。特别是，与其他不使用任何类型的框注释的方法相比，我们在Flickr30k上分别获得84.07%（4.69%的绝对改进），在ReferIt上获得67.40%（7.68%的绝对改善），以及在RefCOCO+测试集A和B上分别获得75.10%和55.49%（平均3.74%的绝对改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04554v1" target="_blank">2312.04554v1</a>
                              </td>
                              <td>Improved Visual Grounding through Self-Consistent Explanations</td>
                              <td>Ruozhen He</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04554v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04554v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04552v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generating Illustrated Instructions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04552v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04552v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04552v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04552v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了生成图解说明的新任务，即根据用户需求定制的视觉说明。我们确定了这项任务特有的需求，并通过一套自动和人工评估指标将其形式化，旨在衡量几代人的有效性、一致性和功效。我们将大型语言模型（LLM）的强大功能与强文本到图像的生成扩散模型相结合，提出了一种称为StackedDiffusion的简单方法，该方法在给定文本作为输入的情况下生成这样的说明性指令。由此产生的模型大大优于基线方法和最先进的多模式LLM；在30%的情况下，用户甚至更喜欢它而不是人工生成的文章。最值得注意的是，它实现了各种新的、令人兴奋的应用程序，远远超出了网络上静态文章所能提供的范围，例如个性化指令，以及响应用户个人情况的中间步骤和图片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04552v1" target="_blank">2312.04552v1</a>
                              </td>
                              <td>Generating Illustrated Instructions</td>
                              <td>Sachit Menon</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04552v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04552v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04539v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Guided Open-Vocabulary Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04539v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04539v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04539v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Models (VLMs) have emerged as promising tools for open-ended image understanding tasks, including open vocabulary segmentation. Yet, direct application of such VLMs to segmentation is non-trivial, since VLMs are trained with image-text pairs and naturally lack pixel-level granularity. Recent works have made advancements in bridging this gap, often by leveraging the shared image-text space in which the image and a provided text prompt are represented. In this paper, we challenge the capabilities of VLMs further and tackle open-vocabulary segmentation without the need for any textual input. To this end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework. Self-Seg is capable of automatically detecting relevant class names from clustered BLIP embeddings and using these for accurate semantic segmentation. In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to effectively assess predicted open-vocabulary class names. We achieve state-of-the-art results on Pascal VOC, ADE20K and CityScapes for open-vocabulary segmentation without given class names, as well as competitive performance with methods where class names are given. All code and data will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04539v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM）已成为开放式图像理解任务的有前途的工具，包括开放式词汇分割。然而，将这种VLM直接应用于分割是不平凡的，因为VLM是用图像-文本对训练的，并且自然缺乏像素级粒度。最近的工作在弥合这一差距方面取得了进展，通常是通过利用共享的图像-文本空间来表示图像和提供的文本提示。在本文中，我们进一步挑战VLM的能力，并在不需要任何文本输入的情况下解决开放式词汇分割问题。为此，我们提出了一种新的自引导语义分割（Self-Seg）框架。Self-Seg能够从集群BLIP嵌入中自动检测相关类名，并使用这些类名进行准确的语义分割。此外，我们提出了一种基于LLM的开放词汇评估器（LOVE）来有效地评估预测的开放词汇类名。我们在Pascal VOC、ADE20K和CityScapes上实现了最先进的结果，用于在没有给定类名的情况下进行开放式词汇分割，以及使用给定类名的方法的竞争性能。所有代码和数据都将发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04539v1" target="_blank">2312.04539v1</a>
                              </td>
                              <td>Self-Guided Open-Vocabulary Semantic Segmentation</td>
                              <td>Osman Ülger</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04539v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04539v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04535v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Trajeglish: Learning the Language of Driving Scenarios</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04535v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04535v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04535v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04535v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动驾驶开发的一个长期挑战是模拟记录驾驶日志中的动态驾驶场景。为了实现这一功能，我们应用离散序列建模工具来建模车辆、行人和骑自行车的人在驾驶场景中的互动方式。使用一个简单的数据驱动的标记化方案，我们使用一个小词汇将轨迹离散到厘米级的分辨率。然后，我们用类似GPT的编码器-解码器对运动令牌的多智能体序列进行建模，该编码器-解码器在时间上是自回归的，并考虑了智能体之间的时步内交互。从我们的模型中采样的场景展现了最先进的现实主义；我们的模型在Waymo Sim Agents基准测试中名列前茅，在现实主义元指标和交互指标方面分别超过了之前的工作3.3%和9.9%。我们在完全自主和部分自主的设置中阐述了我们的建模选择，并表明我们的模型所学的表示可以快速调整，以提高nuScenes的性能。我们还评估了我们的模型在参数计数和数据集大小方面的可扩展性，并使用我们模型的密度估计来量化流量建模任务的上下文长度和时间步长内交互的显著性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04535v1" target="_blank">2312.04535v1</a>
                              </td>
                              <td>Trajeglish: Learning the Language of Driving Scenarios</td>
                              <td>Jonah Philion</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04535v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04535v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04528v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using Large Language Models for Hyperparameter Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04528v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04528v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04528v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper studies using foundational large language models (LLMs) to make decisions during hyperparameter optimization (HPO). Empirical evaluations demonstrate that in settings with constrained search budgets, LLMs can perform comparably or better than traditional HPO methods like random search and Bayesian optimization on standard benchmarks. Furthermore, we propose to treat the code specifying our model as a hyperparameter, which the LLM outputs, going beyond the capabilities of existing HPO approaches. Our findings suggest that LLMs are a promising tool for improving efficiency in the traditional decision-making problem of hyperparameter optimization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04528v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了在超参数优化（HPO）过程中使用基础大型语言模型（LLM）进行决策。经验评估表明，在搜索预算有限的情况下，LLM可以在标准基准上与随机搜索和贝叶斯优化等传统HPO方法相比或更好地执行。此外，我们建议将指定我们的模型的代码视为LLM输出的超参数，这超出了现有HPO方法的能力。我们的研究结果表明，在传统的超参数优化决策问题中，LLM是一种很有前途的提高效率的工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04528v1" target="_blank">2312.04528v1</a>
                              </td>
                              <td>Using Large Language Models for Hyperparameter Optimization</td>
                              <td>Michael R. Zhang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04528v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04528v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03514v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Large Language Models Transform Computational Social Science?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03514v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03514v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03514v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in} social science analysis in partnership with humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03514v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）能够成功地执行许多语言处理任务零样本（无需训练数据）。如果零样本LLM也能可靠地对说服力和政治意识形态等社会现象进行分类和解释，那么LLM可以在重要方面增强计算社会科学（CSS）的管道。这项工作为使用LLM作为CSS工具提供了一个路线图。为此，我们提供了一组提示最佳实践和一个广泛的评估管道，以衡量13个语言模型在25个具有代表性的英语CSS基准上的零样本性能。在分类标记任务（分类）中，LLM未能优于最佳微调模型，但仍与人类达成了相当程度的一致。在自由形式的编码任务（生成）中，LLM产生的解释往往超过众包工作者的黄金参考的质量。我们得出的结论是，当今LLM的性能可以通过两种方式增强CSS研究管道：（1）在人类注释团队中充当零样本数据注释器，以及（2）引导具有挑战性的创造性生成任务（例如，解释文本的基本属性）。总之，LLM被要求与人类合作，有意义地参与社会科学分析。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03514v2" target="_blank">2305.03514v2</a>
                              </td>
                              <td>Can Large Language Models Transform Computational Social Science?</td>
                              <td>Caleb Ziems</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03514v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03514v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/salt-nlp/llms_for_css" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04511v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An LLM Compiler for Parallel Function Calling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04511v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04511v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04511v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04511v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经在各种复杂的推理基准上显示出显著的结果。LLM的推理能力使它们能够执行函数调用，使用用户提供的函数来克服其固有的局限性，如知识截断、算术技能差或无法访问私人数据。这一发展扩大了LLM的范围，包括多功能调用，LLM配备了各种功能，并根据上下文选择适当的功能。LLM的多功能调用能力促进了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多功能调用方法通常需要对每个函数进行顺序推理和操作，这可能会导致高延迟、高成本，有时还会导致不准确的行为。为了解决这个问题，我们引入了LLMCompiler，它并行执行函数以有效地协调多功能调用。LLMCompiler借鉴了经典编译器的原理，通过三个组件简化了并行函数调用：（i）LLM Planner，制定执行策略和依赖关系；（ii）任务获取单元，用于调度调用任务的功能；以及（iii）并行执行这些任务的执行器。LLMCompiler自动计算函数调用的优化编排，并可与LLaMA-2等开源模型一起使用。我们已经在一系列任务上对LLMCompiler进行了基准测试，包括函数调用之间具有非平凡相互依赖性的情况，以及需要基于中间结果进行动态重新规划的情况。我们观察到，与ReAct相比，一致的延迟加速高达3.7倍，成本节约高达6.7倍，准确性提高高达~9%。此外，LLMCompiler实现了比OpenAI最近的并行函数调用高达1.35倍的延迟增益，同时实现了类似的精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04511v1" target="_blank">2312.04511v1</a>
                              </td>
                              <td>An LLM Compiler for Parallel Function Calling</td>
                              <td>Sehoon Kim</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04511v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04511v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/squeezeailab/llmcompiler" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04510v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04510v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04510v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04510v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04510v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，基于能量的语言建模是可控文本生成的有效框架，因为它能够灵活集成任意鉴别器。然而，由于基于能量的LMs是全局归一化的，因此需要像Metropolis Hastings（MH）这样的近似技术来进行推理。过去的工作在很大程度上探索了一次修改单个令牌的简单提议分布，比如吉布斯采样。在本文中，我们开发了一种新的MH采样器，相比之下，它提出通过大型语言模型的迭代提示在每个步骤中重写整个序列。我们的新采样器（a）允许从目标分布进行更高效和准确的采样，（b）允许通过采样程序确定生成长度，而不是像过去的工作所要求的那样预先固定。我们在两个受控生成任务上进行了实验，与单令牌提议技术相比，显示了下游性能增益和更准确的目标分布采样。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04510v1" target="_blank">2312.04510v1</a>
                              </td>
                              <td>A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation</td>
                              <td>Jarad Forristal</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04510v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04510v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04494v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04494v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04494v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04494v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With recent advances in multi-modal foundation models, the previously text-only large language models (LLM) have evolved to incorporate visual input, opening up unprecedented opportunities for various applications in visualization. Our work explores the utilization of the visual perception ability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs) that can interpret and accomplish user-defined visualization objectives through natural language. We propose the first framework for the design of AVAs and present several usage scenarios intended to demonstrate the general applicability of the proposed paradigm. The addition of visual perception allows AVAs to act as the virtual visualization assistant for domain experts who may lack the knowledge or expertise in fine-tuning visualization outputs. Our preliminary exploration and proof-of-concept agents suggest that this approach can be widely applicable whenever the choices of appropriate visualization parameters require the interpretation of previous visual output. Feedback from unstructured interviews with experts in AI research, medical visualization, and radiology has been incorporated, highlighting the practicality and potential of AVAs. Our study indicates that AVAs represent a general paradigm for designing intelligent visualization systems that can achieve high-level visualization goals, which pave the way for developing expert-level visualization agents in the future.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04494v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着多模态基础模型的最新进展，以前的纯文本大型语言模型（LLM）已经发展到包含视觉输入，为可视化中的各种应用开辟了前所未有的机会。我们的工作探索了利用多模态LLM的视觉感知能力来开发自主可视化代理（AVA），该代理可以通过自然语言解释和实现用户定义的可视化目标。我们提出了AVA设计的第一个框架，并提出了几个使用场景，旨在证明所提出的范式的普遍适用性。视觉感知的添加使AVA能够充当领域专家的虚拟可视化助手，这些领域专家可能缺乏微调可视化输出的知识或专业知识。我们的初步探索和概念验证代理表明，只要选择适当的可视化参数需要解释以前的视觉输出，这种方法就可以广泛应用。纳入了对人工智能研究、医学可视化和放射学专家的非结构化采访的反馈，突出了AVA的实用性和潜力。我们的研究表明，AVAs代表了设计智能可视化系统的通用范式，可以实现高级可视化目标，为未来开发专家级可视化代理铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04494v1" target="_blank">2312.04494v1</a>
                              </td>
                              <td>AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making</td>
                              <td>Shusen Liu</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04494v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04494v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_09820v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Stability Analysis of Fine-Tuning a Pre-Trained Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_09820v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_09820v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_09820v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT, etc.) has proven to be one of the most promising paradigms in recent NLP research. However, numerous recent works indicate that fine-tuning suffers from the instability problem, i.e., tuning the same model under the same setting results in significantly different performance. Many recent works have proposed different methods to solve this problem, but there is no theoretical understanding of why and how these methods work. In this paper, we propose a novel theoretical stability analysis of fine-tuning that focuses on two commonly used settings, namely, full fine-tuning and head tuning. We define the stability under each setting and prove the corresponding stability bounds. The theoretical bounds explain why and how several existing methods can stabilize the fine-tuning procedure. In addition to being able to explain most of the observed empirical discoveries, our proposed theoretical analysis framework can also help in the design of effective and provable methods. Based on our theory, we propose three novel strategies to stabilize the fine-tuning procedure, namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self Unsupervised Re-Training (SURT). We extensively evaluate our proposed approaches on 11 widely used real-world benchmark datasets, as well as hundreds of synthetic classification datasets. The experiment results show that our proposed methods significantly stabilize the fine-tuning procedure and also corroborate our theoretical analysis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_09820v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>微调预先训练的模型（如BERT、ALBERT、RoBERTa、T5、GPT等）已被证明是最近NLP研究中最有前途的范式之一。然而，最近的许多工作表明，微调存在不稳定性问题，即在相同设置下调整同一模型会导致显著不同的性能。最近的许多工作都提出了不同的方法来解决这个问题，但对于这些方法为什么以及如何工作，还没有理论上的理解。在本文中，我们提出了一种新的微调理论稳定性分析，重点关注两种常用的设置，即全微调和头部微调。我们定义了每个设置下的稳定性，并证明了相应的稳定性边界。理论界解释了为什么以及如何几种现有的方法可以稳定微调过程。除了能够解释大多数观察到的经验发现外，我们提出的理论分析框架还可以帮助设计有效和可证明的方法。基于我们的理论，我们提出了三种新的策略来稳定微调过程，即最大裕度正则化器（MMR）、多头损失（MHLoss）和自无监督再训练（SURT）。我们在11个广泛使用的真实世界基准数据集以及数百个合成分类数据集上广泛评估了我们提出的方法。实验结果表明，我们提出的方法显著稳定了微调过程，也证实了我们的理论分析。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.09820v2" target="_blank">2301.09820v2</a>
                              </td>
                              <td>A Stability Analysis of Fine-Tuning a Pre-Trained Model</td>
                              <td>Zihao Fu</td>
                              <td>2023-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_09820v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.09820v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04455v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04455v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04455v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04455v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in large language models (LLMs) have significantly expanded their functionality and skills as tool agents. In this paper, we argue that a waveform pattern in the model's attention allocation has an impact on the tool use performance, which degrades when the position of essential information hits the trough zone. To address this issue, we propose a novel inference method named Attention Buckets. This approach enables LLMs to handle context by conducting parallel processes, each featuring a unique RoPE angle base that shapes the attention waveform. Attention Buckets ensures that an attention trough of a particular process can be compensated with an attention peak of another run, reducing the risk of the LLM missing essential information residing within the attention trough. Our extensive experiments on the widely recognized tool use benchmark demonstrate the efficacy of our approach, where a 7B-parameter open-source model enhanced by Attention Buckets achieves SOTA performance on par with GPT-4.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04455v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展显著扩展了其作为工具代理的功能和技能。在本文中，我们认为模型注意力分配中的波形模式对工具使用性能有影响，当基本信息的位置到达波谷区时，工具使用性能会降低。为了解决这个问题，我们提出了一种新的推理方法——注意力桶。这种方法使LLM能够通过进行并行处理来处理上下文，每个处理都有一个独特的RoPE角度基础，可以塑造注意力波形。注意力桶确保特定过程的注意力低谷可以用另一次运行的注意力峰值来补偿，从而降低LLM丢失位于注意力低谷内的重要信息的风险。我们在广泛认可的工具使用基准上进行的广泛实验证明了我们方法的有效性，其中由注意力桶增强的7B参数开源模型实现了与GPT-4相当的SOTA性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04455v1" target="_blank">2312.04455v1</a>
                              </td>
                              <td>Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use</td>
                              <td>Yuhan Chen</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04455v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04455v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alibabaresearch/damo-convai" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04440v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04440v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04440v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04440v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document \textit{open} aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04440v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，自动摘要模型的性能有了显著的提高。然而，在现实世界场景中，在满足用户的特定信息需求方面仍然存在差距，特别是当寻求有针对性的摘要时，例如在本文所针对的有用的基于方面的摘要设置中。此前针对这一背景的数据集和研究主要集中在一组有限的预定义方面，仅关注单个文档输入，或依赖于合成数据。为了推进对更现实场景的研究，我们引入了OpenAsp，这是一个基于多文档\textit｛open｝方面的摘要的基准。该基准是使用一种新颖且具有成本效益的注释协议创建的，通过该协议，从现有的通用多文档摘要数据集中派生出一个开放方面的数据集。我们分析了OpenAsp的特性，展示了其高质量的内容。此外，我们还表明，在OpenAsp中实现的现实的开放方面设置对当前最先进的摘要模型以及大型语言模型提出了挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04440v1" target="_blank">2312.04440v1</a>
                              </td>
                              <td>OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization</td>
                              <td>Shmuel Amar</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04440v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04440v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/liatschiff/openasp" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04372v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04372v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04372v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04372v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives. This approach aims to address the challenge of interpreting and executing spontaneous user instructions such as "overtake the car ahead," which have typically posed difficulties for existing frameworks. We introduce the LaMPilot benchmark specifically designed to quantitatively evaluate the efficacy of Large Language Models (LLMs) in translating human directives into actionable driving policies. We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark. The results of the experiments showed that GPT-4, with human feedback, achieved an impressive task completion rate of 92.7% and a minimal collision rate of 0.9%. To encourage further investigation in this area, our code and dataset will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04372v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了LaMPilot，这是一种用于自动驾驶领域规划的新框架，将任务重新思考为利用已建立的行为原语的代码生成过程。这种方法旨在解决解释和执行自发用户指令（如“超越前方汽车”）的挑战，这些指令通常会给现有框架带来困难。我们引入了LaMPilot基准，专门用于定量评估大型语言模型（LLM）在将人类指令转化为可操作的驾驶政策方面的功效。然后，我们在LaMPilot基准测试的任务上评估了一系列最先进的代码生成语言模型。实验结果表明，GPT-4在人类反馈的情况下，实现了令人印象深刻的92.7%的任务完成率和0.9%的最小碰撞率。为了鼓励在这一领域进行进一步的研究，我们将提供代码和数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04372v1" target="_blank">2312.04372v1</a>
                              </td>
                              <td>LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs</td>
                              <td>Yunsheng Ma</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04372v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04372v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16733v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMs for Science: Usage for Code Generation and Data Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16733v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16733v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16733v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have been touted to enable increased productivity in many areas of today's work life. Scientific research as an area of work is no exception: the potential of LLM-based tools to assist in the daily work of scientists has become a highly discussed topic across disciplines. However, we are only at the very onset of this subject of study. It is still unclear how the potential of LLMs will materialise in research practice. With this study, we give first empirical evidence on the use of LLMs in the research process. We have investigated a set of use cases for LLM-based tools in scientific research, and conducted a first study to assess to which degree current tools are helpful. In this paper we report specifically on use cases related to software engineering, such as generating application code and developing scripts for data analytics. While we studied seemingly simple use cases, results across tools differ significantly. Our results highlight the promise of LLM-based tools in general, yet we also observe various issues, particularly regarding the integrity of the output these tools provide.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16733v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）被吹捧为能够在当今工作生活的许多领域提高生产力。科学研究作为一个工作领域也不例外：基于LLM的工具辅助科学家日常工作的潜力已成为跨学科高度讨论的话题。然而，我们才刚刚开始这一研究课题。目前尚不清楚LLM的潜力将如何在研究实践中实现。通过这项研究，我们首次提供了LLM在研究过程中使用的经验证据。我们调查了一组基于LLM的工具在科学研究中的用例，并进行了第一项研究，以评估当前工具的帮助程度。在本文中，我们特别报告了与软件工程相关的用例，例如生成应用程序代码和开发数据分析脚本。虽然我们研究了看似简单的用例，但不同工具的结果却存在显著差异。我们的结果突出了基于LLM的工具的前景，但我们也观察到了各种问题，特别是关于这些工具提供的输出的完整性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16733v3" target="_blank">2311.16733v3</a>
                              </td>
                              <td>LLMs for Science: Usage for Code Generation and Data Analysis</td>
                              <td>Mohamed Nejjar</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16733v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16733v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luuca78/llms4science" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04350v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04350v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04350v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04350v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04350v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>进行因果推理的能力被广泛认为是智力的核心特征。在这项工作中，我们研究了大型语言模型（LLM）是否能够连贯地推理因果关系。自然语言处理（NLP）中的许多现有工作都集中在评估LLM中的常识性因果推理，从而未能评估模型是否能够根据一组定义明确的形式规则进行因果推理。为了解决这一问题，我们提出了一个新的NLP任务，即自然语言中的因果推理，其灵感来自Judea Pearl等人假设的“因果推理引擎”。我们用10K个样本组成了一个大型数据集CLadder：基于因果图和查询（关联、介入和反事实）的集合，我们获得了符号问题和基本事实答案，通过oracle因果推理引擎。然后将其翻译成自然语言。我们在数据集上评估了多个LLM，并引入和评估了一种定制的思想链提示策略CausalCoT。我们表明，我们的任务对LLM来说极具挑战性，我们进行了深入分析，以更深入地了解LLM的因果推理能力。我们的数据来源于https://huggingface.co/datasets/causalnlp/cladder，我们的代码可以在https://github.com/causalnlp/cladder.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04350v1" target="_blank">2312.04350v1</a>
                              </td>
                              <td>CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models</td>
                              <td>Zhijing Jin</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04350v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04350v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/causalnlp/cladder" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04344v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04344v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04344v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04344v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued considerable interest for its potential in medical applications. Despite its promise, recent studies and internal reviews highlight its underperformance in specialized medical tasks. This paper explores the boundary of GPT-4V's capabilities in medicine, particularly in processing complex imaging data from endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we assessed its foundational competencies, identifying substantial areas for enhancement. Our research emphasizes prompt engineering, an often-underutilized strategy for improving AI responsiveness. Through iterative testing, we refined the model's prompts, significantly improving its interpretative accuracy and relevance in medical imaging. From our comprehensive evaluations, we distilled 10 effective prompt engineering techniques, each fortifying GPT-4V's medical acumen. These methodical enhancements facilitate more reliable, precise, and clinically valuable insights from GPT-4V, advancing its operability in critical healthcare environments. Our findings are pivotal for those employing AI in medicine, providing clear, actionable guidance on harnessing GPT-4V's full diagnostic potential.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04344v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>OpenAI最新的大型视觉语言模型（LVLM）GPT-4V（vision）因其在医学应用中的潜力而引起了人们的极大兴趣。尽管它有前景，但最近的研究和内部审查强调了它在专业医疗任务方面的表现不佳。本文探讨了GPT-4V在医学上的能力边界，特别是在处理来自内镜、CT扫描和核磁共振成像等的复杂成像数据方面。利用开源数据集，我们评估了其基本能力，确定了大量的增强领域。我们的研究强调快速工程，这是一种通常未被充分利用的提高人工智能响应能力的策略。通过迭代测试，我们改进了模型的提示，显著提高了其在医学成像中的解释准确性和相关性。从我们的综合评估中，我们提炼出了10种有效的快速工程技术，每种技术都强化了GPT-4V的医学敏锐性。这些有条不紊的增强有助于GPT-4V获得更可靠、更精确和更有临床价值的见解，提高其在关键医疗保健环境中的可操作性。我们的发现对那些在医学中使用人工智能的人来说至关重要，为利用GPT-4V的全部诊断潜力提供了明确、可行的指导。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04344v1" target="_blank">2312.04344v1</a>
                              </td>
                              <td>Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies</td>
                              <td>Pengcheng Chen</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04344v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04344v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04333v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond Surface: Probing LLaMA Across Scales and Layers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04333v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04333v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04333v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04333v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对大型语言模型（LLM）进行了深入分析，重点介绍了LLaMA，这是自然语言处理中一个突出的开源基础模型。我们没有通过LLaMA的生成输出来评估它，而是设计了多项选择任务，以探索它在推理和计算等高阶任务中的内在理解。我们横向检查模型，比较不同的尺寸，纵向评估不同的层。基于设计的探测任务，我们揭示了几个关键而不常见的发现：（1）从水平上看，扩大模型大小几乎不能自动赋予额外的知识或计算能力。相反，它可以增强推理能力，尤其是在数学问题解决方面，并有助于减少幻觉，但只能超过一定的大小阈值；（2） 在垂直分析中，LLaMA的底层缺乏大量的算术和事实知识，表现出逻辑思维、多语言和识别能力，顶层拥有大部分计算能力和现实世界知识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04333v1" target="_blank">2312.04333v1</a>
                              </td>
                              <td>Beyond Surface: Probing LLaMA Across Scales and Layers</td>
                              <td>Nuo Chen</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04333v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04333v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nuochenpku/llama_analysis" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16427v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16427v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16427v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16427v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16427v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高效的、针对特定任务的提示通常由专家精心设计，以在深入了解大型语言模型（LLM）的直觉和目标任务复杂性的基础上，集成详细的说明和领域见解。然而，这种专家级提示的自动生成仍然难以捉摸。现有的提示优化方法往往忽视了领域知识的深度，难以有效地探索专家级提示的广阔空间。针对这一点，我们提出了PromptAgent，这是一种自主制作质量与专家手工制作的提示相当的优化方法。其核心是，PromptAgent将提示优化视为一个战略规划问题，并采用植根于蒙特卡洛树搜索的原则性规划算法，在专家级提示空间中进行战略性导航。受类似人类的试错探索的启发，PromptAgent通过反思模型错误和生成建设性的错误反馈，诱导精确的专家级见解和深入的指导。这种新颖的框架允许代理迭代地检查中间提示（状态），基于错误反馈（动作）对其进行细化，模拟未来的奖励，并搜索导致专家提示的高奖励路径。我们将PromptAgent应用于跨越三个实际领域的12个任务：BIG Bench Hard（BBH），以及特定领域和通用NLP任务，表明它显著优于强思想链和最近的提示优化基线。广泛的分析强调其能够以极高的效率和可推广性来制作专家级的、详细的、深入领域的提示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16427v2" target="_blank">2310.16427v2</a>
                              </td>
                              <td>PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</td>
                              <td>Xinyuan Wang</td>
                              <td>2023-10-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16427v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16427v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xinyuanwangcs/promptagent" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11426v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Post Hoc Explanations of Language Models Can Improve Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11426v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11426v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11426v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11426v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在执行复杂任务方面表现出了非凡的能力。此外，最近的研究表明，在上下文学习过程中结合人类注释的理性（例如，思维链提示）可以显著提高这些模型的性能，特别是在需要推理能力的任务中。然而，纳入这些理由在可扩展性方面带来了挑战，因为这需要高度的人员参与。在这项工作中，我们提出了一个新的框架，即通过利用上下文内学习和事后解释来提高模型性能（AMPLIFY），该框架通过自动化原理生成过程来解决上述挑战。为此，我们利用事后解释方法，输出归因得分（解释），捕捉每个输入特征对模型预测的影响。更具体地说，我们构建了自动化的自然语言推理，嵌入了事后解释的见解，为LLM提供了纠正信号。对真实世界数据集的广泛实验表明，我们的框架AMPLIFY在一系列任务中使预测准确率提高了约10-25%，包括那些依赖于人类注释原理（如思维链提示）的先前方法达不到要求的任务。我们的工作首次尝试强调事后解释作为提高LLM有效性的宝贵工具的潜力。此外，我们进行了额外的实证分析和消融研究，以证明AMPLIFY每一个组成部分的影响，这反过来又为上下文学习中的精炼提供了关键的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11426v3" target="_blank">2305.11426v3</a>
                              </td>
                              <td>Post Hoc Explanations of Language Models Can Improve Language Models</td>
                              <td>Satyapriya Krishna</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11426v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11426v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04318v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MIMo: A Multi-Modal Infant Model for Studying Cognitive Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04318v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04318v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04318v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human intelligence and human consciousness emerge gradually during the process of cognitive development. Understanding this development is an essential aspect of understanding the human mind and may facilitate the construction of artificial minds with similar properties. Importantly, human cognitive development relies on embodied interactions with the physical and social environment, which is perceived via complementary sensory modalities. These interactions allow the developing mind to probe the causal structure of the world. This is in stark contrast to common machine learning approaches, e.g., for large language models, which are merely passively ``digesting'' large amounts of training data, but are not in control of their sensory inputs. However, computational modeling of the kind of self-determined embodied interactions that lead to human intelligence and consciousness is a formidable challenge. Here we present MIMo, an open-source multi-modal infant model for studying early cognitive development through computer simulations. MIMo's body is modeled after an 18-month-old child with detailed five-fingered hands. MIMo perceives its surroundings via binocular vision, a vestibular system, proprioception, and touch perception through a full-body virtual skin, while two different actuation models allow control of his body. We describe the design and interfaces of MIMo and provide examples illustrating its use. All code is available at https://github.com/trieschlab/MIMo .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04318v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类的智力和意识是在认知发展的过程中逐渐显现出来的。理解这一发展是理解人类思维的一个重要方面，可能有助于构建具有类似性质的人造思维。重要的是，人类的认知发展依赖于与物理和社会环境的具体互动，这是通过互补的感官模式感知的。这些相互作用使发展中的心智能够探究世界的因果结构。这与常见的机器学习方法形成了鲜明对比，例如，对于大型语言模型，它们只是被动地“消化”大量的训练数据，但不能控制其感官输入。然而，对导致人类智力和意识的那种自我决定的具体互动进行计算建模是一个巨大的挑战。在这里，我们介绍了MIMo，一个开源的多模式婴儿模型，用于通过计算机模拟研究早期认知发展。米莫的身体模仿了一个18个月大的孩子，有着精致的五指手。MIMo通过双眼视觉、前庭系统、本体感觉和通过全身虚拟皮肤的触摸感知来感知周围环境，而两种不同的驱动模型允许控制他的身体。我们描述了MIMo的设计和接口，并提供了说明其使用的示例。所有代码均可在https://github.com/trieschlab/mimo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04318v1" target="_blank">2312.04318v1</a>
                              </td>
                              <td>MIMo: A Multi-Modal Infant Model for Studying Cognitive Development</td>
                              <td>Dominik Mattern</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04318v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04318v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/trieschlab/mimo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04316v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Knowledge-driven Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04316v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04316v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04316v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores the emerging knowledge-driven autonomous driving technologies. Our investigation highlights the limitations of current autonomous driving systems, in particular their sensitivity to data bias, difficulty in handling long-tail scenarios, and lack of interpretability. Conversely, knowledge-driven methods with the abilities of cognition, generalization and life-long learning emerge as a promising way to overcome these challenges. This paper delves into the essence of knowledge-driven autonomous driving and examines its core components: dataset \& benchmark, environment, and driver agent. By leveraging large language models, world models, neural rendering, and other advanced artificial intelligence techniques, these components collectively contribute to a more holistic, adaptive, and intelligent autonomous driving system. The paper systematically organizes and reviews previous research efforts in this area, and provides insights and guidance for future research and practical applications of autonomous driving. We will continually share the latest updates on cutting-edge developments in knowledge-driven autonomous driving along with the relevant valuable open-source resources at: \url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04316v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文探讨了新兴的知识驱动的自动驾驶技术。我们的调查强调了当前自动驾驶系统的局限性，特别是它们对数据偏差的敏感性、处理长尾场景的困难以及缺乏可解释性。相反，具有认知、概括和终身学习能力的知识驱动方法是克服这些挑战的一种很有前途的方法。本文深入研究了知识驱动的自动驾驶的本质，并考察了其核心组成部分：数据集和基准、环境和驾驶员代理。通过利用大型语言模型、世界模型、神经渲染和其他先进的人工智能技术，这些组件共同为更全面、自适应和智能的自动驾驶系统做出了贡献。本文系统地组织和回顾了以往在这一领域的研究工作，并为自动驾驶的未来研究和实际应用提供了见解和指导。我们将不断分享知识驱动自动驾驶前沿发展的最新进展，以及相关宝贵的开源资源，网址为：\url{https://github.com/pjlab-adg/awesome-knowledge-driven-ad}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04316v1" target="_blank">2312.04316v1</a>
                              </td>
                              <td>Towards Knowledge-driven Autonomous Driving</td>
                              <td>Xin Li</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04316v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04316v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/pjlab-adg/awesome-knowledge-driven-ad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04314v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04314v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04314v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04314v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning scene graphs from natural language descriptions has proven to be a cheap and promising scheme for Scene Graph Generation (SGG). However, such unstructured caption data and its processing are troubling the learning an acurrate and complete scene graph. This dilema can be summarized as three points. First, traditional language parsers often fail to extract meaningful relationship triplets from caption data. Second, grounding unlocalized objects in parsed triplets will meet ambiguity in visual-language alignment. Last, caption data typically are sparse and exhibit bias to partial observations of image content. These three issues make it hard for the model to generate comprehensive and accurate scene graphs. To fill this gap, we propose a simple yet effective framework, GPT4SGG, to synthesize scene graphs from holistic and region-specific narratives. The framework discards traditional language parser, and localize objects before obtaining relationship triplets. To obtain relationship triplets, holistic and dense region-specific narratives are generated from the image. With such textual representation of image data and a task-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene graph as "pseudo labels". Experimental results showcase GPT4SGG significantly improves the performance of SGG models trained on image-caption data. We believe this pioneering work can motivate further research into mining the visual reasoning capabilities of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04314v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从自然语言描述中学习场景图已被证明是一种廉价且有前途的场景图生成（SGG）方案。然而，这种非结构化字幕数据及其处理给准确完整的场景图的学习带来了麻烦。这种困境可以概括为三点。首先，传统的语言解析器往往无法从字幕数据中提取出有意义的关系三元组。第二，在解析的三元组中固定未定标的对象会遇到视觉语言对齐中的歧义。最后，字幕数据通常是稀疏的，并且表现出对图像内容的部分观察的偏差。这三个问题使得模型很难生成全面准确的场景图。为了填补这一空白，我们提出了一个简单而有效的框架GPT4SGG，用于从整体和特定区域的叙述中合成场景图。该框架抛弃了传统的语言解析器，在获得关系三元组之前对对象进行定位。为了获得关系三元组，从图像中生成整体的、密集的区域特定叙事。有了这种图像数据的文本表示和特定任务的提示，LLM，特别是GPT-4，直接将场景图合成为“伪标签”。实验结果表明，GPT4SGG显著提高了在图像字幕数据上训练的SGG模型的性能。我们相信，这项开创性的工作可以激励人们进一步研究LLM的视觉推理能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04314v1" target="_blank">2312.04314v1</a>
                              </td>
                              <td>GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives</td>
                              <td>Zuyao Chen</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04314v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04314v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04302v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04302v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04302v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04302v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and 1552.5 in MME-perception. The code is available at: https://github.com/dvlab-research/Prompt-Highlighter/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04302v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究针对多模态LLM（LLM&VLM）推理的一个关键方面：显式可控文本生成。多模态LLM赋予多模态理解语义生成的能力，但由于其自回归生成性质，其可解释性较低，对提示内容的依赖性较重。虽然操作提示格式可以提高输出，但为每个任务设计具体而精确的提示可能具有挑战性且无效。为了解决这个问题，我们引入了一种新的推理方法Prompt Highlighter，它使用户能够突出显示特定的提示跨度，以在生成过程中交互式控制焦点。在无分类器扩散引导的激励下，我们基于高亮标记形成规则和无条件的上下文对，证明模型中的自回归生成可以以无分类器的方式进行引导。值得注意的是，我们发现，在推理过程中，通过注意力权重引导具有高亮标记的模型会产生更多期望的输出。我们的方法与当前的LLM和VLM兼容，无需培训即可实现令人印象深刻的定制生成结果。实验证实了它在关注输入上下文和生成可靠内容方面的有效性。在不调整LLaVA-v1.5的情况下，我们的方法在MMBench测试中获得69.5，在MME感知中获得1552.5。代码位于：https://github.com/dvlab-research/prompt-highlighter/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04302v1" target="_blank">2312.04302v1</a>
                              </td>
                              <td>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</td>
                              <td>Yuechen Zhang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04302v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04302v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dvlab-research/prompt-highlighter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04293v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04293v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04293v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04293v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance across various multimodal tasks. However, its efficacy in emotion recognition remains a question. This paper quantitatively evaluates GPT-4V's capabilities in multimodal emotion understanding, encompassing tasks such as facial emotion recognition, visual sentiment analysis, micro-expression recognition, dynamic facial emotion recognition, and multimodal emotion recognition. Our experiments show that GPT-4V exhibits impressive multimodal and temporal understanding capabilities, even surpassing supervised systems in some tasks. Despite these achievements, GPT-4V is currently tailored for general domains. It performs poorly in micro-expression recognition that requires specialized expertise. The main purpose of this paper is to present quantitative results of GPT-4V on emotion understanding and establish a zero-shot benchmark for future research. Code and evaluation results are available at: https://github.com/zeroQiaoba/gpt4v-emotion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04293v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，GPT-4 with Vision（GPT-4V）在各种多模式任务中表现出了显著的性能。然而，它在情绪识别方面的功效仍然是个问题。本文定量评估了GPT-4V在多模式情绪理解方面的能力，包括面部情绪识别、视觉情绪分析、微表情识别、动态面部情绪识别和多模式情绪识别等任务。我们的实验表明，GPT-4V表现出令人印象深刻的多模式和时间理解能力，甚至在某些任务中超过了监督系统。尽管取得了这些成就，GPT-4V目前是为通用领域量身定制的。它在需要专业知识的微表情识别方面表现不佳。本文的主要目的是呈现GPT-4V在情绪理解方面的定量结果，并为未来的研究建立零样本基准。代码和评估结果可在以下网站获取：https://github.com/zeroqiaoba/gpt4v-emotion.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04293v1" target="_blank">2312.04293v1</a>
                              </td>
                              <td>GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion Understanding</td>
                              <td>Zheng Lian</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04293v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04293v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zeroqiaoba/gpt4v-emotion" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05876v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05876v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05876v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05876v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations. In order to address these challenges, researchers have pursued two primary strategies, knowledge editing and retrieval augmentation, to enhance LLMs by incorporating external information from different aspects. Nevertheless, there is still a notable absence of a comprehensive survey. In this paper, we propose a review to discuss the trends in integration of knowledge and large language models, including taxonomy of methods, benchmarks, and applications. In addition, we conduct an in-depth analysis of different methods and point out potential research directions in the future. We hope this survey offers the community quick access and a comprehensive overview of this research area, with the intention of inspiring future research endeavors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05876v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在各种自然语言任务上表现出优异的性能，但它们容易受到过时数据和特定领域限制所带来的问题的影响。为了应对这些挑战，研究人员采用了两种主要策略，即知识编辑和检索增强，通过整合来自不同方面的外部信息来增强LLM。尽管如此，仍然明显缺乏全面的调查。在本文中，我们提出了一个综述来讨论知识和大型语言模型集成的趋势，包括方法的分类、基准和应用程序。此外，我们对不同的方法进行了深入分析，并指出了未来潜在的研究方向。我们希望这项调查能为社区提供对这一研究领域的快速访问和全面概述，以激励未来的研究努力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05876v2" target="_blank">2311.05876v2</a>
                              </td>
                              <td>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications</td>
                              <td>Zhangyin Feng</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05876v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05876v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00502v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient LLM Inference on CPUs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00502v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00502v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00502v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00502v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在广泛的任务中表现出了卓越的性能和巨大的潜力。然而，由于模型参数的天文数字，部署这些模型一直具有挑战性，这需要对大内存容量和高内存带宽的需求。在本文中，我们提出了一种有效的方法，可以更有效地部署LLM。我们支持自动的INT4纯权重量化流，并设计了一个具有高度优化内核的特殊LLM运行时，以加速CPU上的LLM推理。我们展示了我们的方法在流行的LLM上的普遍适用性，包括Llama2、Llama、GPT-NeoX，并展示了CPU上的极端推理效率。该代码可在以下网址公开获取：https://github.com/intel/intel-extension-for-transformers.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00502v2" target="_blank">2311.00502v2</a>
                              </td>
                              <td>Efficient LLM Inference on CPUs</td>
                              <td>Haihao Shen</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00502v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00502v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/intel/intel-extension-for-transformers" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03549v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03549v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03549v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03549v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks. However, training these models can incur significant expenses, often requiring tens of thousands of GPUs for months of continuous operation. Typically, this training is carried out in specialized GPU clusters equipped with homogeneous high-speed Remote Direct Memory Access (RDMA) network interface cards (NICs). The acquisition and maintenance of such dedicated clusters is challenging. Current LLM training frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on optimizing training within homogeneous cluster settings. In this paper, we introduce Holmes, a training framework for LLMs that employs thoughtfully crafted data and model parallelism strategies over the heterogeneous NIC environment. Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs. Furthermore, our proposed framework, utilizing pipeline parallel techniques, demonstrates scalability to multiple GPU clusters, even in scenarios without high-speed interconnects between nodes in distinct clusters. We conducted comprehensive experiments that involved various scenarios in the heterogeneous NIC environment. In most cases, our framework achieves performance levels close to those achievable with homogeneous RDMA-capable networks (InfiniBand or RoCE), significantly exceeding training efficiency within the pure Ethernet environment. Additionally, we verified that our framework outperforms other mainstream LLM frameworks under heterogeneous NIC environment in terms of training efficiency and can be seamlessly integrated with them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03549v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM），如GPT-3、OPT和LLaMA，在广泛的任务中表现出了显著的准确性。然而，训练这些模型可能会产生巨大的费用，通常需要数万个GPU才能连续运行数月。通常，这种训练是在配备了同质高速远程直接内存访问（RDMA）网络接口卡（NIC）的专用GPU集群中进行的。购置和维护这些专用集群具有挑战性。当前的LLM训练框架，如威震天LM和威震天DeepSpeed，主要侧重于在同质集群设置中优化训练。在本文中，我们介绍了Holmes，这是一个LLM的训练框架，它在异构NIC环境中采用了精心设计的数据和模型并行策略。我们的主要技术贡献在于一种新的调度方法，该方法根据连接的NIC的特性，在LLM训练中智能地将不同的计算小任务分配给特定的GPU设备组。此外，我们提出的框架利用流水线并行技术，展示了对多个GPU集群的可扩展性，即使在不同集群中的节点之间没有高速互连的情况下也是如此。我们进行了全面的实验，涉及异构NIC环境中的各种场景。在大多数情况下，我们的框架实现的性能水平接近于支持RDMA的同质网络（InfiniBand或RoCE），大大超过了纯以太网环境中的训练效率。此外，我们验证了在异构NIC环境下，我们的框架在训练效率方面优于其他主流LLM框架，并且可以与它们无缝集成。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03549v2" target="_blank">2312.03549v2</a>
                              </td>
                              <td>Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment</td>
                              <td>Fei Yang</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03549v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03549v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04160v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Text as Image: Learning Transferable Adapter for Multi-Label Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04160v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04160v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04160v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained vision-language models have notably accelerated progress of open-world concept recognition. Their impressive zero-shot ability has recently been transferred to multi-label image classification via prompt tuning, enabling to discover novel labels in an open-vocabulary manner. However, this paradigm suffers from non-trivial training costs, and becomes computationally prohibitive for a large number of candidate labels. To address this issue, we note that vision-language pre-training aligns images and texts in a unified embedding space, making it potential for an adapter network to identify labels in visual modality while be trained in text modality. To enhance such cross-modal transfer ability, a simple yet effective method termed random perturbation is proposed, which enables the adapter to search for potential visual embeddings by perturbing text embeddings with noise during training, resulting in better performance in visual modality. Furthermore, we introduce an effective approach to employ large language models for multi-label instruction-following text generation. In this way, a fully automated pipeline for visual label recognition is developed without relying on any manual data. Extensive experiments on public benchmarks show the superiority of our method in various multi-label classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04160v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过预训练的视觉语言模型显著加速了开放世界概念识别的发展。他们令人印象深刻的零样本能力最近通过即时调整转移到了多标签图像分类，从而能够以开放词汇的方式发现新标签。然而，这种范式承受着非琐碎的训练成本，并且在计算上对大量候选标签来说变得令人望而却步。为了解决这个问题，我们注意到视觉语言预训练在统一的嵌入空间中对齐图像和文本，这使得适配器网络有可能在接受文本模态训练的同时识别视觉模态中的标签。为了增强这种跨模态传递能力，提出了一种称为随机扰动的简单而有效的方法，该方法使适配器能够通过在训练过程中用噪声扰动文本嵌入来搜索潜在的视觉嵌入，从而在视觉模态中获得更好的性能。此外，我们还介绍了一种在文本生成后使用大型语言模型进行多标签教学的有效方法。通过这种方式，在不依赖任何手动数据的情况下，开发了用于视觉标签识别的全自动管道。在公共基准上进行的大量实验表明，我们的方法在各种多标签分类任务中具有优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04160v1" target="_blank">2312.04160v1</a>
                              </td>
                              <td>Text as Image: Learning Transferable Adapter for Multi-Label Classification</td>
                              <td>Xuelin Zhu</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04160v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04160v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04134v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using a Large Language Model to generate a Design Structure Matrix</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04134v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04134v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04134v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Design Structure Matrix (DSM) is an established method used in dependency modelling, especially in the design of complex engineering systems. The generation of DSM is traditionally carried out through manual means and can involve interviewing experts to elicit critical system elements and the relationships between them. Such manual approaches can be time-consuming and costly. This paper presents a workflow that uses a Large Language Model (LLM) to support the generation of DSM and improve productivity. A prototype of the workflow was developed in this work and applied on a diesel engine DSM published previously. It was found that the prototype could reproduce 357 out of 462 DSM entries published (i.e. 77.3%), suggesting that the work can aid DSM generation. A no-code version of the prototype is made available online to support future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04134v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>设计结构矩阵（DSM）是一种用于相关性建模的既定方法，尤其是在复杂工程系统的设计中。DSM的生成传统上是通过手动方式进行的，可以包括采访专家，以引出关键的系统元素及其之间的关系。这种手动方法可能耗时且成本高昂。本文提出了一个使用大型语言模型（LLM）来支持DSM生成并提高生产力的工作流程。本工作开发了工作流程的原型，并将其应用于之前发布的柴油机DSM。研究发现，原型可以复制462个已发表的DSM条目中的357个（即77.3%），这表明该工作可以帮助DSM的生成。该原型的无代码版本在网上提供，以支持未来的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04134v1" target="_blank">2312.04134v1</a>
                              </td>
                              <td>Using a Large Language Model to generate a Design Structure Matrix</td>
                              <td>Edwin C. Y. Koh</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04134v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05470v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generative Judge for Evaluating Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05470v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05470v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05470v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05470v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的快速发展大大扩展了它们可以处理的任务范围。在自然语言处理（NLP）领域，研究人员已将重点从传统的NLP任务（如序列标记和解析）转移到围绕与人类需求相一致的任务（如头脑风暴和电子邮件写作）。任务分布的这种转变对评估这些一致的模型提出了新的要求，包括通用性（即评估不同场景下的性能）、灵活性（即在不同协议下进行检查）和可解释性（即通过解释仔细检查模型）。在本文中，我们提出了一种具有13B参数的生成法官Auto-J，旨在解决这些挑战。我们的模型在大量真实世界场景下基于用户查询和LLM生成的响应进行训练，并适应不同的评估协议（例如，成对响应比较和单响应评估）和结构良好的自然语言批评。为了证明我们的方法的有效性，我们构建了一个涵盖58种不同场景的新试验台。在实验上，Auto-J以很大的优势超过了一系列强大的竞争对手，包括开源和闭源模型。我们还提供了详细的分析和案例研究，以进一步揭示我们的方法的潜力，并在https://github.com/gair-nlp/auto-j.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05470v2" target="_blank">2310.05470v2</a>
                              </td>
                              <td>Generative Judge for Evaluating Alignment</td>
                              <td>Junlong Li</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05470v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05470v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gair-nlp/auto-j" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04127v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04127v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04127v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04127v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, in specific scenarios, LLMs still generate harmful responses when faced with malicious instructions, a phenomenon referred to as "Jailbreak Attack". In our research, we introduce a novel jailbreak attack method (\textbf{RADIAL}), which consists of two steps: 1) Inherent Response Tendency Analysis: we analyze the inherent affirmation and rejection tendency of LLMs to react to real-world instructions. 2) Real-World Instructions-Driven Jailbreak: based on our analysis, we strategically choose several real-world instructions and embed malicious instructions into them to amplify the LLM's potential to generate harmful responses. On three open-source human-aligned LLMs, our method achieves excellent jailbreak attack performance for both Chinese and English malicious instructions. Besides, we guided detailed ablation experiments and verified the effectiveness of our core idea "Inherent Response Tendency Analysis". Our exploration also exposes the vulnerability of LLMs to being induced into generating more detailed harmful responses in subsequent rounds of dialogue.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04127v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大量工作致力于改进大型语言模型（LLM）的安全机制。然而，在特定情况下，LLM在面对恶意指令时仍然会产生有害的响应，这种现象被称为“越狱攻击”。在我们的研究中，我们介绍了一种新的越狱攻击方法（\textbf｛RADIAL｝），该方法由两个步骤组成：1）固有响应趋势分析：我们分析LLM对现实世界指令做出反应的固有肯定和拒绝趋势。2） 真实世界指令驱动的越狱：基于我们的分析，我们战略性地选择了几个真实世界的指令，并将恶意指令嵌入其中，以放大LLM产生有害响应的潜力。在三个开源的人工对齐LLM上，我们的方法对中英文恶意指令都实现了出色的越狱攻击性能。此外，我们指导了详细的消融实验，并验证了我们的核心思想“固有反应趋势分析”的有效性。我们的探索还暴露了LLM在随后几轮对话中容易被诱导产生更详细的有害反应。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04127v1" target="_blank">2312.04127v1</a>
                              </td>
                              <td>Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak</td>
                              <td>Yanrui Du</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04127v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04127v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05140v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05140v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05140v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05140v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05140v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同情对话是建立和谐社会关系不可或缺的一部分，有助于开发有用的人工智能。以前的方法主要基于精细的小规模语言模型。随着ChatGPT的出现，大型语言模型（LLM）在该领域的应用效果引起了人们的极大关注。这项工作实证研究了LLM在产生移情反应方面的表现，并提出了三种改进方法，即上下文学习中的语义相似、两阶段交互式生成和与知识库相结合。大量实验表明，LLM可以从我们提出的方法中显著受益，并能够在自动和人工评估中实现最先进的性能。此外，我们还探索了GPT-4模拟人类评估者的可能性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05140v3" target="_blank">2310.05140v3</a>
                              </td>
                              <td>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</td>
                              <td>Yushan Qian</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05140v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05140v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/27182812/LLM4ED" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04087v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04087v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04087v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04087v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With recent advancements in Large Multimodal Models (LMMs) across various domains, a novel prompting method called visual referring prompting has emerged, showing significant potential in enhancing human-computer interaction within multimodal systems. This method offers a more natural and flexible approach to human interaction with these systems compared to traditional text descriptions or coordinates. However, the categorization of visual referring prompting remains undefined, and its impact on the performance of LMMs has yet to be formally examined. In this study, we conduct the first comprehensive analysis of LMMs using a variety of visual referring prompting strategies. We introduce a benchmark dataset called VRPTEST, comprising 3 different visual tasks and 2,275 images, spanning diverse combinations of prompt strategies. Using VRPTEST, we conduct a comprehensive evaluation of eight versions of prominent open-source and proprietary foundation models, including two early versions of GPT-4V. We develop an automated assessment framework based on software metamorphic testing techniques to evaluate the accuracy of LMMs without the need for human intervention or manual labeling. We find that the current proprietary models generally outperform the open-source ones, showing an average accuracy improvement of 22.70%; however, there is still potential for improvement. Moreover, our quantitative analysis shows that the choice of prompt strategy significantly affects the accuracy of LMMs, with variations ranging from -17.5% to +7.3%. Further case studies indicate that an appropriate visual referring prompting strategy can improve LMMs' understanding of context and location information, while an unsuitable one might lead to answer rejection. We also provide insights on minimizing the negative impact of visual referring prompting on LMMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04087v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型多模态模型（LMM）在各个领域的最新进展，一种称为视觉参考提示的新提示方法已经出现，在增强多模态系统中的人机交互方面显示出巨大的潜力。与传统的文本描述或坐标相比，这种方法为人类与这些系统的交互提供了一种更自然、更灵活的方法。然而，视觉参照提示的分类仍不明确，其对LMM性能的影响尚待正式研究。在本研究中，我们首次使用各种视觉参考提示策略对LMM进行了全面分析。我们介绍了一个名为VRPTEST的基准数据集，包括3个不同的视觉任务和2275张图像，涵盖了提示策略的不同组合。使用VRPTEST，我们对八个版本的著名开源和专有基础模型进行了全面评估，其中包括GPT-4V的两个早期版本。我们开发了一个基于软件变形测试技术的自动评估框架，以评估LMM的准确性，而无需人工干预或手动标记。我们发现，目前的专有模型通常优于开源模型，平均准确率提高了22.70%；然而，仍有改进的潜力。此外，我们的定量分析表明，提示策略的选择对LMM的准确性有显著影响，变化范围从-17.5%到+7.3%。进一步的案例研究表明，适当的视觉推荐提示策略可以提高LMM对上下文和位置信息的理解，而不合适的提示策略可能会导致拒绝回答。我们还提供了关于最大限度地减少视觉推荐提示对LMM的负面影响的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04087v1" target="_blank">2312.04087v1</a>
                              </td>
                              <td>VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models</td>
                              <td>Zongjie Li</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04087v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04087v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04076v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models are Good Prompt Learners for Low-Shot Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04076v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04076v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04076v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04076v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在训练图像有限或无法访问的情况下，低镜头图像分类受益于具有较强可推广性的预训练视觉语言（VL）模型（如CLIP）的最新进展。使用VL模型构建的即时学习方法从仅具有有限类特定信息的类名生成文本特征。大型语言模型（LLM）以其丰富的百科全书式知识作为补充。因此，在本文中，我们讨论了LLM的集成，以增强预训练的VL模型，特别是在低炮点分类方面。然而，语言和视觉之间的领域差距阻碍了LLM的直接应用。因此，我们提出了LLaMP，即作为提示学习者的大型语言模型，它为CLIP文本编码器产生自适应提示，并将其建立为连接桥梁。实验表明，与其他最先进的即时学习方法相比，在11个数据集的范围内，LLaMP在零样本泛化和少热点图像分类方面都具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04076v1" target="_blank">2312.04076v1</a>
                              </td>
                              <td>Large Language Models are Good Prompt Learners for Low-Shot Image Classification</td>
                              <td>Zhaoheng Zheng</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04076v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04076v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04059v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04059v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04059v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04059v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automated coaching messages for weight control can save time and costs, but their repetitive, generic nature may limit their effectiveness compared to human coaching. Large language model (LLM) based artificial intelligence (AI) chatbots, like ChatGPT, could offer more personalized and novel messages to address repetition with their data-processing abilities. While LLM AI demonstrates promise to encourage healthier lifestyles, studies have yet to examine the feasibility and acceptability of LLM-based BWL coaching. 87 adults in a weight-loss trial rated ten coaching messages' helpfulness (five human-written, five ChatGPT-generated) using a 5-point Likert scale, providing additional open-ended feedback to justify their ratings. Participants also identified which messages they believed were AI-generated. The evaluation occurred in two phases: messages in Phase 1 were perceived as impersonal and negative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated messages were rated less helpful than human-written ones, with 66 percent receiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI messages matched the human-written ones regarding helpfulness, with 82% scoring three or above. Additionally, 50% were misidentified as human-written, suggesting AI's sophistication in mimicking human-generated content. A thematic analysis of open-ended feedback revealed that participants appreciated AI's empathy and personalized suggestions but found them more formulaic, less authentic, and too data-focused. This study reveals the preliminary feasibility and acceptability of LLM AIs, like ChatGPT, in crafting potentially effective weight control coaching messages. Our findings also underscore areas for future enhancement.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04059v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于体重控制的自动化指导信息可以节省时间和成本，但与人工指导相比，其重复性和通用性可能会限制其有效性。基于大型语言模型（LLM）的人工智能聊天机器人，如ChatGPT，可以通过其数据处理能力提供更个性化和新颖的消息来解决重复问题。虽然LLM人工智能有望鼓励更健康的生活方式，但研究尚未检验基于LLM的BWL辅导的可行性和可接受性。在一项减肥试验中，87名成年人使用5分的Likert量表对10条辅导信息（5条由人撰写，5条由ChatGPT生成）的有益性进行了评分，并提供了额外的开放式反馈来证明他们的评分是合理的。参与者还确定了他们认为哪些信息是人工智能生成的。评估分为两个阶段：第一阶段的信息被认为是非个人的和负面的，促使对第二阶段的信息进行修订。在第一阶段，人工智能生成的信息被评为不如人类书写的信息有用，66%的人的有用性评级为3或更高。然而，在第二阶段，人工智能信息与人类书写的关于乐于助人的信息相匹配，82%的人得分在三分或三分以上。此外，50%被误认为是人类书写的，这表明人工智能在模仿人类生成的内容方面很复杂。对开放式反馈的主题分析显示，参与者欣赏人工智能的同理心和个性化建议，但发现它们更公式化，不太真实，而且过于注重数据。这项研究揭示了LLM AI（如ChatGPT）在制定潜在有效的体重控制指导信息方面的初步可行性和可接受性。我们的研究结果还强调了未来需要改进的领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04059v1" target="_blank">2312.04059v1</a>
                              </td>
                              <td>Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss</td>
                              <td>Zhuoran Huang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04059v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04059v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13255v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13255v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13255v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13255v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to "a blindfolded text-based game." Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model designed to address this limitation. Steve-Eye integrates the LLM with a visual encoder which enables it to process visual-text inputs and generate multimodal feedback. In addition, we use a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, empowering our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out extensive experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. Codes and datasets will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13255v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的研究提供了令人信服的证据，证明大型语言模型（LLM）可以为嵌入的代理提供与世界互动的自我驱动能力，这标志着向通用机器人迈出了第一步。然而，这些努力往往忽视了开放世界的视觉丰富性，使整个互动过程类似于“蒙着眼睛的基于文本的游戏”。因此，基于LLM的代理在直观地理解周围环境和产生易于理解的反应方面经常遇到挑战。在本文中，我们提出了Steve Eye，这是一个端到端训练的大型多模式模型，旨在解决这一限制。Steve Eye将LLM与视觉编码器集成，使其能够处理视觉文本输入并生成多模式反馈。此外，我们使用半自动策略来收集包括850K个开放世界指令对的广泛数据集，使我们的模型能够涵盖代理的三个基本功能：多模式感知、基础知识库以及技能预测和规划。最后，我们开发了三个开放世界评估基准，然后从广泛的角度进行了广泛的实验，以验证我们的模型的战略行动和计划能力。将发布代码和数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13255v2" target="_blank">2310.13255v2</a>
                              </td>
                              <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
                              <td>Sipeng Zheng</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13255v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13255v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03409v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models as Optimizers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03409v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03409v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03409v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03409v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>优化无处不在。虽然基于导数的算法是解决各种问题的强大工具，但梯度的缺乏给许多现实世界的应用带来了挑战。在这项工作中，我们提出了PROmpting优化（OPRO），这是一种利用大型语言模型（LLM）作为优化器的简单有效的方法，其中优化任务用自然语言描述。在每个优化步骤中，LLM从包含先前生成的解及其值的提示中生成新的解，然后评估新的解并将其添加到下一优化步骤的提示中。我们首先展示了关于线性回归和旅行推销员问题的OPRO，然后继续进行提示优化，目标是找到最大限度提高任务准确性的指令。通过各种LLM，我们证明了OPRO优化的最佳提示在GSM8K上比人类设计的提示高出8%，在大型钳工任务上高出50%。代码位于https://github.com/google-deepmind/opro.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03409v2" target="_blank">2309.03409v2</a>
                              </td>
                              <td>Large Language Models as Optimizers</td>
                              <td>Chengrun Yang</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03409v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03409v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-deepmind/opro" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00273v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mark My Words: Analyzing and Evaluating Language Model Watermarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00273v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00273v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00273v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. In this context, the ability to distinguish machine-generated text from human-authored content becomes important. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on text watermarking techniques - as opposed to image watermarks - and proposes MARKMYWORDS, a comprehensive benchmark for them under different tasks as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. Current watermarking techniques are good enough to be deployed: Kirchenbauer et al. [1] can watermark Llama2-7B-chat with no perceivable loss in quality, the watermark can be detected with fewer than 100 tokens, and the scheme offers good tamper-resistance to simple attacks. We argue that watermark indistinguishability, a criteria emphasized in some prior works, is too strong a requirement: schemes that slightly modify logit distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. We publicly release our benchmark (https://github.com/wagner-group/MarkMyWords)</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00273v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大型语言模型的功能显著增长，人们也担心它们被滥用。在这种情况下，区分机器生成的文本和人工创作的内容的能力变得很重要。先前的工作已经提出了许多对文本进行水印的方案，这将受益于系统的评估框架。这项工作专注于文本水印技术，而不是图像水印，并提出了MARKMYWORDS，这是它们在不同任务和实际攻击下的综合基准。我们关注三个主要指标：质量、大小（例如检测水印所需的令牌数量）和抗篡改性。目前的水印技术已经足够好，可以部署：Kirchenbauer等人[1]可以在没有可察觉的质量损失的情况下对Llama2-7B-chat进行水印，水印可以用少于100个令牌来检测，并且该方案对简单的攻击提供了良好的抗篡改性。我们认为，水印的不可区分性，这是一些先前工作中强调的一个标准，是一个太强的要求：稍微修改logit分布的方案优于其不可区分的方案，在生成质量方面没有明显损失。我们公开发布我们的基准(https://github.com/wagner-group/markmywords)</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00273v2" target="_blank">2312.00273v2</a>
                              </td>
                              <td>Mark My Words: Analyzing and Evaluating Language Model Watermarks</td>
                              <td>Julien Piet</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00273v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00273v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wagner-group/markmywords" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01441v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01441v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01441v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01441v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4, our approach elevates the accuracy from COT baseline of 22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in the causal judgment task. Without using few-shot examples or external tools, UPAR significantly outperforms existing prompting methods on SCIBENCH, a challenging dataset containing collegiate-level mathematics, chemistry, and physics scientific problems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01441v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经展示了令人印象深刻的推理能力，许多研究致力于通过提示来增强这种能力。尽管做出了这些努力，但仍然明显缺乏统一的认识论基础。从康德的先验哲学中汲取灵感，我们提出了UPAR提示框架，旨在模仿LLM中的人类认知结构。UPAR框架分为四个阶段：“理解”、“计划”、“行动”和“反思”，能够从复杂的环境中提取结构化信息、预先规划解决方案、按计划执行和自我反思。这种结构显著提高了LLM推理的可解释性和准确性，产生了人类可理解和可检查的推理轨迹。此外，我们的工作为现有的提示技术提供了认识论基础，允许这些方法的可能系统集成。使用GPT-4，我们的方法在具有挑战性的GSM8K子集中将COT基线的准确率从22.92%提高到58.33%，在因果判断任务中将准确率从67.91%提高到75.40%。在不使用少量镜头示例或外部工具的情况下，UPAR显著优于SCIBENCH上现有的提示方法，SCIBENCH是一个具有挑战性的数据集，包含大学级别的数学、化学和物理科学问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01441v2" target="_blank">2310.01441v2</a>
                              </td>
                              <td>UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities</td>
                              <td>Hejia Geng</td>
                              <td>2023-09-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01441v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01441v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04021v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Study on the Calibration of In-context Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04021v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04021v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04021v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern auto-regressive language models are trained to minimize log loss on broad data by predicting the next token so they are expected to get calibrated answers when framing a problem as a next-token prediction task. We study this for in-context learning (ICL), a widely used way to adapt frozen large language models (LLMs) via crafting prompts, and investigate the trade-offs between performance and calibration on a wide range of natural language understanding and reasoning tasks. We conduct extensive experiments to show that such trade-offs may get worse as we increase model size, incorporate more ICL examples, and fine-tune models using instruction, dialog, or reinforcement learning from human feedback (RLHF) on carefully curated datasets. Furthermore, we find that common recalibration techniques that are widely effective such as temperature scaling provide limited gains in calibration errors, suggesting that new methods may be required for settings where models are expected to be reliable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04021v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代自回归语言模型经过训练，通过预测下一个令牌来最大限度地减少广泛数据上的日志损失，因此在将问题定义为下一个代币预测任务时，它们有望得到校准的答案。我们将其用于上下文学习（ICL），这是一种广泛使用的通过制作提示来适应冻结的大型语言模型（LLM）的方法，并研究了在各种自然语言理解和推理任务中性能和校准之间的权衡。我们进行了广泛的实验，以表明随着我们在精心策划的数据集上增加模型大小、引入更多的ICL示例，以及使用指令、对话或从人类反馈中强化学习（RLHF）来微调模型，这种权衡可能会变得更糟。此外，我们发现，广泛有效的常见重新校准技术（如温度缩放）在校准误差方面提供了有限的增益，这表明可能需要新的方法来进行模型可靠的设置。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04021v1" target="_blank">2312.04021v1</a>
                              </td>
                              <td>A Study on the Calibration of In-context Learning</td>
                              <td>Hanlin Zhang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04021v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04021v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04004v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Occlusion-based Detection of Trojan-triggering Inputs in Large Language Models of Code</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04004v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04004v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04004v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are becoming an integrated part of software development. These models are trained on large datasets for code, where it is hard to verify each data point. Therefore, a potential attack surface can be to inject poisonous data into the training data to make models vulnerable, aka trojaned. It can pose a significant threat by hiding manipulative behaviors inside models, leading to compromising the integrity of the models in downstream tasks.   In this paper, we propose an occlusion-based human-in-the-loop technique, OSeql, to distinguish trojan-triggering inputs of code. The technique is based on the observation that trojaned neural models of code rely heavily on the triggering part of input; hence, its removal would change the confidence of the models in their prediction substantially. Our results suggest that OSeql can detect the triggering inputs with almost 100% recall. We discuss the problem of false positives and how to address them. These results provide a baseline for future studies in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04004v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）正在成为软件开发的一个集成部分。这些模型是在大型代码数据集上训练的，很难验证每个数据点。因此，一个潜在的攻击面可能是向训练数据中注入有毒数据，使模型变得脆弱，也就是特洛伊木马。它可以通过将操纵行为隐藏在模型内部，从而在下游任务中损害模型的完整性，从而构成重大威胁。在本文中，我们提出了一种基于阻塞的人在环技术OSeql，用于区分特洛伊木马触发的代码输入。该技术基于以下观察结果：代码的特洛伊木马神经模型严重依赖于输入的触发部分；因此，删除它将大大改变模型对其预测的置信度。我们的结果表明，OSeql可以以几乎100%的召回率检测触发输入。我们讨论了误报的问题以及如何解决这些问题。这些结果为该领域未来的研究提供了基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04004v1" target="_blank">2312.04004v1</a>
                              </td>
                              <td>Occlusion-based Detection of Trojan-triggering Inputs in Large Language Models of Code</td>
                              <td>Aftab Hussain</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04004v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04004v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03121v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Agents using Social Choice Theory</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03121v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03121v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03121v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03121v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们认为，许多普遍的评价问题可以从投票理论的角度来看待。每个任务都被解释为一个单独的投票人，只需要对代理进行顺序排名或成对比较就可以产生整体评估。通过将聚合器视为一种社会福利函数，我们能够利用数百年来对社会选择理论的研究，推导出具有公理基础的原则性评估框架。这些评估具有可解释性和灵活性，同时避免了跨任务评估目前面临的许多问题。我们将这种投票即评估（VasE）框架应用于多种环境，包括强化学习、大型语言模型和人类。在实践中，我们观察到，在复杂的七人游戏中，VasE可以比流行的评估框架（Elo和Nash平均）更稳健，发现仅从分数来看不明显的评估数据属性，并且可以比Elo更好地预测结果。我们确定了一种特定的方法，即最大彩票，它满足与评估相关的重要一致性性质，在计算上是有效的（评估数据大小的多项式），并确定了博弈论循环。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03121v2" target="_blank">2312.03121v2</a>
                              </td>
                              <td>Evaluating Agents using Social Choice Theory</td>
                              <td>Marc Lanctot</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03121v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03121v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/voting" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_02205v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GEMRec: Towards Generative Model Recommendation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_02205v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_02205v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_02205v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recommender Systems are built to retrieve relevant items to satisfy users' information needs. The candidate corpus usually consists of a finite set of items that are ready to be served, such as videos, products, or articles. With recent advances in Generative AI such as GPT and Diffusion models, a new form of recommendation task is yet to be explored where items are to be created by generative models with personalized prompts. Taking image generation as an example, with a single prompt from the user and access to a generative model, it is possible to generate hundreds of new images in a few minutes. How shall we attain personalization in the presence of "infinite" items? In this preliminary study, we propose a two-stage framework, namely Prompt-Model Retrieval and Generated Item Ranking, to approach this new task formulation. We release GEMRec-18K, a prompt-model interaction dataset with 18K images generated by 200 publicly-available generative models paired with a diverse set of 90 textual prompts. Our findings demonstrate the promise of generative model recommendation as a novel personalization problem and the limitations of existing evaluation metrics. We highlight future directions for the RecSys community to advance towards generative recommender systems. Our code and dataset are available at https://github.com/MAPS-research/GEMRec.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_02205v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>推荐系统旨在检索相关项目，以满足用户的信息需求。候选语料库通常由有限的一组准备提供的项目组成，如视频、产品或文章。随着GPT和Diffusion模型等生成人工智能的最新进展，一种新形式的推荐任务有待探索，其中项目将由具有个性化提示的生成模型创建。以图像生成为例，只需用户的一个提示和对生成模型的访问，就可以在几分钟内生成数百张新图像。在“无限”的物品面前，我们该如何实现个性化？在这项初步研究中，我们提出了一个两阶段的框架，即即时模型检索和生成项目排序，来处理这种新的任务公式。我们发布了GEMRec-18K，这是一个提示模型交互数据集，由200个公开可用的生成模型生成18K图像，并与90个不同的文本提示配对。我们的研究结果证明了生成模型推荐作为一个新的个性化问题的前景以及现有评估指标的局限性。我们强调了RecSys社区向生成推荐系统发展的未来方向。我们的代码和数据集可在https://github.com/maps-research/gemrec.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.02205v2" target="_blank">2308.02205v2</a>
                              </td>
                              <td>GEMRec: Towards Generative Model Recommendation</td>
                              <td>Yuanhe Guo</td>
                              <td>2023-08-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_02205v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.02205v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/maps-research/gemrec" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03987v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03987v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03987v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03987v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Entity resolution (ER) is an important data integration task with a wide spectrum of applications. The state-of-the-art solutions on ER rely on pre-trained language models (PLMs), which require fine-tuning on a lot of labeled matching/non-matching entity pairs. Recently, large languages models (LLMs), such as GPT-4, have shown the ability to perform many tasks without tuning model parameters, which is known as in-context learning (ICL) that facilitates effective learning from a few labeled input context demonstrations. However, existing ICL approaches to ER typically necessitate providing a task description and a set of demonstrations for each entity pair and thus have limitations on the monetary cost of interfacing LLMs. To address the problem, in this paper, we provide a comprehensive study to investigate how to develop a cost-effective batch prompting approach to ER. We introduce a framework BATCHER consisting of demonstration selection and question batching and explore different design choices that support batch prompting for ER. We also devise a covering-based demonstration selection strategy that achieves an effective balance between matching accuracy and monetary cost. We conduct a thorough evaluation to explore the design space and evaluate our proposed strategies. Through extensive experiments, we find that batch prompting is very cost-effective for ER, compared with not only PLM-based methods fine-tuned with extensive labeled data but also LLM-based methods with manually designed prompting. We also provide guidance for selecting appropriate design choices for batch prompting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03987v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实体解析（ER）是一项重要的数据集成任务，具有广泛的应用。ER上最先进的解决方案依赖于预训练的语言模型（PLM），这需要对许多标记的匹配/不匹配实体对进行微调。最近，大型语言模型（LLM），如GPT-4，已经显示出在不调整模型参数的情况下执行许多任务的能力，这被称为上下文学习（ICL），有助于从一些标记的输入上下文演示中进行有效学习。然而，现有的ER ICL方法通常需要为每个实体对提供任务描述和一组演示，因此对接口LLM的货币成本有限制。为了解决这个问题，在本文中，我们提供了一个全面的研究来研究如何开发一种具有成本效益的ER批量提示方法。我们引入了一个由演示选择和问题批量组成的框架BATCHER，并探索了支持ER批量提示的不同设计选择。我们还设计了一种基于覆盖的演示选择策略，以实现匹配准确性和货币成本之间的有效平衡。我们进行了一次彻底的评估，以探索设计空间并评估我们提出的策略。通过大量的实验，我们发现批量提示对于ER来说是非常划算的，不仅与使用大量标记数据微调的基于PLM的方法相比，而且与使用手动设计提示的基于LLM的方法相比。我们还为批量提示选择合适的设计选项提供指导。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03987v1" target="_blank">2312.03987v1</a>
                              </td>
                              <td>Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration</td>
                              <td>Meihao Fan</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03987v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fmh1art/batcher" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03970v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03970v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03970v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03970v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Medical report generation demands automatic creation of coherent and precise descriptions for medical images. However, the scarcity of labelled medical image-report pairs poses formidable challenges in developing large-scale neural networks capable of harnessing the potential of artificial intelligence, exemplified by large language models. This study builds upon the state-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2, to customize general large-scale foundation models. Integrating adapter tuning and a medical knowledge enhancement loss, our model significantly improves accuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023 demonstrates our model's prowess, achieving the best-averaged results against several state-of-the-art methods. Significant improvements in ROUGE and CIDEr underscore our method's efficacy, highlighting promising outcomes for the rapid medical-domain adaptation of the vision-language foundation models in addressing challenges posed by data scarcity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03970v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>医疗报告生成要求自动创建连贯和精确的医疗图像描述。然而，标记的医学图像报告对的稀缺性给开发能够利用人工智能潜力的大规模神经网络带来了巨大挑战，例如大型语言模型。这项研究建立在最先进的视觉语言预训练和微调方法BLIP-2的基础上，以定制通用的大型基础模型。将适配器调整和医学知识增强损失相结合，我们的模型显著提高了准确性和一致性。在ImageCLEFmedical 2023的数据集上进行的验证证明了我们的模型的能力，与几种最先进的方法相比，实现了最佳的平均结果。ROUGE和CIDEr的显著改进强调了我们方法的有效性，突出了视觉语言基础模型在应对数据短缺带来的挑战方面的快速医学领域自适应的有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03970v1" target="_blank">2312.03970v1</a>
                              </td>
                              <td>Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models</td>
                              <td>Shibin Wu</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03970v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03970v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01619v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01619v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01619v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01619v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The paper introduces LEMR, a framework that reduces annotation costs for model selection tasks. Our approach leverages ensemble methods to generate pseudo-labels, employs uncertainty sampling for target acquisition, and utilizes a Z-score mechanism for iterative committee reelection to refine model ranks. We present a systematic study across various selection metrics, demonstrating that LEMR achieves comparable results to fully labeled datasets with a fraction of the labeling budget. Our findings indicate that LEMR not only economizes the labeling effort in weak supervision and semi-supervised learning settings but also effectively guides prompt selection for large language models. With extensive experiments across 23 tasks, we reveal that our framework can dramatically decrease the labeling cost without compromising the accuracy of model selection, thereby offering a cost-effective alternative to traditional practices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01619v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了LEMR，这是一个降低模型选择任务注释成本的框架。我们的方法利用集成方法生成伪标签，采用不确定性采样进行目标获取，并利用Z评分机制进行迭代委员会连任，以细化模型等级。我们对各种选择指标进行了系统研究，证明LEMR以很小的标记预算实现了与完全标记数据集相当的结果。我们的研究结果表明，LEMR不仅在弱监督和半监督学习环境中节省了标记工作，而且有效地指导了大型语言模型的及时选择。通过对23项任务的广泛实验，我们发现我们的框架可以在不影响模型选择准确性的情况下显著降低标记成本，从而为传统实践提供了一种具有成本效益的替代方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01619v2" target="_blank">2312.01619v2</a>
                              </td>
                              <td>How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking</td>
                              <td>Zhengyu Hu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01619v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01619v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ppsmk388/morabench" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03905v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03905v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03905v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03905v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample. We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03905v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经符号人工智能弥合了纯符号学习方法和神经学习方法之间的差距。这通常需要最大化神经网络输出分布的符号约束的可能性。这样的输出分布通常被假设为完全因子分解。这限制了神经符号学习对更具表现力的自回归分布的适用性，例如变压器。在这样的分布下，计算即使是简单约束的可能性也是困难的。我们建议对其随机局部近似进行约束，而不是试图对整个输出分布强制执行约束。更准确地说，我们在以模型样本为中心的基于伪似然的近似下优化约束的似然性。我们的近似是因式分解的，允许重用子问题的解决方案，这是有效计算神经符号损失的主要原则。此外，它是似然性的局部高保真近似，在模型样本周围表现出低熵和KL散度。我们评估了我们的数独和最短路径预测方法，称为自回归生成，并观察到我们大大提高了基础模型预测逻辑一致输出的能力。我们还评估了对大型语言模型进行解毒的任务。使用一个简单的约束，不允许列出一系列有毒单词，我们能够引导模型的输出远离有毒的世代，与以前的方法相比，实现了SoTA解毒。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03905v1" target="_blank">2312.03905v1</a>
                              </td>
                              <td>A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints</td>
                              <td>Kareem Ahmed</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03905v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03905v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03884v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WonderJourney: Going from Anywhere to Everywhere</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03884v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03884v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03884v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce WonderJourney, a modularized framework for perpetual 3D scene generation. Unlike prior work on view generation that focuses on a single type of scenes, we start at any user-provided location (by a text description or an image) and generate a journey through a long sequence of diverse yet coherently connected 3D scenes. We leverage an LLM to generate textual descriptions of the scenes in this journey, a text-driven point cloud generation pipeline to make a compelling and coherent sequence of 3D scenes, and a large VLM to verify the generated scenes. We show compelling, diverse visual results across various scene types and styles, forming imaginary "wonderjourneys". Project website: https://kovenyu.com/WonderJourney/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03884v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍WonderJourney，一个用于永久3D场景生成的模块化框架。与之前专注于单一类型场景的视图生成工作不同，我们从任何用户提供的位置（通过文本描述或图像）开始，生成一个穿越一长串多样但连贯连接的3D场景的旅程。我们利用LLM生成旅程中场景的文本描述，利用文本驱动的点云生成管道生成引人注目且连贯的3D场景序列，利用大型VLM验证生成的场景。我们在各种场景类型和风格中展示了引人注目的、多样化的视觉效果，形成了想象中的“奇妙之旅”。项目网站：https://kovenyu.com/wonderjourney/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03884v1" target="_blank">2312.03884v1</a>
                              </td>
                              <td>WonderJourney: Going from Anywhere to Everywhere</td>
                              <td>Hong-Xing Yu</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03884v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03884v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04233v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-tune vision foundation model for crack segmentation in civil infrastructures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04233v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04233v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04233v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale foundation models have become the mainstream method in the field of deep learning, while in civil engineering, the scale of AI models is strictly limited. In this work, vision foundation model is introduced for crack segmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank adaptation, are adopted to fine-tune the foundation model in the field of semantic segmentation: Segment Anything Model (SAM). The fine-tuned model CrackSAM is much larger than all the existing crack segmentation models, but shows excellent performance. To test the zero-shot performance of the proposed method, two unique datasets related to road and exterior wall cracks are collected, annotated and open-sourced, in total 810 images. Comparative experiments are conducted with twelve mature semantic segmentation models. On datasets with artificial noise and previously unseen datasets, the performance of CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits remarkable superiority, particularly in challenging conditions such as dim lighting, shadows, road markings, construction joints, and other interference factors. Such cross-scenario results demonstrate the outstanding zero-shot capability of foundation models, and provide new ideas for the development of vision models in civil engineering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04233v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模的基础模型已经成为深度学习领域的主流方法，而在土木工程中，人工智能模型的规模受到严格限制。在这项工作中，引入视觉基础模型进行裂纹分割。采用自适应和低秩自适应两种参数有效的微调方法对语义分割领域的基础模型进行微调：分段任意模型（SAM）。微调模型CrackSAM比所有现有的裂纹分割模型都大得多，但表现出优异的性能。为了测试所提出方法的零样本性能，收集、注释和开源了两个与道路和外墙裂缝相关的独特数据集，共810张图像。对12个成熟的语义分割模型进行了对比实验。在具有人工噪声的数据集和以前看不见的数据集上，CrackSAM的性能远远超过所有最先进的模型。CrackSAM表现出非凡的优势，尤其是在光线昏暗、阴影、道路标线、施工缝和其他干扰因素等具有挑战性的条件下。这样的跨场景结果展示了基础模型卓越的零样本能力，并为土木工程视觉模型的发展提供了新的思路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04233v1" target="_blank">2312.04233v1</a>
                              </td>
                              <td>Fine-tune vision foundation model for crack segmentation in civil infrastructures</td>
                              <td>Kang Ge</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04233v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04233v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04197v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMBA: A Trainable Segmentation Web-App with Smart Labelling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04197v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04197v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04197v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segmentation is the assigning of a semantic class to every pixel in an image and is a prerequisite for various statistical analysis tasks in materials science, like phase quantification, physics simulations or morphological characterization. The wide range of length scales, imaging techniques and materials studied in materials science means any segmentation algorithm must generalise to unseen data and support abstract, user-defined semantic classes. Trainable segmentation is a popular interactive segmentation paradigm where a classifier is trained to map from image features to user drawn labels. SAMBA is a trainable segmentation tool that uses Meta's Segment Anything Model (SAM) for fast, high-quality label suggestions and a random forest classifier for robust, generalizable segmentations. It is accessible in the browser (https://www.sambasegment.com/) without the need to download any external dependencies. The segmentation backend is run in the cloud, so does not require the user to have powerful hardware.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04197v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分割是将语义类分配给图像中的每个像素，是材料科学中各种统计分析任务的先决条件，如相位量化、物理模拟或形态表征。材料科学中研究的长度尺度、成像技术和材料范围广泛，这意味着任何分割算法都必须推广到看不见的数据，并支持抽象的、用户定义的语义类。可训练分割是一种流行的交互式分割范式，其中分类器被训练为从图像特征映射到用户绘制的标签。SAMBA是一种可训练的分割工具，它使用Meta的Segment Anything Model（SAM）进行快速、高质量的标签建议，并使用随机森林分类器进行稳健、可推广的分割。可以在浏览器中访问(https://www.sambasegment.com/)而不需要下载任何外部依赖项。分段后端在云中运行，因此不需要用户拥有强大的硬件。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04197v1" target="_blank">2312.04197v1</a>
                              </td>
                              <td>SAMBA: A Trainable Segmentation Web-App with Smart Labelling</td>
                              <td>Ronan Docherty</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04197v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04197v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03628v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Segment Anything Model Towards Open-Vocabulary Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03628v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03628v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03628v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent Segment Anything Model (SAM) has emerged as a new paradigmatic vision foundation model, showcasing potent zero-shot generalization and flexible prompting. Despite SAM finding applications and adaptations in various domains, its primary limitation lies in the inability to grasp object semantics. In this paper, we present Sambor to seamlessly integrate SAM with the open-vocabulary object detector in an end-to-end framework. While retaining all the remarkable capabilities inherent to SAM, we enhance it with the capacity to detect arbitrary objects based on human inputs like category names or reference expressions. To accomplish this, we introduce a novel SideFormer module that extracts SAM features to facilitate zero-shot object localization and inject comprehensive semantic information for open-vocabulary recognition. In addition, we devise an open-set region proposal network (Open-set RPN), enabling the detector to acquire the open-set proposals generated by SAM. Sambor demonstrates superior zero-shot performance across benchmarks, including COCO and LVIS, proving highly competitive against previous SoTA methods. We aspire for this work to serve as a meaningful endeavor in endowing SAM to recognize diverse object categories and advancing open-vocabulary learning with the support of vision foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03628v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的Segment Anything Model（SAM）已经成为一种新的范式视觉基础模型，展示了强大的零样本泛化和灵活的提示。尽管SAM在各个领域都能找到应用程序和调整，但其主要局限性在于无法掌握对象语义。在本文中，我们提出了Sambor，以在端到端的框架中将SAM与开放词汇表对象检测器无缝集成。在保留SAM固有的所有显著功能的同时，我们增强了它基于人工输入（如类别名称或引用表达式）检测任意对象的能力。为了实现这一点，我们引入了一个新颖的SideFormer模块，该模块提取SAM特征以促进零样本对象定位，并为开放式词汇识别注入全面的语义信息。此外，我们设计了一个开放集区域建议网络（open-set-RPN），使检测器能够获取SAM生成的开放集建议。Sambor在包括COCO和LVIS在内的基准测试中表现出优异的零样本性能，证明与以前的SoTA方法相比具有高度竞争力。我们希望这项工作能成为一项有意义的努力，使SAM能够识别不同的对象类别，并在愿景基础模型的支持下推进开放式词汇学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03628v1" target="_blank">2312.03628v1</a>
                              </td>
                              <td>Boosting Segment Anything Model Towards Open-Vocabulary Learning</td>
                              <td>Xumeng Han</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03628v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03628v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ucas-vg/sambor" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03585v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Model Assisted Weakly Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03585v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03585v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03585v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03585v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作旨在利用预先训练的基础模型，如对比语言图像预训练（CLIP）和任意分割模型（SAM），来解决使用图像级标签的弱监督语义分割（WSSS）问题。为此，我们提出了一种基于CLIP和SAM的从粗到细的框架，用于生成高质量的分割种子。具体来说，我们构建了一个图像分类任务和一个种子分割任务，这两个任务由具有冻结权重的CLIP和两组可学习的任务特定提示联合执行。设计了基于SAM的种子（SAMS）模块，并将其应用于每个任务，以生成粗略或精细的种子图。此外，我们设计了由图像级标签监督的多标签对比损失和由生成的粗种子图监督的CAM激活损失。这些损失用于学习提示，这是我们框架中唯一需要学习的部分。一旦学习了提示，我们将每个图像以及学习到的分割特定提示输入到CLIP和SAMS模块中，以产生高质量的分割种子。像其他两阶段WSSS方法一样，这些种子充当伪标签来训练现成的分割网络。实验表明，我们的方法在PASCAL VOC 2012上取得了最先进的性能，在MS COCO 2014上取得了具有竞争力的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03585v1" target="_blank">2312.03585v1</a>
                              </td>
                              <td>Foundation Model Assisted Weakly Supervised Semantic Segmentation</td>
                              <td>Xiaobo Yang</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03585v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03585v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03502v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03502v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03502v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03502v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything(SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03502v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的成功启发了计算机视觉界探索能够通过提示工程进行零/少镜头泛化的图像分割基础模型。Segment Anything（SAM）是最先进的图像分割基础模型，展示了强大的零/少镜头泛化能力。尽管取得了成功，但最近的研究揭示了SAM在强分布转变下的弱点。特别是，SAM在损坏的自然图像、伪装图像、医学图像等上表现不佳。受观察结果的启发，我们旨在开发一种基于自我训练的策略，使SAM适应目标分布。鉴于大源数据集、高计算成本和不正确的伪标签的独特挑战，我们提出了一种具有锚正则化和低秩微调的弱监督自训练架构，以提高自适应的鲁棒性和计算效率。我们验证了5种类型的下游分割任务的有效性，包括自然清洁/损坏图像、医学图像、伪装图像和机器人图像。我们提出的方法本质上是任务不可知的，在具有相同测试提示输入的几乎所有下游任务上都优于预先训练的SAM和最先进的领域自适应方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03502v1" target="_blank">2312.03502v1</a>
                              </td>
                              <td>Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</td>
                              <td>Haojie Zhang</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03502v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03502v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhang-haojie/wesam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_06444v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_06444v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_06444v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_06444v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tongue segmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, when confronted with tongue images that differ from the training set or possess challenging backgrounds, these methods demonstrate limited performance. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment Anything Model). SAM is a large-scale pretrained interactive segmentation model known for its powerful zero-shot generalization capability. Applying SAM to tongue segmentation leverages its learned prior knowledge from natural images, enabling the achievement of zero-shot segmentation for various types of tongue images. In this study, a Prompt Generator based on object detection is integrated into SAM to enable an end-to-end automated tongue segmentation method. Experiments demonstrate that TongueSAM achieves exceptional performance across various of tongue segmentation datasets, particularly under zero-shot. Even when dealing with challenging background tongue images, TongueSAM achieves a mIoU of 95.23\% under zero-shot conditions, surpassing other segmentation methods. As far as we know, this is the first application of large-scale pretrained model for tongue segmentation. The project mentioned in this paper is currently publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_06444v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>舌头分割是中医舌诊自动化的首要步骤，对舌诊结果起着重要作用。目前，许多基于深度学习的方法已经取得了可喜的成果。然而，当面对不同于训练集或具有挑战性背景的舌头图像时，这些方法表现出有限的性能。针对这一问题，本文提出了一种基于SAM（Segment Anything model）的通用舌头分割模型TongueSAM。SAM是一种大规模的预训练交互式分割模型，以其强大的零样本泛化能力而闻名。将SAM应用于舌头分割利用了其从自然图像中学习到的先验知识，实现了对各种类型的舌头图像的零样本分割。在这项研究中，基于对象检测的提示生成器被集成到SAM中，以实现端到端的自动舌头分割方法。实验表明，TongueSAM在各种舌头分割数据集上都取得了优异的性能，尤其是在零样本下。即使在处理具有挑战性的背景舌头图像时，TongueSAM在零样本条件下也达到了95.23\%的mIoU，超过了其他分割方法。据我们所知，这是大规模预训练模型在舌头分割中的首次应用。本文中提到的项目目前已公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.06444v3" target="_blank">2308.06444v3</a>
                              </td>
                              <td>TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot</td>
                              <td>Shan Cao</td>
                              <td>2023-08-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_06444v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.06444v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cshan-github/tonguesam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03209v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cache Me if You Can: Accelerating Diffusion Models through Block Caching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03209v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03209v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03209v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03209v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>扩散模型由于其生成真实感图像的能力，最近在图像合成领域发生了革命性的变化。然而，扩散模型的主要缺点之一是图像生成过程成本高昂。为了从随机噪声中迭代地细化图像，必须多次应用大型图像到图像网络。虽然最近的许多工作提出了减少所需步骤数量的技术，但它们通常将底层去噪网络视为黑盒。在这项工作中，我们研究了网络中各层的行为，发现1）各层的输出随时间平稳变化，2）各层显示出不同的变化模式，3）从一步到另一步的变化通常非常小。我们假设去噪网络中的许多层计算是冗余的。利用这一点，我们引入了块缓存，在块缓存中，我们重用前面步骤的层块的输出，以加快推理速度。此外，我们提出了一种基于每个块随时间步长的变化来自动确定缓存时间表的技术。在我们的实验中，我们通过FID、人类评估和定性分析表明，块缓存可以在相同的计算成本下生成具有更高视觉质量的图像。我们为不同的最先进的模型（LDM和EMU）和求解器（DDIM和DPM）演示了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03209v1" target="_blank">2312.03209v1</a>
                              </td>
                              <td>Cache Me if You Can: Accelerating Diffusion Models through Block Caching</td>
                              <td>Felix Wimbauer</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03209v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03209v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03203v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03203v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03203v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03203v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework leads to warp-level divergence. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03203v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，3D场景表示已经获得了巨大的普及。使用神经辐射场的方法适用于传统任务，如新颖的视图合成。近年来，出现了一些工作，旨在将NeRF的功能扩展到视图合成之外，用于语义感知任务，如使用2D基础模型中的3D特征场提取进行编辑和分割。然而，这些方法有两个主要的局限性：（a）它们受到NeRF管道的渲染速度的限制，以及（b）隐式表示的特征场受到连续性伪影的影响，从而降低了特征质量。最近，3D高斯散射在实时辐射场渲染方面表现出了最先进的性能。在这项工作中，我们更进一步：除了辐射场渲染外，我们还通过2D基础模型蒸馏实现了对任意维度语义特征的3D高斯飞溅。这种转换并不简单：天真地将特征字段合并到3DGS框架中会导致扭曲级别的差异。我们建议对架构和培训进行更改，以有效地避免此问题。我们提出的方法是通用的，我们的实验展示了新颖的视图语义分割、语言引导编辑和通过从最先进的2D基础模型（如SAM和CLIP-LSeg）中学习特征字段来分割任何内容。在整个实验中，我们的蒸馏方法能够提供类似或更好的结果，同时训练和渲染速度明显更快。此外，据我们所知，我们是第一个通过利用SAM模型启用点和边界框提示进行辐射场操作的方法。项目网站：https://feature-3dgs.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03203v1" target="_blank">2312.03203v1</a>
                              </td>
                              <td>Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</td>
                              <td>Shijie Zhou</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03203v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03203v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03119v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI-SAM: Automatic and Interactive Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03119v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03119v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03119v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic segmentation is a core task in computer vision. Existing methods are generally divided into two categories: automatic and interactive. Interactive approaches, exemplified by the Segment Anything Model (SAM), have shown promise as pre-trained models. However, current adaptation strategies for these models tend to lean towards either automatic or interactive approaches. Interactive methods depend on prompts user input to operate, while automatic ones bypass the interactive promptability entirely. Addressing these limitations, we introduce a novel paradigm and its first model: the Automatic and Interactive Segment Anything Model (AI-SAM). In this paradigm, we conduct a comprehensive analysis of prompt quality and introduce the pioneering Automatic and Interactive Prompter (AI-Prompter) that automatically generates initial point prompts while accepting additional user inputs. Our experimental results demonstrate AI-SAM's effectiveness in the automatic setting, achieving state-of-the-art performance. Significantly, it offers the flexibility to incorporate additional user prompts, thereby further enhancing its performance. The project page is available at https://github.com/ymp5078/AI-SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03119v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割是计算机视觉的核心任务。现有的方法通常分为两类：自动和交互式。交互式方法，以分段任意模型（SAM）为例，已经显示出作为预训练模型的前景。然而，目前这些模型的适应策略往往倾向于自动或交互式方法。交互式方法依赖于提示用户输入进行操作，而自动方法则完全绕过了交互式提示性。针对这些局限性，我们引入了一种新的范式及其第一个模型：自动和交互式分段任意模型（AI-SAM）。在这个范式中，我们对提示质量进行了全面的分析，并介绍了开创性的自动交互提示器（AI Promoter），它在接受额外用户输入的同时自动生成初始点提示。我们的实验结果证明了AI-SAM在自动设置方面的有效性，实现了最先进的性能。值得注意的是，它提供了包含额外用户提示的灵活性，从而进一步提高了性能。项目页面位于https://github.com/ymp5078/ai-sam.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03119v1" target="_blank">2312.03119v1</a>
                              </td>
                              <td>AI-SAM: Automatic and Interactive Segment Anything Model</td>
                              <td>Yimu Pan</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03119v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03119v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15776v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stable Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15776v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15776v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15776v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) achieves remarkable promptable segmentation given high-quality prompts which, however, often require good skills to specify. To make SAM robust to casual prompts, this paper presents the first comprehensive analysis on SAM's segmentation stability across a diverse spectrum of prompt qualities, notably imprecise bounding boxes and insufficient points. Our key finding reveals that given such low-quality prompts, SAM's mask decoder tends to activate image features that are biased towards the background or confined to specific object parts. To mitigate this issue, our key idea consists of calibrating solely SAM's mask attention by adjusting the sampling locations and amplitudes of image features, while the original SAM model architecture and weights remain unchanged. Consequently, our deformable sampling plugin (DSP) enables SAM to adaptively shift attention to the prompted target regions in a data-driven manner, facilitated by our effective robust training strategy (RTS). During inference, dynamic routing plugin (DRP) is proposed that toggles SAM between the deformable and regular grid sampling modes, conditioned on the input prompt quality. Thus, our solution, termed Stable-SAM, offers several advantages: 1) improved SAM's segmentation stability across a wide range of prompt qualities, while 2) retaining SAM's powerful promptable segmentation efficiency and generality, with 3) minimal learnable parameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive experiments across multiple datasets validate the effectiveness and advantages of our approach, underscoring Stable-SAM as a more robust solution for segmenting anything. Codes will be released upon acceptance. https://github.com/fanq15/Stable-SAM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15776v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在给出高质量提示的情况下实现了显著的可提示分段，然而，这通常需要良好的指定技能。为了使SAM对偶然提示具有鲁棒性，本文首次全面分析了SAM在各种提示质量范围内的分割稳定性，特别是不精确的边界框和不足的点。我们的关键发现表明，在这种低质量提示的情况下，SAM的掩码解码器倾向于激活偏向背景或局限于特定对象部分的图像特征。为了缓解这个问题，我们的关键思想包括通过调整图像特征的采样位置和幅度来单独校准SAM的掩模注意力，而原始SAM模型架构和权重保持不变。因此，我们的可变形采样插件（DSP）使SAM能够以数据驱动的方式自适应地将注意力转移到提示的目标区域，这得益于我们有效的鲁棒训练策略（RTS）。在推理过程中，提出了动态路由插件（DRP），以输入提示质量为条件，在可变形网格采样模式和规则网格采样模式之间切换SAM。因此，我们的解决方案，称为稳定SAM，提供了几个优势：1）在广泛的提示质量范围内提高了SAM的分割稳定性，同时2）保留了SAM强大的可提示分割效率和通用性，具有3）最小的可学习参数（0.08M）和快速适应（通过1个训练历元）。跨多个数据集的广泛实验验证了我们方法的有效性和优势，强调稳定SAM是一种更稳健的分割任何内容的解决方案。代码将在验收后发布。https://github.com/fanq15/stable-sam</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15776v2" target="_blank">2311.15776v2</a>
                              </td>
                              <td>Stable Segment Anything Model</td>
                              <td>Qi Fan</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15776v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15776v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fanq15/stable-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10121v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Slide-SAM: Medical SAM Meets Sliding Window</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10121v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10121v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10121v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has achieved a notable success in two-dimensional image segmentation in natural images. However, the substantial gap between medical and natural images hinders its direct application to medical image segmentation tasks. Particularly in 3D medical images, SAM struggles to learn contextual relationships between slices, limiting its practical applicability. Moreover, applying 2D SAM to 3D images requires prompting the entire volume, which is time- and label-consuming. To address these problems, we propose Slide-SAM, which treats a stack of three adjacent slices as a prediction window. It firstly takes three slices from a 3D volume and point- or bounding box prompts on the central slice as inputs to predict segmentation masks for all three slices. Subsequently, the masks of the top and bottom slices are then used to generate new prompts for adjacent slices. Finally, step-wise prediction can be achieved by sliding the prediction window forward or backward through the entire volume. Our model is trained on multiple public and private medical datasets and demonstrates its effectiveness through extensive 3D segmetnation experiments, with the help of minimal prompts. Code is available at \url{https://github.com/Curli-quan/Slide-SAM}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10121v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在自然图像的二维图像分割中取得了显著的成功。然而，医学图像和自然图像之间的巨大差距阻碍了其直接应用于医学图像分割任务。特别是在3D医学图像中，SAM难以学习切片之间的上下文关系，限制了其实际应用性。此外，将2D SAM应用于3D图像需要提示整个体积，这是耗时且耗费标签的。为了解决这些问题，我们提出了Slide SAM，它将三个相邻切片的堆栈视为预测窗口。它首先从3D体积中提取三个切片，并在中心切片上提示点或边界框作为输入，以预测所有三个切片的分割掩码。随后，顶部和底部切片的掩码用于生成相邻切片的新提示。最后，可以通过在整个体积中向前或向后滑动预测窗口来实现逐步预测。我们的模型在多个公共和私人医疗数据集上进行了训练，并在最小提示的帮助下，通过广泛的3D分割实验证明了其有效性。代码位于\url{https://github.com/curli-quan/slide-sam}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10121v2" target="_blank">2311.10121v2</a>
                              </td>
                              <td>Slide-SAM: Medical SAM Meets Sliding Window</td>
                              <td>Quan Quan</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10121v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10121v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/curli-quan/slide-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02464v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02464v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02464v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02464v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic segmentation of remote sensing imagery plays a pivotal role in extracting precise information for diverse down-stream applications. Recent development of the Segment Anything Model (SAM), an advanced general-purpose segmentation model, has revolutionized this field, presenting new avenues for accurate and efficient segmentation. However, SAM is limited to generating segmentation results without class information. Consequently, the utilization of such a powerful general vision model for semantic segmentation in remote sensing images has become a focal point of research. In this paper, we present a streamlined framework aimed at leveraging the raw output of SAM by exploiting two novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary (SGB). More specifically, we propose a novel object loss and further introduce a boundary loss as augmentative components to aid in model optimization in a general semantic segmentation framework. Taking into account the content characteristics of SGO, we introduce the concept of object consistency to leverage segmented regions lacking semantic information. By imposing constraints on the consistency of predicted values within objects, the object loss aims to enhance semantic segmentation performance. Furthermore, the boundary loss capitalizes on the distinctive features of SGB by directing the model's attention to the boundary information of the object. Experimental results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness of our proposed method. The source code for this work will be accessible at https://github.com/sstary/SSRS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02464v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>遥感图像的语义分割在为各种下游应用提取精确信息方面发挥着关键作用。Segment Anything Model（SAM）是一种先进的通用分割模型，它的最新发展彻底改变了这一领域，为准确高效的分割提供了新的途径。然而，SAM仅限于在没有类信息的情况下生成分割结果。因此，利用这种强大的通用视觉模型进行遥感图像的语义分割已成为研究的热点。在本文中，我们提出了一个简化的框架，旨在通过利用两个新概念，即SAM生成的对象（SGO）和SAM生成的边界（SGB），利用SAM的原始输出。更具体地说，我们提出了一种新的对象损失，并进一步引入了边界损失作为增强组件，以帮助在通用语义分割框架中进行模型优化。考虑到SGO的内容特征，我们引入了对象一致性的概念来利用缺乏语义信息的分割区域。通过对对象内预测值的一致性施加约束，对象丢失旨在提高语义分割性能。此外，边界损失利用了SGB的独特特征，将模型的注意力引向对象的边界信息。在两个著名的数据集，即ISPRS Vaihingen和LoveDA Urban上的实验结果证明了我们提出的方法的有效性。此作品的源代码可访问https://github.com/sstary/ssrs.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02464v1" target="_blank">2312.02464v1</a>
                              </td>
                              <td>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</td>
                              <td>Xianping Ma</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02464v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02464v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sstary/ssrs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11905v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11905v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11905v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11905v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A critical aspect of their effectiveness is the Engagement Zone (EZ), the spatial region within which a SAM can effectively engage and neutralize a target. Notably, the EZ is intrinsically related to the missile's maximum range; it defines the furthest distance at which a missile can intercept a target. The accurate computation of this EZ is essential but challenging due to the dynamic and complex factors involved, which often lead to high computational costs and extended processing times when using conventional simulation methods. In light of these challenges, our study investigates the potential of machine learning techniques, proposing an approach that integrates machine learning with a custom-designed simulation tool to train supervised algorithms. We leverage a comprehensive dataset of pre-computed SAM EZ simulations, enabling our model to accurately predict the SAM EZ for new input parameters. It accelerates SAM EZ simulations, enhances air defense strategic planning, and provides real-time insights, improving SAM system performance. The study also includes a comparative analysis of machine learning algorithms, illuminating their capabilities and performance metrics and suggesting areas for future research, highlighting the transformative potential of machine learning in SAM EZ simulations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11905v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>地对空导弹在现代防空系统中至关重要。其有效性的一个关键方面是交战区（EZ），这是SAM可以有效交战和压制目标的空间区域。值得注意的是，EZ本质上与导弹的最大射程有关；它定义了导弹拦截目标的最远距离。由于所涉及的动态和复杂因素，该EZ的精确计算是必不可少的，但具有挑战性，在使用传统模拟方法时，这些因素往往导致高计算成本和延长处理时间。鉴于这些挑战，我们的研究调查了机器学习技术的潜力，提出了一种将机器学习与定制设计的模拟工具相结合的方法，以训练监督算法。我们利用预先计算的SAM EZ模拟的综合数据集，使我们的模型能够准确预测新输入参数的SAM EZ。它加速SAM EZ模拟，增强防空战略规划，并提供实时见解，提高SAM系统性能。该研究还包括对机器学习算法的比较分析，阐明了它们的能力和性能指标，并提出了未来研究的领域，强调了机器学习在SAM EZ模拟中的变革潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11905v2" target="_blank">2311.11905v2</a>
                              </td>
                              <td>Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning</td>
                              <td>Joao P. A. Dantas</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11905v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11905v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jpadantas/sam-ez" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02420v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Granularity-adjusted Pixel-level Semantic Annotation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02420v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02420v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02420v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in computer vision predominantly rely on learning-based systems, leveraging annotations as the driving force to develop specialized models. However, annotating pixel-level information, particularly in semantic segmentation, presents a challenging and labor-intensive task, prompting the need for autonomous processes. In this work, we propose GranSAM which distinguishes itself by providing semantic segmentation at the user-defined granularity level on unlabeled data without the need for any manual supervision, offering a unique contribution in the realm of semantic mask annotation method. Specifically, we propose an approach to enable the Segment Anything Model (SAM) with semantic recognition capability to generate pixel-level annotations for images without any manual supervision. For this, we accumulate semantic information from synthetic images generated by the Stable Diffusion model or web crawled images and employ this data to learn a mapping function between SAM mask embeddings and object class labels. As a result, SAM, enabled with granularity-adjusted mask recognition, can be used for pixel-level semantic annotation purposes. We conducted experiments on the PASCAL VOC 2012 and COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU, respectively, compared to existing state-of-the-art methods when evaluated under our problem setting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02420v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉的最新进展主要依赖于基于学习的系统，利用注释作为开发专门模型的驱动力。然而，注释像素级信息，特别是在语义分割中，是一项具有挑战性和劳动密集型的任务，这促使人们需要自主处理。在这项工作中，我们提出了GranSAM，它通过在用户定义的粒度级别上对未标记的数据提供语义分割来区分自己，而不需要任何手动监督，在语义掩码注释方法领域提供了独特的贡献。具体来说，我们提出了一种方法，使具有语义识别能力的分段任意模型（SAM）能够在没有任何手动监督的情况下为图像生成像素级注释。为此，我们从稳定扩散模型生成的合成图像或网络爬行图像中积累语义信息，并使用这些数据来学习SAM掩码嵌入和对象类标签之间的映射函数。因此，通过粒度调整的掩码识别启用的SAM可以用于像素级语义注释目的。我们在PASCAL VOC 2012和COCO-80数据集上进行了实验，在我们的问题设置下进行评估时，与现有的最先进的方法相比，mIoU分别增加了17.95%和5.17%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02420v1" target="_blank">2312.02420v1</a>
                              </td>
                              <td>Towards Granularity-adjusted Pixel-level Semantic Annotation</td>
                              <td>Rohit Kundu</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02420v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02420v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03015v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03015v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03015v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03015v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-world 3D part segmentation is pivotal in diverse applications such as robotics and AR/VR. Traditional supervised methods often grapple with limited 3D data availability and struggle to generalize to unseen object categories. PartSLIP, a recent advancement, has made significant strides in zero- and few-shot 3D part segmentation. This is achieved by harnessing the capabilities of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic method for converting and lifting multi-view 2D bounding box predictions into 3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced version designed to overcome the limitations of its predecessor. Our approach incorporates two major improvements. First, we utilize a pre-trained 2D segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more precise and accurate annotations than the 2D bounding boxes used in PartSLIP. Second, PartSLIP++ replaces the heuristic 3D conversion process with an innovative modified Expectation-Maximization algorithm. This algorithm conceptualizes 3D instance segmentation as unobserved latent variables, and then iteratively refines them through an alternating process of 2D-3D matching and optimization with gradient descent. Through extensive evaluations, we show that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot 3D semantic and instance-based object part segmentation tasks. Code released at https://github.com/zyc00/PartSLIP2.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03015v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开放世界3D零件分割在机器人和AR/VR等各种应用中至关重要。传统的监督方法往往难以解决有限的3D数据可用性问题，难以推广到看不见的对象类别。PartSLIP是最近的一项进步，在零镜头和少镜头3D零件分割方面取得了重大进展。这是通过利用2D开放词汇检测模块GLIP的能力，并引入用于将多视图2D边界框预测转换和提升为3D分割掩模的启发式方法来实现的。在本文中，我们介绍了PartSLIP++，这是一个旨在克服其前代限制的增强版本。我们的方法包括两个主要改进。首先，我们利用预先训练的2D分割模型SAM来产生逐像素的2D分割，产生比PartSLIP中使用的2D边界框更精确的注释。其次，PartSLIP++用一种创新的改进的期望最大化算法取代了启发式的3D转换过程。该算法将3D实例分割概念化为未观察到的潜在变量，然后通过2D-3D匹配和梯度下降优化的交替过程对其进行迭代细化。通过广泛的评估，我们发现PartSLIP++在低镜头3D语义和基于实例的对象零件分割任务中都比PartSLIP表现出更好的性能。代码发布于https://github.com/zyc00/partslip2.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03015v1" target="_blank">2312.03015v1</a>
                              </td>
                              <td>PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation</td>
                              <td>Yuchen Zhou</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03015v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03015v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyc00/partslip2" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15138v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15138v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15138v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15138v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Climate change is increasingly disrupting worldwide agriculture, making global food production less reliable. To tackle the growing challenges in feeding the planet, cutting-edge management strategies, such as precision agriculture, empower farmers and decision-makers with rich and actionable information to increase the efficiency and sustainability of their farming practices. Crop-type maps are key information for decision-support tools but are challenging and costly to generate. We investigate the capabilities of Meta AI's Segment Anything Model (SAM) for crop-map prediction task, acknowledging its recent successes at zero-shot image segmentation. However, SAM being limited to up-to 3 channel inputs and its zero-shot usage being class-agnostic in nature pose unique challenges in using it directly for crop-type mapping. We propose using clustering consensus metrics to assess SAM's zero-shot performance in segmenting satellite imagery and producing crop-type maps. Although direct crop-type mapping is challenging using SAM in zero-shot setting, experiments reveal SAM's potential for swiftly and accurately outlining fields in satellite images, serving as a foundation for subsequent crop classification. This paper attempts to highlight a use-case of state-of-the-art image segmentation models like SAM for crop-type mapping and related specific needs of the agriculture industry, offering a potential avenue for automatic, efficient, and cost-effective data products for precision agriculture practices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15138v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>气候变化正在日益扰乱全球农业，使全球粮食生产变得不那么可靠。为了应对养活地球方面日益增长的挑战，精准农业等尖端管理战略为农民和决策者提供了丰富而可操作的信息，以提高其农业实践的效率和可持续性。作物类型图是决策支持工具的关键信息，但生成起来具有挑战性且成本高昂。我们研究了Meta AI的Segment Anything Model（SAM）用于crop-map预测任务的能力，承认其最近在零样本图像分割方面的成功。然而，SAM被限制为最多3个通道输入，并且其零样本的使用本质上是类认知的，这在将其直接用于crop型映射方面带来了独特的挑战。我们建议使用聚类一致性度量来评估SAM在分割卫星图像和生成crop型地图方面的零样本性能。尽管在零样本环境中使用SAM进行直接作物类型测绘具有挑战性，但实验揭示了SAM在卫星图像中快速准确地勾勒田地轮廓的潜力，为后续作物分类奠定了基础。本文试图强调最先进的图像分割模型（如SAM）在作物类型测绘和农业相关特定需求中的使用案例，为精确农业实践提供自动化、高效和成本效益高的数据产品提供潜在途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15138v2" target="_blank">2311.15138v2</a>
                              </td>
                              <td>Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture</td>
                              <td>Rutuja Gurav</td>
                              <td>2023-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15138v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15138v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02312v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02312v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02312v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02312v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video games have served as useful benchmarks for the decision making community, but going beyond Atari games towards training agents in modern games has been prohibitively expensive for the vast majority of the research community. Recent progress in the research, development and open release of large vision models has the potential to amortize some of these costs across the community. However, it is currently unclear which of these models have learnt representations that retain information critical for sequential decision making. Towards enabling wider participation in the research of gameplaying agents in modern games, we present a systematic study of imitation learning with publicly available visual encoders compared to the typical, task-specific, end-to-end training approach in Minecraft, Minecraft Dungeons and Counter-Strike: Global Offensive.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02312v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>电子游戏已经成为决策群体的有用基准，但对于绝大多数研究群体来说，超越雅达利游戏，在现代游戏中培养代理人的成本高得令人望而却步。最近在大型视觉模型的研究、开发和公开发布方面取得的进展有可能在整个社区分摊其中的一些成本。然而，目前尚不清楚这些模型中的哪一个已经学会了保留对顺序决策至关重要的信息的表示。为了更广泛地参与现代游戏中游戏代理的研究，我们对使用公开的视觉编码器的模仿学习进行了系统研究，与《我的世界》、《我的地下城》和《反恐精英：全球攻势》中典型的、特定任务的端到端训练方法进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02312v1" target="_blank">2312.02312v1</a>
                              </td>
                              <td>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</td>
                              <td>Lukas Schäfer</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02312v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02312v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02231v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quality Diversity in the Amorphous Fortress (QD-AF): Evolving for Complexity in 0-Player Games</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02231v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02231v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02231v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We explore the generation of diverse environments using the Amorphous Fortress (AF) simulation framework. AF defines a set of Finite State Machine (FSM) nodes and edges that can be recombined to control the behavior of agents in the `fortress' grid-world. The behaviors and conditions of the agents within the framework are designed to capture the common building blocks of multi-agent artificial life and reinforcement learning environments. Using quality diversity evolutionary search, we generate diverse sets of environments. These environments exhibit certain types of complexity according to measures of agents' FSM architectures and activations, and collective behaviors. Our approach, Quality Diversity in Amorphous Fortress (QD-AF) generates families of 0-player games akin to simplistic ecological models, and we identify the emergence of both competitive and co-operative multi-agent and multi-species survival dynamics. We argue that these generated worlds can collectively serve as training and testing grounds for learning algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02231v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们使用无定形堡垒（AF）模拟框架来探索不同环境的生成。AF定义了一组有限状态机（FSM）节点和边，这些节点和边可以重新组合以控制“堡垒”网格世界中代理的行为。框架内智能体的行为和条件旨在捕捉多智能体人工生命和强化学习环境的常见构建块。使用质量多样性进化搜索，我们生成不同的环境集。根据代理的FSM架构和激活以及集体行为的测量，这些环境表现出某些类型的复杂性。我们的方法，无定形堡垒中的质量多样性（QD-AF）生成了类似于简单生态模型的0人游戏家族，我们确定了竞争和合作的多主体和多物种生存动态的出现。我们认为，这些生成的世界可以共同作为学习算法的训练和测试场地。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02231v1" target="_blank">2312.02231v1</a>
                              </td>
                              <td>Quality Diversity in the Amorphous Fortress (QD-AF): Evolving for Complexity in 0-Player Games</td>
                              <td>Sam Earle</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02231v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02231v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01197v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Meets Point Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01197v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01197v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01197v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has established itself as a powerful zero-shot image segmentation model, enabled by efficient point-centric annotation and prompt-based models. While click and brush interactions are both well explored in interactive image segmentation, the existing methods on videos focus on mask annotation and propagation. This paper presents SAM-PT, a novel method for point-centric interactive video segmentation, empowered by SAM and long-term point tracking. SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation. Compared to traditional object-centric mask propagation strategies, we uniquely use point propagation to exploit local structure information agnostic to object semantics. We highlight the merits of point-based tracking through direct evaluation on the zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our experiments on popular video object segmentation and multi-object segmentation tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a point-based segmentation tracker yields better zero-shot performance and efficient interactions. We release our code that integrates different point trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01197v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）已经成为一个强大的零样本图像分割模型，通过高效的点中心注释和基于提示的模型实现。虽然点击和刷式交互在交互式图像分割中都得到了很好的探索，但现有的视频方法侧重于掩码注释和传播。本文提出了一种新的以点为中心的交互式视频分割方法SAM-PT，该方法由SAM和长期点跟踪相结合。SAM-PT利用稳健和稀疏的点选择和传播技术来生成掩模。与传统的以对象为中心的掩码传播策略相比，我们独特地使用点传播来利用与对象语义无关的局部结构信息。我们通过对零样本开放世界未识别视频对象（UVO）基准的直接评估，强调了基于点的跟踪的优点。我们在流行的视频对象分割和多对象分割跟踪基准（包括DAVIS、YouTube-VOS和BDD100K）上的实验表明，基于点的分割跟踪器可以产生更好的零样本性能和有效的交互。我们在上发布了集成了不同点跟踪器和视频分割基准的代码https://github.com/syscv/sam-pt.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01197v2" target="_blank">2307.01197v2</a>
                              </td>
                              <td>Segment Anything Meets Point Tracking</td>
                              <td>Frano Rajič</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01197v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01197v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/syscv/sam-pt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01531v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SANeRF-HQ: Segment Anything for NeRF in High Quality</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01531v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01531v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01531v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Additional information can be found at https://lyclyc52.github.io/SANeRF-HQ/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01531v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，Segment Anything Model（SAM）展示了零样本分割的显著能力，而NeRF（Neural Radiance Fields）作为一种解决新视图合成之外的各种3D问题的方法而广受欢迎。尽管最初尝试将这两种方法结合到3D分割中，但它们面临着在复杂场景中准确、一致地分割对象的挑战。在本文中，我们介绍了高质量NeRF的任何分割（SANeRF-HQ），以实现给定场景中任何对象的高质量3D分割。SANeRF HQ利用SAM在用户提供的提示的指导下进行开放世界对象分割，同时利用NeRF从不同角度聚合信息。为了克服上述挑战，我们在聚合过程中使用密度场和RGB相似性来提高分割边界的准确性。强调分割的准确性，我们在多个NeRF数据集上定量评估我们的方法，在这些数据集上可以获得或手动注释高质量的基本事实。SANeRF HQ在NeRF对象分割方面比以前最先进的方法有了显著的质量改进，为对象定位提供了更高的灵活性，并实现了跨多个视图的更一致的对象分割。其他信息可在https://lyclyc52.github.io/sanerf-hq/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01531v1" target="_blank">2312.01531v1</a>
                              </td>
                              <td>SANeRF-HQ: Segment Anything for NeRF in High Quality</td>
                              <td>Yichen Liu</td>
                              <td>2023-12-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01531v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01531v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_17769v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social Contract AI: Aligning AI Assistants with Implicit Group Norms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_17769v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_17769v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_17769v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation frameworks in which AI assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_17769v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们探索了通过从观察到的交互中反转用户（未知）偏好的模型来调整人工智能助手的想法。为了验证我们的提议，我们在经济最后通牒游戏中进行了概念验证模拟，将用户偏好正式化为指导模拟玩家行动的政策。我们发现，人工智能助手准确地将其行为与经济文献中的标准政策（例如，自私、无私）相匹配。然而，当面对助理的培训分配中不包括的货币（如药品克数）时，助理的学习政策缺乏稳健性，在分配外的环境中表现出有限的通用性。此外，我们发现，当语言使用与未知政策（例如，利他主义政策与粗鲁语言相结合）之间的关系不一致时，助手对该政策的学习会减慢。总的来说，我们的初步结果表明，开发人工智能助手需要从不同用户那里推断偏好的模拟框架，可以为研究实际的对齐问题提供一种有价值的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.17769v2" target="_blank">2310.17769v2</a>
                              </td>
                              <td>Social Contract AI: Aligning AI Assistants with Implicit Group Norms</td>
                              <td>Jan-Philipp Fränken</td>
                              <td>2023-10-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_17769v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.17769v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/janphilippfranken/scai" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00749v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SEED: Domain-Specific Data Curation With Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00749v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00749v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00749v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Data curation tasks that prepare data for analytics are critical for turning data into actionable insights. However, due to the diverse requirements of applications in different domains, generic off-the-shelf tools are typically insufficient. As a result, data scientists often have to develop domain-specific solutions tailored to both the dataset and the task, e.g. writing domain-specific code or training machine learning models on a sufficient number of annotated examples. This process is notoriously difficult and time-consuming. We present SEED, an LLM-as-compiler approach that automatically generates domain-specific data curation solutions via Large Language Models (LLMs). Once the user describes a task, input data, and expected output, the SEED compiler produces an executable pipeline composed of LLM-generated code, small model, and data access modules. SEED uses these generated modules to process most of the data records and dynamically decides when the LLM should step in to directly process some individual records, possibly using the data-access modules to retrieve relevant information from the data sources to assist the LLM in solving the task. To validate this new, revolutionary approach, we conducted experiments on 9 datasets spanning over 5 data curation tasks. The results show that SEED generates domain-specific solutions that significantly outperform their generic counterparts, often approaching the performance of the manually curated solutions that use thousands of labeled training examples. Moreover, in comparison to solutions that use the LLM on every data record, SEED achieves state-of-the-art or comparable few-shot performance, while significantly reducing the number of LLM calls.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00749v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为分析准备数据的数据管理任务对于将数据转化为可操作的见解至关重要。然而，由于不同领域中应用程序的不同需求，通用的现成工具通常是不够的。因此，数据科学家通常必须开发针对数据集和任务的特定领域解决方案，例如编写特定领域代码或在足够数量的注释示例上训练机器学习模型。这个过程是出了名的困难和耗时。我们介绍了SEED，这是一种LLM即编译器的方法，通过大型语言模型（LLM）自动生成特定于领域的数据管理解决方案。一旦用户描述了任务、输入数据和预期输出，SEED编译器就会生成一个由LLM生成的代码、小模型和数据访问模块组成的可执行管道。SEED使用这些生成的模块来处理大多数数据记录，并动态决定LLM何时介入直接处理一些单独的记录，可能使用数据访问模块从数据源检索相关信息，以帮助LLM解决任务。为了验证这种新的革命性方法，我们在5个数据管理任务的9个数据集上进行了实验。结果表明，SEED生成的领域特定解决方案明显优于其通用解决方案，通常接近使用数千个标记训练示例的手动策划解决方案的性能。此外，与在每个数据记录上使用LLM的解决方案相比，SEED实现了最先进或相当的少镜头性能，同时显著减少了LLM调用的数量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00749v2" target="_blank">2310.00749v2</a>
                              </td>
                              <td>SEED: Domain-Specific Data Curation With Large Language Models</td>
                              <td>Zui Chen</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00749v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00749v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00869v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Caption Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00869v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00869v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00869v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions. SAM presents strong generalizability to segment anything while is short for semantic understanding. By introducing a lightweight query-based feature mixer, we align the region-specific features with the embedding space of language models for later caption generation. As the number of trainable parameters is small (typically in the order of tens of millions), it costs less computation, less memory usage, and less communication bandwidth, resulting in both fast and scalable training. To address the scarcity problem of regional caption data, we propose to first pre-train our model on objection detection and segmentation tasks. We call this step weak supervision pretraining since the pre-training data only contains category names instead of full-sentence descriptions. The weak supervision pretraining allows us to leverage many publicly available object detection and segmentation datasets. We conduct extensive experiments to demonstrate the superiority of our method and validate each design choice. This work serves as a stepping stone towards scaling up regional captioning data and sheds light on exploring efficient ways to augment SAM with regional semantics. The project page, along with the associated code, can be accessed via the following https://xk-huang.github.io/segment-caption-anything/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00869v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种方法来有效地为分段任意模型（SAM）配备生成区域字幕的能力。SAM表现出很强的可推广性来分割任何东西，而缺乏语义理解。通过引入一个轻量级的基于查询的特征混合器，我们将特定于区域的特征与语言模型的嵌入空间对齐，以便稍后生成字幕。由于可训练参数的数量很少（通常在数千万量级），因此计算成本更低，内存使用更少，通信带宽更少，从而实现快速且可扩展的训练。为了解决区域字幕数据的稀缺性问题，我们建议首先对我们的模型进行异议检测和分割任务的预训练。我们称这一步骤为弱监督预训练，因为预训练数据只包含类别名称，而不是完整的句子描述。弱监督预训练使我们能够利用许多公开可用的对象检测和分割数据集。我们进行了大量的实验来证明我们的方法的优越性，并验证了每一个设计选择。这项工作是扩大区域字幕数据的垫脚石，并有助于探索用区域语义增强SAM的有效方法。项目页面以及相关代码可以通过以下方式访问https://xk-huang.github.io/segment-caption-anything/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00869v1" target="_blank">2312.00869v1</a>
                              </td>
                              <td>Segment and Caption Anything</td>
                              <td>Xiaoke Huang</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00869v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00869v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00863v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00863v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00863v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00863v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, light-weight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00863v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）已成为众多视觉应用程序的强大工具。在广泛的高质量SA-1B数据集上训练的超大型变压器模型是推动零样本传输和高通用性的令人印象深刻的性能的关键组件。虽然SAM模型是有益的，但其巨大的计算成本限制了其在更广泛的现实世界应用中的应用。为了解决这一限制，我们提出了高效SAM，这是一种轻量级的SAM模型，它表现出良好的性能，并大大降低了复杂性。我们的想法是基于利用掩蔽图像预训练，SAMI，它学习从SAM图像编码器重建特征，以进行有效的视觉表示学习。此外，我们采用SAMI预训练的轻量级图像编码器和掩码解码器来构建高效SAM，并在SA-1B上对模型进行微调，以执行任何分割任务。我们对包括图像分类、对象检测、实例分割和语义对象检测在内的多个视觉任务进行了评估，发现我们提出的预训练方法SAMI始终优于其他掩蔽图像预训练方法。在分割任何任务（如零样本实例分割）上，与其他快速SAM模型相比，我们的带有SAMI预训练的轻量级图像编码器的高效SAM表现良好，具有显著的增益（例如，COCO/LVIS上的~4 AP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00863v1" target="_blank">2312.00863v1</a>
                              </td>
                              <td>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</td>
                              <td>Yunyang Xiong</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00863v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00732v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Grouping: Segment and Edit Anything in 3D Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00732v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00732v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00732v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by SAM, along with introduced 3D spatial consistency regularization. Comparing to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization and scene recomposition. Our code and models will be at https://github.com/lkeab/gaussian-grouping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00732v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的Gaussian Splatting实现了3D场景的高质量实时新颖视图合成。然而，它只专注于外观和几何建模，而缺乏细粒度的对象级场景理解。为了解决这个问题，我们提出了高斯分组，它扩展了高斯飞溅，以联合重建和分割开放世界3D场景中的任何内容。我们用紧凑的身份编码来增强每个高斯，允许高斯根据其在3D场景中的对象实例或物质成员身份进行分组。我们没有求助于昂贵的3D标签，而是通过利用SAM的2D掩模预测以及引入的3D空间一致性正则化，在可微分渲染期间监督身份编码。与隐式NeRF表示相比，我们表明离散和分组的3D高斯可以以高视觉质量、细粒度和效率重建、分割和编辑3D中的任何内容。在高斯分组的基础上，我们进一步提出了一种局部高斯编辑方案，该方案在多功能场景编辑应用中显示了有效性，包括3D对象去除、修复、着色和场景重组。我们的代码和模型将在https://github.com/lkeab/gaussian-grouping.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00732v1" target="_blank">2312.00732v1</a>
                              </td>
                              <td>Gaussian Grouping: Segment and Edit Anything in 3D Scenes</td>
                              <td>Mingqiao Ye</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00732v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00732v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lkeab/gaussian-grouping" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00312v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00312v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00312v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00312v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Polyp segmentation plays a vital role in accurately locating polyps at an early stage, which holds significant clinical importance for the prevention of colorectal cancer. Various polyp segmentation methods have been developed using fully-supervised deep learning techniques. However, pixel-wise annotation for polyp images by physicians during the diagnosis is both time-consuming and expensive. Moreover, visual foundation models such as the Segment Anything Model (SAM) have shown remarkable performance. Nevertheless, directly applying SAM to medical segmentation may not produce satisfactory results due to the inherent absence of medical knowledge. In this paper, we propose a novel SAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised polyp segmentation, enabling a collaborative learning process between our segmentation network and SAM to boost the model performance. Specifically, we first propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for weakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level Enhancement Module (CEM) that integrates the adjacent features to enhance the representation capabilities of different resolution features. Additionally, a Feature Aggregation Module (FAM) is employed to capture richer features across multiple levels. Moreover, we present a box-augmentation strategy that combines the segmentation maps generated by CEA-Net with scribble annotations to create more precise prompts. These prompts are then fed into SAM, generating segmentation SAM-guided masks, which can provide additional supervision to train CEA-Net effectively. Furthermore, we present an Image-level Filtering Mechanism to filter out unreliable SAM-guided masks. Extensive experimental results show that our SAM-CLNet outperforms state-of-the-art weakly-supervised segmentation methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00312v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>息肉分割在早期准确定位息肉中起着至关重要的作用，对癌症的预防具有重要的临床意义。已经使用完全监督的深度学习技术开发了各种息肉分割方法。然而，医生在诊断过程中对息肉图像进行逐像素注释既耗时又昂贵。此外，视觉基础模型，如分段任意模型（SAM），已经显示出显著的性能。然而，由于固有的医学知识的缺乏，直接将SAM应用于医学分割可能不会产生令人满意的结果。在本文中，我们提出了一种新的SAM引导的协作学习网络（SAM CLNet），用于涂鸦监督息肉分割，使我们的分割网络和SAM之间的协作学习过程能够提高模型性能。具体来说，我们首先提出了一种用于弱监督息肉分割的跨级别增强和聚合网络（CEA-Net）。在CEA-Net中，我们提出了一种跨级别增强模块（CEM），该模块集成了相邻的特征，以增强不同分辨率特征的表示能力。此外，还采用了特征聚合模块（FAM）来跨多个级别捕获更丰富的特征。此外，我们提出了一种方框扩充策略，该策略将CEA-Net生成的分割图与涂鸦注释相结合，以创建更精确的提示。然后将这些提示输入SAM，生成分段SAM引导的掩码，这可以提供额外的监督来有效地训练CEA-Net。此外，我们提出了一种图像级过滤机制来过滤不可靠的SAM引导掩模。大量的实验结果表明，我们的SAM CLNet优于最先进的弱监督分割方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00312v1" target="_blank">2312.00312v1</a>
                              </td>
                              <td>Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation</td>
                              <td>Yiming Zhao</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00312v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00312v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03775v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Matched Pair Calibration for Ranking Fairness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03775v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03775v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03775v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03775v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于分数的排名系统中的公平性测试，称为匹配对校准。我们的方法构建了一组匹配的项目对，在计算该组的适当排名误差之前，子组之间的混杂差异最小。匹配步骤确保我们比较相同评分项目之间的亚组结果，以便测量的表现差异直接意味着亚组水平暴露的不公平。我们展示了我们的方法如何将校准的公平直觉从二元分类设置推广到排名，并将我们的方法与其他排名公平措施的建议联系起来。此外，我们的策略显示了边际结果测试的逻辑如何扩展到分析师可以访问模型分数的情况。最后，我们提供了一个将匹配对校准应用于真实单词排名数据集的例子，以证明其在检测排名偏差方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03775v3" target="_blank">2306.03775v3</a>
                              </td>
                              <td>Matched Pair Calibration for Ranking Fairness</td>
                              <td>Hannah Korevaar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03775v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03775v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01442v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01442v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01442v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01442v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation, model inputs, model targets, time series per model, and computational budget.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01442v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型，尤其是Transformers，在包括时间序列预测在内的各个领域都取得了令人印象深刻的成果。虽然现有的时间序列文献主要集中在模型架构修改和数据扩充技术上，但本文探索了时间序列的深度学习模型的训练模式；如何训练模型，而不管其体系结构如何。我们进行了广泛的实验，以研究在公共时间序列数据集上训练的几个Transformer模型中深度双下降的发生。我们证明了历元深度双下降，并且可以使用更多的历元来恢复过度拟合。利用这些发现，我们在测试的72个基准中的近70%中实现了最先进的长序列时间序列预测结果。这表明文献中的许多模型可能具有尚未开发的潜力。此外，我们还介绍了一种分类法，用于对训练模式修改进行分类，包括数据扩充、模型输入、模型目标、每个模型的时间序列和计算预算。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01442v3" target="_blank">2311.01442v3</a>
                              </td>
                              <td>Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</td>
                              <td>Valentino Assandri</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01442v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01442v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17707v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17707v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17707v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17707v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SAMPro3D for zero-shot 3D indoor scene segmentation. Given the 3D point cloud and multiple posed 2D frames of 3D scenes, our approach segments 3D scenes by applying the pretrained Segment Anything Model (SAM) to 2D frames. Our key idea involves locating 3D points in scenes as natural 3D prompts to align their projected pixel prompts across frames, ensuring frame-consistency in both pixel prompts and their SAM-predicted masks. Moreover, we suggest filtering out low-quality 3D prompts based on feedback from all 2D frames, for enhancing segmentation quality. We also propose to consolidate different 3D prompts if they are segmenting the same object, bringing a more comprehensive segmentation. Notably, our method does not require any additional training on domain-specific data, enabling us to preserve the zero-shot power of SAM. Extensive qualitative and quantitative results show that our method consistently achieves higher quality and more diverse segmentation than previous zero-shot or fully supervised approaches, and in many cases even surpasses human-level annotations. The project page can be accessed at https://mutianxu.github.io/sampro3d/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17707v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SAMPro3D用于零样本三维室内场景分割。给定3D场景的3D点云和多个姿势的2D帧，我们的方法通过将预训练的分段任何模型（SAM）应用于2D帧来分割3D场景。我们的关键思想包括将场景中的3D点定位为自然的3D提示，以使其投影的像素提示跨帧对齐，确保像素提示和SAM预测遮罩的帧一致性。此外，我们建议基于所有2D帧的反馈过滤掉低质量的3D提示，以提高分割质量。我们还建议，如果不同的3D提示正在分割同一对象，则可以合并它们，从而实现更全面的分割。值得注意的是，我们的方法不需要对特定领域的数据进行任何额外的训练，使我们能够保持SAM的零样本能力。广泛的定性和定量结果表明，与以前的零样本或全监督方法相比，我们的算法始终实现更高质量和更多样的分割，在许多情况下甚至超过了人类层面的注释。项目页面可访问https://mutianxu.github.io/sampro3d/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17707v1" target="_blank">2311.17707v1</a>
                              </td>
                              <td>SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation</td>
                              <td>Mutian Xu</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17707v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17707v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16269v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16269v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16269v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16269v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Leveraging the extensive training data from SA-1B, the Segment Anything Model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this paper, we aim to develop an automated instance segmentation approach for remote sensing images, based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building, NWPU VHR-10, and SSDD datasets, validate the effectiveness of our proposed method. The code for our method is publicly available at kychen.me/RSPrompter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16269v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用SA-1B的大量训练数据，Segment Anything Model（SAM）展示了显著的泛化和零样本能力。然而，作为一种类别不可知的实例分割方法，SAM在很大程度上依赖于先前的手动指导，包括点、框和粗粒度掩码。此外，它在遥感图像分割任务中的性能在很大程度上仍未被探索和证实。在本文中，我们的目标是开发一种基于基本SAM模型并结合语义类别信息的遥感图像自动实例分割方法。从提示学习中获得灵感，我们提出了一种学习SAM适当提示生成的方法。这使SAM能够为遥感图像产生语义上可辨别的分割结果，我们称之为RSPrompter。我们还提出了一些正在进行的衍生工具，例如分割任务，利用SAM社区的最新进展，并将其与RSpromoter的性能进行比较。从WHU大楼、NWPU VHR-10和SSDD数据集获得的大量实验结果验证了我们提出的方法的有效性。我们方法的代码可在kychen.me/RSPrompter上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16269v2" target="_blank">2306.16269v2</a>
                              </td>
                              <td>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</td>
                              <td>Keyan Chen</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16269v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16269v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/KyanChen/RSPrompter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17539v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17539v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17539v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17539v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training an overparameterized neural network can yield minimizers of the same level of training loss and yet different generalization capabilities. With evidence that indicates a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. This sharpness-aware minimization (SAM) strategy, however, has not been studied much yet as to how overparameterization can actually affect its behavior. In this work, we analyze SAM under varying degrees of overparameterization and present both empirical and theoretical results that suggest a critical influence of overparameterization on SAM. Specifically, we first use standard techniques in optimization to prove that SAM can achieve a linear convergence rate under overparameterization in a stochastic setting. We also show that the linearly stable minima found by SAM are indeed flatter and have more uniformly distributed Hessian moments compared to those of SGD. These results are corroborated with our experiments that reveal a consistent trend that the generalization improvement made by SAM continues to increase as the model becomes more overparameterized. We further present that sparsity can open up an avenue for effective overparameterization in practice.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17539v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练过参数化的神经网络可以产生相同水平的训练损失和不同泛化能力的最小化器。有证据表明，极小值的锐度与其推广误差之间存在相关性，因此，人们越来越努力开发一种优化方法，将平坦极小值明确地作为更具推广性的解决方案。然而，对于过度参数化如何实际影响其行为，这种敏锐度感知最小化（SAM）策略还没有得到太多研究。在这项工作中，我们分析了不同程度的过参数化下的SAM，并给出了经验和理论结果，这些结果表明过参数化对SAM的关键影响。具体而言，我们首先使用优化中的标准技术来证明在随机环境中，在过参数化的情况下，SAM可以实现线性收敛速度。我们还表明，与SGD相比，SAM发现的线性稳定极小值确实更平坦，并且具有更均匀分布的Hessian矩。这些结果与我们的实验相证实，这些实验揭示了一个一致的趋势，即随着模型变得更加参数化，SAM所做的泛化改进继续增加。我们进一步提出，稀疏性可以在实践中为有效的过参数化开辟一条途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17539v1" target="_blank">2311.17539v1</a>
                              </td>
                              <td>The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis</td>
                              <td>Sungbin Shin</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17539v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17539v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17960v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17960v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17960v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17960v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Cell segmentation in histopathological images plays a crucial role in understanding, diagnosing, and treating many diseases. However, data annotation for this is expensive since there can be a large number of cells per image, and expert pathologists are needed for labelling images. Instead, our paper focuses on using weak supervision -- annotation from related tasks -- to induce a segmenter. Recent foundation models, such as Segment Anything (SAM), can use prompts to leverage additional supervision during inference. SAM has performed remarkably well in natural image segmentation tasks; however, its applicability to cell segmentation has not been explored.   In response, we investigate guiding the prompting procedure in SAM for weakly supervised cell segmentation when only bounding box supervision is available. We develop two workflows: (1) an object detector's output as a test-time prompt to SAM (D-SAM), and (2) SAM as pseudo mask generator over training data to train a standalone segmentation model (SAM-S). On finding that both workflows have some complementary strengths, we develop an integer programming-based approach to reconcile the two sets of segmentation masks, achieving yet higher performance. We experiment on three publicly available cell segmentation datasets namely, ConSep, MoNuSeg, and TNBC, and find that all SAM-based solutions hugely outperform existing weakly supervised image segmentation models, obtaining 9-15 pt Dice gains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17960v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组织病理学图像中的细胞分割在理解、诊断和治疗许多疾病方面起着至关重要的作用。然而，这方面的数据注释是昂贵的，因为每个图像可能有大量的细胞，并且需要专业的病理学家来标记图像。相反，我们的论文侧重于使用弱监督——相关任务的注释——来诱导分割器。最近的基础模型，如Segment Anything（SAM），可以在推理过程中使用提示来利用额外的监督。SAM在自然图像分割任务中表现非常好；然而，它在细胞分割中的适用性尚未得到探索。作为回应，我们研究了当只有边界框监督可用时，在SAM中指导弱监督细胞分割的提示过程。我们开发了两个工作流程：（1）对象检测器的输出作为SAM的测试时间提示（D-SAM），以及（2）SAM作为训练数据上的伪掩码生成器来训练独立分割模型（SAM-s）。在发现两个工作流都有一些互补的优势后，我们开发了一种基于整数编程的方法来协调两组分割掩码，从而实现更高的性能。我们在三个公开可用的细胞分割数据集上进行了实验，即ConSep、MoNuSeg和TNBC，发现所有基于SAM的解决方案都大大优于现有的弱监督图像分割模型，获得了9-15 pt的Dice增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17960v1" target="_blank">2311.17960v1</a>
                              </td>
                              <td>Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images</td>
                              <td>Aayush Kumar Tyagi</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17960v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17960v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04567v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scaling Laws of Synthetic Images for Model Training ... for Now</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04567v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04567v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04567v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images, potentially overcoming the difficulty of collecting curated data at scale. It is unclear, however, how these models behave at scale, as more synthetic data is added to the training set. In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models, for the training of supervised models: image classifiers with label supervision, and CLIP with language supervision. We identify several factors, including text prompts, classifier-free guidance scale, and types of text-to-image models, that significantly affect scaling behavior. After tuning these factors, we observe that synthetic images demonstrate a scaling trend similar to, but slightly less effective than, real images in CLIP training, while they significantly underperform in scaling when training supervised image classifiers. Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts, a limitation that significantly impairs the training of image classifiers. Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g., fewer than 0.5 million images in ImageNet), (2) when the evaluation dataset diverges significantly from the training data, indicating the out-of-distribution scenario, or (3) when synthetic data is used in conjunction with real images, as demonstrated in the training of CLIP models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04567v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像模型的最新重大进展开启了使用合成图像训练视觉系统的可能性，有可能克服大规模收集策划数据的困难。然而，随着更多的合成数据被添加到训练集中，目前尚不清楚这些模型在规模上的表现。在本文中，我们研究了由最先进的文本生成的合成图像到图像模型的缩放规律，用于训练监督模型：具有标签监督的图像分类器和具有语言监督的CLIP。我们确定了几个显著影响缩放行为的因素，包括文本提示、无分类器引导量表和文本到图像模型的类型。在调整这些因素后，我们观察到合成图像在CLIP训练中表现出类似于真实图像的缩放趋势，但略低于真实图像，而在训练监督图像分类器时，合成图像在缩放方面明显表现不佳。我们的分析表明，这种表现不佳的主要原因是现成的文本到图像模型无法生成某些概念，这一限制大大削弱了图像分类器的训练。我们的研究结果还表明，缩放合成数据在以下情况下可能特别有效：（1）当监督问题的真实图像供应有限时（例如，ImageNet中的图像少于50万张），（2）当评估数据集与训练数据显著偏离时，表明分布外的情况，或者（3）当合成数据与真实图像结合使用时，如CLIP模型的训练所示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04567v1" target="_blank">2312.04567v1</a>
                              </td>
                              <td>Scaling Laws of Synthetic Images for Model Training ... for Now</td>
                              <td>Lijie Fan</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04567v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04567v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/syn-rep-learn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04539v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Guided Open-Vocabulary Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04539v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04539v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04539v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Models (VLMs) have emerged as promising tools for open-ended image understanding tasks, including open vocabulary segmentation. Yet, direct application of such VLMs to segmentation is non-trivial, since VLMs are trained with image-text pairs and naturally lack pixel-level granularity. Recent works have made advancements in bridging this gap, often by leveraging the shared image-text space in which the image and a provided text prompt are represented. In this paper, we challenge the capabilities of VLMs further and tackle open-vocabulary segmentation without the need for any textual input. To this end, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework. Self-Seg is capable of automatically detecting relevant class names from clustered BLIP embeddings and using these for accurate semantic segmentation. In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) to effectively assess predicted open-vocabulary class names. We achieve state-of-the-art results on Pascal VOC, ADE20K and CityScapes for open-vocabulary segmentation without given class names, as well as competitive performance with methods where class names are given. All code and data will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04539v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM）已成为开放式图像理解任务的有前途的工具，包括开放式词汇分割。然而，将这种VLM直接应用于分割是不平凡的，因为VLM是用图像-文本对训练的，并且自然缺乏像素级粒度。最近的工作在弥合这一差距方面取得了进展，通常是通过利用共享的图像-文本空间来表示图像和提供的文本提示。在本文中，我们进一步挑战VLM的能力，并在不需要任何文本输入的情况下解决开放式词汇分割问题。为此，我们提出了一种新的自引导语义分割（Self-Seg）框架。Self-Seg能够从集群BLIP嵌入中自动检测相关类名，并使用这些类名进行准确的语义分割。此外，我们提出了一种基于LLM的开放词汇评估器（LOVE）来有效地评估预测的开放词汇类名。我们在Pascal VOC、ADE20K和CityScapes上实现了最先进的结果，用于在没有给定类名的情况下进行开放式词汇分割，以及使用给定类名的方法的竞争性能。所有代码和数据都将发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04539v1" target="_blank">2312.04539v1</a>
                              </td>
                              <td>Self-Guided Open-Vocabulary Semantic Segmentation</td>
                              <td>Osman Ülger</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04539v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04539v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04534v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04534v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04534v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04534v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human images. Unlike prior arts constrained by specific input types, our method allows flexible specification of style (text or image) and texture (full garment, cropped sections, or texture patches) conditions. To address the entanglement challenge when using full garment images as conditions, we develop a two-stage pipeline with explicit disentanglement of style and texture. In the first stage, we generate a human parsing map reflecting the desired style conditioned on the input. In the second stage, we composite textures onto the parsing map areas based on the texture input. To represent complex and non-stationary textures that have never been achieved in previous fashion editing works, we first propose extracting hierarchical and balanced CLIP features and applying position encoding in VTON. Experiments demonstrate superior synthesis quality and personalization enabled by our method. The flexible control over style and texture mixing brings virtual try-on to a new level of user experience for online shopping and fashion design.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04534v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新颖的无约束设计虚拟试穿（ucVTON）任务，以实现对输入人体图像的个性化合成服装的真实感合成。与受特定输入类型限制的现有技术不同，我们的方法允许灵活指定风格（文本或图像）和纹理（整件衣服、裁剪部分或纹理补丁）条件。为了解决使用完整服装图像作为条件时的纠缠挑战，我们开发了一个两阶段的管道，该管道具有明确的风格和纹理解纠缠。在第一阶段，我们生成一个人工解析图，反映出以输入为条件的所需风格。在第二阶段，我们根据纹理输入将纹理合成到解析图区域上。为了表示以前的时尚编辑工作中从未实现过的复杂和非平稳纹理，我们首先提出提取分层和平衡的CLIP特征，并在VTON中应用位置编码。实验证明，我们的方法能够实现卓越的合成质量和个性化。对风格和纹理混合的灵活控制将虚拟试穿带到了网上购物和时尚设计的用户体验的新水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04534v1" target="_blank">2312.04534v1</a>
                              </td>
                              <td>PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns</td>
                              <td>Shuliang Ning</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04534v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04534v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04326v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04326v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04326v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04326v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the open-sourcing of text-to-image models (T2I) such as stable diffusion (SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned in specific domains based on the open-source SD model, such as in anime, character portraits, etc. However, there are few specialized models in certain domains, such as interior design, which is attributed to the complex textual descriptions and detailed visual elements inherent in design, alongside the necessity for adaptable resolution. Therefore, text-to-image models for interior design are required to have outstanding prompt-following capabilities, as well as iterative collaboration with design professionals to achieve the desired outcome. In this paper, we collect and optimize text-image data in the design field and continue training in both English and Chinese on the basis of the open-source CLIP model. We also proposed a fine-tuning strategy with curriculum learning and reinforcement learning from CLIP feedback to enhance the prompt-following capabilities of our approach so as to improve the quality of image generation. The experimental results on the collected dataset demonstrate the effectiveness of the proposed approach, which achieves impressive results and outperforms strong baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04326v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着文本到图像模型（T2I）的开源，如稳定扩散（SD）和稳定扩散XL（SD-XL），基于开源SD模型在特定领域进行微调的模型大量涌入，如动漫、人物肖像等。然而，在某些领域，如室内设计，很少有专门的模型，这归因于设计中固有的复杂文本描述和详细视觉元素，以及自适应分辨率的必要性。因此，室内设计的文本到图像模型需要具有出色的即时跟随能力，以及与设计专业人员的迭代协作，以实现所需的结果。在本文中，我们在设计领域收集和优化文本图像数据，并在开源CLIP模型的基础上继续进行中英文培训。我们还提出了一种微调策略，即从CLIP反馈中进行课程学习和强化学习，以增强我们方法的即时跟随能力，从而提高图像生成的质量。在收集的数据集上的实验结果证明了所提出的方法的有效性，该方法取得了令人印象深刻的结果，并优于强基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04326v1" target="_blank">2312.04326v1</a>
                              </td>
                              <td>iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image Diffusion Model for Interior Design</td>
                              <td>Ruyi Gan</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04326v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04326v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04248v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04248v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04248v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04248v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent progress in the text-driven 3D stylization of a single object has been considerably promoted by CLIP-based methods. However, the stylization of multi-object 3D scenes is still impeded in that the image-text pairs used for pre-training CLIP mostly consist of an object. Meanwhile, the local details of multiple objects may be susceptible to omission due to the existing supervision manner primarily relying on coarse-grained contrast of image-text pairs. To overcome these challenges, we present a novel framework, dubbed TeMO, to parse multi-object 3D scenes and edit their styles under the contrast supervision at multiple levels. We first propose a Decoupled Graph Attention (DGA) module to distinguishably reinforce the features of 3D surface points. Particularly, a cross-modal graph is constructed to align the object points accurately and noun phrases decoupled from the 3D mesh and textual description. Then, we develop a Cross-Grained Contrast (CGC) supervision system, where a fine-grained loss between the words in the textual description and the randomly rendered images are constructed to complement the coarse-grained loss. Extensive experiments show that our method can synthesize high-quality stylized content and outperform the existing methods over a wide range of multi-object 3D meshes. Our code and results will be made publicly available</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04248v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于CLIP的方法极大地促进了单个对象的文本驱动三维风格化的最新进展。然而，多对象3D场景的风格化仍然受到阻碍，因为用于预训练CLIP的图像-文本对大多由对象组成。同时，由于现有的监督方式主要依赖于图像-文本对的粗粒度对比，多个对象的局部细节可能容易被遗漏。为了克服这些挑战，我们提出了一种新的框架，称为TeMO，用于解析多对象3D场景，并在多个级别的对比度监督下编辑其风格。我们首先提出了一个解耦图注意力（DGA）模块来区分地增强三维表面点的特征。特别地，构建了一个跨模态图，以准确地对齐对象点，并将名词短语与3D网格和文本描述解耦。然后，我们开发了一个跨粒度对比度（CGC）监督系统，其中构建文本描述中的单词和随机渲染的图像之间的细粒度损失，以补充粗粒度损失。大量实验表明，我们的方法可以合成高质量的风格化内容，并在广泛的多对象三维网格上优于现有方法。我们的代码和结果将公开</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04248v1" target="_blank">2312.04248v1</a>
                              </td>
                              <td>TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes</td>
                              <td>Xuying Zhang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04248v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04248v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13355v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SILC: Improving Vision Language Pretraining with Self-Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13355v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13355v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13355v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image-Text pretraining on web-scale image caption datasets has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense prediction tasks and have shown the emergence of open-set abilities. However, the contrastive objective used by these models only focuses on image-text alignment and does not incentivise image feature learning for dense prediction tasks. In this work, we introduce SILC, a novel framework for vision language pretraining. SILC improves image-text contrastive learning with the simple addition of local-to-global correspondence learning by self-distillation. We show that distilling local image features from an exponential moving average (EMA) teacher model significantly improves model performance on dense predictions tasks like detection and segmentation, while also providing improvements on image-level tasks such as classification and retrieval. SILC models sets a new state of the art for zero-shot classification, few shot classification, image and text retrieval, zero-shot segmentation, and open vocabulary segmentation. We further show that SILC features greatly benefit open vocabulary detection, captioning and visual question answering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13355v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于CLIP及其变体的成功，在网络规模的图像字幕数据集上的图像文本预训练已成为开放词汇分类和检索模型的默认配方。一些工作也将CLIP特征用于密集预测任务，并显示了开集能力的出现。然而，这些模型使用的对比目标仅关注图像-文本对齐，而没有激励密集预测任务的图像特征学习。在这项工作中，我们介绍了一种新的视觉语言预训练框架SILC。SILC通过自蒸馏简单地添加局部到全局的对应学习来改进图像-文本对比学习。我们表明，从指数移动平均（EMA）教师模型中提取局部图像特征显著提高了模型在检测和分割等密集预测任务上的性能，同时也改善了分类和检索等图像级任务。SILC模型为零样本分类、少镜头分类、图像和文本检索、零样本分割和开放式词汇分割树立了新的技术水平。我们进一步表明，SILC特征极大地有利于开放式词汇检测、字幕和视觉问答。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13355v2" target="_blank">2310.13355v2</a>
                              </td>
                              <td>SILC: Improving Vision Language Pretraining with Self-Distillation</td>
                              <td>Muhammad Ferjad Naeem</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13355v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13355v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04145v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diffusing Colors: Image Colorization with Text Guided Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04145v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04145v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04145v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The colorization of grayscale images is a complex and subjective task with significant challenges. Despite recent progress in employing large-scale datasets with deep neural networks, difficulties with controllability and visual quality persist. To tackle these issues, we present a novel image colorization framework that utilizes image diffusion techniques with granular text prompts. This integration not only produces colorization outputs that are semantically appropriate but also greatly improves the level of control users have over the colorization process. Our method provides a balance between automation and control, outperforming existing techniques in terms of visual quality and semantic coherence. We leverage a pretrained generative Diffusion Model, and show that we can finetune it for the colorization task without losing its generative power or attention to text prompts. Moreover, we present a novel CLIP-based ranking model that evaluates color vividness, enabling automatic selection of the most suitable level of vividness based on the specific scene semantics. Our approach holds potential particularly for color enhancement and historical image colorization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04145v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>灰度图像的彩色化是一项复杂而主观的任务，具有重大的挑战性。尽管最近在将大规模数据集与深度神经网络结合使用方面取得了进展，但可控性和视觉质量方面的困难依然存在。为了解决这些问题，我们提出了一种新的图像彩色化框架，该框架利用了具有细粒度文本提示的图像扩散技术。这种集成不仅产生语义上合适的着色输出，而且大大提高了用户对着色过程的控制水平。我们的方法提供了自动化和控制之间的平衡，在视觉质量和语义连贯性方面优于现有技术。我们利用了一个预训练的生成扩散模型，并表明我们可以为着色任务对其进行微调，而不会失去其生成能力或对文本提示的关注。此外，我们提出了一种新的基于CLIP的排名模型，该模型评估颜色的生动性，从而能够根据特定的场景语义自动选择最合适的生动性级别。我们的方法具有潜力，特别是在色彩增强和历史图像着色方面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04145v1" target="_blank">2312.04145v1</a>
                              </td>
                              <td>Diffusing Colors: Image Colorization with Text Guided Diffusion</td>
                              <td>Nir Zabari</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04145v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04145v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04089v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Vocabulary Segmentation with Semantic-Assisted Calibration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04089v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04089v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04089v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper studies open-vocabulary segmentation (OVS) through calibrating in-vocabulary and domain-biased embedding space with generalized contextual prior of CLIP. As the core of open-vocabulary understanding, alignment of visual content with the semantics of unbounded text has become the bottleneck of this field. To address this challenge, recent works propose to utilize CLIP as an additional classifier and aggregate model predictions with CLIP classification results. Despite their remarkable progress, performance of OVS methods in relevant scenarios is still unsatisfactory compared with supervised counterparts. We attribute this to the in-vocabulary embedding and domain-biased CLIP prediction. To this end, we present a Semantic-assisted CAlibration Network (SCAN). In SCAN, we incorporate generalized semantic prior of CLIP into proposal embedding to avoid collapsing on known categories. Besides, a contextual shift strategy is applied to mitigate the lack of global context and unnatural background noise. With above designs, SCAN achieves state-of-the-art performance on all popular open-vocabulary segmentation benchmarks. Furthermore, we also focus on the problem of existing evaluation system that ignores semantic duplication across categories, and propose a new metric called Semantic-Guided IoU (SG-IoU).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04089v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文通过对CLIP的广义上下文先验在词汇和域偏嵌入空间中的校准，研究了开放式词汇分割（OVS）。作为开放式词汇理解的核心，视觉内容与无界文本语义的一致性已成为该领域的瓶颈。为了应对这一挑战，最近的工作提出利用CLIP作为额外的分类器，并将模型预测与CLIP分类结果进行聚合。尽管取得了显著进展，但与受监督的方法相比，OVS方法在相关场景中的性能仍然不令人满意。我们将其归因于词汇内嵌入和领域偏见的CLIP预测。为此，我们提出了一个语义辅助的C库网络（SCAN）。在SCAN中，我们将CLIP的广义语义先验引入到建议嵌入中，以避免在已知类别上崩溃。此外，应用上下文转移策略来缓解全局上下文的缺乏和非自然背景噪声。通过以上设计，SCAN在所有流行的开放式词汇分割基准上都实现了最先进的性能。此外，我们还关注了现有评估系统忽视跨类别语义重复的问题，并提出了一种新的度量标准，称为语义引导IoU（SG-IoU）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04089v1" target="_blank">2312.04089v1</a>
                              </td>
                              <td>Open-Vocabulary Segmentation with Semantic-Assisted Calibration</td>
                              <td>Yong Liu</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04089v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04089v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/workforai/scan" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04086v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MTVG : Multi-text Video Generation with Text-to-Video Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04086v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04086v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04086v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, video generation has attracted massive attention and yielded noticeable outcomes. Concerning the characteristics of video, multi-text conditioning incorporating sequential events is necessary for next-step video generation. In this work, we propose a novel multi-text video generation~(MTVG) by directly utilizing a pre-trained diffusion-based text-to-video~(T2V) generation model without additional fine-tuning. To generate consecutive video segments, visual consistency generated by distinct prompts is necessary with diverse variations, such as motion and content-related transitions. Our proposed MTVG includes Dynamic Noise and Last Frame Aware Inversion which reinitialize the noise latent to preserve visual coherence between videos of different prompts and prevent repetitive motion or contents. Furthermore, we present Structure Guiding Sampling to maintain the global appearance across the frames in a single video clip, where we leverage iterative latent updates across the preceding frame. Additionally, our Prompt Generator allows for arbitrary format of text conditions consisting of diverse events. As a result, our extensive experiments, including diverse transitions of descriptions, demonstrate that our proposed methods show superior generated outputs in terms of semantically coherent and temporally seamless video.Video examples are available in our project page: https://kuai-lab.github.io/mtvg-page.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04086v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，视频生成引起了人们的广泛关注，并取得了显著的成果。考虑到视频的特点，结合顺序事件的多文本条件处理对于下一步视频生成是必要的。在这项工作中，我们提出了一种新的多文本视频生成~（MTVG），通过直接利用预先训练的基于扩散的文本到视频~（T2V）生成模型，而无需额外的微调。为了生成连续的视频片段，通过不同的提示生成的视觉一致性对于不同的变化是必要的，例如运动和内容相关的转换。我们提出的MTVG包括动态噪声和最后一帧感知反转，它们重新初始化潜在的噪声，以保持不同提示的视频之间的视觉连贯性，并防止重复运动或内容。此外，我们提出了结构引导采样，以在单个视频剪辑中保持跨帧的全局外观，其中我们利用前一帧的迭代潜在更新。此外，我们的提示生成器允许由各种事件组成的任意格式的文本条件。因此，我们的大量实验，包括描述的不同转换，证明了我们提出的方法在语义连贯和时间无缝的视频方面显示出优越的生成输出。我们的项目页面中提供了视频示例：https://kuai-lab.github.io/mtvg-page.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04086v1" target="_blank">2312.04086v1</a>
                              </td>
                              <td>MTVG : Multi-text Video Generation with Text-to-Video Models</td>
                              <td>Gyeongrok Oh</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04086v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04086v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04076v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models are Good Prompt Learners for Low-Shot Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04076v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04076v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04076v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pre-trained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMP, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04076v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在训练图像有限或无法访问的情况下，低镜头图像分类受益于具有较强可推广性的预训练视觉语言（VL）模型（如CLIP）的最新进展。使用VL模型构建的即时学习方法从仅具有有限类特定信息的类名生成文本特征。大型语言模型（LLM）以其丰富的百科全书式知识作为补充。因此，在本文中，我们讨论了LLM的集成，以增强预训练的VL模型，特别是在低炮点分类方面。然而，语言和视觉之间的领域差距阻碍了LLM的直接应用。因此，我们提出了LLaMP，即作为提示学习者的大型语言模型，它为CLIP文本编码器产生自适应提示，并将其建立为连接桥梁。实验表明，与其他最先进的即时学习方法相比，在11个数据集的范围内，LLaMP在零样本泛化和少热点图像分类方面都具有更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04076v1" target="_blank">2312.04076v1</a>
                              </td>
                              <td>Large Language Models are Good Prompt Learners for Low-Shot Image Classification</td>
                              <td>Zhaoheng Zheng</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04076v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04076v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04464v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04464v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04464v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04464v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning generalized representations from limited training samples is crucial for applying deep neural networks in low-resource scenarios. Recently, methods based on Contrastive Language-Image Pre-training (CLIP) have exhibited promising performance in few-shot adaptation tasks. To avoid catastrophic forgetting and overfitting caused by few-shot fine-tuning, existing works usually freeze the parameters of CLIP pre-trained on large-scale datasets, overlooking the possibility that some parameters might not be suitable for downstream tasks. To this end, we revisit CLIP's visual encoder with a specific focus on its distinctive attention pooling layer, which performs a spatial weighted-sum of the dense feature maps. Given that dense feature maps contain meaningful semantic information, and different semantics hold varying importance for diverse downstream tasks (such as prioritizing semantics like ears and eyes in pet classification tasks rather than side mirrors), using the same weighted-sum operation for dense features across different few-shot tasks might not be appropriate. Hence, we propose fine-tuning the parameters of the attention pooling layer during the training process to encourage the model to focus on task-specific semantics. In the inference process, we perform residual blending between the features pooled by the fine-tuned and the original attention pooling layers to incorporate both the few-shot knowledge and the pre-trained CLIP's prior knowledge. We term this method as Semantic-Aware FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot CLIP and is compatible with the existing adapter approach (termed SAFE-A).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04464v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的训练样本中学习广义表示对于在低资源场景中应用深度神经网络至关重要。近年来，基于对比语言图像预训练（CLIP）的方法在少数镜头自适应任务中表现出了良好的性能。为了避免少数镜头微调导致的灾难性遗忘和过拟合，现有工作通常冻结在大规模数据集上预训练的CLIP的参数，忽略了一些参数可能不适合下游任务的可能性。为此，我们重新审视CLIP的视觉编码器，特别关注其独特的注意力池层，该层执行密集特征图的空间加权和。考虑到密集特征图包含有意义的语义信息，并且不同的语义对不同的下游任务具有不同的重要性（例如在宠物分类任务中优先考虑耳朵和眼睛等语义，而不是侧视镜），在不同的少镜头任务中对密集特征使用相同的加权和运算可能是不合适的。因此，我们建议在训练过程中微调注意力池层的参数，以鼓励模型专注于特定任务的语义。在推理过程中，我们在由微调层和原始注意力池化层池化的特征之间进行残差混合，以合并少镜头知识和预训练的CLIP的先验知识。我们将这种方法称为语义感知FinE调优（SAFE）。SAFE在增强传统的少镜头CLIP方面是有效的，并且与现有的适配器方法（称为SAFE-A）兼容。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04464v3" target="_blank">2311.04464v3</a>
                              </td>
                              <td>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning</td>
                              <td>Yao Zhu</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04464v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04464v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03936v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Potential of Vision-Language Models for Content Moderation of Children's Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03936v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03936v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03936v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural language supervision has been shown to be effective for zero-shot learning in many computer vision tasks, such as object detection and activity recognition. However, generating informative prompts can be challenging for more subtle tasks, such as video content moderation. This can be difficult, as there are many reasons why a video might be inappropriate, beyond violence and obscenity. For example, scammers may attempt to create junk content that is similar to popular educational videos but with no meaningful information. This paper evaluates the performance of several CLIP variations for content moderation of children's cartoons in both the supervised and zero-shot setting. We show that our proposed model (Vanilla CLIP with Projection Layer) outperforms previous work conducted on the Malicious or Benign (MOB) benchmark for video content moderation. This paper presents an in depth analysis of how context-specific language prompts affect content moderation performance. Our results indicate that it is important to include more context in content moderation prompts, particularly for cartoon videos as they are not well represented in the CLIP training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03936v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在许多计算机视觉任务中，如物体检测和活动识别，自然语言监督对零样本学习是有效的。然而，对于更微妙的任务，如视频内容审核，生成信息提示可能具有挑战性。这可能很困难，因为除了暴力和淫秽之外，视频可能不合适还有很多原因。例如，骗子可能试图创建类似于流行教育视频但没有任何有意义信息的垃圾内容。本文评估了几种CLIP变体在监督和零样本环境下对儿童动画片内容调节的性能。我们表明，我们提出的模型（带投影层的Vanilla CLIP）优于之前在视频内容审核的恶意或良性（MOB）基准上进行的工作。本文深入分析了特定于上下文的语言提示如何影响内容审核性能。我们的研究结果表明，在内容审核提示中包含更多上下文是很重要的，尤其是对于卡通视频，因为它们在CLIP训练数据中没有很好地表现出来。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03936v1" target="_blank">2312.03936v1</a>
                              </td>
                              <td>The Potential of Vision-Language Models for Content Moderation of Children's Videos</td>
                              <td>Syed Hammad Ahmed</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03936v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03936v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03818v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03818v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03818v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03818v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03818v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）在从不同任务的图像中提取有价值的内容信息方面发挥着重要作用。它将文本和视觉模式对齐，以理解整个图像，包括所有细节，甚至是与特定任务无关的细节。然而，为了更好地理解和控制图像的编辑，关注特定的感兴趣区域变得至关重要，这些区域可以由人类或感知模型指示为点、掩码或框。为了满足这些要求，我们引入了Alpha CLIP，这是CLIP的增强版，具有辅助Alpha通道，用于建议关注区域，并通过构建数百万个RGBA区域文本对进行微调。Alpha CLIP不仅保留了CLIP的视觉识别能力，而且能够精确控制图像内容的重点。它展示了在各种任务中的有效性，包括但不限于开放世界识别、多模式大型语言模型和条件2D/3D生成。它具有强大的潜力，可以作为图像相关任务的通用工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03818v1" target="_blank">2312.03818v1</a>
                              </td>
                              <td>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</td>
                              <td>Zeyi Sun</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03818v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03818v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunzey/alphaclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03585v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Model Assisted Weakly Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03585v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03585v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03585v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03585v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作旨在利用预先训练的基础模型，如对比语言图像预训练（CLIP）和任意分割模型（SAM），来解决使用图像级标签的弱监督语义分割（WSSS）问题。为此，我们提出了一种基于CLIP和SAM的从粗到细的框架，用于生成高质量的分割种子。具体来说，我们构建了一个图像分类任务和一个种子分割任务，这两个任务由具有冻结权重的CLIP和两组可学习的任务特定提示联合执行。设计了基于SAM的种子（SAMS）模块，并将其应用于每个任务，以生成粗略或精细的种子图。此外，我们设计了由图像级标签监督的多标签对比损失和由生成的粗种子图监督的CAM激活损失。这些损失用于学习提示，这是我们框架中唯一需要学习的部分。一旦学习了提示，我们将每个图像以及学习到的分割特定提示输入到CLIP和SAMS模块中，以产生高质量的分割种子。像其他两阶段WSSS方法一样，这些种子充当伪标签来训练现成的分割网络。实验表明，我们的方法在PASCAL VOC 2012上取得了最先进的性能，在MS COCO 2014上取得了具有竞争力的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03585v1" target="_blank">2312.03585v1</a>
                              </td>
                              <td>Foundation Model Assisted Weakly Supervised Semantic Segmentation</td>
                              <td>Xiaobo Yang</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03585v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03585v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03805v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03805v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03805v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03805v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt learning is a powerful technique for transferring Vision-Language Models (VLMs) such as CLIP to downstream tasks. However, the prompt-based methods that are fine-tuned solely with base classes may struggle to generalize to novel classes in open-vocabulary scenarios, especially when data are limited. To address this issue, we propose an innovative approach called SYNC-CLIP that leverages SYNthetiC data for enhancing the generalization capability of CLIP. Based on the observation of the distribution shift between the real and synthetic samples, we treat real and synthetic samples as distinct domains and propose to optimize separate domain prompts to capture domain-specific information, along with the shared visual prompts to preserve the semantic consistency between two domains. By aligning the cross-domain features, the synthetic data from novel classes can provide implicit guidance to rebalance the decision boundaries. Experimental results on three model generalization tasks demonstrate that our method performs very competitively across various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art competitor PromptSRC by an average improvement of 3.0% on novel classes across 11 datasets in open-vocabulary scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03805v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>即时学习是一种将视觉语言模型（VLM）（如CLIP）转移到下游任务的强大技术。然而，仅使用基类进行微调的基于提示的方法可能难以在开放词汇场景中推广到新的类，尤其是在数据有限的情况下。为了解决这个问题，我们提出了一种称为SYNC-CLIP的创新方法，该方法利用SYNthetiC数据来增强CLIP的泛化能力。基于对真实样本和合成样本之间分布变化的观察，我们将真实样本和综合样本视为不同的域，并建议优化单独的域提示以捕获域特定信息，以及共享的视觉提示以保持两个域之间的语义一致性。通过对齐跨领域特征，来自新类的合成数据可以提供隐含的指导，以重新平衡决策边界。在三个模型泛化任务上的实验结果表明，我们的方法在各种基准测试中表现得非常有竞争力。值得注意的是，在开放词汇场景中，SYNC-CLIP在11个数据集的新类上的平均改进率为3.0%，优于最先进的竞争对手PromptSRC。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03805v1" target="_blank">2312.03805v1</a>
                              </td>
                              <td>SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios</td>
                              <td>Mushui Liu</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03805v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03805v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17903v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deeply Coupled Cross-Modal Prompt Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17903v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17903v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17903v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly accommodates the interplay between vision and language with a Cross-Modal Prompt Attention (CMPA) mechanism, which enables the mutual exchange of respective representation through a well-connected multi-head attention module progressively and strongly. We then conduct comprehensive few-shot learning experiments on 11 image classification datasets and analyze the robustness to domain shift as well. Thorough experimental analysis evidently demonstrates the superb few-shot generalization and compelling domain adaption capacity of a well-executed DCP. The code can be found at https://github.com/GingL/CMPA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17903v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态基础模型（例如CLIP）的最新进展在零样本泛化方面表现出色。最近，从基础模型到下游任务的知识转移中涉及的及时调整引起了人们的极大关注。然而，现有的跨模态学习中的即时调整方法要么只关注语言分支，要么在肤浅的机制中学习视觉语言交互。在此背景下，我们提出了一种基于CLIP的深度耦合跨模态提示学习（DCP）方法。DCP通过跨模态即时注意力（CMPA）机制灵活地适应了视觉和语言之间的相互作用，通过一个连接良好的多头注意力模块，可以逐步而有力地相互交换各自的表示。然后，我们在11个图像分类数据集上进行了全面的少镜头学习实验，并分析了其对域移动的鲁棒性。全面的实验分析清楚地证明了一个执行良好的DCP具有出色的少镜头泛化能力和令人信服的域自适应能力。代码可在找到https://github.com/gingl/cmpa.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17903v3" target="_blank">2305.17903v3</a>
                              </td>
                              <td>Deeply Coupled Cross-Modal Prompt Learning</td>
                              <td>Xuejing Liu</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17903v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17903v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gingl/cmpa" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03792v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PCDP-SGD: Improving the Convergence of Differentially Private SGD via Projection in Advance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03792v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03792v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03792v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The paradigm of Differentially Private SGD~(DP-SGD) can provide a theoretical guarantee for training data in both centralized and federated settings. However, the utility degradation caused by DP-SGD limits its wide application in high-stakes tasks, such as medical image diagnosis. In addition to the necessary perturbation, the convergence issue is attributed to the information loss on the gradient clipping. In this work, we propose a general framework PCDP-SGD, which aims to compress redundant gradient norms and preserve more crucial top gradient components via projection operation before gradient clipping. Additionally, we extend PCDP-SGD as a fundamental component in differential privacy federated learning~(DPFL) for mitigating the data heterogeneous challenge and achieving efficient communication. We prove that pre-projection enhances the convergence of DP-SGD by reducing the dependence of clipping error and bias to a fraction of the top gradient eigenspace, and in theory, limits cross-client variance to improve the convergence under heterogeneous federation. Experimental results demonstrate that PCDP-SGD achieves higher accuracy compared with state-of-the-art DP-SGD variants in computer vision tasks. Moreover, PCDP-SGD outperforms current federated learning frameworks when DP is guaranteed on local training sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03792v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>差分专用SGD~（DP-SGD）范式可以为集中式和联邦环境中的数据训练提供理论保证。然而，DP-SGD引起的效用下降限制了其在高风险任务（如医学图像诊断）中的广泛应用。除了必要的扰动外，收敛问题还归因于梯度剪裁上的信息损失。在这项工作中，我们提出了一个通用框架PCDP-SGD，旨在压缩冗余梯度范数，并在梯度裁剪之前通过投影操作保留更关键的顶部梯度分量。此外，我们将PCDP-SGD扩展为差分隐私联合学习（DPFL）的基本组件，以缓解数据异构挑战并实现高效通信。我们证明了预投影通过将削波误差和偏差的依赖性降低到顶部梯度本征空间的一小部分来增强DP-SGD的收敛性，并在理论上限制跨客户端方差以提高异构联邦下的收敛性。实验结果表明，与最先进的DP-SGD变体相比，PCDP-SGD在计算机视觉任务中实现了更高的精度。此外，当DP在本地训练集上得到保证时，PCDP-SGD优于当前的联合学习框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03792v1" target="_blank">2312.03792v1</a>
                              </td>
                              <td>PCDP-SGD: Improving the Convergence of Differentially Private SGD via Projection in Advance</td>
                              <td>Haichao Sha</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03792v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03792v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03479v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03479v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03479v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03479v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language. Code is available at https://github.com/supersational/JAMMIN-GPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03479v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一个系统，允许Ableton Live的用户通过音乐描述来命名MIDI片段。用户可以直接在Ableton的剪辑视图中键入所需的音乐内容，然后由我们的集成系统插入。这允许用户在快速产生音乐创意的同时，保持创作过程的流畅性。该系统的工作原理是提示ChatGPT使用几种基于文本的音乐格式之一进行回复，如ABC符号、和弦符号或鼓表。这是将生成性人工智能工具集成到预先存在的音乐工作流程中的重要一步，对于喜欢通过描述性语言表达创意的内容制作者来说可能很有价值。代码位于https://github.com/supersational/jammin-gpt.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03479v1" target="_blank">2312.03479v1</a>
                              </td>
                              <td>JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live</td>
                              <td>Sven Hollowell</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03479v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03479v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/supersational/jammin-gpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08577v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08577v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08577v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08577v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released at https://github.com/bethgelab/DataTypeIdentification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08577v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM）开发的最新进展在识别视觉语义内容方面取得了显著成功，包括令人印象深刻的合成图像理解实例。在这里，我们介绍了视觉数据类型识别的新任务，这是一项基本的感知技能，对数据管理（例如，从大型数据集中去除噪声数据，特定领域检索）和自主视觉（例如，区分不断变化的天气条件和相机镜头染色）具有启示意义。我们开发了两个数据集，由27种不同视觉数据类型的动物图像组成，涵盖四大类。对39个VLM（参数范围从100M到80B）进行了广泛的零样本评估，显示了细微的性能状况。虽然VLM相当善于识别某些风格的\textit｛数据类型｝，如漫画和草图，但它们很难处理由图像旋转或附加噪声等基本操作产生的更简单的数据类型。我们的研究结果表明，（i）对于像CLIP这样的对比训练模型，单独的模型缩放产生了边际收益，以及（ii）对于像OpenFlamingo这样的最大的自动回归训练的VLM，性能显著下降。这一发现指出了当前前沿VLM的一个盲点：它们擅长识别语义内容，但无法通过缩放来理解视觉数据类型。通过分析这些模型的预训练分布，并在微调期间将数据类型信息纳入字幕，我们实现了性能的显著增强。通过探索这项以前未知的任务，我们的目标是为进一步推进VLM奠定基础，使其具备视觉数据类型理解能力。代码和数据集发布于https://github.com/bethgelab/datatypeidentification.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08577v3" target="_blank">2310.08577v3</a>
                              </td>
                              <td>Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models</td>
                              <td>Vishaal Udandarao</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08577v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08577v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/bethgelab/DataTypeIdentification" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03781v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lite-Mind: Towards Efficient and Versatile Brain Representation Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03781v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03781v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03781v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind, a lightweight, efficient, and versatile brain representation network based on discrete Fourier transform, that efficiently aligns fMRI voxels to fine-grained information of CLIP. Our experiments demonstrate that Lite-Mind achieves an impressive 94.3% fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind is also proven to be able to be migrated to smaller brain datasets and establishes a new state-of-the-art for zero-shot classification on the GOD dataset. The code is available at https://github.com/gongzix/Lite-Mind.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03781v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>解码大脑视觉信息的研究，特别是通过非侵入性fMRI方法，进展迅速。这一挑战源于有限的数据可用性和fMRI信号的低信噪比，导致fMRI对图像检索的精度较低。最先进的MindEye通过利用具有高参数计数数量级的深度MLP，即每个受试者996M MLP主干，将fMRI嵌入与CLIP视觉转换器的最终隐藏层对齐，显著提高了fMRI到图像检索的性能。然而，受试者之间存在显著的个体差异，即使在相同的实验设置中也是如此，这就要求对特定受试者模型进行训练。这些实质性参数对在实际设备上部署fMRI解码提出了重大挑战，尤其是在每个受试者都需要特定模型的情况下。为此，我们提出了Lite Mind，这是一种基于离散傅立叶变换的轻量级、高效和通用的大脑表示网络，可以有效地将fMRI体素与CLIP的细粒度信息对齐。我们的实验表明，Lite Mind在受试者1的NSD数据集上实现了令人印象深刻的94.3%的fMRI到图像检索准确率，比MindEye少98.7%的参数。Lite-Mind也被证明能够迁移到较小的大脑数据集，并在GOD数据集上建立了一种新的零样本分类技术。代码位于https://github.com/gongzix/lite-mind.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03781v1" target="_blank">2312.03781v1</a>
                              </td>
                              <td>Lite-Mind: Towards Efficient and Versatile Brain Representation Network</td>
                              <td>Zixuan Gong</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03781v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03781v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2208_14085v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Point Cloud from Moving Camera Videos: A No-Reference Metric</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2208_14085v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2208_14085v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2208_14085v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Point cloud is one of the most widely used digital representation formats for three-dimensional (3D) contents, the visual quality of which may suffer from noise and geometric shift distortions during the production procedure as well as compression and downsampling distortions during the transmission process. To tackle the challenge of point cloud quality assessment (PCQA), many PCQA methods have been proposed to evaluate the visual quality levels of point clouds by assessing the rendered static 2D projections. Although such projection-based PCQA methods achieve competitive performance with the assistance of mature image quality assessment (IQA) methods, they neglect that the 3D model is also perceived in a dynamic viewing manner, where the viewpoint is continually changed according to the feedback of the rendering device. Therefore, in this paper, we evaluate the point clouds from moving camera videos and explore the way of dealing with PCQA tasks via using video quality assessment (VQA) methods. First, we generate the captured videos by rotating the camera around the point clouds through several circular pathways. Then we extract both spatial and temporal quality-aware features from the selected key frames and the video clips through using trainable 2D-CNN and pre-trained 3D-CNN models respectively. Finally, the visual quality of point clouds is represented by the video quality values. The experimental results reveal that the proposed method is effective for predicting the visual quality levels of the point clouds and even competitive with full-reference (FR) PCQA methods. The ablation studies further verify the rationality of the proposed framework and confirm the contributions made by the quality-aware features extracted via the dynamic viewing manner. The code is available at https://github.com/zzc-1998/VQA_PC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2208_14085v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>点云是用于三维（3D）内容的最广泛使用的数字表示格式之一，其视觉质量可能在制作过程中受到噪声和几何偏移失真的影响，以及在传输过程中受到压缩和下采样失真的影响。为了应对点云质量评估（PCQA）的挑战，已经提出了许多PCQA方法来通过评估渲染的静态2D投影来评估点云的视觉质量水平。尽管这种基于投影的PCQA方法在成熟的图像质量评估（IQA）方法的帮助下实现了有竞争力的性能，但它们忽略了3D模型也以动态观看的方式被感知，其中视点根据渲染设备的反馈不断地改变。因此，在本文中，我们评估了移动摄像机视频中的点云，并探索了使用视频质量评估（VQA）方法处理PCQA任务的方法。首先，我们通过几个圆形路径围绕点云旋转相机来生成捕获的视频。然后，我们分别使用可训练的2D-CNN和预训练的3D-CNN模型，从选定的关键帧和视频片段中提取空间和时间质量感知特征。最后，点云的视觉质量由视频质量值表示。实验结果表明，该方法在预测点云的视觉质量水平方面是有效的，甚至与完全参考（FR）PCQA方法具有竞争力。消融研究进一步验证了所提出框架的合理性，并证实了通过动态观察方式提取的质量感知特征所做出的贡献。代码位于https://github.com/zzc-1998/vqa_pc.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2208.14085v3" target="_blank">2208.14085v3</a>
                              </td>
                              <td>Evaluating Point Cloud from Moving Camera Videos: A No-Reference Metric</td>
                              <td>Zicheng Zhang</td>
                              <td>2022-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2208_14085v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2208.14085v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zzc-1998/vqa_pc" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00351v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Manipulating the Label Space for In-Context Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00351v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00351v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00351v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>After pre-training by generating the next word conditional on previous words, the Language Model (LM) acquires the ability of In-Context Learning (ICL) that can learn a new task conditional on the context of the given in-context examples (ICEs). Similarly, visually-conditioned Language Modelling is also used to train Vision-Language Models (VLMs) with ICL ability. However, such VLMs typically exhibit weaker classification abilities compared to contrastive learning-based models like CLIP, since the Language Modelling objective does not directly contrast whether an object is paired with a text. To improve the ICL of classification, using more ICEs to provide more knowledge is a straightforward way. However, this may largely increase the selection time, and more importantly, the inclusion of additional in-context images tends to extend the length of the in-context sequence beyond the processing capacity of a VLM. To alleviate these limitations, we propose to manipulate the label space of each ICE to increase its knowledge density, allowing for fewer ICEs to convey as much information as a larger set would. Specifically, we propose two strategies which are Label Distribution Enhancement and Visual Descriptions Enhancement to improve In-context classification performance on diverse datasets, including the classic ImageNet and more fine-grained datasets like CUB-200. Specifically, using our approach on ImageNet, we increase accuracy from 74.70\% in a 4-shot setting to 76.21\% with just 2 shots. surpassing CLIP by 0.67\%. On CUB-200, our method raises 1-shot accuracy from 48.86\% to 69.05\%, 12.15\% higher than CLIP. The code is given in https://anonymous.4open.science/r/MLS_ICC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00351v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在通过生成以先前单词为条件的下一个单词进行预训练之后，语言模型（LM）获得了上下文内学习（ICL）的能力，该能力可以学习以给定上下文内示例（ICE）的上下文为条件的新任务。同样，视觉条件语言建模也用于训练具有ICL能力的视觉语言模型。然而，与CLIP等基于对比学习的模型相比，这种VLM通常表现出较弱的分类能力，因为语言建模目标不会直接对比对象是否与文本配对。为了提高分类的ICL，使用更多的ICEs来提供更多的知识是一种简单的方法。然而，这可能在很大程度上增加选择时间，更重要的是，包含额外的上下文内图像往往会将上下文内序列的长度扩展到VLM的处理能力之外。为了缓解这些限制，我们建议操纵每个ICE的标签空间，以增加其知识密度，允许更少的ICE传达与更大集合一样多的信息。具体来说，我们提出了两种策略，即标签分布增强和视觉描述增强，以提高不同数据集上的上下文内分类性能，包括经典的ImageNet和更细粒度的数据集，如CUB-200。具体来说，使用我们在ImageNet上的方法，我们将精度从4次拍摄时的74.70\%提高到仅拍摄2次的76.21\%。在CUB-200上，我们的方法将单次射击精度从48.86%提高到69.05%，比CLIP高12.15%。代码在中给出https://anonymous.4open.science/r/mls_icc.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00351v2" target="_blank">2312.00351v2</a>
                              </td>
                              <td>Manipulating the Label Space for In-Context Classification</td>
                              <td>Haokun Chen</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00351v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00351v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03203v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03203v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03203v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03203v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework leads to warp-level divergence. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03203v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，3D场景表示已经获得了巨大的普及。使用神经辐射场的方法适用于传统任务，如新颖的视图合成。近年来，出现了一些工作，旨在将NeRF的功能扩展到视图合成之外，用于语义感知任务，如使用2D基础模型中的3D特征场提取进行编辑和分割。然而，这些方法有两个主要的局限性：（a）它们受到NeRF管道的渲染速度的限制，以及（b）隐式表示的特征场受到连续性伪影的影响，从而降低了特征质量。最近，3D高斯散射在实时辐射场渲染方面表现出了最先进的性能。在这项工作中，我们更进一步：除了辐射场渲染外，我们还通过2D基础模型蒸馏实现了对任意维度语义特征的3D高斯飞溅。这种转换并不简单：天真地将特征字段合并到3DGS框架中会导致扭曲级别的差异。我们建议对架构和培训进行更改，以有效地避免此问题。我们提出的方法是通用的，我们的实验展示了新颖的视图语义分割、语言引导编辑和通过从最先进的2D基础模型（如SAM和CLIP-LSeg）中学习特征字段来分割任何内容。在整个实验中，我们的蒸馏方法能够提供类似或更好的结果，同时训练和渲染速度明显更快。此外，据我们所知，我们是第一个通过利用SAM模型启用点和边界框提示进行辐射场操作的方法。项目网站：https://feature-3dgs.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03203v1" target="_blank">2312.03203v1</a>
                              </td>
                              <td>Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</td>
                              <td>Shijie Zhou</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03203v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03203v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04749v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04749v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04749v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04749v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04749v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着潜在扩散模型的出现，文本条件图像生成领域取得了前所未有的进展。虽然值得注意的是，随着给定文本输入的复杂性增加，最先进的扩散模型可能仍然无法生成准确传达给定提示语义的图像。此外，已经观察到，预训练的多模态模型（如CLIP）往往无法检测到这种错位。为了解决这些问题，在本文中，我们探索了一种简单而有效的分解方法来评估和改进文本到图像的对齐。特别地，我们首先引入一个分解对齐分数，该分数在给定复杂提示的情况下将其分解为一组不相交的断言。然后使用VQA模型来测量每个断言与生成的图像的对准。最后，将不同断言的对齐分数进行后验组合，以给出最终的文本到图像的对齐分数。实验分析表明，与传统的CLIP、BLIP评分相比，所提出的比对指标与人类评分的相关性显著更高。此外，我们还发现断言水平对齐分数提供了有用的反馈，然后可以在简单的迭代过程中使用该反馈来逐渐增加不同断言在最终图像输出中的表达。人类用户研究表明，所提出的方法在整体文本到图像对齐精度方面超过了以前的技术水平8.7%。我们论文的项目页面可在https://1jsingh.github.io/divide-evaluate-and-refine</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04749v2" target="_blank">2307.04749v2</a>
                              </td>
                              <td>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</td>
                              <td>Jaskirat Singh</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04749v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04749v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02974v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Describing Differences in Image Sets with Natural Language</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02974v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02974v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02974v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02974v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>两组图像有何不同？辨别集合级差异对于理解模型行为和分析数据集至关重要，但手动筛选数千张图像是不切实际的。为了帮助这个发现过程，我们探索了自动描述两个$\textbf｛sets｝$图像之间的差异的任务，我们将其称为Set Difference Captioning。此任务接收图像集$D_A$和$D_B$，并输出一个在$D_A$$D_B$D_A美元上更为真实的描述。我们概述了一种分两阶段的方法，该方法首先从图像集提出候选差异描述，然后通过检查它们对两个集的区分程度来对候选进行重新排序。我们介绍了VisDiff，它首先为图像加上字幕，并提示语言模型提出候选描述，然后使用CLIP对这些描述进行重新排序。为了评估VisDiff，我们收集了VisDiffBench，这是一个包含187个配对图像集的数据集，具有基本事实差异描述。我们将VisDiff应用于不同的领域，例如比较数据集（例如，ImageNet与ImageNetV2），比较分类模型（例如，零样本CLIP与监督ResNet），总结模型故障模式（监督ResNet。使用VisDiff，我们能够在数据集和模型中发现有趣的和以前未知的差异，证明了它在揭示细微见解方面的实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02974v1" target="_blank">2312.02974v1</a>
                              </td>
                              <td>Describing Differences in Image Sets with Natural Language</td>
                              <td>Lisa Dunlap</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02974v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02974v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/understanding-visual-datasets/visdiff" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02918v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02918v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02918v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02918v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations. This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration. Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation. Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations. Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation. To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks. Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive experiments on 16 IR tasks and 26 benchmarks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02918v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管取得了实质性进展，但一体化图像恢复（IR）在处理复杂的真实世界退化方面仍面临着持续的挑战。本文介绍了MPerciver：一种新的多模式即时学习方法，它利用稳定扩散（SD）先验来增强一体化图像恢复的自适应性、可推广性和保真度。具体来说，我们开发了一个双分支模块来掌握两种类型的SD提示：用于整体表示的文本提示和用于多尺度细节表示的视觉提示。这两个提示都通过CLIP图像编码器的退化预测进行动态调整，从而实现对各种未知退化的自适应响应。此外，插件细节细化模块通过直接编码器到解码器的信息转换来提高恢复保真度。为了评估我们的方法，MPerciver接受了9项任务的一体式IR训练，在大多数任务中都优于最先进的特定任务方法。在多任务预训练后，MPerciver在低水平视觉中获得了广泛的表现，在看不见的任务中表现出显著的零样本和少速能力。在16个IR任务和26个基准上进行的大量实验强调了MPerciver在适应性、可推广性和保真度方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02918v1" target="_blank">2312.02918v1</a>
                              </td>
                              <td>Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</td>
                              <td>Yuang Ai</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02918v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02918v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11300v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11300v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11300v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11300v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap between the General Vision-Language Model (GVLM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DVLM. Experimental results show that our proposed dataset is highly effective for various tasks, and our model GeoRSCLIP improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$ in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo) tasks. Dataset and models have been released in: \url{https://github.com/om-ai-lab/RS5M}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11300v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用大量图像-文本配对数据的预训练视觉语言模型（VLM）展示了前所未有的图像-文本关联能力，在各种下游任务中取得了显著的结果。一个关键的挑战是如何利用现有的大规模预训练VLM（在公共对象上训练）来执行特定领域的转移，以完成与领域相关的下游任务。一个关键的挑战是如何利用现有的大规模预训练VLM（在公共对象上训练）来执行特定领域的转移，以完成与领域相关的下游任务。在本文中，我们提出了一个新的框架，其中包括领域预训练视觉语言模型（DVLM），弥合了通用视觉语言模型和特定领域下游任务之间的差距。此外，我们还提出了遥感领域的图像-文本配对数据集RS5M，该数据集有500万张英文描述的遥感图像。该数据集是通过过滤公开可用的图像-文本配对数据集和使用预训练的VLM进行字幕标记的仅RS数据集而获得的。这些构成了第一个大规模的RS图像-文本配对数据集。此外，我们对CLIP模型进行了微调，并在RS5M上尝试了几种参数高效微调方法来实现DVLM。实验结果表明，我们提出的数据集对于各种任务都是非常有效的，并且我们的模型GeoRSCLIP在零样本分类（ZSC）中比基线或先前最先进的模型改进了$3\%\sim20\%$，在遥感跨模式文本图像检索（RSCTIR）中改进了$3\\%\sim6\%$和在语义定位（SeLo）任务中改进了$4\%\%\sim5\%$。数据集和模型已在以下位置发布：\url{https://github.com/om-ai-lab/rs5m}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11300v4" target="_blank">2306.11300v4</a>
                              </td>
                              <td>RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model</td>
                              <td>Zilun Zhang</td>
                              <td>2023-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11300v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11300v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/om-ai-lab/rs5m" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00613v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Controllable Music Production with Diffusion Models and Guidance Gradients</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00613v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00613v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00613v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model. Audio samples are available at https://machinelearning.apple.com/research/controllable-music</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00613v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们展示了如何使用扩散模型的条件生成来处理44.1kHz立体声音频中的音乐制作中的各种现实任务，并提供采样时间指导。我们考虑的场景包括音乐音频的延续、修复和再生，在两个不同的音乐曲目之间创建平滑的过渡，以及将所需的风格特征转移到现有的音频片段中。我们通过在一个简单的框架中应用采样时的指导来实现这一点，该框架支持重建和分类损失，或两者的任何组合。这种方法确保生成的音频可以匹配其周围的上下文，或者符合相对于任何合适的预训练分类器或嵌入模型指定的类分布或潜在表示。音频样本可在https://machinelearning.apple.com/research/controllable-music</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00613v2" target="_blank">2311.00613v2</a>
                              </td>
                              <td>Controllable Music Production with Diffusion Models and Guidance Gradients</td>
                              <td>Mark Levy</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00613v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00613v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14062v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hardware Resilience Properties of Text-Guided Image Classifiers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14062v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14062v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14062v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel method to enhance the reliability of image classification models during deployment in the face of transient hardware errors. By utilizing enriched text embeddings derived from GPT-3 with question prompts per class and CLIP pretrained text encoder, we investigate their impact as an initialization for the classification layer. Our approach achieves a remarkable $5.5\times$ average increase in hardware reliability (and up to $14\times$) across various architectures in the most critical layer, with minimal accuracy drop ($0.3\%$ on average) compared to baseline PyTorch models. Furthermore, our method seamlessly integrates with any image classification backbone, showcases results across various network architectures, decreases parameter and FLOPs overhead, and follows a consistent training recipe. This research offers a practical and efficient solution to bolster the robustness of image classification models against hardware failures, with potential implications for future studies in this domain. Our code and models are released at https://github.com/TalalWasim/TextGuidedResilience.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14062v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的方法来提高图像分类模型在部署过程中面对瞬态硬件错误的可靠性。通过利用从GPT-3派生的每个类都有问题提示的丰富文本嵌入和CLIP预训练的文本编码器，我们研究了它们作为分类层初始化的影响。与基线PyTorch模型相比，我们的方法在最关键层的各种架构中实现了显著的硬件可靠性平均5.5美元的增长（最高可达14美元），精度下降最小（平均0.3美元）。此外，我们的方法与任何图像分类主干无缝集成，展示了各种网络架构的结果，减少了参数和FLOP开销，并遵循一致的训练方法。这项研究提供了一种实用而有效的解决方案，以增强图像分类模型对硬件故障的鲁棒性，对该领域的未来研究具有潜在意义。我们的代码和模型发布于https://github.com/talalwasim/textguidedresilience.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14062v2" target="_blank">2311.14062v2</a>
                              </td>
                              <td>Hardware Resilience Properties of Text-Guided Image Classifiers</td>
                              <td>Syed Talal Wasim</td>
                              <td>2023-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14062v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14062v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/talalwasim/textguidedresilience" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02568v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02568v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02568v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02568v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores promptable NeRF generation (e.g., text prompt or single image prompt) for direct conditioning and fast generation of NeRF parameters for the underlying 3D scenes, thus undoing complex intermediate steps while providing full 3D generation with conditional control. Unlike previous diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations, Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single forward pass, leveraging a pre-trained implicit latent space of NeRF parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that the NeRFs produced by our method serve as semantically informative initializations, significantly accelerating the inference process of existing prompt-to-NeRF methods. Specifically, we will show that our approach speeds up the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the image-to-NeRF method Zero-1-to-3 by 3 to 5 times.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02568v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文探索了可提示的NeRF生成（例如，文本提示或单个图像提示），用于直接调节和快速生成底层3D场景的NeRF参数，从而在提供具有条件控制的完整3D生成的同时，省去复杂的中间步骤。与之前涉及乏味的逐提示优化的基于扩散CLIP的管道不同，Prompt2NeRF PIL能够利用预先训练的NeRF参数的隐式潜在空间，通过单次前向传递生成各种3D对象。此外，在零样本任务中，我们的实验表明，由我们的方法产生的NeRF用作语义信息初始化，显著加速了现有提示-NeRF方法的推理过程。具体来说，我们将展示我们的方法将文本到NeRF模型DreamFusion的速度和图像到NeRF方法Zero-1-to-3的3D重建速度提高了3到5倍。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02568v1" target="_blank">2312.02568v1</a>
                              </td>
                              <td>Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent</td>
                              <td>Jianmeng Liu</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02568v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02568v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02546v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02546v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02546v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02546v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although vision models such as Contrastive Language-Image Pre-Training (CLIP) show impressive generalization performance, their zero-shot robustness is still limited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead of undesirably providing human supervision as commonly done, it is possible to take advantage of Multi-modal Large Language Models (MLLMs) that hold powerful visual understanding abilities. However, MLLMs are shown to struggle with vision problems due to the incompatibility of tasks, thus hindering their utilization. In this paper, we propose to effectively leverage MLLMs to conduct Machine Vision Therapy which aims to rectify the noisy predictions from vision models. By fine-tuning with the denoised labels, the learning model performance can be boosted in an unsupervised manner. To solve the incompatibility issue, we propose a novel Denoising In-Context Learning (DICL) strategy to align vision tasks with MLLMs. Concretely, by estimating a transition matrix that captures the probability of one class being confused with another, an instruction containing a correct exemplar and an erroneous one from the most probable noisy class can be constructed. Such an instruction can help any MLLMs with ICL ability to detect and rectify incorrect predictions of vision models. Through extensive experiments on ImageNet, WILDS, DomainBed, and other OOD datasets, we carefully validate the quantitative and qualitative effectiveness of our method. Our code is available at https://github.com/tmllab/Machine_Vision_Therapy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02546v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管对比语言图像预训练（CLIP）等视觉模型显示出令人印象深刻的泛化性能，但在没有微调的情况下，其零样本鲁棒性在分布外（OOD）场景下仍然有限。可以利用具有强大视觉理解能力的多模态大型语言模型（MLLMs），而不是像通常那样提供不期望的人工监督。然而，由于任务的不兼容性，MLLMs被证明难以解决视力问题，从而阻碍了它们的利用。在本文中，我们建议有效地利用MLLMs进行机器视觉治疗，旨在纠正视觉模型中的噪声预测。通过对去噪标签进行微调，可以以无监督的方式提高学习模型的性能。为了解决不兼容性问题，我们提出了一种新的去噪上下文学习（DICL）策略，以使视觉任务与MLLMs保持一致。具体地，通过估计捕获一个类与另一个类混淆的概率的转移矩阵，可以构造包含来自最可能的噪声类的正确样本和错误样本的指令。这样的指令可以帮助任何具有ICL能力的MLLM检测和纠正视觉模型的错误预测。通过在ImageNet、WILDS、DomainBed和其他OOD数据集上进行广泛的实验，我们仔细验证了我们方法的定量和定性有效性。我们的代码可在https://github.com/tmllab/machine_vision_therapy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02546v1" target="_blank">2312.02546v1</a>
                              </td>
                              <td>Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning</td>
                              <td>Zhuo Huang</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02546v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02546v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tmllab/machine_vision_therapy" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02400v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02400v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02400v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02400v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DP-SGD has emerged as a popular method to protect personally identifiable information in deep learning applications. Unfortunately, DP-SGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility. To enhance the model's utility, researchers proposed various adaptive DP-SGD methods. However, we examine and discover that these techniques result in greater privacy leakage or lower accuracy than the traditional DP-SGD method, or a lack of evaluation on a complex data set such as CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our method automates clipping threshold estimation based on the DL model's gradient norm and scales the gradients of each training sample without losing gradient information. This helps to improve the algorithm's utility while using a less privacy budget. To further improve accuracy, we introduce automatic noise multiplier decay mechanisms to decrease the noise multiplier after every epoch. Finally, we develop closed-form mathematical expressions using tCDP accountant for automatic noise multiplier and automatic clipping threshold estimation. Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms existing SOTA DP-SGD methods in privacy and accuracy on various benchmark datasets. We also show that privacy can be improved by lowering the scale factor and using learning rate schedulers without significantly reducing accuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier, improves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10, CIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a substantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37 for the corresponding data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02400v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DP-SGD已成为深度学习应用程序中保护个人身份信息的一种流行方法。不幸的是，DP-SGD的每样本梯度削波和训练期间的均匀噪声添加会显著降低模型的实用性。为了提高模型的实用性，研究人员提出了各种自适应的DP-SGD方法。然而，我们检查并发现，与传统的DP-SGD方法相比，这些技术会导致更大的隐私泄露或更低的准确性，或者缺乏对复杂数据集（如CIFAR100）的评估。为了解决这些限制，我们提出了一种自动DP-SGD。我们的方法基于DL模型的梯度范数自动进行裁剪阈值估计，并在不丢失梯度信息的情况下缩放每个训练样本的梯度。这有助于提高算法的实用性，同时使用更少的隐私预算。为了进一步提高精度，我们引入了自动噪声乘数衰减机制，以在每个历元后降低噪声乘数。最后，我们使用tCDP会计为自动噪声乘法器和自动削波阈值估计开发了闭合形式的数学表达式。通过广泛的实验，我们证明了在各种基准数据集上，Auto DP-SGD在隐私和准确性方面优于现有的SOTA DP-SGD方法。我们还表明，可以通过降低比例因子和使用学习率调度器来改善隐私，而不会显著降低准确性。具体而言，当与阶跃噪声乘法器一起使用时，Auto DP-SGD将MNIST、CIFAR10、CIFAR100和AG News Corpus数据集的精度分别提高了3.20、1.57、6.73和1.42。此外，它大大减少了相应数据集的隐私预算94.9、79.16、67.36和53.37。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02400v1" target="_blank">2312.02400v1</a>
                              </td>
                              <td>Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation</td>
                              <td>Sai Venkatesh Chilukoti</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02400v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02400v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02345v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPDrawX: Primitive-based Explanations for Text Guided Sketch Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02345v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02345v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02345v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the goal of understanding the visual concepts that CLIP associates with text prompts, we show that the latent space of CLIP can be visualized solely in terms of linear transformations on simple geometric primitives like circles and straight lines. Although existing approaches achieve this by sketch-synthesis-through-optimization, they do so on the space of B\'ezier curves, which exhibit a wastefully large set of structures that they can evolve into, as most of them are non-essential for generating meaningful sketches. We present CLIPDrawX, an algorithm that provides significantly better visualizations for CLIP text embeddings, using only simple primitive shapes like straight lines and circles. This constrains the set of possible outputs to linear transformations on these primitives, thereby exhibiting an inherently simpler mathematical form. The synthesis process of CLIPDrawX can be tracked end-to-end, with each visual concept being explained exclusively in terms of primitives. Implementation will be released upon acceptance. Project Page: $\href{https://clipdrawx.github.io/}{\text{https://clipdrawx.github.io/}}$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02345v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了理解CLIP与文本提示相关联的视觉概念，我们证明了CLIP的潜在空间可以仅通过简单几何图元（如圆和直线）上的线性变换来可视化。尽管现有的方法是通过优化进行草图合成来实现这一点的，但它们是在B’zier曲线的空间上实现的，B曲线展示了一组浪费的大结构，它们可以进化成这些结构，因为它们中的大多数对于生成有意义的草图来说都不是必不可少的。我们介绍了CLIPDrawX，这是一种算法，它只使用简单的基本形状（如直线和圆），为CLIP文本嵌入提供了更好的可视化效果。这将一组可能的输出约束为这些基元上的线性变换，从而呈现出一种固有的更简单的数学形式。CLIPDrawX的合成过程可以端到端跟踪，每个视觉概念都只根据基元进行解释。验收合格后发布实施情况。项目页面：$\href{https://clipdrawx.github.io/}｛\text{https://clipdrawx.github.io/}}$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02345v1" target="_blank">2312.02345v1</a>
                              </td>
                              <td>CLIPDrawX: Primitive-based Explanations for Text Guided Sketch Synthesis</td>
                              <td>Nityanand Mathur</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02345v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02345v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18784v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18784v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18784v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18784v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantization. Further, existing results with heavy-tailed noise assume bounded $\eta$-th central moments, with $\eta \in (1,2]$. In contrast, our refined analysis works even for $\eta=1$, strictly relaxing the noise moment assumptions in the literature.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18784v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的几项工作研究了随机梯度下降（SGD）及其截断变量的高概率收敛性。与普通SGD相比，削波SGD实际上更稳定，并具有对数依赖于故障概率的额外理论优势。然而，实现提高通信效率或加速收敛的SGD的其他实际非线性变体（例如符号SGD、量化SGD和归一化SGD）的收敛性却知之甚少。在这项工作中，我们研究了一类广义非线性SGD方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，我们证明了对失效概率的对数依赖性，即使当噪声是重尾噪声时也是如此。严格地说，我们的结果比削波SGD的结果更一般，适用于具有有界（分量或联合）输出的任何非线性，如削波、归一化和量化。此外，具有重尾噪声的现有结果假设第$\eta$个中心矩有界，其中$\eta\in（1,2]$。相比之下，我们的精细分析甚至适用于$\eta=1$，严格放宽了文献中的噪声矩假设。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18784v3" target="_blank">2310.18784v3</a>
                              </td>
                              <td>High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise</td>
                              <td>Aleksandar Armacki</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18784v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18784v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02310v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02310v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02310v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02310v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in language-model-based video understanding have been progressing at a remarkable pace, spurred by the introduction of Large Language Models (LLMs). However, the focus of prior research has been predominantly on devising a projection layer that maps video features to tokens, an approach that is both rudimentary and inefficient. In our study, we introduce a cutting-edge framework, VaQuitA, designed to refine the synergy between video and textual information. At the data level, instead of sampling frames uniformly, we implement a sampling method guided by CLIP-score rankings, which enables a more aligned selection of frames with the given question. At the feature level, we integrate a trainable Video Perceiver alongside a Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the interplay between the input question and the video features. We also discover that incorporating a simple prompt, "Please be critical", into the LLM input can substantially enhance its video comprehension capabilities. Our experimental results indicate that VaQuitA consistently sets a new benchmark for zero-shot video question-answering tasks and is adept at producing high-quality, multi-turn video dialogues with users.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02310v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在引入大型语言模型（LLM）的推动下，基于语言模型的视频理解的最新进展取得了显著进展。然而，先前的研究重点主要集中在设计一种将视频特征映射到标记的投影层上，这种方法既初级又低效。在我们的研究中，我们引入了一个尖端的框架VaQuitA，旨在完善视频和文本信息之间的协同作用。在数据层面，我们实现了一种以CLIP分数排名为指导的采样方法，而不是统一地对帧进行采样，这使得对给定问题的帧的选择更加一致。在功能层面，我们将可训练的视频感知器与视觉查询转换器（缩写为VQ Former）集成在一起，这增强了输入问题和视频功能之间的相互作用。我们还发现，在LLM输入中加入一个简单的提示“Please be critical”，可以大大增强其视频理解能力。我们的实验结果表明，VaQuitA始终为零样本视频问答任务设定了一个新的基准，并擅长与用户进行高质量、多焦点的视频对话。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02310v1" target="_blank">2312.02310v1</a>
                              </td>
                              <td>VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding</td>
                              <td>Yizhou Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02310v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02310v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02147v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rejuvenating image-GPT as Strong Visual Representation Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02147v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02147v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement of D-iGPT is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT achieves 89.5\% top-1 accuracy with a vanilla ViT-Large model. This model also shows strong generalization on the downstream task and robustness on out-of-distribution samples. Code is avaiable at \href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02147v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文增强了图像GPT（iGPT），这是引入自回归预训练来预测视觉表示学习的下一个像素的开创性工作之一。进行了两个简单但重要的更改。首先，我们将预测目标从原始像素转移到语义标记，从而实现对视觉内容的更高层次理解。其次，我们通过指示模型不仅预测下一个令牌，而且预测可见令牌来补充自回归建模。当语义标记由经过区别训练的模型（如CLIP）编码时，这种管道尤其有效。我们将这种新方法称为D-iGPT。大量实验表明，D-iGPT在视觉表示方面表现出色：D-iGPT的一个显著成就是其在ImageNet-1K数据集上的令人信服的性能——通过在公开可用的数据集上进行训练，D-iGPT在普通ViT-Large模型中实现了89.5\%的前1级准确率。该模型对下游任务也表现出较强的泛化能力，对分布外样本也表现出鲁棒性。代码可在\href获得{https://github.com/oliverrensu/d-igpt}{https://github.com/oliverrensu/d-igpt}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02147v1" target="_blank">2312.02147v1</a>
                              </td>
                              <td>Rejuvenating image-GPT as Strong Visual Representation Learners</td>
                              <td>Sucheng Ren</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02147v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/oliverrensu/d-igpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02253v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02253v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02253v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02253v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifier's performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02253v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成式深度学习的最新进展使得能够在文本到图像生成中创建高质量的合成图像。先前的工作表明，在ImageNet上微调预训练的扩散模型并从微调的模型生成合成训练图像可以提高ImageNet分类器的性能。然而，随着合成图像数量超过真实图像，性能会下降。在本文中，我们探讨了生成微调是否对这种改进至关重要，以及是否有可能使用更多的合成数据进一步扩大训练规模。我们提出了一个新的框架，利用现成的生成模型来生成合成训练图像，解决了多重挑战：类名模糊、天真提示缺乏多样性和领域转换。具体来说，我们利用大型语言模型（LLM）和CLIP来解决类名的歧义。为了使图像多样化，我们提出了情境化多样化（CD）和程式化多样化（SD）方法，也是由LLM推动的。最后，为了减轻域偏移，我们利用域自适应技术对合成图像进行辅助批量归一化。我们的框架通过更多的合成数据不断提高识别模型的性能，高达原始ImageNet大小的6倍，展示了合成数据在改进识别模型和强大的域外泛化方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02253v1" target="_blank">2312.02253v1</a>
                              </td>
                              <td>Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images</td>
                              <td>Zhuoran Yu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02253v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02253v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02021v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02021v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02021v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02021v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNN), where domain shifts occur due to lighting, weather, or geolocation changes. In this work, we propose VLTSeg to enhance domain generalization in semantic segmentation, where the network is solely trained on the source domain and evaluated on unseen target domains. Our method leverages the inherent semantic robustness of vision-language models. First, by substituting traditional vision-only backbones with pre-trained encoders from CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG, vision-language pre-training significantly outperforms supervised and self-supervised vision pre-training. We thus propose a new vision-language approach for domain generalized segmentation, which improves the domain generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset. We further show the superior generalization capabilities of vision-language segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test set at the time of writing. Additionally, our approach shows strong in-domain generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set, resulting in a shared first place with the previous SOTA on the current leaderboard at the time of submission.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02021v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>领域泛化（DG）仍然是基于深度神经网络（DNN）的感知的一个重大挑战，在深度神经网络中，由于照明、天气或地理位置的变化，领域会发生变化。在这项工作中，我们提出了VLTSeg来增强语义分割中的域泛化，其中网络仅在源域上进行训练，并在看不见的目标域上进行评估。我们的方法利用了视觉语言模型固有的语义稳健性。首先，通过用CLIP和EVA-CLIP的预训练编码器代替传统的纯视觉骨干作为迁移学习设置，我们发现在DG领域，视觉语言预训练显著优于监督和自监督视觉预训练。因此，我们提出了一种新的用于领域广义分割的视觉语言方法，当在合成GTA5数据集上进行训练时，该方法将领域广义SOTA提高了7.6%mIoU。我们进一步展示了视觉语言分割模型的优越泛化能力，在流行的Cityscapes到ACDC基准上达到了76.48%的mIoU，在撰写本文时的测试集上比以前的SOTA方法高出6.9%mIoU。此外，我们的方法在Cityscapes测试集中显示出86.1%mIoU所示的强大的域内泛化能力，导致在提交时与前一个SOTA在当前排行榜上并列第一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02021v1" target="_blank">2312.02021v1</a>
                              </td>
                              <td>VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation</td>
                              <td>Christoph Hümmer</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02021v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02021v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01998v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language-only Efficient Training of Zero-shot Composed Image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01998v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01998v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01998v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Composed image retrieval (CIR) task takes a composed query of image and text, aiming to search relative images for both conditions. Conventional CIR approaches need a training dataset composed of triplets of query image, query text, and target image, which is very expensive to collect. Several recent works have worked on the zero-shot (ZS) CIR paradigm to tackle the issue without using pre-collected triplets. However, the existing ZS-CIR methods show limited backbone scalability and generalizability due to the lack of diversity of the input texts during training. We propose a novel CIR framework, only using language for its training. Our LinCIR (Language-only training for CIR) can be trained only with text datasets by a novel self-supervision named self-masking projection (SMP). We project the text latent embedding to the token embedding space and construct a new text by replacing the keyword tokens of the original text. Then, we let the new and original texts have the same latent embedding vector. With this simple strategy, LinCIR is surprisingly efficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in 48 minutes and shows the best ZS-CIR performances on four different CIR benchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised method on FashionIQ. Code is available at https://github.com/navervision/lincir</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01998v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>组合图像检索（CIR）任务采用图像和文本的组合查询，旨在搜索这两种条件下的相对图像。传统的CIR方法需要由查询图像、查询文本和目标图像的三元组组成的训练数据集，这是非常昂贵的收集。最近的几项工作已经致力于零样本（ZS）CIR范式，以在不使用预先收集的三元组的情况下解决这个问题。然而，由于训练过程中输入文本缺乏多样性，现有的ZS-CIR方法显示出有限的主干可扩展性和可推广性。我们提出了一个新的CIR框架，只使用语言进行训练。我们的LinCIR（仅针对CIR的语言训练）只能通过一种新的称为自掩蔽投影（SMP）的自监督来使用文本数据集进行训练。我们将文本潜在嵌入投影到标记嵌入空间，并通过替换原始文本的关键字标记来构建新的文本。然后，我们让新文本和原始文本具有相同的潜在嵌入向量。通过这种简单的策略，LinCIR的效率和高效性令人惊讶；具有CLIP-ViT-G骨干的LinCIR在48分钟内完成训练，并在四个不同的CIR基准（CIRCO、GeneCIS、FashionIQ和CIRR）上显示出最佳的ZS-CIR性能，甚至优于Fashion智商上的监督方法。代码位于https://github.com/navervision/lincir</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01998v1" target="_blank">2312.01998v1</a>
                              </td>
                              <td>Language-only Efficient Training of Zero-shot Composed Image Retrieval</td>
                              <td>Geonmo Gu</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01998v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01998v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/navervision/lincir" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01987v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bootstrapping SparseFormers from Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01987v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01987v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recently proposed SparseFormer architecture provides an alternative approach to visual understanding by utilizing a significantly lower number of visual tokens via adjusting RoIs, greatly reducing computational costs while still achieving promising performance. However, training SparseFormers from scratch is still expensive, and scaling up the number of parameters can be challenging. In this paper, we propose to bootstrap SparseFormers from ViT-based vision foundation models in a simple and efficient way. Since the majority of SparseFormer blocks are the standard transformer ones, we can inherit weights from large-scale pre-trained vision transformers and freeze them as much as possible. Therefore, we only need to train the SparseFormer-specific lightweight focusing transformer to adjust token RoIs and fine-tune a few early pre-trained blocks to align the final token representation. In such a way, we can bootstrap SparseFormer architectures from various large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs or CLIPs) using a rather smaller amount of training samples (e.g., IN-1K) and without labels or captions within just a few hours. As a result, the bootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9% accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer from CLIPs also demonstrates notable zero-shot performance with highly reduced computational cost without seeing any caption during the bootstrapping procedure. In addition, CLIP-bootstrapped SparseFormers, which align the output space with language without seeing a word, can serve as efficient vision encoders in multimodal large language models. Code will be publicly available at https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01987v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近提出的SparseFormer架构提供了一种视觉理解的替代方法，通过调整ROI来利用数量显著减少的视觉标记，大大降低了计算成本，同时仍然实现了有希望的性能。然而，从头开始训练SparseFormers仍然是昂贵的，并且扩大参数的数量可能具有挑战性。在本文中，我们建议以一种简单有效的方式从基于ViT的视觉基础模型中引导SparseFormers。由于大多数SparseFormer块都是标准的transformer块，我们可以从大规模预训练的视觉transformer中继承权重，并尽可能多地冻结它们。因此，我们只需要训练SparseFormer特定的轻量级聚焦转换器来调整令牌ROI，并微调一些早期预训练的块来对齐最终的令牌表示。通过这种方式，我们可以在短短几个小时内使用少量的训练样本（例如，In-1K）从各种大规模预训练模型（例如，In-21K预训练AugRegs或CLIP）中引导SparseFormer架构，而不需要标签或字幕。因此，自举单模态SparseFormer（来自AugReg-ViT-L/16-384）在IN-1K上仅使用49个令牌就可以达到84.9%的准确率，而来自CLIP的多模态SparseFormer也表现出显著的零样本性能，大大降低了计算成本，而在自举过程中看不到任何说明。此外，CLIP自举的SparseFormers可以在不看单词的情况下将输出空间与语言对齐，可以在多模式大型语言模型中用作高效的视觉编码器。代码将在公开https://github.com/showlab/sparseformer</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01987v1" target="_blank">2312.01987v1</a>
                              </td>
                              <td>Bootstrapping SparseFormers from Vision Foundation Models</td>
                              <td>Ziteng Gao</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01987v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/showlab/sparseformer" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01897v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01897v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01897v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01897v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pre-training. Yet, it still remains unclear how to adapt these pre-trained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective cross-snippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone. For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.0 average mAP on THUMOS14, 37.12 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01897v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉转换器（ViT）由于其灵活的设计、适应性强的自我注意机制和掩蔽预训练的有效性，在视频识别中显示出了很高的潜力。然而，如何将这些预先训练的短期ViT应用于未经处理的视频中的时间动作检测（TAD）仍不清楚。现有的工作将它们视为每个短切片段的现成特征提取器，而没有在更广泛的时间上下文中捕捉不同片段之间的细粒度关系。为了缓解这一问题，本文重点设计了一种新的机制，将这些预先训练的ViT模型作为统一的长格式视频转换器，以充分释放其在捕捉片段间关系方面的建模能力，同时保持低计算开销和内存消耗，实现高效的TAD。为此，我们设计了有效的跨片段传播模块，从两个层面逐步在不同片段之间交换短期视频信息。对于内部主干信息传播，我们引入了一种跨片段传播策略，以实现主干内部的多片段时间特征交互。对于主干后信息传播，我们提出了用于进一步剪辑级建模的时间变换器层。使用VideoMAE预训练的普通ViT-B，我们的端到端时间动作检测器（ViT-TAD）产生了与以前的时间动作检测器相比非常有竞争力的性能，在THUMOS14上的平均mAP高达69.0，在ActivityNet-1.3上的平均mAP高达37.12，在FineAction上的平均mAP高达17.20。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01897v1" target="_blank">2312.01897v1</a>
                              </td>
                              <td>Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</td>
                              <td>Min Yang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01897v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01897v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01758v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01758v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01758v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01758v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot age estimation aims to learn feature information about age from input images and make inferences about a given person's image or video frame without specific sample data. The development of zero-shot age estimation can improve the efficiency and accuracy of various applications (e.g., age verification and secure access control, etc.), while also promoting research on multi-modal and zero-shot learning in the social media field. For example, zero-sample age estimation can be used to create social networks focused on specific age groups. However, existing methods mainly focus on supervised, labeled age estimation learning, and the prediction effect of zero-shot learning is very poor. To tackle the above issues, we propose a novel CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation (CZL-CIAE). Specifically, we first introduce the CLIP model to extract image features and text semantic information respectively, and map them into a highly semantically aligned high-dimensional feature space. Next, we designed a new Transformer architecture (i.e., FourierFormer) to achieve channel evolution and spatial interaction of images, and to fuse image and text semantic information. Finally, we introduce reversible age estimation, which uses end-to-end error feedback to reduce the error rate of age predictions. Through extensive experiments on multiple data sets, CZL-CIAE has achieved better age prediction results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01758v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本年龄估计旨在从输入图像中学习关于年龄的特征信息，并在没有特定样本数据的情况下对给定人的图像或视频帧进行推断。零样本年龄估计的发展可以提高各种应用（如年龄验证和安全访问控制等）的效率和准确性，同时也促进了社交媒体领域对多模式和零样本学习的研究。例如，零样本年龄估计可以用于创建专注于特定年龄组的社交网络。然而，现有的方法主要集中在监督、标记的年龄估计学习上，零样本学习的预测效果非常差。为了解决上述问题，我们提出了一种新的CLIP驱动的零样本学习用于校正逆年龄估计（CZL-CIAE）。具体来说，我们首先引入CLIP模型来分别提取图像特征和文本语义信息，并将它们映射到语义高度对齐的高维特征空间中。接下来，我们设计了一种新的Transformer架构（即FourierFormer），以实现图像的通道进化和空间交互，并融合图像和文本的语义信息。最后，我们介绍了可逆年龄估计，它使用端到端的误差反馈来降低年龄预测的错误率。通过对多个数据集的广泛实验，CZL-CIAE取得了较好的年龄预测结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01758v1" target="_blank">2312.01758v1</a>
                              </td>
                              <td>CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation</td>
                              <td>Yuntao Shou</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01758v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01758v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01640v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01640v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01640v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01640v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current pedestrian attribute recognition (PAR) algorithms are developed based on multi-label or multi-task learning frameworks, which aim to discriminate the attributes using specific classification heads. However, these discriminative models are easily influenced by imbalanced data or noisy samples. Inspired by the success of generative models, we rethink the pedestrian attribute recognition scheme and believe the generative models may perform better on modeling dependencies and complexity between human attributes. In this paper, we propose a novel sequence generation paradigm for pedestrian attribute recognition, termed SequencePAR. It extracts the pedestrian features using a pre-trained CLIP model and embeds the attribute set into query tokens under the guidance of text prompts. Then, a Transformer decoder is proposed to generate the human attributes by incorporating the visual features and attribute query tokens. The masked multi-head attention layer is introduced into the decoder module to prevent the model from remembering the next attribute while making attribute predictions during training. Extensive experiments on multiple widely used pedestrian attribute recognition datasets fully validated the effectiveness of our proposed SequencePAR. The source code and pre-trained models will be released at https://github.com/Event-AHU/OpenPAR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01640v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的行人属性识别（标准杆数）算法是基于多标签或多任务学习框架开发的，旨在使用特定的分类头来识别属性。然而，这些判别模型很容易受到不平衡数据或噪声样本的影响。受生成模型成功的启发，我们重新思考了行人属性识别方案，并认为生成模型可能在建模人类属性之间的依赖性和复杂性方面表现得更好。在本文中，我们提出了一种新的行人属性识别序列生成范式，称为SequencePAR。它使用预先训练的CLIP模型提取行人特征，并在文本提示的指导下将属性集嵌入到查询标记中。然后，提出了一种Transformer解码器，通过结合视觉特征和属性查询令牌来生成人类属性。在解码器模块中引入了掩蔽的多头注意力层，以防止模型在训练期间进行属性预测时记住下一个属性。在多个广泛使用的行人属性识别数据集上进行的大量实验充分验证了我们提出的SequencePAR的有效性。源代码和预训练模型将在https://github.com/event-ahu/openpar.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01640v1" target="_blank">2312.01640v1</a>
                              </td>
                              <td>SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm</td>
                              <td>Jiandong Jin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01640v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01640v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01629v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLAMP: Contrastive LAnguage Model Prompt-tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01629v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01629v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have emerged as powerful general-purpose interfaces for many machine learning problems. Recent work has adapted LLMs to generative visual tasks like image captioning, visual question answering, and visual chat, using a relatively small amount of instruction-tuning data. In this paper, we explore whether modern LLMs can also be adapted to classifying an image into a set of categories. First, we evaluate multimodal LLMs that are tuned for generative tasks on zero-shot image classification and find that their performance is far below that of specialized models like CLIP. We then propose an approach for light fine-tuning of LLMs using the same contrastive image-caption matching objective as CLIP. Our results show that LLMs can, indeed, achieve good image classification performance when adapted this way. Our approach beats state-of-the-art mLLMs by 13% and slightly outperforms contrastive learning with a custom text model, while also retaining the LLM's generative abilities. LLM initialization appears to particularly help classification in domains under-represented in the visual pre-training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01629v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经成为许多机器学习问题的强大通用接口。最近的工作使用相对少量的指令调整数据，使LLM适应生成视觉任务，如图像字幕、视觉问答和视觉聊天。在本文中，我们探讨了现代LLM是否也可以适用于将图像分类为一组类别。首先，我们评估了针对零样本图像分类的生成任务调整的多模式LLM，发现它们的性能远低于CLIP等专门模型。然后，我们提出了一种使用与CLIP相同的对比图像字幕匹配目标对LLM进行光微调的方法。我们的结果表明，当采用这种方式时，LLM确实可以实现良好的图像分类性能。我们的方法以13%的优势击败了最先进的mLLM，略优于使用自定义文本模型的对比学习，同时还保留了LLM的生成能力。LLM初始化似乎特别有助于在视觉预训练数据中表现不足的领域中进行分类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01629v1" target="_blank">2312.01629v1</a>
                              </td>
                              <td>CLAMP: Contrastive LAnguage Model Prompt-tuning</td>
                              <td>Piotr Teterwak</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01629v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01629v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01597v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01597v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01597v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01597v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in contrastive language-image pretraining (CLIP) have demonstrated strong capabilities in zero-shot classification by aligning visual representations with target text embeddings in an image level. However, in dense prediction tasks, CLIP often struggles to localize visual features within an image and fails to give accurate pixel-level predictions, which prevents it from functioning as a generalized visual foundation model. In this work, we aim to enhance CLIP's potential for semantic segmentation with minimal modifications to its pretrained models. By rethinking self-attention, we surprisingly find that CLIP can adapt to dense prediction tasks by simply introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically, we replace the traditional self-attention block of CLIP vision encoder's last layer by our CSA module and reuse its pretrained projection matrices of query, key, and value, leading to a training-free adaptation approach for CLIP's zero-shot semantic segmentation. Extensive experiments show the advantage of CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic segmentation benchmarks highlighted in this paper, significantly outperforming the existing SoTA's 33.9% and the vanilla CLIP's 14.1%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01597v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）的最新进展表明，通过在图像级别中将视觉表示与目标文本嵌入对齐，在零样本分类中具有强大的能力。然而，在密集的预测任务中，CLIP经常难以定位图像中的视觉特征，并且无法给出准确的像素级预测，这使其无法作为通用的视觉基础模型发挥作用。在这项工作中，我们的目标是通过对其预训练模型的最小修改来增强CLIP的语义分割潜力。通过重新思考自我注意，我们惊讶地发现，CLIP可以通过简单地引入一种新的相关自我注意（CSA）机制来适应密集的预测任务。具体来说，我们用CSA模块取代了CLIP视觉编码器最后一层的传统自注意块，并重用其预先训练的查询、关键字和值的投影矩阵，从而为CLIP的零样本语义分割提供了一种无训练适配方法。广泛的实验表明了CSA的优势：我们在本文强调的八个语义分割基准中获得了38.2%的平均零样本mIoU，显著优于现有的SoTA的33.9%和普通CLIP的14.1%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01597v1" target="_blank">2312.01597v1</a>
                              </td>
                              <td>SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference</td>
                              <td>Feng Wang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01597v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01597v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01564v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01564v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01564v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01564v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01564v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>输入文本提示的选择对CLIP等视觉语言预训练（VLP）模型的性能起着至关重要的作用。我们介绍了APoLLo，这是一种统一的多模式方法，结合了视觉语言模型的适配器和提示学习。我们的方法旨在大幅提高VLP模型的泛化能力，当它们在几个镜头设置中进行微调时。我们将可训练的基于交叉注意力的适配器层与视觉和语言编码器结合起来，以加强两种模式之间的一致性。我们加强各个编码器分支（接收增强输入）之间的一致性，以防止下游任务中的过拟合。我们的方法在三个具有代表性的任务上进行了评估：对新类的泛化、跨数据集评估和看不见的领域转移。在实践中，APoLLo在10个不同图像识别数据集的新类上实现了比MaPLe（SOTA）高达6.03%的相对增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01564v1" target="_blank">2312.01564v1</a>
                              </td>
                              <td>APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</td>
                              <td>Sanjoy Chowdhury</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01564v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01564v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_04337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-View Unsupervised Image Generation with Cross Attention Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing interest in novel view synthesis, driven by Neural Radiance Field (NeRF) models, is hindered by scalability issues due to their reliance on precisely annotated multi-view images. Recent models address this by fine-tuning large text2image diffusion models on synthetic multi-view data. Despite robust zero-shot generalization, they may need post-processing and can face quality issues due to the synthetic-real domain gap. This paper introduces a novel pipeline for unsupervised training of a pose-conditioned diffusion model on single-category datasets. With the help of pretrained self-supervised Vision Transformers (DINOv2), we identify object poses by clustering the dataset through comparing visibility and locations of specific object parts. The pose-conditioned diffusion model, trained on pose labels, and equipped with cross-frame attention at inference time ensures cross-view consistency, that is further aided by our novel hard-attention guidance. Our model, MIRAGE, surpasses prior work in novel view synthesis on real images. Furthermore, MIRAGE is robust to diverse textures and geometries, as demonstrated with our experiments on synthetic images generated with pretrained Stable Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRF）模型的驱动下，人们对新视图合成越来越感兴趣，但由于它们依赖于精确注释的多视图图像，因此受到可扩展性问题的阻碍。最近的模型通过在合成多视图数据上微调大文本2图像扩散模型来解决这一问题。尽管有强大的零样本泛化，但它们可能需要后处理，并可能由于合成域间隙而面临质量问题。本文介绍了一种新的流水线，用于在单类别数据集上对姿态条件扩散模型进行无监督训练。在预训练的自监督视觉变换器（DINOv2）的帮助下，我们通过比较特定对象部分的可见性和位置来对数据集进行聚类，从而识别对象姿态。在姿势标签上训练并在推理时配备跨帧注意力的姿势条件扩散模型确保了跨视图的一致性，这进一步得益于我们新颖的硬注意力引导。我们的模型，MIRAGE，在真实图像的新颖视图合成方面超越了以往的工作。此外，正如我们在使用预训练的稳定扩散生成的合成图像上的实验所证明的那样，MIRAGE对不同的纹理和几何形状是鲁棒的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04337v1" target="_blank">2312.04337v1</a>
                              </td>
                              <td>Multi-View Unsupervised Image Generation with Cross Attention Guidance</td>
                              <td>Llukman Cerkezi</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别优于DINO和OpenCLIP 19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v2" target="_blank">2306.03881v2</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01677v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-task Image Restoration Guided By Robust DINO Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01677v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task image restoration has gained significant interest due to its inherent versatility and efficiency compared to its single-task counterpart. Despite its potential, performance degradation is observed with an increase in the number of tasks, primarily attributed to the distinct nature of each restoration task. Addressing this challenge, we introduce \mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approach leveraging robust features extracted from DINOv2. Our empirical analysis shows that while shallow features of DINOv2 capture rich low-level image characteristics, the deep features ensure a robust semantic representation insensitive to degradations while preserving high-frequency contour details. Building on these features, we devise specialized components, including multi-layer semantic fusion module, DINO-Restore adaption and fusion module, and DINO perception contrastive loss, to integrate DINOv2 features into the restoration paradigm. Equipped with the aforementioned components, our DINO-IR performs favorably against existing multi-task image restoration approaches in various tasks by a large margin, indicating the superiority and necessity of reinforcing the robust features for multi-task image restoration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01677v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与单任务图像恢复相比，多任务图像恢复由于其固有的多功能性和效率而引起了人们的极大兴趣。尽管有其潜力，但随着任务数量的增加，性能会下降，这主要归因于每个恢复任务的不同性质。为了应对这一挑战，我们引入了\box｛\textbf｛DINO-IR｝｝，这是一种利用从DINOv2中提取的鲁棒特征的新的多任务图像恢复方法。我们的实证分析表明，虽然DINOv2的浅层特征捕捉到了丰富的低层图像特征，但深层特征确保了对退化不敏感的鲁棒语义表示，同时保留了高频轮廓细节。基于这些特征，我们设计了专门的组件，包括多层语义融合模块、DINO恢复适应和融合模块以及DINO感知对比损失，以将DINOv2特征整合到恢复范式中。配备了上述组件，我们的DINO-IR在各种任务中与现有的多任务图像恢复方法相比表现良好，这表明了增强多任务图像修复的鲁棒性特征的优越性和必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01677v2" target="_blank">2312.01677v2</a>
                              </td>
                              <td>Multi-task Image Restoration Guided By Robust DINO Features</td>
                              <td>Xin Lin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01677v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01677v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00230v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00230v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00230v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉位置识别（VPR）技术来确定公开可用图像的地理位置是现实世界VPR应用的一个紧迫问题。尽管目前大多数VPR方法在理想条件下都能获得良好的结果，但它们在复杂环境中的性能通常不令人满意，这些环境的特点是光照变化、季节变化和移动物体引起的遮挡。在这项研究中，我们利用DINOv2模型作为骨干网络进行修剪和微调，以提取稳健的图像特征。我们提出了一种新的VPR架构，称为DINO Mix，它将基础视觉模型与特征聚合相结合。该架构依赖于基础视觉模型强大的图像特征提取能力。我们使用基于MLP Mixer的混合模块来聚合图像特征，从而产生全局鲁棒和可推广的描述符，从而实现高精度的VPR。我们通过实验证明，所提出的DINO-Mix架构显著优于当前最先进的（SOTA）方法。在具有光照变化、季节变化和遮挡的测试集（Tokyo24/7，Nordland，SF-XL-Testv1）中，我们提出的DINO Mix架构分别实现了91.75%、80.18%和82%的Top-1准确率。与SOTA方法相比，我们的体系结构的平均准确度提高了5.14%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00230v2" target="_blank">2311.00230v2</a>
                              </td>
                              <td>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</td>
                              <td>Gaoshuang Huang</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00230v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00230v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/GaoShuang98/DINO-Mix" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02366v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02366v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02366v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02366v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of deep learning systems into the medical domain has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2, an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images, excels in extracting general-purpose visual representations, exhibiting promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and the clarity on whether its features are sufficiently general to benefit radiology image analysis is yet to be established. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). Tasks include disease classification and organ segmentation on both 2D and 3D images, evaluated under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the effectiveness and generalizability of the DINOv2 feature embeddings. Comparative analyses with established medical image analysis models, U-Net and TransUnet for segmentation, and CNN and ViT models pre-trained via supervised, weakly supervised, and self-supervised learning for classification, reveal DINOv2's superior performance in segmentation tasks and competitive results in disease classification. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02366v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据注释的资源密集型过程以及这些系统无法推广到不同的数据分布，阻碍了深度学习系统与医学领域的集成。基础模型是在大型数据集上预先训练的模型，已成为减少对注释数据的依赖并增强模型可推广性和稳健性的解决方案。DINOv2是一个开源的基础模型，在1.42亿张策划的自然图像上经过自我监督学习预训练，擅长提取通用视觉表示，在各种视觉任务中表现出有希望的能力。然而，关于DINOv2对放射学成像的适应性，一个关键问题仍未得到解答，其特征是否足够通用以有利于放射学图像分析的明确性尚待确定。因此，本研究全面评估了DINOv2的放射学，在不同的模式（X射线、CT和MRI）下进行了100多项实验。任务包括2D和3D图像上的疾病分类和器官分割，在不同的设置下进行评估，如kNN、少镜头学习、线性探测、端到端微调和参数有效微调，以测量DINOv2特征嵌入的有效性和可推广性。与已建立的医学图像分析模型U-Net和TransUnet进行分割，以及通过监督、弱监督和自监督学习预训练的CNN和ViT模型进行分类的比较分析，揭示了DINOv2在分割任务中的优越性能和在疾病分类中的竞争结果。这些发现有助于深入了解优化医学成像预训练策略的潜在途径，并增进对DINOv2在弥合自然图像分析和放射学图像分析之间差距方面的作用的更广泛理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02366v1" target="_blank">2312.02366v1</a>
                              </td>
                              <td>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</td>
                              <td>Mohammed Baharoon</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02366v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02366v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mohammedsb/dinov2formedical" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the groundwork for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练的模型性能略有提高，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了小幅的性能提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，并可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v2" target="_blank">2310.03513v2</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO-ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和看不见的数据之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v2" target="_blank">2310.02048v2</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v3" target="_blank">2310.03940v3</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18809v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FoundPose: Unseen Object Pose Estimation with Foundation Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18809v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose FoundPose, a method for 6D pose estimation of unseen rigid objects from a single RGB image. The method assumes that 3D models of the objects are available but does not require any object-specific training. This is achieved by building upon DINOv2, a recent vision foundation model with impressive generalization capabilities. An online pose estimation stage is supported by a minimal object representation that is built during a short onboarding stage from DINOv2 patch features extracted from rendered object templates. Given a query image with an object segmentation mask, FoundPose first rapidly retrieves a handful of similarly looking templates by a DINOv2-based bag-of-words approach. Pose hypotheses are then generated from 2D-3D correspondences established by matching DINOv2 patch features between the query image and a retrieved template, and finally optimized by featuremetric refinement. The method can handle diverse objects, including challenging ones with symmetries and without any texture, and noticeably outperforms existing RGB methods for coarse pose estimation in both accuracy and speed on the standard BOP benchmark. With the featuremetric and additional MegaPose refinement, which are demonstrated complementary, the method outperforms all RGB competitors. Source code is at: evinpinar.github.io/foundpose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18809v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了FoundPose，这是一种从单个RGB图像中对看不见的刚性物体进行6D姿态估计的方法。该方法假设对象的3D模型是可用的，但不需要任何特定于对象的训练。这是在DINOv2的基础上实现的，DINOv2是一个最近的视觉基础模型，具有令人印象深刻的泛化能力。在线姿态估计阶段由最小对象表示支持，该最小对象表示是在短期入职阶段根据从渲染对象模板中提取的DINOv2补丁特征构建的。给定一个带有对象分割掩码的查询图像，FoundPose首先通过基于DINOv2的单词袋方法快速检索一些外观相似的模板。然后，从通过匹配查询图像和检索到的模板之间的DINOv2补丁特征而建立的2D-3D对应关系中生成姿势假设，并最终通过特征度量细化进行优化。该方法可以处理不同的对象，包括具有对称性且没有任何纹理的具有挑战性的对象，并且在标准BOP基准上，在精度和速度方面明显优于现有的RGB方法进行粗略姿态估计。该方法的特征度量和额外的MegaPose细化被证明是互补的，优于所有RGB竞争对手。源代码位于：evinpinar.github.io/foundpose。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18809v1" target="_blank">2311.18809v1</a>
                              </td>
                              <td>FoundPose: Unseen Object Pose Estimation with Foundation Features</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18809v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18809v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00079v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00079v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores advancements in high-fidelity personalized image generation through the utilization of pre-trained text-to-image diffusion models. While previous approaches have made significant strides in generating versatile scenes based on text descriptions and a few input images, challenges persist in maintaining the subject fidelity within the generated images. In this work, we introduce an innovative algorithm named HiFi Tuner to enhance the appearance preservation of objects during personalized image generation. Our proposed method employs a parameter-efficient fine-tuning framework, comprising a denoising process and a pivotal inversion process. Key enhancements include the utilization of mask guidance, a novel parameter regularization technique, and the incorporation of step-wise subject representations to elevate the sample fidelity. Additionally, we propose a reference-guided generation approach that leverages the pivotal inversion of a reference image to mitigate unwanted subject variations and artifacts. We further extend our method to a novel image editing task: substituting the subject in an image through textual manipulations. Experimental evaluations conducted on the DreamBooth dataset using the Stable Diffusion model showcase promising results. Fine-tuning solely on textual embeddings improves CLIP-T score by 3.6 points and improves DINO score by 9.6 points over Textual Inversion. When fine-tuning all parameters, HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2 points over DreamBooth, establishing a new state of the art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00079v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文通过利用预先训练的文本到图像扩散模型，探索了高保真个性化图像生成的进展。虽然以前的方法在基于文本描述和一些输入图像生成多功能场景方面取得了重大进展，但在保持生成图像中的主题保真度方面仍然存在挑战。在这项工作中，我们引入了一种名为HiFi Tuner的创新算法，以在个性化图像生成过程中增强对象的外观保护。我们提出的方法采用了一个参数有效的微调框架，包括去噪过程和关键反演过程。关键的增强包括利用掩模引导、一种新的参数正则化技术，以及结合逐步主题表示来提高样本保真度。此外，我们提出了一种参考引导生成方法，该方法利用参考图像的关键反转来减轻不必要的受试者变化和伪影。我们进一步将我们的方法扩展到一个新颖的图像编辑任务：通过文本操作替换图像中的主体。使用稳定扩散模型在DreamBooth数据集上进行的实验评估显示了有希望的结果。与文本反转相比，仅对文本嵌入进行微调可将CLIP-T分数提高3.6分，将DINO分数提高9.6分。当对所有参数进行微调时，HiFi Tuner将CLIP-T分数提高了1.2分，并将DINO分数提高了比DreamBooth高1.2分，建立了新的技术水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00079v1" target="_blank">2312.00079v1</a>
                              </td>
                              <td>HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</td>
                              <td>Zhonghao Wang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00079v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00079v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17893v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17893v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective approach for self-supervised video object segmentation (VOS). Our key insight is that the inherent structural dependencies present in DINO-pretrained Transformers can be leveraged to establish robust spatio-temporal correspondences in videos. Furthermore, simple clustering on this correspondence cue is sufficient to yield competitive segmentation results. Previous self-supervised VOS techniques majorly resort to auxiliary modalities or utilize iterative slot attention to assist in object discovery, which restricts their general applicability and imposes higher computational requirements. To deal with these challenges, we develop a simplified architecture that capitalizes on the emerging objectness from DINO-pretrained Transformers, bypassing the need for additional modalities or slot attention. Specifically, we first introduce a single spatio-temporal Transformer block to process the frame-wise DINO features and establish spatio-temporal dependencies in the form of self-attention. Subsequently, utilizing these attention maps, we implement hierarchical clustering to generate object segmentation masks. To train the spatio-temporal block in a fully self-supervised manner, we employ semantic and dynamic motion consistency coupled with entropy normalization. Our method demonstrates state-of-the-art performance across multiple unsupervised VOS benchmarks and particularly excels in complex real-world multi-object video segmentation tasks such as DAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will be released at https://github.com/shvdiwnkozbw/SSL-UVOS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17893v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种简单而有效的自监督视频对象分割（VOS）方法。我们的关键见解是，DINO预训练的变形金刚中存在的固有结构依赖性可以用来在视频中建立强大的时空对应关系。此外，在该对应线索上的简单聚类足以产生有竞争力的分割结果。以前的自监督VOS技术主要采用辅助模态或利用迭代槽注意力来辅助对象发现，这限制了它们的普遍适用性，并提出了更高的计算要求。为了应对这些挑战，我们开发了一种简化的架构，该架构利用了DINO预训练变压器中出现的对象性，绕过了对额外模式或插槽关注的需求。具体来说，我们首先引入单个时空变换器块来处理逐帧的DINO特征，并以自注意的形式建立时空依赖关系。随后，利用这些注意力图，我们实现了分层聚类来生成对象分割掩码。为了以完全自监督的方式训练时空块，我们使用语义和动态运动一致性以及熵归一化。我们的方法在多个无监督VOS基准测试中展示了最先进的性能，尤其擅长复杂的真实世界多对象视频分割任务，如DAVIS-17-无监督和YouTube-VIS-19。代码和模型检查点将在发布https://github.com/shvdiwnkozbw/ssl-uvos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17893v1" target="_blank">2311.17893v1</a>
                              </td>
                              <td>Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</td>
                              <td>Shuangrui Ding</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17893v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17893v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/shvdiwnkozbw/ssl-uvos" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上与SOTA表示相似。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果令人惊讶地好，并且在这些融合特征上使用最近邻居的零样本评估在基准数据集（例如，SPair-71k、PF-Pascal和TSS）上提供了比现有技术方法显著的性能增益。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v2" target="_blank">2305.15347v2</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15937v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimal Transport Aggregation for Visual Place Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15937v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a 'dustbin' cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15937v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉位置识别（VPR）的任务旨在仅依靠视觉提示，将查询图像与来自不同位置的图像的广泛数据库中的参考进行匹配。最先进的管道专注于从深层主干提取的特征的聚合，以便为每个图像形成全局描述符。在这种情况下，我们引入了SALAD（局部聚合描述符的Sinkhorn算法），它将NetVLAD的局部特征到簇的软分配重新表述为最优传输问题。在SALAD中，我们考虑了特征到聚类和聚类到特征的关系，我们还引入了一个“垃圾箱”聚类，旨在选择性地丢弃被视为非信息性的特征，提高整体描述符质量。此外，我们利用并微调DINOv2作为主干，它为局部特征提供了增强的描述能力，并显著减少了所需的训练时间。因此，我们的单阶段方法不仅超过了公共VPR数据集中的单阶段基线，而且还超过了以显著更高的成本添加重新排序的两阶段方法。代码和型号可在https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15937v1" target="_blank">2311.15937v1</a>
                              </td>
                              <td>Optimal Transport Aggregation for Visual Place Recognition</td>
                              <td>Sergio Izquierdo</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15937v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/serizba/salad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14665v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14665v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) can be used to solve complex visual tasks without human labels. Self-supervised representations encode useful semantic information about images, and as a result, they have already been used for tasks such as unsupervised semantic segmentation. In this paper, we investigate self-supervised representations for instance segmentation without any manual annotations. We find that the features of different SSL methods vary in their level of instance-awareness. In particular, DINO features, which are known to be excellent semantic descriptors, lack behind MAE features in their sensitivity for separating instances.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14665v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）可以用于解决没有人为标签的复杂视觉任务。自监督表示对图像的有用语义信息进行编码，因此，它们已经被用于无监督语义分割等任务。在本文中，我们研究了在没有任何手动注释的情况下进行实例分割的自监督表示。我们发现，不同SSL方法的特性在实例感知级别上有所不同。特别是，众所周知，DINO特征是优秀的语义描述符，但在分离实例的敏感性方面却落后于MAE特征。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14665v1" target="_blank">2311.14665v1</a>
                              </td>
                              <td>Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</td>
                              <td>Paul Engstler</td>
                              <td>2023-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14665v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14665v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13110v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13110v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13110v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们认为表示学习的一个自然目标是将数据的分布（比如令牌集）压缩和转换为非相干子空间上支持的低维高斯混合。这种表示的优度可以通过一种称为稀疏率降低的原则性度量来评估，该度量同时最大化学习表示的内在信息增益和外在稀疏性。从这个角度来看，包括transformer在内的流行的深度网络架构可以被视为实现了优化这一措施的迭代方案。特别地，我们从该目标部分的交替优化中推导出一个变换器块：多头自注意算子通过对特征的编码率实现近似梯度下降步骤来压缩表示，随后的多层感知器对特征进行稀疏化。这导致了一系列类似白盒变压器的深度网络架构，称为CRATE，在数学上是完全可解释的。我们通过去噪和压缩之间的新连接表明，上述压缩编码的逆编码可以通过同一类CRATE架构来实现。因此，如此导出的白盒架构对于编码器和解码器都是通用的。实验表明，尽管这些网络很简单，但它们确实学会了压缩和稀疏大规模真实世界图像和文本数据集的表示，并实现了非常接近高度工程化的基于转换器的模型的性能：ViT、MAE、DINO、BERT和GPT2。我们认为，从数据压缩的统一角度来看，所提出的计算框架在弥合深度学习理论和实践之间的差距方面显示出巨大的潜力。代码位于：https://ma-lab-berkeley.github.io/crate。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13110v2" target="_blank">2311.13110v2</a>
                              </td>
                              <td>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</td>
                              <td>Yaodong Yu</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13110v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13110v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13717v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13717v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr\'echet Inception Distance is a widely used metric for evaluating synthetic image quality that utilizes an ImageNet-trained InceptionV3 network as a feature extractor. However, its application in medical imaging lacks a standard feature extractor, leading to biased and inconsistent comparisons. This study aimed to compare state-of-the-art feature extractors for computing Fr\'echet Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data augmentation techniques tailored for limited data domains on datasets comprising three medical imaging modalities and four anatomical locations. Human evaluation of generative quality (via a visual Turing test) was compared to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer architectures, in addition to an InceptionV3 network trained on a large medical dataset, RadImageNet. All ImageNet-based extractors were consistent with each other, but only SwAV was significantly correlated with medical expert judgment. The RadImageNet-based FD showed volatility and lacked correlation with human judgment. Caution is advised when using medical image-trained extraction networks in the FD calculation. These networks should be rigorously evaluated on the imaging modality under consideration and publicly released. ImageNet-based extractors, while imperfect, are consistent and widely understood. Training extraction networks with SwAV is a promising approach for synthetic medical image evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13717v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr’echet Inception Distance是一种广泛用于评估合成图像质量的指标，它利用ImageNet训练的InceptionV3网络作为特征提取器。然而，它在医学成像中的应用缺乏标准的特征提取器，导致了有偏差和不一致的比较。本研究旨在比较用于计算医学成像中Fr’chet距离（FD）的最先进的特征提取器。StyleGAN2网络使用针对数据集上的有限数据域量身定制的数据增强技术进行训练，数据集包括三种医学成像模式和四个解剖位置。将人类对生成质量的评估（通过视觉图灵测试）与使用ImageNet训练的InceptionV3、ResNet50、SwAV、DINO和Swin Transformer架构以及在大型医学数据集RadImageNet上训练的InceptV3网络计算的FD进行比较。所有基于ImageNet的提取器彼此一致，但只有SwAV与医学专家的判断显著相关。基于RadImageNet的FD显示出波动性，并且缺乏与人类判断的相关性。当在FD计算中使用医学图像训练的提取网络时，建议谨慎。这些网络应根据正在考虑的成像模式进行严格评估并公开发布。基于ImageNet的提取器虽然不完善，但却是一致的，并得到了广泛的理解。用SwAV训练提取网络是一种很有前途的合成医学图像评估方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13717v1" target="_blank">2311.13717v1</a>
                              </td>
                              <td>Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</td>
                              <td>McKell Woodland</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13717v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13717v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mckellwoodland/fid-med-eval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12969v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Every Thing with Few Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12969v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12969v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开集对象检测的目的是检测训练中发现的任意类别之外的任意类别。最近的进展采用了开放词汇范式，利用视觉语言主干来用语言表示类别。在本文中，我们介绍了DE ViT，这是一种开放集对象检测器，它使用仅视觉的DINOv2主干，并通过示例图像而不是语言来学习新的类别。为了提高一般检测能力，我们将多分类任务转换为二进制分类任务，同时绕过每类推理，并提出了一种新的区域传播定位技术。我们使用COCO和LVIS在开放词汇、少镜头和单镜头物体检测基准上评估了DE-ViT。对于COCO，DE ViT比开放词汇SoTA高6.9 AP50，并在新类中达到50 AP50。DE ViT在10次发射时以15毫安时的容量超过了少数发射的SoTA，在30次发射时则以7.2毫安时的体积超过了SoTA，而在一次发射时仅以2.8 AP50的容量超过SoTA。对于LVIS，DE ViT比开放词汇表SoTA高2.2掩码AP，达到34.3掩码AP。代码可在https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12969v2" target="_blank">2309.12969v2</a>
                              </td>
                              <td>Detect Every Thing with Few Examples</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12969v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12969v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mlzxy/devit" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是不计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价性。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初与语法学习有关，最近又与进一步的闭包性质和一元二阶逻辑定义有关。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并在无星假设下定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v7" target="_blank">2006.01236v7</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v7" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11754v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11754v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automotive related datasets have previously been used for training autonomous driving systems or vehicle classification tasks. However, there is a lack of datasets in the field of automotive AI for car parts detection, and most available datasets are limited in size and scope, struggling to cover diverse scenarios. To address this gap, this paper presents a large-scale and fine-grained automotive dataset consisting of 84,162 images for detecting 12 different types of car parts. This dataset was collected from natural cameras and online websites which covers various car brands, scenarios, and shooting angles. To alleviate the burden of manual annotation, we propose a novel semi-supervised auto-labeling method that leverages state-of-the-art pre-trained detectors. Moreover, we study the limitations of the Grounding DINO approach for zero-shot labeling. Finally, we evaluate the effectiveness of our proposed dataset through fine-grained car parts detection by training several lightweight YOLO-series detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11754v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>汽车相关数据集先前已用于训练自动驾驶系统或车辆分类任务。然而，用于汽车零部件检测的汽车人工智能领域缺乏数据集，大多数可用数据集的大小和范围都有限，难以覆盖各种场景。为了解决这一差距，本文提出了一个由84162张图像组成的大规模细粒度汽车数据集，用于检测12种不同类型的汽车零件。该数据集是从自然相机和在线网站收集的，涵盖了各种汽车品牌、场景和拍摄角度。为了减轻手动注释的负担，我们提出了一种新的半监督自动标记方法，该方法利用了最先进的预训练检测器。此外，我们还研究了用于零样本标记的Grounding DINO方法的局限性。最后，我们通过训练几个轻量级YOLO系列检测器，通过细粒度的汽车零件检测来评估我们提出的数据集的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11754v1" target="_blank">2311.11754v1</a>
                              </td>
                              <td>A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</td>
                              <td>Wang Jie</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11754v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12079v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FreeKD: Knowledge Distillation via Semantic Frequency Prompt</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12079v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12079v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12079v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge distillation (KD) has been applied to various tasks successfully, and mainstream methods typically boost the student model via spatial imitation losses. However, the consecutive downsamplings induced in the spatial domain of teacher model is a type of corruption, hindering the student from analyzing what specific information needs to be imitated, which results in accuracy degradation. To better understand the underlying pattern of corrupted feature maps, we shift our attention to the frequency domain. During frequency distillation, we encounter a new challenge: the low-frequency bands convey general but minimal context, while the high are more informative but also introduce noise. Not each pixel within the frequency bands contributes equally to the performance. To address the above problem: (1) We propose the Frequency Prompt plugged into the teacher model, absorbing the semantic frequency context during finetuning. (2) During the distillation period, a pixel-wise frequency mask is generated via Frequency Prompt, to localize those pixel of interests (PoIs) in various frequency bands. Additionally, we employ a position-aware relational frequency loss for dense prediction tasks, delivering a high-order spatial enhancement to the student model. We dub our Frequency Knowledge Distillation method as FreeKD, which determines the optimal localization and extent for the frequency distillation. Extensive experiments demonstrate that FreeKD not only outperforms spatial-based distillation methods consistently on dense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on COCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys more robustness to the student. Notably, we also validate the generalization of our approach on large-scale vision models (e.g., DINO and SAM).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12079v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>知识提取（KD）已成功应用于各种任务，主流方法通常通过空间模仿损失来提升学生模型。然而，在教师模型的空间域中引发的连续下采样是一种腐败，阻碍了学生分析需要模仿的特定信息，从而导致准确性下降。为了更好地理解损坏特征图的潜在模式，我们将注意力转移到频域。在频率提取过程中，我们遇到了一个新的挑战：低频带传达了一般但最小的上下文，而高频带信息量更大，但也引入了噪声。并非频带内的每个像素对性能的贡献都相等。为了解决上述问题：（1）我们提出了插入教师模型的频率提示，在微调过程中吸收语义频率上下文。（2） 在提取期间，通过频率提示生成逐像素的频率掩码，以在各个频带中定位那些感兴趣的像素（PoI）。此外，我们在密集预测任务中使用了位置感知的关系频率损失，为学生模型提供了高阶空间增强。我们将我们的频率知识提取方法称为FreeKD，它确定了频率提取的最佳定位和范围。大量实验表明，FreeKD不仅在密集的预测任务上始终优于基于空间的蒸馏方法（例如，FreeKD在COCO2017上为RepPoints-R50带来了3.8的AP增益，在Cityscapes上为PSPNet-R18带来了4.55 mIoU增益），而且还向学生传达了更大的鲁棒性。值得注意的是，我们还验证了我们的方法在大规模视觉模型（例如，DINO和SAM）上的通用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12079v1" target="_blank">2311.12079v1</a>
                              </td>
                              <td>FreeKD: Knowledge Distillation via Semantic Frequency Prompt</td>
                              <td>Yuan Zhang</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12079v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12079v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11125v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11125v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11125v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11125v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11125v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类别级物体姿态估计旨在从已知类别中预测物体的6D姿态和3D尺寸，通常难以应对大的类内形状变化。利用平均形状的现有作品往往无法捕捉到这种变化。为了解决这个问题，我们提出了SecondPose，这是一种将DINOv2中特定于对象的几何特征与语义类别先验相结合的新方法。利用DINOv2在提供SE（3）一致语义特征方面的优势，我们分层提取了两种类型的SE（3（3）不变几何特征，以进一步封装局部到全局的特定对象信息。然后，这些几何特征与DINOv2特征点对齐，以在SE（3）变换下建立一致的对象表示，促进从相机空间到预定义规范空间的映射，从而进一步增强姿态估计。在NOCS-REAL275上进行的大量实验表明，SecondPose比最先进的技术进步了12.4%。此外，在更复杂的数据集HouseCat6D上，SecondPose仍然以很大的优势超过了其他竞争对手，该数据集提供了具有光度挑战性的物体。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11125v1" target="_blank">2311.11125v1</a>
                              </td>
                              <td>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</td>
                              <td>Yamei Chen</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11125v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09770v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09770v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09770v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09770v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent progress in self-supervised representation learning has opened up new opportunities for training from unlabeled data and has been a growing trend in voice conversion. However, unsupervised training of voice cloning seems to remain a challenging task. In this paper we propose a semi-supervised zero-shot voice cloning approach that works by adapting a HuBERT-based voice conversion system to the voice cloning task and shows the robustness of such a system to noises both in training data (we add noises resulting in up to 0db signal-to-noise-ratio to 35% of training data with no significant degradation of evaluation metrics) and in the target speaker reference audio at inference. Moreover, such a method does not require any type of denoising or noise-labeling of training data. Finally, we introduce a novel multi-tasking approach by incorporating self-supervised DINO loss into joint training of a CAM++ based speaker verification system and a unit-based VITS cloning system. We show that it significantly improves the quality of generated audio over baselines, especially for noisy target speaker references.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09770v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督表示学习的最新进展为从未标记数据中进行训练开辟了新的机会，并成为语音转换的一个日益增长的趋势。然而，语音克隆的无监督训练似乎仍然是一项具有挑战性的任务。在本文中，我们提出了一种半监督的零样本语音克隆方法，该方法通过使基于HuBERT的语音转换系统适应语音克隆任务来工作，并显示了这种系统对训练数据中的噪声（我们将导致高达0db信号噪声比的噪声添加到35%的训练数据中，而评估指标没有显著退化）和目标中的噪声的鲁棒性说话者在推理时参考音频。此外，这种方法不需要对训练数据进行任何类型的去噪或噪声标记。最后，我们介绍了一种新的多任务方法，将自我监督的DINO损失纳入基于CAM++的说话人验证系统和基于单元的VITS克隆系统的联合训练中。我们发现，它显著提高了生成的音频质量，特别是对于有噪声的目标扬声器参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09770v2" target="_blank">2311.09770v2</a>
                              </td>
                              <td>DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss</td>
                              <td>Vikentii Pankov</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09770v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09770v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10125v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10125v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10125v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10125v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the current landscape of artificial intelligence, foundation models serve as the bedrock for advancements in both language and vision domains. OpenAI GPT-4 has emerged as the pinnacle in large language models (LLMs), while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models such as Meta's SAM and DINO, and YOLOS. However, the financial and computational burdens of training new models from scratch remain a significant barrier to progress. In response to this challenge, we introduce UnifiedVisionGPT, a novel framework designed to consolidate and automate the integration of SOTA vision models, thereby facilitating the development of vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key features: (1) provides a versatile multimodal framework adaptable to a wide range of applications, building upon the strengths of multimodal foundation models; (2) seamlessly integrates various SOTA vision models to create a comprehensive multimodal platform, capitalizing on the best components of each model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in the CV domain compared to the current trajectory of LLMs; and (4) introduces automation in the selection of SOTA vision models, generating optimal results based on diverse multimodal inputs such as text prompts and images. This paper outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, generalization, and performance. Our implementation, along with the unified multimodal framework and comprehensive dataset, is made publicly available at https://github.com/LHBuilder/SA-Segment-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10125v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在当前的人工智能领域，基础模型是语言和视觉领域进步的基石。OpenAI GPT-4已成为大型语言模型（LLM）的巅峰，而计算机视觉（CV）领域拥有大量最先进的（SOTA）模型，如Meta的SAM、DINO和YOLOS。然而，从头开始训练新模型的财务和计算负担仍然是取得进展的重大障碍。为了应对这一挑战，我们引入了UnifiedVisionGPT，这是一种新颖的框架，旨在整合和自动化SOTA视觉模型的集成，从而促进面向视觉的人工智能的发展。UnifiedVision通过四个关键功能脱颖而出：（1）提供了一个适用于广泛应用的多模式框架，利用多模态基础模型的优势；（2） 无缝集成各种SOTA愿景模型，利用每个模型的最佳组件，创建一个全面的多模式平台；（3） 优先考虑面向视觉的人工智能，确保与LLM的当前轨迹相比，CV领域的进展更快；以及（4）在SOTA视觉模型的选择中引入自动化，基于不同的多模式输入（如文本提示和图像）生成最佳结果。本文概述了UnifiedVisionGPT的体系结构和功能，展示了其通过提高效率、多功能性、通用性和性能来彻底改变计算机视觉领域的潜力。我们的实现，以及统一的多模式框架和全面的数据集，在https://github.com/lhbuilder/sa-segment-anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10125v1" target="_blank">2311.10125v1</a>
                              </td>
                              <td>UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</td>
                              <td>Chris Kelly</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10125v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lhbuilder/sa-segment-anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09350v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Imitation Learning Through Pre-Trained Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09350v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09350v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09350v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09350v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们利用自监督视觉转换器模型及其涌现的语义能力来提高模仿学习策略的泛化能力。我们介绍了BC-ViT，这是一种模仿学习算法，它利用丰富的DINO预训练的Visual Transformer（ViT）补丁级嵌入，在通过演示进行学习时获得更好的泛化能力。我们的学习者通过将外观特征聚类为语义概念来看待世界，形成稳定的关键点，这些关键点在广泛的外观变化和对象类型中进行概括。我们展示了这种表示通过评估对象操作任务的不同数据集的模仿学习来实现广义行为。我们提供的方法、数据和评估方法有助于进一步研究模仿学习者的泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09350v1" target="_blank">2311.09350v1</a>
                              </td>
                              <td>Generalizable Imitation Learning Through Pre-Trained Representations</td>
                              <td>Wei-Di Chang</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09350v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09350v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09118v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WildlifeDatasets: An open-source toolkit for animal re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09118v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09118v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09118v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09118v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了WildlifeDataset(https://github.com/wildlifedatasets/wildlife-datasets)-主要面向生态学家和计算机视觉/机器学习研究人员的开源工具包。WildlifeDataset是用Python编写的，允许直接访问公开可用的野生动物数据集，并为数据集预处理、性能分析和模型微调提供了多种方法。我们在各种场景和基线实验中展示了该工具包，据我们所知，包括对野生动物重新识别的数据集和方法进行最全面的实验比较，包括局部描述符和深度学习方法。此外，我们提供了第一个用于广泛物种内个体重新识别的基础模型MegaDescriptor，该模型在动物重新识别数据集上提供了最先进的性能，并显著优于其他预先训练的模型，如CLIP和DINOv2。为了向公众提供该模型，并允许与任何现有的野生动物监测应用程序轻松集成，我们通过HuggingFace中心提供多种MegaDescriptor风格（即小型、中型和大型）(https://huggingface.co/bvra)。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09118v1" target="_blank">2311.09118v1</a>
                              </td>
                              <td>WildlifeDatasets: An open-source toolkit for animal re-identification</td>
                              <td>Vojtěch Čermák</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09118v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09118v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/WildlifeDatasets/wildlife-datasets" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wildlifedatasets/wildlife-datasets" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19522v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Natural Domain Foundation Models Useful for Medical Image Classification?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19522v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19522v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19522v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19522v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习领域正朝着使用通用基础模型的方向发展，这些模型可以很容易地适应不同的任务。虽然这种范式转变已经成为自然语言处理领域的普遍做法，但计算机视觉的进展却较慢。在本文中，我们试图通过研究各种最先进的基础模型对医学图像分类任务的可转移性来解决这个问题。具体而言，我们评估了五个基础模型的性能，即SAM、SEEM、DINOv2、BLIP和OpenCLIP，它们跨越了四个成熟的医学成像数据集。我们探索了不同的训练环境，以充分利用这些模型的潜力。我们的研究结果喜忧参半。DINOv2始终优于ImageNet预训练的标准实践。然而，其他基础模型未能始终超过这一既定基线，这表明它们在医学图像分类任务中的可转移性存在局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19522v2" target="_blank">2310.19522v2</a>
                              </td>
                              <td>Are Natural Domain Foundation Models Useful for Medical Image Classification?</td>
                              <td>Joana Palés Huix</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19522v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19522v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04010v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04010v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04010v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04010v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Due to scarcity of anomaly situations in the early manufacturing stage, an unsupervised anomaly detection (UAD) approach is widely adopted which only uses normal samples for training. This approach is based on the assumption that the trained UAD model will accurately reconstruct normal patterns but struggles with unseen anomalous patterns. To enhance the UAD performance, reconstruction-by-inpainting based methods have recently been investigated, especially on the masking strategy of suspected defective regions. However, there are still issues to overcome: 1) time-consuming inference due to multiple masking, 2) output inconsistency by random masking strategy, and 3) inaccurate reconstruction of normal patterns when the masked area is large. Motivated by this, we propose a novel reconstruction-by-inpainting method, dubbed Excision And Recovery (EAR), that features single deterministic masking based on the ImageNet pre-trained DINO-ViT and visual obfuscation for hint-providing. Experimental results on the MVTec AD dataset show that deterministic masking by pre-trained attention effectively cuts out suspected defective regions and resolve the aforementioned issues 1 and 2. Also, hint-providing by mosaicing proves to enhance the UAD performance than emptying those regions by binary masking, thereby overcomes issue 3. Our approach achieves a high UAD performance without any change of the neural network structure. Thus, we suggest that EAR be adopted in various manufacturing industries as a practically deployable solution.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04010v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于制造早期异常情况的稀缺性，一种仅使用正态样本进行训练的无监督异常检测（UAD）方法被广泛采用。这种方法基于这样的假设，即经过训练的无人机模型将准确地重建正常模式，但要与看不见的异常模式作斗争。为了提高UAD的性能，最近研究了通过基于修复的方法进行重建，特别是在可疑缺陷区域的掩蔽策略上。然而，仍有一些问题需要克服：1）由于多次掩蔽而导致的耗时推理，2）通过随机掩蔽策略的输出不一致，以及3）当掩蔽区域较大时，对正常模式的重建不准确。受此启发，我们提出了一种新的修复重建方法，称为切除和恢复（EAR），其特征是基于ImageNet预训练的DINO ViT的单一确定性掩蔽和用于提示提供的视觉模糊。在MVTec AD数据集上的实验结果表明，通过预先训练的注意力进行的确定性掩蔽有效地去除了可疑的缺陷区域，并解决了上述问题1和2。此外，通过镶嵌提供的提示被证明比通过二进制掩码清空那些区域增强了UAD性能，从而克服了问题3。我们的方法在不改变神经网络结构的情况下实现了高UAD性能。因此，我们建议在各种制造业中采用EAR作为一种可实际部署的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04010v2" target="_blank">2310.04010v2</a>
                              </td>
                              <td>Excision And Recovery: Visual Defect Obfuscation Based Self-Supervised Anomaly Detection Strategy</td>
                              <td>YeongHyeon Park</td>
                              <td>2023-10-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04010v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04010v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03570v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cal-DETR: Calibrated Detection Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03570v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03570v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03570v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管深度神经网络在一些计算机视觉任务中表现出令人印象深刻的预测性能，但它容易做出过于自信的预测。这限制了DNN在许多安全关键应用中的采用和更广泛的利用。然而，最近一直在努力校准DNN，几乎所有的努力都集中在分类任务上。令人惊讶的是，很少有人关注校准现代基于DNN的目标检测器，尤其是检测变压器，它们最近表现出了很好的检测性能，并在许多决策系统中具有影响力。在这项工作中，我们通过提出一种校准检测变压器（Cal-DETR）的机制来解决这个问题，特别是对于可变形DETR、UP-DETR和DINO。我们追求列车时间校准路线，并做出以下贡献。首先，我们提出了一种简单而有效的方法来量化基于变换器的物体检测器的不确定性。其次，我们开发了一种不确定性引导的logit调制机制，该机制利用不确定性来调制类logit。第三，我们开发了一种logit混合方法，该方法作为具有检测特定损耗的正则化子，也是对不确定性引导的logit调制技术的补充，以进一步提高校准性能。最后，我们在三个域内和四个域外场景中进行了广泛的实验。结果证实了Cal DETR在校准域内和域外检测方面的有效性，同时保持甚至提高了检测性能。我们的代码库和预训练模型可以访问\url{https://github.com/akhtarvision/cal-detr}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03570v1" target="_blank">2311.03570v1</a>
                              </td>
                              <td>Cal-DETR: Calibrated Detection Transformer</td>
                              <td>Muhammad Akhtar Munir</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03570v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03570v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/akhtarvision/cal-detr" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg,MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation.Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV17 object detection on UAVDT, and video instance segmentation on DAVIS 2017.We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https://github.com/ISL-CV/FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集的预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，称为特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值转移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为骨干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，以及40.8（+2.3）%AP、42.1%AP的实例分割。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的UAV17对象检测和DAVIS 2017上的视频实例分割。我们通过展示可视化和各种消融研究来更好地了解FLSL的成功。源代码位于https://github.com/isl-cv/flsl.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v4" target="_blank">2306.06203v4</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_03053v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masking Hyperspectral Imaging Data with Pretrained Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_03053v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_03053v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_03053v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与潜在噪声和未知光谱特性相关联的不期望的背景区域的存在降低了高光谱数据处理的性能。掩盖不需要的区域是解决这一问题的关键。仅处理感兴趣的区域在计算成本、所需内存和总体性能方面产生了显著的改进。所提出的处理管道包括两个基本部分：感兴趣区域掩模生成，然后仅在新掩模的高光谱立方体上应用高光谱数据处理技术。我们工作的新颖之处在于用于初步图像分割的方法。我们使用Segment Anything Model（SAM）来提取数据集中的所有对象，然后使用零样本Grounding Dino对象检测器来细化片段，然后执行交集和排除过滤步骤，而无需进行微调或重新训练。为了说明掩蔽过程的有效性，将所提出的方法部署在三个需要精确掩蔽的具有挑战性的应用场景中；碎塑料表征、岩芯扫描和垃圾监测。提供了所提出的掩蔽方法对三个应用的数值评估以及所使用的超参数。该方法的脚本将在https://github.com/hifexplo/masking.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.03053v1" target="_blank">2311.03053v1</a>
                              </td>
                              <td>Masking Hyperspectral Imaging Data with Pretrained Models</td>
                              <td>Elias Arbash</td>
                              <td>2023-11-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_03053v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.03053v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hifexplo/masking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01584v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01584v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01584v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tax incentives and fiscal bonuses have had a significant impact on the Italian economy over the past decade. In particular, the "Superbonus 110" tax relief in 2020, offering a generous 110% deduction for expenses related to energy efficiency improvements and seismic risk reduction in buildings, has played a pivotal role. However, the surge in construction activities has also brought about an unfortunate increase in fraudulent activities. To address this challenge, our research introduces a practical system for monitoring and managing the entire process of the Superbonus 110 tax credit, from its initiation to redemption. This system leverages artificial intelligence and blockchain technology to streamline tax credit management and incorporates controllers based on a Decentralised Autonomous Organisation architecture, bolstered by a Multi-agent System. The outcome of our work is a system capable of establishing a tokenomics framework that caters to the needs and functionalities of both investors and operators. Moreover, it features a robust control system to prevent inadvertent errors like double spending, overspending, and deceitful practices such as false claims of completed work. The collaborative approach between the Decentralised Autonomous Organisation and the Multi-agent System enhances trust and security levels among participants in a competitive environment where potential fraudsters might attempt to exploit the system. It also enables comprehensive tracking and monitoring of the entire Superbonus process. In the realm of engineering, our project represents an innovative fusion of blockchain technology and Multi-agent Systems, advancing the application of artificial intelligence. This integration guarantees the validation, recording, and execution of transactions with a remarkable level of trust and transparency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01584v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>过去十年，税收优惠和财政奖金对意大利经济产生了重大影响。特别是，2020年的“超级奖金110”税收减免发挥了关键作用，为提高建筑能效和降低地震风险的相关费用提供了110%的慷慨减免。然而，建筑活动的激增也带来了欺诈活动的不幸增加。为了应对这一挑战，我们的研究引入了一个实用的系统，用于监控和管理超级奖金110税收抵免的整个过程，从启动到赎回。该系统利用人工智能和区块链技术简化税收抵免管理，并结合了基于分散自治组织架构的控制器，由多代理系统支持。我们的工作成果是建立一个能够建立一个符合投资者和运营商需求和功能的标记基因组学框架的系统。此外，它还具有一个强大的控制系统，以防止意外的错误，如重复支出、超支和欺诈行为，如对已完成工作的虚假声明。分散自治组织和多代理系统之间的协作方法提高了参与者之间的信任和安全水平，在竞争环境中，潜在的欺诈者可能会试图利用该系统。它还可以全面跟踪和监控整个超级奖金流程。在工程领域，我们的项目代表了区块链技术和多智能体系统的创新融合，推动了人工智能的应用。这种集成保证了交易的验证、记录和执行具有显著的信任和透明度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01584v2" target="_blank">2311.01584v2</a>
                              </td>
                              <td>Secured Fiscal Credit Model: Multi-Agent Systems And Decentralized Autonomous Organisations For Tax Credit's Tracking</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01584v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01584v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models: A Baseline Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transfer capabilities in a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first observe that, unlike fine-tuning from ImageNet pre-trained models, as previous methods do, fine-tuning from foundation models yields significantly poorer results, sometimes even worse than training from scratch. While freezing the backbones, we demonstrate that although the foundation models greatly improve the performance of the baseline method that trains the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. Based on these findings, we introduce \textit{CLIP distillation}, a parameter-free method specifically designed to distill target knowledge from CLIP models. The core of our \textit{CLIP distillation} lies in a self-calibration technique for automatic temperature scaling, a feature that significantly enhances the baseline's out-class detection capability. Although simple, our method outperforms previous approaches in most benchmark tasks, excelling in evaluation metrics including H-score/H$^3$-score and the newly proposed universal classification rate (UCR) metric. We hope that our investigation and the proposed simple framework can serve as a strong baseline to facilitate future studies in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务中显示出了令人印象深刻的学习和迁移能力。然而，有趣的是，基础模型还没有被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先观察到，与之前的方法一样，从ImageNet预训练模型进行微调不同，从基础模型进行微调会产生明显较差的结果，有时甚至比从头开始训练更差。在冻结主干的同时，我们证明，尽管基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。基于这些发现，我们介绍了\textit｛CLIP蒸馏｝，这是一种专门设计用于从CLIP模型中提取目标知识的无参数方法。我们的\textit｛CLIP蒸馏｝的核心在于自动温度标度的自校准技术，这一功能显著增强了基线的超一流检测能力。尽管简单，但我们的方法在大多数基准任务中都优于以前的方法，在评估指标方面表现出色，包括H-核心/H$^3$-得分和新提出的通用分类率（UCR）指标。我们希望我们的调查和拟议的简单框架能够作为一个强有力的基线，促进该领域未来的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v2" target="_blank">2305.11092v2</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models: A Baseline Study</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/szubing/uniood" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08854v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rank-DETR for High Quality Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08854v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08854v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design that prioritizes predictions of more accurate localization accuracy during ranking to boost the AP under high IoU thresholds. We apply our method to improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong COCO object detection results when using different backbones such as ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08854v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代检测变换器（DETR）使用一组对象查询来预测边界框的列表，根据它们的分类置信度得分对它们进行排序，并选择排名靠前的预测作为给定输入图像的最终检测结果。高性能的对象检测器需要边界框预测的准确排序。对于基于DETR的检测器，由于分类分数和定位精度之间的不对准，排名靠前的边界框的定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向秩的设计，结合称为秩DETR，介绍了一种简单且高性能的基于DETR的对象检测器。我们的主要贡献包括：（i）一种面向秩的架构设计，它可以提示正预测并抑制负预测，以确保更低的假阳性率，以及（ii）一种基于秩的损失函数和匹配成本设计，它在排序过程中优先考虑更准确定位精度的预测，以在高IoU阈值下提高AP。我们应用我们的方法改进了最近的SOTA方法（例如，H-DETR和DINO-DETR），并在使用不同的主干（如ResNet-$50$、Swin-T和Swin-L）时报告了强大的COCO对象检测结果，证明了我们方法的有效性。代码位于\url{https://github.com/leaplabthu/rank-detr}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08854v3" target="_blank">2310.08854v3</a>
                              </td>
                              <td>Rank-DETR for High Quality Object Detection</td>
                              <td>Yifan Pu</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08854v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08854v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01646v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01646v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01646v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new competitive baseline on the more challenging benchmarks SemiAves, SemiCUB, SemiFungi and Semi-iNat.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01646v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了SemiGPC，这是一种基于高斯过程的分布感知标签细化策略，其中模型的预测是从标签的后验分布导出的。与其他基于缓冲区的半监督方法（如CoMatch和SimMatch）不同，我们的SemiGPC包括一个归一化项，该项可以解决全局数据分布的不平衡问题，同时保持局部敏感性。这种显式控制允许SemiGPC对确认偏差更具鲁棒性，尤其是在类不平衡的情况下。我们发现，当与不同的半监督方法（如FixMatch、ReMixMatch、SimMatch和FreeMatch）以及不同的预训练策略（包括MSN和Dino）配对时，SemiGPC可以提高性能。我们还表明，SemiGPC在标准CIFAR10-LT/CIFAR100-LT的不同程度的类不平衡下，尤其是在低数据状态下，实现了最先进的结果。在更具挑战性的基准SemiAves、SemiCUB、SemiFormics和Semi-iNat上，与新的竞争基线相比，使用SemiGPC还导致平均准确率增加约2%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01646v1" target="_blank">2311.01646v1</a>
                              </td>
                              <td>SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes</td>
                              <td>Abdelhak Lemkhenter</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01646v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01646v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10726v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Few-Shot Panoptic Segmentation With Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10726v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10726v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10726v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前最先进的全景分割方法需要大量的带注释的训练数据，这既困难又昂贵，对其广泛采用构成了重大挑战。与此同时，视觉表示学习的最新突破引发了范式的转变，导致了可以用完全未标记的图像训练的大型基础模型的出现。在这项工作中，我们建议利用这种任务不可知的图像特征，通过提供具有近0个标签的分割全景信息（SPINO）来实现少镜头全景分割。详细地说，我们的方法将DINOv2主干与轻量级网络头相结合，用于语义分割和边界估计。我们表明，尽管我们的方法仅用10幅带注释的图像进行训练，但它预测了可用于任何现有全景分割方法的高质量伪标签。值得注意的是，我们证明，与完全监督的基线相比，SPINO在使用不到0.3%的基本事实标签的情况下取得了有竞争力的结果，为利用基础模型学习复杂的视觉识别任务铺平了道路。为了说明其普遍适用性，我们进一步将SPINO部署在室外和室内环境的真实世界机器人视觉系统上。为了促进未来的研究，我们在http://spino.cs.uni-freiburg.de.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10726v2" target="_blank">2309.10726v2</a>
                              </td>
                              <td>Few-Shot Panoptic Segmentation With Foundation Models</td>
                              <td>Markus Käppeler</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10726v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10726v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/robot-learning-freiburg/SPINO" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了跨越语义多样的图像数据集的各种生成模型，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过进行迄今为止最大的实验来评估生成模型，从而测量生成样本的人类对图像真实感的感知，并发现没有任何现有的指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性、稀有性和记忆性的17个现代指标相比，我们发现，人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，目前的指标并不能正确地检测记忆：文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块化库，以计算9个不同编码器的17个通用指标https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v2" target="_blank">2306.04675v2</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/layer6ai-labs/dgm-eval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_19257v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_19257v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_19257v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_19257v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实例检测（InsDet）是机器人和计算机视觉中一个长期存在的问题，旨在检测杂乱场景中的对象实例（由一些视觉实例预定义）。尽管它具有实际意义，但它的进步被对象检测所掩盖，对象检测旨在检测属于某些预定义类的对象。一个主要原因是，按照今天的标准，当前的InsDet数据集规模太小。例如，流行的InsDet数据集GMU（2016年发布）只有23个实例，远远少于2014年发布的著名对象检测数据集COCO（80个类）。我们有动力引入一个新的InsDet数据集和协议。首先，我们为InsDet定义了一个逼真的设置：训练数据由多视图实例捕获以及不同的场景图像组成，允许通过在其上粘贴带有自由框注释的实例图像来合成训练图像。其次，我们发布了一个真实世界的数据库，其中包含100个对象实例的多视图捕获和高分辨率（6k x 8k）测试图像。第三，我们在数据集上广泛研究了InsDet的基线方法，分析了它们的性能，并提出了未来的工作建议。令人惊讶的是，使用现成的类不可知分割模型（Segment Anything model，SAM）和自监督特征表示DINOv2表现最好，比重新利用对象检测器的端到端训练的InsDet模型（例如，FasterRCNN和RetinaNet）更好地实现了>10 AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.19257v1" target="_blank">2310.19257v1</a>
                              </td>
                              <td>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</td>
                              <td>Qianqian Shen</td>
                              <td>2023-10-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_19257v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.19257v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14736v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14736v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14736v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In Computer Vision, self-supervised contrastive learning enforces similar representations between different views of the same image. The pre-training is most often performed on image classification datasets, like ImageNet, where images mainly contain a single class of objects. However, when dealing with complex scenes with multiple items, it becomes very unlikely for several views of the same image to represent the same object category. In this setting, we propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into semantic regions, then sample the two views from the same region. Preliminary results show empirically that when pre-training on Cityscapes and ADE20K, then evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs at least on par with, and most often significantly outperforms not only SimCLR, but also DINO and MoCo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14736v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉中，自监督对比学习在同一图像的不同视图之间强制执行相似的表示。预训练通常在图像分类数据集上执行，如ImageNet，其中图像主要包含一类对象。然而，当处理具有多个项目的复杂场景时，同一图像的多个视图不太可能表示同一对象类别。在这种设置中，我们提出了SAMCLR，这是SimCLR的一个附加组件，它使用SAM将图像分割成语义区域，然后对同一区域的两个视图进行采样。初步结果实证表明，当在Cityscapes和ADE20K上进行预训练，然后在CIFAR-10、STL10和ImageNette上进行分类评估时，SAMCLR的表现至少与SimCLR不相上下，而且通常显著优于SimCLR，还优于DINO和MoCo。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14736v2" target="_blank">2310.14736v2</a>
                              </td>
                              <td>SAMCLR: Contrastive pre-training on complex scenes using SAM for view sampling</td>
                              <td>Benjamin Missaoui</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14736v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14736v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18642v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One-shot Localization and Segmentation of Medical Images with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18642v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18642v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in Vision Transformers (ViT) and Stable Diffusion (SD) models with their ability to capture rich semantic features of the image have been used for image correspondence tasks on natural images. In this paper, we examine the ability of a variety of pre-trained ViT (DINO, DINOv2, SAM, CLIP) and SD models, trained exclusively on natural images, for solving the correspondence problems on medical images. While many works have made a case for in-domain training, we show that the models trained on natural images can offer good performance on medical images across different modalities (CT,MR,Ultrasound) sourced from various manufacturers, over multiple anatomical regions (brain, thorax, abdomen, extremities), and on wide variety of tasks. Further, we leverage the correspondence with respect to a template image to prompt a Segment Anything (SAM) model to arrive at single shot segmentation, achieving dice range of 62%-90% across tasks, using just one image as reference. We also show that our single-shot method outperforms the recently proposed few-shot segmentation method - UniverSeg (Dice range 47%-80%) on most of the semantic segmentation tasks(six out of seven) across medical imaging modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18642v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉变换器（ViT）和稳定扩散（SD）模型的最新进展及其捕捉图像丰富语义特征的能力已被用于自然图像上的图像对应任务。在本文中，我们检验了各种预训练的ViT（DINO、DINOv2、SAM、CLIP）和SD模型（仅在自然图像上训练）解决医学图像上的对应问题的能力。虽然许多工作已经为域内训练提供了理由，但我们表明，在自然图像上训练的模型可以在来自不同制造商的不同模态（CT、MR、超声）、多个解剖区域（大脑、胸部、腹部、四肢）和各种任务的医学图像上提供良好的性能。此外，我们利用与模板图像的对应关系，提示Segment Anything（SAM）模型实现单次分割，仅使用一张图像作为参考，即可在任务中实现62%-90%的骰子范围。我们还表明，在医学成像模态的大多数语义分割任务（七分之六）上，我们的单镜头方法优于最近提出的少镜头分割方法UniverSeg（Dice范围47%-80%）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18642v1" target="_blank">2310.18642v1</a>
                              </td>
                              <td>One-shot Localization and Segmentation of Medical Images with Foundation Models</td>
                              <td>Deepa Anand</td>
                              <td>2023-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18642v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18642v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18251v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Self-Supervised Approach to Land Cover Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18251v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18251v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Land use/land cover change (LULC) maps are integral resources in earth science and agricultural research. Due to the nature of such maps, the creation of LULC maps is often constrained by the time and human resources necessary to accurately annotate satellite imagery and remote sensing data. While computer vision models that perform semantic segmentation to create detailed labels from such data are not uncommon, litle research has been done on self-supervised and unsupervised approaches to labelling LULC maps without the use of ground-truth masks. Here, we demonstrate a self-supervised method of land cover segmentation that has no need for high-quality ground truth labels. The proposed deep learning employs a frozen pre-trained ViT backbone transferred from DINO in a STEGO architecture and is fine-tuned using a custom dataset consisting of very high resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning, an accuracy of roughly 52% was observed across 5 samples, signifying the feasibility of self-supervised models for the automated labelling of VHR LULC maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18251v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>土地利用/土地覆盖变化图是地球科学和农业研究中不可或缺的资源。由于这类地图的性质，土地利用和土地利用法地图的制作往往受到准确注释卫星图像和遥感数据所需的时间和人力资源的限制。虽然执行语义分割以从这些数据中创建详细标签的计算机视觉模型并不罕见，但很少有人对在不使用地面实况掩码的情况下标记LULC地图的自监督和无监督方法进行研究。在这里，我们展示了一种自监督的土地覆盖分割方法，该方法不需要高质量的地面实况标签。所提出的深度学习采用了从STEGO架构中的DINO转移的冷冻预训练ViT骨干，并使用由超高分辨率（VHR）卫星图像组成的自定义数据集进行微调。仅经过10个时期的微调，在5个样本中观察到约52%的准确率，这表明自监督模型用于VHR-LULC图的自动标记的可行性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18251v1" target="_blank">2310.18251v1</a>
                              </td>
                              <td>A Self-Supervised Approach to Land Cover Segmentation</td>
                              <td>Charles Moore</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18251v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18251v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation, conditional density estimation, and density estimation with missing data tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基本测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间已建立的良好连接的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。它们在各种密度估计、条件密度估计和具有缺失数据任务的密度估计中的效用得到了说明。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v2" target="_blank">2305.13552v2</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>