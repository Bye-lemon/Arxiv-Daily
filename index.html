<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2024-01-11</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_05152v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05152v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to enable multiple robots to operate in complex environments. Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due to the lack of deep understanding of the environment. Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits the scalability of the system. To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered hierarchical and optimizable situational graphs for cooperative map generation and localization while minimizing the information exchanged between the robots. To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization. Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.   Software available within a docker image: https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05152v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作式同时定位和映射（CSLAM）对于使多个机器人能够在复杂环境中操作至关重要。大多数CSLAM技术依赖于原始传感器测量或关键帧描述符等低级特征，由于缺乏对环境的深入了解，这可能导致错误的环路闭合。此外，在机器人之间交换这些测量值和低级特征需要传输大量数据，这限制了系统的可扩展性。为了克服这些限制，我们提出了Multi-S-Graphs，这是一种去中心化的CSLAM系统，它利用嵌入四层分层和可优化的态势图中的高级语义关系信息进行协作地图生成和定位，同时最大限度地减少机器人之间的信息交换。为了支持这一点，我们提出了一种新的基于房间的描述符，该描述符及其连接的墙用于执行机器人间环路闭合，解决了多机器人绑架问题初始化的挑战。在模拟和真实环境中进行的多次实验验证了所提出的方法在准确性和稳健性方面的改进，同时与其他最先进的方法相比，减少了机器人之间交换的数据量。docker映像中可用的软件：https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05152v1" target="_blank">2401.05152v1</a>
                              </td>
                              <td>Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</td>
                              <td>Miguel Fernandez-Cortizas</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05152v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05152v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01657v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01657v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The back-end module of Distributed Collaborative Simultaneous Localization and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO) under a distributed setting, also known as SE(d)-synchronization. Most existing distributed graph optimization algorithms employ a simple sequential partitioning scheme, which may result in unbalanced subgraph dimensions due to the different geographic locations of each robot, and hence imposes extra communication load. Moreover, the performance of current Riemannian optimization algorithms can be further accelerated. In this letter, we propose a novel distributed pose graph optimization algorithm combining multi-level partitioning with an accelerated Riemannian optimization method. Firstly, we employ the multi-level graph partitioning algorithm to preprocess the naive pose graph to formulate a balanced optimization problem. In addition, inspired by the accelerated coordinate descent method, we devise an Improved Riemannian Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is globally optimal. Finally, we evaluate the effects of four common graph partitioning approaches on the correlation of the inter-subgraphs, and discover that the Highest scheme has the best partitioning performance. Also, we implement simulations to quantitatively demonstrate that our proposed algorithm outperforms the state-of-the-art distributed pose graph optimization protocols.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01657v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布式协同同步定位与映射（DCSLAM）的后端模块需要在分布式环境下求解非线性姿态图优化（PGO），也称为SE（d）-同步。大多数现有的分布式图优化算法都采用了简单的顺序划分方案，由于每个机器人的地理位置不同，这可能会导致子图维度不平衡，从而增加额外的通信负载。此外，当前黎曼优化算法的性能可以进一步提高。在这封信中，我们提出了一种新的分布式位姿图优化算法，该算法将多级划分与加速黎曼优化方法相结合。首先，我们采用多级图分割算法对初始姿态图进行预处理，以形成一个平衡优化问题。此外，受加速坐标下降法的启发，我们设计了一种改进的黎曼块坐标下降（IRBCD）算法，得到的临界点是全局最优的。最后，我们评估了四种常见的图划分方法对子图间相关性的影响，发现Highest方案具有最好的划分性能。此外，我们还进行了仿真，定量地证明了我们提出的算法优于最先进的分布式姿态图优化协议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01657v2" target="_blank">2401.01657v2</a>
                              </td>
                              <td>Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</td>
                              <td>Cunhao Li</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01657v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01657v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tjcunhao/distributed-pose-graph" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04791v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04791v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel framework for open-set Simultaneous Localization and Mapping (SLAM) in unstructured environments that uses segmentation to create a map of objects and geometric relationships between objects for localization. Our system consists of 1) a front-end mapping pipeline using a zero-shot segmentation model to extract object masks from images and track them across frames to generate an object-based map and 2) a frame alignment pipeline that uses the geometric consistency of objects to efficiently localize within maps taken in a variety of conditions. This approach is shown to be more robust to changes in lighting and appearance than traditional feature-based SLAM systems or global descriptor methods. This is established by evaluating SOS-SLAM on the Batvik seasonal dataset which includes drone flights collected over a coastal plot of southern Finland during different seasons and lighting conditions. Across flights during varying environmental conditions, our approach achieves higher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes within a reference map up to 14x faster than other feature based approaches and has a map size less than 0.4% the size of the most compact other maps. When considering localization performance from varying viewpoints, our approach outperforms all benchmarks from the same viewpoint and most benchmarks from different viewpoints. SOS-SLAM is a promising new approach for SLAM in unstructured environments that is robust to changes in lighting and appearance and is more computationally efficient than other approaches. We release our code and datasets: https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04791v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种在非结构化环境中用于开放集同时定位和映射（SLAM）的新框架，该框架使用分割来创建用于定位的对象和对象之间的几何关系的映射。我们的系统由1）前端映射管道和2）帧对齐管道组成，前者使用零样本分割模型从图像中提取对象掩码，并在帧间跟踪它们以生成基于对象的地图，后者使用对象的几何一致性在各种条件下拍摄的地图内有效定位。与传统的基于特征的SLAM系统或全局描述符方法相比，这种方法对照明和外观的变化更具鲁棒性。这是通过评估巴特维克季节数据集上的SOS-SLAM来建立的，该数据集包括在不同季节和光照条件下在芬兰南部沿海地区收集的无人机飞行。在不同环境条件下的飞行中，我们的方法实现了比基准方法更高的召回率，精度为1.0。SOS-SLAM在参考地图内的定位速度比其他基于特征的方法快14倍，并且地图大小小于最紧凑的其他地图大小的0.4%。当从不同的角度考虑本地化性能时，我们的方法优于来自同一角度的所有基准测试和来自不同角度的大多数基准测试。SOS-SLAM是非结构化环境中SLAM的一种很有前途的新方法，它对光照和外观的变化具有鲁棒性，并且在计算上比其他方法更高效。我们发布我们的代码和数据集：https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04791v1" target="_blank">2401.04791v1</a>
                              </td>
                              <td>SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</td>
                              <td>Jouko Kinnari</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04791v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04791v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03398v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03398v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>AI and robotics technologies have witnessed remarkable advancements in the past decade, revolutionizing work patterns and opportunities in various domains. The application of these technologies has propelled society towards an era of symbiosis between humans and machines. To facilitate efficient communication between humans and intelligent robots, we propose the "Avatar" system, an immersive low-latency panoramic human-robot interaction platform. We have designed and tested a prototype of a rugged mobile platform integrated with edge computing units, panoramic video capture devices, power batteries, robot arms, and network communication equipment. Under favorable network conditions, we achieved a low-latency high-definition panoramic visual experience with a delay of 357ms. Operators can utilize VR headsets and controllers for real-time immersive control of robots and devices. The system enables remote control over vast physical distances, spanning campuses, provinces, countries, and even continents (New York to Shenzhen). Additionally, the system incorporates visual SLAM technology for map and trajectory recording, providing autonomous navigation capabilities. We believe that this intuitive system platform can enhance efficiency and situational experience in human-robot collaboration, and with further advancements in related technologies, it will become a versatile tool for efficient and symbiotic cooperation between AI and humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03398v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能和机器人技术在过去十年中取得了显著进步，改变了各个领域的工作模式和机会。这些技术的应用将社会推向了一个人与机器共生的时代。为了促进人类与智能机器人之间的高效通信，我们提出了“阿凡达”系统，这是一个沉浸式低延迟全景人机交互平台。我们设计并测试了一个坚固的移动平台原型，该平台集成了边缘计算单元、全景视频捕获设备、动力电池、机械臂和网络通信设备。在良好的网络条件下，我们实现了延迟357ms的低延迟高清全景视觉体验。操作员可以利用VR耳机和控制器对机器人和设备进行实时沉浸式控制。该系统能够实现跨越校园、省份、国家甚至大洲（纽约到深圳）的远距离远程控制。此外，该系统结合了用于地图和轨迹记录的视觉SLAM技术，提供了自主导航功能。我们相信，这个直观的系统平台可以提高人机协作的效率和情景体验，随着相关技术的进一步进步，它将成为人工智能与人类高效共生合作的通用工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03398v2" target="_blank">2401.03398v2</a>
                              </td>
                              <td>Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</td>
                              <td>Junjie Li</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03398v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03398v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11598v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11598v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11598v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习神经隐式表示在多视图图像的三维重建中取得了显著的性能。当前的方法使用体积渲染将隐式表示渲染为RGB或深度图像，这些图像由多视图地面实况监督。然而，每次渲染视图都会遇到孔的深度不完整以及深度监督对遮挡结构的不了解，这严重影响了通过体绘制进行几何推断的准确性。为了解决这个问题，我们建议通过具有注意深度融合先验的体绘制，从多视图RGBD图像中学习神经隐式表示。我们的先验允许神经网络从从可用于渲染的所有深度图像中融合的截断有符号距离函数（TSDF）中感知粗略的3D结构。TSDF能够访问一个深度图像上孔的缺失深度以及当前视图中不可见的遮挡部分。通过引入一种新的注意力机制，我们允许神经网络直接使用具有推断占用率的深度融合先验作为学习的隐函数。我们的注意力机制与表示整个场景的一次性融合TSDF或表示同步定位和映射（SLAM）上下文中的部分场景的增量融合TSDF一起工作。我们对广泛使用的基准（包括合成扫描和真实世界扫描）的评估表明，我们优于最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11598v2" target="_blank">2310.11598v2</a>
                              </td>
                              <td>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</td>
                              <td>Pengchong Hu</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11598v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11598v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/MachinePerceptionLab/Attentive_DFPrior" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03604v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03604v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03604v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性里程计（VIO）算法通过使用相机和惯性测量单元（IMU）传感器来估计精确的相机轨迹。VIO的应用范围广泛，包括增强现实和室内导航。VIO算法有可能促进视障人士在室内和室外环境中的导航。然而，最先进的VIO算法在动态环境中，特别是在人口稠密的走廊中，遇到了巨大的挑战。现有的VIO数据集，例如ADVIO，通常无法有效利用这些挑战。在本文中，我们引入了Amirkabir校园数据集（AUT-VI）来解决上述问题并改进导航系统。AUT-VI是一个新颖且极具挑战性的数据集，包含17个不同位置的126个不同序列。该数据集包含动态对象、具有挑战性的回路闭合/地图重用、不同的照明条件、反射和相机突然移动，以覆盖所有极端导航场景。此外，为了支持正在进行的开发工作，我们向公众发布了用于数据捕获的Android应用程序。这使得其他研究人员能够轻松地捕捉他们定制的VIO数据集变体。此外，我们在数据集上评估了最先进的视觉惯性里程计（VIO）和视觉里程计（VO）方法，强调了对这一具有挑战性的数据集的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03604v1" target="_blank">2401.03604v1</a>
                              </td>
                              <td>Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</td>
                              <td>Ali Samadzadeh</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03604v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03604v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/A3DV/VIRec" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02816v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02816v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we conducted a comparative evaluation of three RGB-D SLAM (Simultaneous Localization and Mapping) algorithms: RTAB-Map, ORB-SLAM3, and OpenVSLAM for SURENA-V humanoid robot localization and mapping. Our test involves the robot to follow a full circular pattern, with an Intel RealSense D435 RGB-D camera installed on its head. In assessing localization accuracy, ORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map at 0.1641 and OpenVSLAM at 0.1847. However, it should be noted that both ORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when the robot encountered a wall with limited feature points. Nevertheless, OpenVSLAM demonstrated the ability to detect loop closures and successfully relocalize itself within the map when the robot approached its initial location. The investigation also extended to mapping capabilities, where RTAB-Map excelled by offering diverse mapping outputs, including dense, OctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM provided only sparse maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02816v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对用于SURENA-V人形机器人定位和映射的三种RGB-D SLAM（同时定位和映射）算法：RTAB-Map、ORB-SLAM3和OpenVSLAM进行了比较评估。我们的测试涉及机器人遵循全圆形模式，其头部安装了Intel RealSense D435 RGB-D相机。在评估定位精度方面，ORB-SLAM3的ATE为0.1073，优于其他公司，其次是RTAB-Map，为0.1641，OpenVSLAM为0.1847。然而，应该注意的是，当机器人遇到具有有限特征点的墙壁时，ORB-SLAM3和OpenVSLAM在保持准确的里程测量方面都面临挑战。尽管如此，OpenVSLAM展示了检测环路闭合的能力，并在机器人接近其初始位置时成功地在地图内重新定位自己。调查还扩展到了地图功能，RTAB Map在地图功能方面表现出色，提供了多种地图输出，包括密集、OctoMap和占用网格地图。相比之下，ORB-SLAM3和OpenVSLAM都只提供了稀疏映射。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02816v1" target="_blank">2401.02816v1</a>
                              </td>
                              <td>Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</td>
                              <td>Amirhosein Vedadi</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02816v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02816v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01081v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01081v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial SLAM is crucial in various fields, such as aerial vehicles, industrial robots, and autonomous driving. The fusion of camera and inertial measurement unit (IMU) makes up for the shortcomings of a signal sensor, which significantly improves the accuracy and robustness of localization in challenging environments. This article presents PLE-SLAM, an accurate and real-time visual-inertial SLAM algorithm based on point-line features and efficient IMU initialization. First, we use parallel computing methods to extract features and compute descriptors to ensure real-time performance. Adjacent short line segments are merged into long line segments, and isolated short line segments are directly deleted. Second, a rotation-translation-decoupled initialization method is extended to use both points and lines. Gyroscope bias is optimized by tightly coupling IMU measurements and image observations. Accelerometer bias and gravity direction are solved by an analytical method for efficiency. To improve the system's intelligence in handling complex environments, a scheme of leveraging semantic information and geometric constraints to eliminate dynamic features and A solution for loop detection and closed-loop frame pose estimation using CNN and GNN are integrated into the system. All networks are accelerated to ensure real-time performance. The experiment results on public datasets illustrate that PLE-SLAM is one of the state-of-the-art visual-inertial SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01081v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性SLAM在飞行器、工业机器人和自动驾驶等各个领域都至关重要。相机和惯性测量单元（IMU）的融合弥补了信号传感器的不足，显著提高了在具有挑战性的环境中定位的准确性和鲁棒性。本文提出了一种基于点线特征和有效IMU初始化的精确实时视觉惯性SLAM算法PLE-SLAM。首先，我们使用并行计算方法来提取特征并计算描述符，以确保实时性能。相邻的短线段合并为长线段，孤立的短线段直接删除。其次，将旋转-平移解耦初始化方法扩展为同时使用点和线。陀螺仪偏置通过紧密耦合IMU测量和图像观测进行优化。加速度计偏置和重力方向通过效率分析方法求解。为了提高系统在处理复杂环境中的智能性，将一种利用语义信息和几何约束来消除动态特征的方案以及一种使用CNN和GNN的环路检测和闭环帧姿态估计的解决方案集成到系统中。所有网络都被加速以确保实时性能。在公共数据集上的实验结果表明，PLE-SLAM是最先进的视觉惯性SLAM系统之一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01081v2" target="_blank">2401.01081v2</a>
                              </td>
                              <td>PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</td>
                              <td>Jiaming He</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01081v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01081v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hjmgarmin/ple-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11700v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11700v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11700v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了$\textbf{GS-SLAM}$，它首先在同步定位和映射（SLAM）系统中使用3D高斯表示。它有助于更好地平衡效率和准确性。与最近使用神经隐式表示的SLAM方法相比，我们的方法使用了实时可微分的飞溅渲染管道，大大加快了地图优化和RGB-D重新渲染的速度。具体而言，我们提出了一种自适应扩展策略，该策略添加新的或删除有噪声的3D高斯，以有效地重建新的观测场景几何结构并改进先前观测区域的映射。该策略对于扩展3D高斯表示以重建整个场景而不是在现有方法中合成静态对象至关重要。此外，在姿态跟踪过程中，设计了一种有效的从粗到细的技术来选择可靠的3D高斯表示来优化相机姿态，从而减少了运行时间并实现了稳健的估计。与Replica、TUM-RGBD数据集上现有的最先进的实时方法相比，我们的方法实现了具有竞争力的性能。源代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11700v3" target="_blank">2311.11700v3</a>
                              </td>
                              <td>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</td>
                              <td>Chi Yan</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11700v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11700v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01887v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01887v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01887v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计基于视觉输入来估计移动的相机的运动。现有的方法大多侧重于双视点跟踪，往往忽略了图像序列中丰富的时间上下文，从而忽略了全局运动模式，并且没有提供对完整轨迹可靠性的评估。这些缺点阻碍了在具有遮挡、动态对象和低纹理区域的场景中的性能。为了应对这些挑战，我们提出了长期有效的任意点跟踪（LEAP）模块。LEAP创新地将视觉、轨迹间和时间线索与精心选择的锚点相结合，用于动态轨迹估计。此外，LEAP的时间概率公式将分布更新集成到可学习的迭代精化模块中，以推理逐点不确定性。基于这些特点，我们开发了LEAP-VO，这是一种强大的视觉里程计系统，擅长处理遮挡和动态场景。我们的专注集成展示了一种新颖的做法，将长期点跟踪作为前端。大量实验表明，在各种视觉里程计基准测试中，所提出的管道显著优于现有基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01887v1" target="_blank">2401.01887v1</a>
                              </td>
                              <td>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</td>
                              <td>Weirong Chen</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01887v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01887v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01545v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01545v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01545v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01545v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system designed for dynamic scenes. While existing neural implicit SLAM systems perform well in static scenes, they often encounter challenges in real-world environments with dynamic interferences, leading to ineffective tracking and mapping. DDN-SLAM utilizes the priors provided by the deep semantic system, combined with conditional probability fields, for segmentation.By constructing depth-guided static masks and employing joint multi-resolution hashing encoding, we ensure fast hole filling and high-quality mapping while mitigating the effects of dynamic information interference. To enhance tracking robustness, we utilize sparse feature points validated with optical flow and keyframes, enabling loop closure detection and global bundle optimization. Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real datasets demonstrate that our method outperforms state-of-the-art approaches in both dynamic and static scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01545v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了DDN-SLAM，一个针对动态场景设计的实时密集神经隐式语义SLAM系统。虽然现有的神经隐式SLAM系统在静态场景中表现良好，但它们在具有动态干扰的真实世界环境中经常遇到挑战，导致跟踪和映射无效。DDN-SLAM利用深度语义系统提供的先验，结合条件概率场进行分割。通过构建深度引导的静态掩模并采用联合多分辨率哈希编码，我们确保了快速的孔洞填充和高质量的映射，同时减轻了动态信息干扰的影响。为了增强跟踪鲁棒性，我们利用经过光流和关键帧验证的稀疏特征点，实现闭环检测和全局束优化。此外，DDN-SLAM支持单眼、立体声和RGB-D输入，在20-30Hz的频率下稳定工作。在6个虚拟/真实数据集上进行的大量实验表明，我们的方法在动态和静态场景中都优于最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01545v1" target="_blank">2401.01545v1</a>
                              </td>
                              <td>DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding</td>
                              <td>Mingrui Li</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01545v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01545v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01189v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01189v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01189v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01189v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit representations have been explored to enhance visual SLAM algorithms, especially in providing high-fidelity dense map. Existing methods operate robustly in static scenes but struggle with the disruption caused by moving objects. In this paper we present NID-SLAM, which significantly improves the performance of neural SLAM in dynamic environments. We propose a new approach to enhance inaccurate regions in semantic masks, particularly in marginal areas. Utilizing the geometric information present in depth images, this method enables accurate removal of dynamic objects, thereby reducing the probability of camera drift. Additionally, we introduce a keyframe selection strategy for dynamic scenes, which enhances camera tracking robustness against large-scale objects and improves the efficiency of mapping. Experiments on publicly available RGB-D datasets demonstrate that our method outperforms competitive neural SLAM approaches in tracking accuracy and mapping quality in dynamic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01189v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>已经探索了神经隐式表示来增强视觉SLAM算法，特别是在提供高保真密集地图方面。现有的方法在静态场景中运行稳健，但难以应对移动对象造成的干扰。在本文中，我们提出了NID-SLAM，它显著提高了神经SLAM在动态环境中的性能。我们提出了一种新的方法来增强语义掩码中的不准确区域，特别是在边缘区域。利用深度图像中存在的几何信息，该方法能够准确地去除动态对象，从而降低相机漂移的概率。此外，我们还引入了一种动态场景的关键帧选择策略，该策略增强了相机对大型对象的跟踪鲁棒性，并提高了映射效率。在公开的RGB-D数据集上的实验表明，我们的方法在动态环境中的跟踪精度和映射质量方面优于竞争性的神经SLAM方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01189v1" target="_blank">2401.01189v1</a>
                              </td>
                              <td>NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments</td>
                              <td>Ziheng Xu</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01189v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is a fundamental task for numerous applications such as autonomous navigation and exploration. Despite many SLAM datasets have been released, current SLAM solutions still struggle to have sustained and resilient performance. One major issue is the absence of high-quality datasets including diverse all-weather conditions and a reliable metric for assessing robustness. This limitation significantly restricts the scalability and generalizability of SLAM technologies, impacting their development, validation, and deployment. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push SLAM towards all-weather environments to pursue the most robust SLAM performance. It contains multi-degraded environments including over 30 diverse scenes such as structureless corridors, varying lighting conditions, and perceptual obscurants like smoke and dust; multimodal sensors such as LiDAR, fisheye camera, IMU, and thermal camera; and multiple locomotions like aerial, legged, and wheeled robots. We develop accuracy and robustness evaluation tracks for SLAM and introduced novel robustness metrics. Comprehensive studies are performed, revealing new observations, challenges, and opportunities for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和测绘（SLAM）是许多应用（如自主导航和勘探）的基本任务。尽管已经发布了许多SLAM数据集，但当前的SLAM解决方案仍难以获得持续和有弹性的性能。一个主要问题是缺乏高质量的数据集，包括不同的全天候条件和评估稳健性的可靠指标。这种限制极大地限制了SLAM技术的可扩展性和可推广性，影响了它们的开发、验证和部署。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在将SLAM推向全天候环境，以追求最稳健的SLAM性能。它包含了多种退化环境，包括30多个不同的场景，如无结构走廊、不同的照明条件和烟雾和灰尘等感知障碍物；多模式传感器，如激光雷达、鱼眼相机、IMU和热像仪；以及多种运动方式，如空中机器人、腿式机器人和轮式机器人。我们为SLAM开发了准确性和稳健性评估跟踪，并引入了新的稳健性度量。进行了全面的研究，揭示了新的观察结果、挑战和未来研究的机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v4" target="_blank">2307.07607v4</a>
                              </td>
                              <td>SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17110v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17110v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17110v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17110v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agricultural robotics is an active research area due to global population growth and expectations of food and labor shortages. Robots can potentially help with tasks such as pruning, harvesting, phenotyping, and plant modeling. However, agricultural automation is hampered by the difficulty in creating high resolution 3D semantic maps in the field that would allow for safe manipulation and navigation. In this paper, we build toward solutions for this issue and showcase how the use of semantics and environmental priors can help in constructing accurate 3D maps for the target application of sorghum. Specifically, we 1) use sorghum seeds as semantic landmarks to build a visual Simultaneous Localization and Mapping (SLAM) system that enables us to map 78\\% of a sorghum range on average, compared to 38% with ORB-SLAM2; and 2) use seeds as semantic features to improve 3D reconstruction of a full sorghum panicle from images taken by a robotic in-hand camera.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17110v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于全球人口增长以及对粮食和劳动力短缺的预期，农业机器人是一个活跃的研究领域。机器人可能有助于完成修剪、收割、表型分析和植物建模等任务。然而，农业自动化受到阻碍，因为难以在该领域创建高分辨率3D语义地图，从而实现安全操作和导航。在本文中，我们致力于解决这一问题，并展示了语义和环境先验的使用如何帮助为高粱的目标应用构建准确的3D地图。具体而言，我们1）使用高粱种子作为语义地标来构建视觉同步定位和映射（SLAM）系统，该系统使我们能够平均映射78%的高粱范围，而ORB-SLAM2的映射率为38%；和2）使用种子作为语义特征来改进由机器人手持相机拍摄的图像对完整高粱穗的3D重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17110v1" target="_blank">2312.17110v1</a>
                              </td>
                              <td>Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants</td>
                              <td>Mohamad Qadri</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17110v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17110v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing LiDAR-inertial-visual odometry and mapping (LIV-SLAM) systems mainly utilize the LiDAR-inertial odometry (LIO) module for structure reconstruction and the visual-inertial odometry (VIO) module for color rendering. However, the accuracy of VIO is often compromised by photometric changes, weak textures and motion blur, unlike the more robust LIO. This paper introduces SR-LIVO, an advanced and novel LIV-SLAM system employing sweep reconstruction to align reconstructed sweeps with image timestamps. This allows the LIO module to accurately determine states at all imaging moments, enhancing pose accuracy and processing efficiency. Experimental results on two public datasets demonstrate that: 1) our SRLIVO outperforms existing state-of-the-art LIV-SLAM systems in both pose accuracy and time efficiency; 2) our LIO-based pose estimation prove more accurate than VIO-based ones in several mainstream LIV-SLAM systems (including ours). We have released our source code to contribute to the community development in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的激光雷达惯性视觉里程计和测绘（LIV-SLAM）系统主要利用激光雷达惯性里程计（LIO）模块进行结构重建和视觉惯性里程计模块进行颜色渲染。然而，与更稳健的LIO不同，VIO的准确性经常受到光度变化、弱纹理和运动模糊的影响。本文介绍了SR-LIVO，这是一种先进新颖的LIV-SLAM系统，它采用扫描重建来将重建的扫描与图像时间戳对齐。这使得LIO模块能够准确地确定所有成像时刻的状态，从而提高姿态精度和处理效率。在两个公共数据集上的实验结果表明：1）我们的SRLIVO在姿态精度和时间效率方面都优于现有最先进的LIV-SLAM系统；2） 在几个主流的LIV-SLAM系统（包括我们的系统）中，我们基于LIO的姿态估计被证明比基于VIO的姿态估算更准确。我们已经发布了我们的源代码，为该领域的社区开发做出贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16800v1" target="_blank">2312.16800v1</a>
                              </td>
                              <td>SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep Reconstruction</td>
                              <td>Zikang Yuan</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ZikangYuan/sr_livo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06141v5_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Active Semantic Localization with Graph Neural Embedding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06141v5_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06141v5_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06141v5_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications (e.g., point-goal navigation, object-goal navigation, vision language navigation) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). However, most existing works on semantic localization focus on passive vision tasks without viewpoint planning, or rely on additional rich modalities (e.g., depth measurements). Thus, the problem is largely unsolved. In this work, we explore a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph, which combines the viewpoint- and appearance- invariance of local and global features; (2) Graph neural network, which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a reinforcement-learning planner for active vision. Experiments on two scenarios, self-supervised learning and unsupervised domain adaptation, using a photo-realistic Habitat simulator validate the effectiveness of the proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06141v5_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义定位，即具有语义图像模态的机器人自我定位，在最近出现的嵌入式人工智能应用（如点目标导航、对象目标导航、视觉语言导航）和拓扑映射应用（如图神经SLAM、以自我为中心的拓扑图）中至关重要。然而，大多数现有的语义定位工作都集中在被动视觉任务上，而没有视点规划，或者依赖于额外的丰富模式（例如，深度测量）。因此，这个问题在很大程度上没有得到解决。在这项工作中，我们探索了一种轻量级的、完全基于CPU的、领域自适应的语义定位框架，称为图神经定位器。我们的方法受到了两种最近出现的技术的启发：（1）场景图，它结合了局部和全局特征的视点和外观不变性；（2） 图形神经网络，能够直接学习/识别图形数据（即非矢量数据）。具体来说，首先将图卷积神经网络训练为被动视觉的场景图分类器，然后将其知识转移到主动视觉的强化学习规划器中。在自监督学习和无监督领域自适应两种场景下，使用照片逼真的Habitat模拟器进行实验，验证了所提出方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06141v5" target="_blank">2305.06141v5</a>
                              </td>
                              <td>Active Semantic Localization with Graph Neural Embedding</td>
                              <td>Mitsuki Yoshida</td>
                              <td>2023-05-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06141v5_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06141v5" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的局限性。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景的地面机器人收集的公共数据集M2DGR的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v3" target="_blank">2307.07763v3</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15679v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15679v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15679v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15679v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Purpose: Common dense stereo Simultaneous Localization and Mapping (SLAM) approaches in Minimally Invasive Surgery (MIS) require high-end parallel computational resources for real-time implementation. Yet, it is not always feasible since the computational resources should be allocated to other tasks like segmentation, detection, and tracking. To solve the problem of limited parallel computational power, this research aims at a lightweight dense stereo SLAM system that works on a single-core CPU and achieves real-time performance (more than 30 Hz in typical scenarios). Methods: A new dense stereo mapping module is integrated with the ORB-SLAM2 system and named BDIS-SLAM. Our new dense stereo mapping module includes stereo matching and 3D dense depth mosaic methods. Stereo matching is achieved with the recently proposed CPU-level real-time matching algorithm Bayesian Dense Inverse Searching (BDIS). A BDIS-based shape recovery and a depth mosaic strategy are integrated as a new thread and coupled with the backbone ORB-SLAM2 system for real-time stereo shape recovery. Results: Experiments on in-vivo data sets show that BDIS-SLAM runs at over 30 Hz speed on modern single-core CPU in typical endoscopy/colonoscopy scenarios. BDIS-SLAM only consumes around an additional 12% time compared with the backbone ORB-SLAM2. Although our lightweight BDIS-SLAM simplifies the process by ignoring deformation and fusion procedures, it can provide a usable dense mapping for modern MIS on computationally constrained devices. Conclusion: The proposed BDIS-SLAM is a lightweight stereo dense SLAM system for MIS. It achieves 30 Hz on a modern single-core CPU in typical endoscopy/colonoscopy scenarios (image size around 640*480). BDIS-SLAM provides a low-cost solution for dense mapping in MIS and has the potential to be applied in surgical robots and AR systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15679v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目的：微创外科（MIS）中常见的密集立体同时定位和映射（SLAM）方法需要高端并行计算资源才能实时实现。然而，这并不总是可行的，因为计算资源应该分配给其他任务，如分割、检测和跟踪。为了解决并行计算能力有限的问题，本研究旨在开发一种在单核CPU上工作并实现实时性能（在典型场景中超过30 Hz）的轻量级密集立体声SLAM系统。方法：将一种新的稠密立体映射模块与ORB-SLAM2系统集成，命名为BDIS-SLAM。我们新的密集立体映射模块包括立体匹配和3D密集深度镶嵌方法。最近提出的CPU级实时匹配算法贝叶斯密集逆搜索（BDIS）实现了立体匹配。基于BDIS的形状恢复和深度镶嵌策略被集成为一个新的线程，并与骨干ORB-SLAM2系统耦合，用于实时立体形状恢复。结果：在体内数据集上的实验表明，在典型的内窥镜/结肠镜检查场景中，BDIS-SLAM在现代单核CPU上以超过30Hz的速度运行。与骨干ORB-SLAM2相比，BDIS-SLAM仅额外消耗约12%的时间。尽管我们的轻量级BDIS-SLAM通过忽略变形和融合过程简化了过程，但它可以在计算受限的设备上为现代MIS提供可用的密集映射。结论：BDIS-SLAM是一种用于MIS的轻量级立体密集SLAM系统。在典型的内窥镜/结肠镜检查场景中，它在现代单核CPU上实现了30 Hz（图像大小约为640*480）。BDIS-SLAM为MIS中的密集映射提供了一种低成本的解决方案，并有可能应用于外科机器人和AR系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15679v1" target="_blank">2312.15679v1</a>
                              </td>
                              <td>BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery</td>
                              <td>Jingwei Song</td>
                              <td>2023-12-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15679v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15679v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jingweisong/bdis-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11310v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Twilight SLAM: Navigating Low-Light Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11310v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11310v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11310v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a detailed examination of low-light visual Simultaneous Localization and Mapping (SLAM) pipelines, focusing on the integration of state-of-the-art (SOTA) low-light image enhancement algorithms with standard and contemporary SLAM frameworks. The primary objective of our work is to address a pivotal question: Does illuminating visual input significantly improve localization accuracy in both semi-dark and dark environments? In contrast to previous works that primarily address partially dim-lit datasets, we comprehensively evaluate various low-light SLAM pipelines across obscurely-lit environments. Employing a meticulous experimental approach, we qualitatively and quantitatively assess different combinations of image enhancers and SLAM frameworks, identifying the best-performing combinations for feature-based visual SLAM. The findings advance low-light SLAM by highlighting the practical implications of enhancing visual input for improved localization accuracy in challenging lighting conditions. This paper also offers valuable insights, encouraging further exploration of visual enhancement strategies for enhanced SLAM performance in real-world scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11310v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文详细研究了微光视觉同步定位和映射（SLAM）管道，重点是将最先进的微光图像增强算法与标准和现代SLAM框架相结合。我们工作的主要目标是解决一个关键问题：在半暗和暗环境中，照明视觉输入是否能显著提高定位精度？与之前主要处理部分昏暗数据集的工作相比，我们全面评估了昏暗环境中的各种微光SLAM管道。采用细致的实验方法，我们定性和定量地评估了图像增强器和SLAM框架的不同组合，确定了基于特征的视觉SLAM的最佳组合。该发现通过强调在具有挑战性的照明条件下增强视觉输入以提高定位精度的实际意义，推进了微光SLAM。本文还提供了有价值的见解，鼓励进一步探索在现实世界场景中增强SLAM性能的视觉增强策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11310v4" target="_blank">2304.11310v4</a>
                              </td>
                              <td>Twilight SLAM: Navigating Low-Light Environments</td>
                              <td>Surya Pratap Singh</td>
                              <td>2023-04-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11310v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11310v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13332v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13332v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13332v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13332v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The opacity of rigid 3D scenes with opaque surfaces is considered to be of a binary type. However, we observed that this property is not followed by the existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization through the volumetric rendering function does not facilitate easy integration of the desired prior. Instead, we observed that the opacity of ternary-type (TT) is well supported. In this work, we study why ternary-type opacity is well-suited and desired for the task at hand. In particular, we provide theoretical insights into the process of jointly optimizing radiance and opacity through the volumetric rendering process. Through exhaustive experiments on benchmark datasets, we validate our claim and provide insights into the optimization process, which we believe will unleash the potential of RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple yet novel visual odometry scheme that uses a hybrid combination of volumetric and warping-based image renderings. More specifically, the proposed hybrid odometry (HO) additionally uses image warping-based coarse odometry, leading up to an order of magnitude final speed-up. Furthermore, we show that the proposed TT and HO well complement each other, offering state-of-the-art results on benchmark datasets in terms of both speed and accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13332v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有不透明表面的刚性3D场景的不透明度被认为是二元类型。然而，我们观察到，现有的仅RGB的NeRF SLAM并没有遵循这一特性。因此，我们有动机将此先验引入仅RGB的NeRF SLAM流水线。不幸的是，通过体积渲染函数的优化不利于所需先验的容易集成。相反，我们观察到三元类型（TT）的不透明性得到了很好的支持。在这项工作中，我们研究了为什么三元型不透明性非常适合并期望用于手头的任务。特别是，我们为通过体积渲染过程联合优化辐射和不透明度的过程提供了理论见解。通过在基准数据集上进行详尽的实验，我们验证了我们的说法，并深入了解了优化过程，我们相信这将释放仅RGB的NeRF SLAM的潜力。为了促进这一研究方向，我们还提出了一种简单而新颖的视觉里程计方案，该方案使用体积和基于扭曲的图像渲染的混合组合。更具体地说，所提出的混合里程计（HO）还使用了基于图像扭曲的粗略里程计，导致了一个数量级的最终加速。此外，我们还表明，所提出的TT和HO很好地相互补充，在速度和准确性方面，在基准数据集上提供了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13332v2" target="_blank">2312.13332v2</a>
                              </td>
                              <td>Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM</td>
                              <td>Junru Lin</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13332v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13332v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13802v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Dense Subframe-based SLAM Framework with Side-scan Sonar</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13802v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13802v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13802v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Side-scan sonar (SSS) is a lightweight acoustic sensor that is commonly deployed on autonomous underwater vehicles (AUVs) to provide high-resolution seafloor images. However, leveraging side-scan images for simultaneous localization and mapping (SLAM) presents a notable challenge, primarily due to the difficulty of establishing sufficient amount of accurate correspondences between these images. To address this, we introduce a novel subframe-based dense SLAM framework utilizing side-scan sonar data, enabling effective dense matching in overlapping regions of paired side-scan images. With each image being evenly divided into subframes, we propose a robust estimation pipeline to estimate the relative pose between each paired subframes, by using a good inlier set identified from dense correspondences. These relative poses are then integrated as edge constraints in a factor graph to optimize the AUV pose trajectory.   The proposed framework is evaluated on three real datasets collected by a Hugin AUV. Among one of them includes manually-annotated keypoint correspondences as ground truth and is used for evaluation of pose trajectory. We also present a feasible way of evaluating mapping quality against multi-beam echosounder (MBES) data without the influence of pose. Experimental results demonstrate that our approach effectively mitigates drift from the dead-reckoning (DR) system and enables quasi-dense bathymetry reconstruction. An open-source implementation of this work is available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13802v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>侧扫声纳（SSS）是一种轻型声学传感器，通常部署在自主水下航行器（AUV）上，以提供高分辨率海底图像。然而，利用侧扫描图像进行同时定位和映射（SLAM）是一个显著的挑战，主要是由于难以在这些图像之间建立足够数量的精确对应关系。为了解决这一问题，我们引入了一种新的基于子帧的密集SLAM框架，该框架利用侧扫声纳数据，能够在成对侧扫图像的重叠区域进行有效的密集匹配。在将每个图像均匀地划分为子帧的情况下，我们提出了一种稳健的估计流水线，通过使用从密集对应中识别出的良好内部集合来估计每个成对子帧之间的相对姿态。然后将这些相对姿态作为边缘约束集成到因子图中，以优化AUV姿态轨迹。在Hugin AUV收集的三个真实数据集上对所提出的框架进行了评估。其中一个包括手动注释的关键点对应关系作为基本事实，并用于姿态轨迹的评估。我们还提出了一种在不受姿态影响的情况下根据多波束回声测深仪（MBES）数据评估测绘质量的可行方法。实验结果表明，我们的方法有效地缓解了航位推算（DR）系统的漂移，并实现了准密集水深重建。这项工作的开源实现是可用的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13802v1" target="_blank">2312.13802v1</a>
                              </td>
                              <td>A Dense Subframe-based SLAM Framework with Side-scan Sonar</td>
                              <td>Jun Zhang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13802v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13802v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_01854v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Fully-automatic Side-scan Sonar SLAM Framework</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_01854v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_01854v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_01854v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Side-scan sonar (SSS) is a lightweight acoustic sensor that is frequently deployed on autonomous underwater vehicles (AUVs) to provide high-resolution seafloor images. However, using side-scan images to perform simultaneous localization and mapping (SLAM) remains a challenge when there is a lack of 3D bathymetric information and discriminant features in the side-scan images. To tackle this, we propose a feature-based SLAM framework using side-scan sonar, which is able to automatically detect and robustly match keypoints between paired side-scan images. We then use the detected correspondences as constraints to optimize the AUV pose trajectory. The proposed method is evaluated on real data collected by a Hugin AUV, using as a ground truth reference both manually-annotated keypoints and a 3D bathymetry mesh from multibeam echosounder (MBES). Experimental results demonstrate that our approach is able to reduce drifts from the dead-reckoning system. The framework is made publicly available for the benefit of the community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_01854v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>侧扫声纳（SSS）是一种轻型声学传感器，经常部署在自主水下航行器（AUV）上，以提供高分辨率海底图像。然而，当侧扫描图像中缺乏3D测深信息和判别特征时，使用侧扫描图像来执行同时定位和映射（SLAM）仍然是一个挑战。为了解决这一问题，我们提出了一种使用侧扫声纳的基于特征的SLAM框架，该框架能够自动检测并稳健地匹配成对侧扫图像之间的关键点。然后，我们使用检测到的对应关系作为约束来优化AUV姿态轨迹。所提出的方法是在Hugin AUV收集的真实数据上进行评估的，使用手动注释的关键点和多波束回声测深仪（MBES）的3D测深网格作为地面实况参考。实验结果表明，我们的方法能够减少航位推算系统的漂移。该框架是为了社区的利益而公开提供的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.01854v2" target="_blank">2304.01854v2</a>
                              </td>
                              <td>A Fully-automatic Side-scan Sonar SLAM Framework</td>
                              <td>Jun Zhang</td>
                              <td>2023-04-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_01854v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.01854v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/halajun/diasss" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13471v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13471v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13471v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13471v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that integrates learning-based sparse visual odometry for low-latency camera tracking and a neural radiance scene representation for sophisticated dense reconstruction and novel view synthesis. Our system initializes camera poses using sparse visual odometry and obtains view-dependent dense geometry priors from a monocular depth prediction network. We harmonize the scale of poses and dense geometry, treating them as supervisory cues to train a neural implicit scene representation. NeRF-VO demonstrates exceptional performance in both photometric and geometric fidelity of the scene representation by jointly optimizing a sliding window of keyframed poses and the underlying dense geometry, which is accomplished through training the radiance field with volume rendering. We surpass state-of-the-art methods in pose estimation accuracy, novel view synthesis fidelity, and dense reconstruction quality across a variety of synthetic and real-world datasets, while achieving a higher camera tracking frequency and consuming less GPU memory.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13471v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种新的单目视觉里程计（VO）系统NeRF VO，该系统集成了用于低延迟相机跟踪的基于学习的稀疏视觉里程计和用于复杂密集重建和新颖视图合成的神经辐射场景表示。我们的系统使用稀疏视觉里程计初始化相机姿态，并从单目深度预测网络中获得与视图相关的密集几何先验。我们协调姿势的尺度和密集的几何体，将它们视为训练神经隐式场景表示的监督线索。NeRF VO通过联合优化关键帧姿势的滑动窗口和底层密集几何体，在场景表示的光度和几何保真度方面表现出非凡的性能，这是通过使用体渲染训练辐射场来实现的。我们在各种合成和真实世界数据集的姿态估计精度、新颖的视图合成保真度和密集的重建质量方面超过了最先进的方法，同时实现了更高的相机跟踪频率和更少的GPU内存消耗。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13471v1" target="_blank">2312.13471v1</a>
                              </td>
                              <td>NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields</td>
                              <td>Jens Naumann</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13471v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13471v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14972v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's GPT-4 with Self-Hosted Open Source SLMs in Production</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14972v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14972v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14972v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine both the quality and the performance characteristics of modern SLMs relative to an existing customer-facing OpenAI-based implementation. We find that across 9 SLMs and 29 variants, we observe competitive quality-of-results for our use case, significant performance consistency improvement, and a cost reduction of 5x-29x when compared to OpenAI GPT-4.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14972v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多公司依靠OpenAI的GPT-4等托管人工智能模型的API在其产品中创建人工智能体验。除了易用性和缩短生产时间的好处外，这种对专有API的依赖在模型控制、性能可靠性、运行时间可预测性和成本方面也有缺点。与此同时，出现了一系列可供商业使用的开源小语言模型（SLM）。然而，它们取代现有能力的准备情况尚不清楚，而且还没有现成的系统方法来测试这些模型。在本文中，我们提出了现代开源SLM的系统评估方法和特征，以及在将专有LLM API替换为真实世界的产品功能时的权衡。我们设计了SLaM，这是一种自动化分析工具，能够利用任意SLM对产品特征进行定量和定性测试。使用SLaM，我们检查了现代SLM相对于现有面向客户的基于OpenAI的实现的质量和性能特征。我们发现，在9个SLM和29个变体中，与OpenAI GPT-4相比，我们观察到用例的结果质量具有竞争力，性能一致性显著提高，成本降低了5x-29x。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14972v1" target="_blank">2312.14972v1</a>
                              </td>
                              <td>Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's GPT-4 with Self-Hosted Open Source SLMs in Production</td>
                              <td>Chandra Irugalbandara</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14972v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14972v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13385v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ORBSLAM3-Enhanced Autonomous Toy Drones: Pioneering Indoor Exploration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13385v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13385v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13385v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigating toy drones through uncharted GPS-denied indoor spaces poses significant difficulties due to their reliance on GPS for location determination. In such circumstances, the necessity for achieving proper navigation is a primary concern. In response to this formidable challenge, we introduce a real-time autonomous indoor exploration system tailored for drones equipped with a monocular \emph{RGB} camera.   Our system utilizes \emph{ORB-SLAM3}, a state-of-the-art vision feature-based SLAM, to handle both the localization of toy drones and the mapping of unmapped indoor terrains. Aside from the practicability of \emph{ORB-SLAM3}, the generated maps are represented as sparse point clouds, making them prone to the presence of outlier data. To address this challenge, we propose an outlier removal algorithm with provable guarantees. Furthermore, our system incorporates a novel exit detection algorithm, ensuring continuous exploration by the toy drone throughout the unfamiliar indoor environment. We also transform the sparse point to ensure proper path planning using existing path planners.   To validate the efficacy and efficiency of our proposed system, we conducted offline and real-time experiments on the autonomous exploration of indoor spaces. The results from these endeavors demonstrate the effectiveness of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13385v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于玩具无人机依赖GPS进行定位，因此在未经GPS验证的室内空间导航玩具无人机会带来重大困难。在这种情况下，实现适当导航的必要性是一个主要问题。为了应对这一严峻挑战，我们推出了一种实时自主室内探索系统，该系统专为配备单目摄像机的无人机量身定制。我们的系统利用\emph｛ORB-SLAM3｝，一种最先进的基于视觉特征的SLAM，来处理玩具无人机的定位和未映射的室内地形的映射。除了\emph{ORB-SLAM3}的实用性之外，生成的地图被表示为稀疏点云，这使得它们容易出现异常数据。为了应对这一挑战，我们提出了一种具有可证明保证的异常值去除算法。此外，我们的系统结合了一种新颖的出口检测算法，确保玩具无人机在陌生的室内环境中不断探索。我们还转换稀疏点，以确保使用现有路径规划器进行正确的路径规划。为了验证我们提出的系统的有效性和效率，我们对室内空间的自主探索进行了离线和实时实验。这些努力的结果证明了我们的方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13385v1" target="_blank">2312.13385v1</a>
                              </td>
                              <td>ORBSLAM3-Enhanced Autonomous Toy Drones: Pioneering Indoor Exploration</td>
                              <td>Murad Tukan</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13385v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13385v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13162v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Brain-Inspired Visual Odometry: Balancing Speed and Interpretability through a System of Systems Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13162v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13162v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13162v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, we address the critical challenge of balancing speed and accuracy while maintaining interpretablity in visual odometry (VO) systems, a pivotal aspect in the field of autonomous navigation and robotics. Traditional VO systems often face a trade-off between computational speed and the precision of pose estimation. To tackle this issue, we introduce an innovative system that synergistically combines traditional VO methods with a specifically tailored fully connected network (FCN). Our system is unique in its approach to handle each degree of freedom independently within the FCN, placing a strong emphasis on causal inference to enhance interpretability. This allows for a detailed and accurate assessment of relative pose error (RPE) across various degrees of freedom, providing a more comprehensive understanding of parameter variations and movement dynamics in different environments. Notably, our system demonstrates a remarkable improvement in processing speed without compromising accuracy. In certain scenarios, it achieves up to a 5% reduction in Root Mean Square Error (RMSE), showcasing its ability to effectively bridge the gap between speed and accuracy that has long been a limitation in VO research. This advancement represents a significant step forward in developing more efficient and reliable VO systems, with wide-ranging applications in real-time navigation and robotic systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13162v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们解决了视觉里程计（VO）系统在保持可解释性的同时平衡速度和准确性的关键挑战，这是自主导航和机器人领域的一个关键方面。传统的VO系统经常面临计算速度和姿态估计精度之间的权衡。为了解决这个问题，我们引入了一种创新系统，将传统的VO方法与专门定制的全连接网络（FCN）协同结合。我们的系统在FCN中独立处理每个自由度的方法上是独特的，非常强调因果推理以增强可解释性。这允许对不同自由度的相对姿态误差（RPE）进行详细而准确的评估，从而更全面地了解不同环境中的参数变化和运动动力学。值得注意的是，我们的系统在不影响精度的情况下显著提高了处理速度。在某些情况下，它实现了高达5%的均方根误差（RMSE）降低，展示了其有效弥合速度和精度之间差距的能力，而这一直是VO研究的局限。这一进步标志着在开发更高效、更可靠的VO系统方面迈出了重要一步，在实时导航和机器人系统中有着广泛的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13162v1" target="_blank">2312.13162v1</a>
                              </td>
                              <td>Brain-Inspired Visual Odometry: Balancing Speed and Interpretability through a System of Systems Approach</td>
                              <td>Habib Boloorchi Tabrizi</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13162v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13162v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13005v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accurate Gaussian-Process-based Distance Fields with applications to Echolocation and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13005v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13005v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13005v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a novel method to estimate distance fields from noisy point clouds using Gaussian Process (GP) regression. Distance fields, or distance functions, gained popularity for applications like point cloud registration, odometry, SLAM, path planning, shape reconstruction, etc. A distance field provides a continuous representation of the scene defined as the shortest distance from any query point and the closest surface. The key concept of the proposed method is the transformation of a GP-inferred latent scalar field into an accurate distance field by using a reverting function related to the kernel inverse. The latent field can be interpreted as a smooth occupancy map. This paper provides the theoretical derivation of the proposed method as well as a novel uncertainty proxy for the distance estimates. The improved performance compared with existing distance fields is demonstrated with simulated experiments. The level of accuracy of the proposed approach enables novel applications that rely on precise distance estimation: this work presents echolocation and mapping frameworks for ultrasonic-guided wave sensing in metallic structures. These methods leverage the proposed distance field with a physics-based measurement model accounting for the propagation of the ultrasonic waves in the material. Real-world experiments are conducted to demonstrate the soundness of these frameworks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13005v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种利用高斯过程（GP）回归估计噪声点云距离场的新方法。距离场或距离函数在点云配准、里程计、SLAM、路径规划、形状重建等应用中越来越受欢迎。距离场提供场景的连续表示，该场景定义为距离任何查询点和最近表面的最短距离。所提出的方法的关键概念是通过使用与核逆相关的回归函数将GP推断的潜在标量场转换为精确的距离场。潜场可以被解释为平滑的占有图。本文对所提出的方法进行了理论推导，并为距离估计提供了一种新的不确定性代理。仿真实验表明，与现有的距离场相比，性能有所提高。所提出的方法的精度水平使依赖于精确距离估计的新应用成为可能：这项工作为金属结构中的超声导波传感提供了回声定位和映射框架。这些方法利用所提出的距离场和基于物理的测量模型，考虑超声波在材料中的传播。进行了真实世界的实验来证明这些框架的可靠性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13005v3" target="_blank">2302.13005v3</a>
                              </td>
                              <td>Accurate Gaussian-Process-based Distance Fields with applications to Echolocation and Mapping</td>
                              <td>Cedric Le Gentil</td>
                              <td>2023-02-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13005v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13005v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12680v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Trajectory Approximation of Video Based on Phase Correlation for Forward Facing Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12680v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12680v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12680v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce an innovative approach for extracting trajectories from a camera sensor in GPS-denied environments, leveraging visual odometry. The system takes video footage captured by a forward-facing camera mounted on a vehicle as input, with the output being a chain code representing the camera's trajectory. The proposed methodology involves several key steps. Firstly, we employ phase correlation between consecutive frames of the video to extract essential information. Subsequently, we introduce a novel chain code method termed "dynamic chain code," which is based on the x-shift values derived from the phase correlation. The third step involves determining directional changes (forward, left, right) by establishing thresholds and extracting the corresponding chain code. This extracted code is then stored in a buffer for further processing. Notably, our system outperforms traditional methods reliant on spatial features, exhibiting greater speed and robustness in noisy environments. Importantly, our approach operates without external camera calibration information. Moreover, by incorporating visual odometry, our system enhances its accuracy in estimating camera motion, providing a more comprehensive understanding of trajectory dynamics. Finally, the system culminates in the visualization of the normalized camera motion trajectory.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12680v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了一种创新的方法，利用视觉里程计，在GPS拒绝的环境中从相机传感器中提取轨迹。该系统将安装在车辆上的前向摄像头拍摄的视频片段作为输入，输出是表示摄像头轨迹的链代码。拟议的方法涉及几个关键步骤。首先，我们利用视频连续帧之间的相位相关性来提取基本信息。随后，我们介绍了一种称为“动态链码”的新型链码方法，该方法基于从相位相关性导出的x偏移值。第三步涉及通过建立阈值并提取相应的链代码来确定方向变化（向前、向左、向右）。该提取的代码随后被存储在缓冲器中以供进一步处理。值得注意的是，我们的系统优于依赖于空间特征的传统方法，在噪声环境中表现出更高的速度和鲁棒性。重要的是，我们的方法在没有外部摄像头校准信息的情况下运行。此外，通过结合视觉里程计，我们的系统提高了估计相机运动的准确性，从而更全面地了解轨迹动力学。最后，该系统最终实现了标准化相机运动轨迹的可视化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12680v1" target="_blank">2312.12680v1</a>
                              </td>
                              <td>Trajectory Approximation of Video Based on Phase Correlation for Forward Facing Camera</td>
                              <td>Abdulkadhem A. Abdulkadhem</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12680v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12680v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12204v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhanced Unscented Kalman Filter-Based SLAM in Dynamic Environments: Euclidean Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12204v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12204v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12204v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an innovative approach to Simultaneous Localization and Mapping (SLAM) using the Unscented Kalman Filter (UKF) in a dynamic environment. The UKF is proven to be a robust estimator and demonstrates lower sensitivity to sensor data errors compared to alternative SLAM algorithms. However, conventional algorithms are primarily concerned with stationary landmarks, which might prevent localization in dynamic environments. This paper proposes an Euclidean-based method for handling moving landmarks, calculating and estimating distances between the robot and each moving landmark, and addressing sensor measurement conflicts. The approach is evaluated through simulations in MATLAB and comparing results with the conventional UKF-SLAM algorithm. We also introduce a dataset for filter-based algorithms in dynamic environments, which can be used as a benchmark for evaluating of future algorithms. The outcomes of the proposed algorithm underscore that this simple yet effective approach mitigates the disruptive impact of moving landmarks, as evidenced by a thorough examination involving parameters such as the number of moving and stationary landmarks, waypoints, and computational efficiency. We also evaluated our algorithms in a realistic simulation of a real-world mapping task. This approach allowed us to assess our methods in practical conditions and gain insights for future enhancements. Our algorithm surpassed the performance of all competing methods in the evaluation, showcasing its ability to excel in real-world mapping scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12204v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种在动态环境中使用无迹卡尔曼滤波器（UKF）进行同步定位和映射（SLAM）的创新方法。UKF被证明是一种稳健的估计器，与其他SLAM算法相比，它对传感器数据误差的敏感性较低。然而，传统的算法主要关注静止的地标，这可能会阻止动态环境中的定位。本文提出了一种基于欧几里得的方法来处理移动地标，计算和估计机器人与每个移动地标之间的距离，并解决传感器测量冲突。通过MATLAB仿真对该方法进行了评估，并与传统的UKF-SLAM算法进行了比较。我们还介绍了动态环境中基于滤波器的算法的数据集，该数据集可作为评估未来算法的基准。所提出的算法的结果强调，这种简单而有效的方法减轻了移动地标的破坏性影响，通过对移动和静止地标的数量、航路点和计算效率等参数的彻底检查证明了这一点。我们还在真实世界地图任务的真实模拟中评估了我们的算法。这种方法使我们能够在实际条件下评估我们的方法，并获得未来改进的见解。我们的算法在评估中超过了所有竞争方法的性能，展示了其在真实世界地图场景中的卓越能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12204v1" target="_blank">2312.12204v1</a>
                              </td>
                              <td>Enhanced Unscented Kalman Filter-Based SLAM in Dynamic Environments: Euclidean Approach</td>
                              <td>Masoud Dorvash</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12204v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12204v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_04787v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_04787v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_04787v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_04787v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we present a neural field-based real-time monocular mapping framework for accurate and dense Simultaneous Localization and Mapping (SLAM). Recent neural mapping frameworks show promising results, but rely on RGB-D or pose inputs, or cannot run in real-time. To address these limitations, our approach integrates dense-SLAM with neural implicit fields. Specifically, our dense SLAM approach runs parallel tracking and global optimization, while a neural field-based map is constructed incrementally based on the latest SLAM estimates. For the efficient construction of neural fields, we employ multi-resolution grid encoding and signed distance function (SDF) representation. This allows us to keep the map always up-to-date and adapt instantly to global updates via loop closing. For global consistency, we propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach to run online loop closing and mitigate the pose and scale drift. To enhance depth accuracy further, we incorporate learned monocular depth priors. We propose a novel joint depth and scale adjustment (JDSA) module to solve the scale ambiguity inherent in depth priors. Extensive evaluations across synthetic and real-world datasets validate that our approach outperforms existing methods in accuracy and map completeness while preserving real-time performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_04787v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们提出了一种基于神经场的实时单目映射框架，用于精确和密集的同时定位和映射（SLAM）。最近的神经映射框架显示出有希望的结果，但依赖于RGB-D或姿势输入，或者无法实时运行。为了解决这些局限性，我们的方法将密集SLAM与神经隐式场相结合。具体来说，我们的密集SLAM方法运行并行跟踪和全局优化，而基于神经场的映射是基于最新的SLAM估计逐步构建的。为了有效地构建神经场，我们采用了多分辨率网格编码和符号距离函数（SDF）表示。这使我们能够始终保持地图的最新状态，并通过循环关闭立即适应全球更新。为了全局一致性，我们提出了一种有效的基于Sim（3）的姿态图束调整（PGBA）方法来运行在线闭环并减轻姿态和尺度漂移。为了进一步提高深度精度，我们结合了学习的单目深度先验。我们提出了一种新的深度和尺度联合调整（JDSA）模块来解决深度先验中固有的尺度模糊性。对合成和真实世界数据集的广泛评估验证了我们的方法在准确性和地图完整性方面优于现有方法，同时保持了实时性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.04787v2" target="_blank">2310.04787v2</a>
                              </td>
                              <td>HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields</td>
                              <td>Wei Zhang</td>
                              <td>2023-10-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_04787v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.04787v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_09866v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_09866v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_09866v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_09866v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit scene representations have recently shown encouraging results in dense visual SLAM. However, existing methods produce low-quality scene reconstruction and low-accuracy localization performance when scaling up to large indoor scenes and long sequences. These limitations are mainly due to their single, global radiance field with finite capacity, which does not adapt to large scenarios. Their end-to-end pose networks are also not robust enough with the growth of cumulative errors in large scenes. To this end, we present PLGSLAM, a neural visual SLAM system which performs high-fidelity surface reconstruction and robust camera tracking in real time. To handle large-scale indoor scenes, PLGSLAM proposes a progressive scene representation method which dynamically allocates new local scene representation trained with frames within a local sliding window. This allows us to scale up to larger indoor scenes and improves robustness (even under pose drifts). In local scene representation, PLGSLAM utilizes tri-planes for local high-frequency features. We also incorporate multi-layer perceptron (MLP) networks for the low-frequency feature, smoothness, and scene completion in unobserved areas. Moreover, we propose local-to-global bundle adjustment method with a global keyframe database to address the increased pose drifts on long sequences. Experimental results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction results and tracking performance across various datasets and scenarios (both in small and large-scale indoor environments). The code will be open-sourced upon paper acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_09866v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经隐式场景表示最近在密集视觉SLAM中显示出令人鼓舞的结果。然而，当放大到大的室内场景和长序列时，现有方法产生低质量的场景重建和低精度的定位性能。这些限制主要是由于它们的单个全局辐射场具有有限的容量，不适用于大型场景。随着大型场景中累积误差的增长，它们的端到端姿态网络也不够健壮。为此，我们提出了PLGSLAM，这是一种神经视觉SLAM系统，可以实时执行高保真度表面重建和鲁棒的相机跟踪。为了处理大规模的室内场景，PLGSLAM提出了一种渐进式场景表示方法，该方法动态分配用局部滑动窗口内的帧训练的新的局部场景表示。这使我们能够放大到更大的室内场景，并提高鲁棒性（即使在姿势漂移的情况下）。在局部场景表示中，PLGSLAM利用三平面来进行局部高频特征。我们还结合了多层感知器（MLP）网络，用于未观察区域的低频特征、平滑度和场景完成。此外，我们提出了一种具有全局关键帧数据库的局部到全局束调整方法，以解决长序列上增加的姿态漂移问题。实验结果表明，PLGSLAM在各种数据集和场景（在小型和大型室内环境中）中实现了最先进的场景重建结果和跟踪性能。该代码将在论文验收后开源。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.09866v1" target="_blank">2312.09866v1</a>
                              </td>
                              <td>PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment</td>
                              <td>Tianchen Deng</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_09866v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.09866v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_09800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Event Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_09800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_09800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_09800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event cameras offer the exciting possibility of tracking the camera's pose during high-speed motion and in adverse lighting conditions. Despite this promise, existing event-based monocular visual odometry (VO) approaches demonstrate limited performance on recent benchmarks. To address this limitation, some methods resort to additional sensors such as IMUs, stereo event cameras, or frame-based cameras. Nonetheless, these additional sensors limit the application of event cameras in real-world devices since they increase cost and complicate system requirements. Moreover, relying on a frame-based camera makes the system susceptible to motion blur and HDR. To remove the dependency on additional sensors and to push the limits of using only a single event camera, we present Deep Event VO (DEVO), the first monocular event-only system with strong performance on a large number of real-world benchmarks. DEVO sparsely tracks selected event patches over time. A key component of DEVO is a novel deep patch selection mechanism tailored to event data. We significantly decrease the pose tracking error on seven real-world benchmarks by up to 97% compared to event-only methods and often surpass or are close to stereo or inertial methods. Code is available at https://github.com/tum-vision/DEVO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_09800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>事件摄像机提供了在高速运动和恶劣照明条件下跟踪摄像机姿势的令人兴奋的可能性。尽管有这一前景，但现有的基于事件的单目视觉里程计（VO）方法在最近的基准测试中表现出有限的性能。为了解决这一限制，一些方法采用额外的传感器，如IMU、立体事件摄像机或基于帧的摄像机。尽管如此，这些额外的传感器限制了事件摄像机在现实世界设备中的应用，因为它们增加了成本并使系统需求复杂化。此外，依赖基于帧的相机使系统容易受到运动模糊和HDR的影响。为了消除对额外传感器的依赖，并突破仅使用单个事件相机的限制，我们提出了Deep event VO（DEVO），这是第一个在大量真实世界基准测试上具有强大性能的单目仅事件系统。随着时间的推移，DEVO稀疏地跟踪选定的事件补丁。DEVO的一个关键组件是一种针对事件数据定制的新型深度补丁选择机制。与仅事件的方法相比，我们在七个真实世界基准上的姿态跟踪误差显著降低了97%，并且经常超过或接近立体或惯性方法。代码位于https://github.com/tum-vision/DEVO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.09800v1" target="_blank">2312.09800v1</a>
                              </td>
                              <td>Deep Event Visual Odometry</td>
                              <td>Simon Klenk</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_09800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.09800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tum-vision/devo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_03323v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_03323v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_03323v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_03323v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous vehicles must often contend with conflicting planning requirements, e.g., safety and comfort could be at odds with each other if avoiding a collision calls for slamming the brakes. To resolve such conflicts, assigning importance ranking to rules (i.e., imposing a rule hierarchy) has been proposed, which, in turn, induces rankings on trajectories based on the importance of the rules they satisfy. On one hand, imposing rule hierarchies can enhance interpretability, but introduce combinatorial complexity to planning; while on the other hand, differentiable reward structures can be leveraged by modern gradient-based optimization tools, but are less interpretable and unintuitive to tune. In this paper, we present an approach to equivalently express rule hierarchies as differentiable reward structures amenable to modern gradient-based optimizers, thereby, achieving the best of both worlds. We achieve this by formulating rank-preserving reward functions that are monotonic in the rank of the trajectories induced by the rule hierarchy; i.e., higher ranked trajectories receive higher reward. Equipped with a rule hierarchy and its corresponding rank-preserving reward function, we develop a two-stage planner that can efficiently resolve conflicting planning requirements. We demonstrate that our approach can generate motion plans in ~7-10 Hz for various challenging road navigation and intersection negotiation scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_03323v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动驾驶汽车经常必须应对相互冲突的规划要求，例如，如果避免碰撞需要急刹车，安全性和舒适性可能会相互矛盾。为了解决这种冲突，已经提出了将重要性排序分配给规则（即，施加规则层次结构），这反过来又根据它们所满足的规则的重要性对轨迹进行排序。一方面，强加规则层次结构可以提高可解释性，但会给规划带来组合复杂性；而另一方面，现代基于梯度的优化工具可以利用可微的奖励结构，但其可解释性较差，难以调整。在本文中，我们提出了一种方法，将规则层次等效地表示为适用于现代基于梯度的优化器的可微奖励结构，从而实现两全其美。我们通过制定在由规则层次引起的轨迹的秩中单调的秩保持奖励函数来实现这一点；即排名更高的轨迹获得更高的奖励。配备了规则层次结构及其相应的保秩奖励函数，我们开发了一个两阶段规划器，可以有效地解决冲突的规划需求。我们证明，对于各种具有挑战性的道路导航和交叉口协商场景，我们的方法可以在7-10Hz内生成运动计划。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.03323v3" target="_blank">2212.03323v3</a>
                              </td>
                              <td>Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles</td>
                              <td>Sushant Veer</td>
                              <td>2022-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_03323v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.03323v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nvlabs/rule-hierarchies" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07531v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07531v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07531v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07531v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The estimation of 3D human motion from video has progressed rapidly but current methods still have several key limitations. First, most methods estimate the human in camera coordinates. Second, prior work on estimating humans in global coordinates often assumes a flat ground plane and produces foot sliding. Third, the most accurate methods rely on computationally expensive optimization pipelines, limiting their use to offline applications. Finally, existing video-based methods are surprisingly less accurate than single-frame methods. We address these limitations with WHAM (World-grounded Humans with Accurate Motion), which accurately and efficiently reconstructs 3D human motion in a global coordinate system from video. WHAM learns to lift 2D keypoint sequences to 3D using motion capture data and fuses this with video features, integrating motion context and visual information. WHAM exploits camera angular velocity estimated from a SLAM method together with human motion to estimate the body's global trajectory. We combine this with a contact-aware trajectory refinement method that lets WHAM capture human motion in diverse conditions, such as climbing stairs. WHAM outperforms all existing 3D human motion recovery methods across multiple in-the-wild benchmarks. Code will be available for research purposes at http://wham.is.tue.mpg.de/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07531v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从视频中估计3D人体运动的方法进展迅速，但目前的方法仍有几个关键局限性。首先，大多数方法估计人体在相机坐标中的位置。其次，先前在全球坐标系中估计人类的工作通常假设地面平坦，并产生足部滑动。第三，最准确的方法依赖于计算成本高昂的优化管道，将其限制在离线应用程序中使用。最后，现有的基于视频的方法出人意料地不如单帧方法准确。我们通过WHAM（World grounded Humans with Accurate Motion）解决了这些限制，它可以从视频中准确有效地重建全球坐标系中的3D人体运动。WHAM学习使用运动捕捉数据将2D关键点序列提升到3D，并将其与视频特征融合，集成运动上下文和视觉信息。WHAM利用SLAM方法估计的相机角速度以及人体运动来估计身体的全局轨迹。我们将其与接触感知轨迹细化方法相结合，该方法可以让WHAM捕捉人类在不同条件下的运动，例如爬楼梯。WHAM在多个野外基准测试中优于所有现有的3D人体运动恢复方法。代码可用于研究目的，网址为http://wham.is.tue.mpg.de/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07531v1" target="_blank">2312.07531v1</a>
                              </td>
                              <td>WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</td>
                              <td>Soyong Shin</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07531v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07531v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08769v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08769v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08769v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08769v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper quantifies the performance of visual SLAM that leverages multi-scale fiducial markers (i.e., artificial landmarks that can be detected at a wide range of distances) to show its potential for reliable takeoff and landing navigation in rotorcraft. Prior work has shown that square markers with a black-and-white pattern of grid cells can be used to improve the performance of visual SLAM with color cameras. We extend this prior work to allow nested marker layouts. We evaluate performance during semi-autonomous takeoff and landing operations in a variety of environmental conditions by a DJI Matrice 300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to obtain ground truth pose estimates. Performance measures include absolute trajectory error and the fraction of the number of estimated poses to the total frame. We release all of our results -- our dataset and the code of the implementation of the visual SLAM with fiducial markers -- to the public as open-source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08769v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文量化了视觉SLAM的性能，该视觉SLAM利用多尺度基准标记（即可以在宽距离内检测到的人工地标）来显示其在旋翼飞机中可靠起飞和着陆导航的潜力。先前的工作已经表明，具有黑白网格单元图案的方形标记可以用于提高彩色相机的视觉SLAM的性能。我们扩展了之前的工作，允许嵌套标记布局。我们使用带有两个FLIR Blackfly彩色相机的DJI Matrice 300 RTK旋翼机，使用RTK GNSS获得地面真实姿态估计，评估了在各种环境条件下半自主起飞和着陆操作期间的性能。性能度量包括绝对轨迹误差和估计姿态数量占总帧的比例。我们以开源的形式向公众发布我们的所有结果——我们的数据集和带有基准标记的可视化SLAM的实现代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08769v3" target="_blank">2309.08769v3</a>
                              </td>
                              <td>The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft</td>
                              <td>Jongwon Lee</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08769v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08769v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06991v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06991v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06991v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06991v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the advancement in robotics, it is becoming increasingly common for large factories and warehouses to incorporate visual SLAM (vSLAM) enabled automated robots that operate closely next to humans. This makes any adversarial attacks on vSLAM components potentially detrimental to humans working alongside them. Loop Closure Detection (LCD) is a crucial component in vSLAM that minimizes the accumulation of drift in mapping, since even a small drift can accumulate into a significant drift over time. A prior work by Kim et al., SymbioLCD2, unified visual features and semantic objects into a single graph structure for finding loop closure candidates. While this provided a performance improvement over visual feature-based LCD, it also created a single point of vulnerability for potential graph-based adversarial attacks. Unlike previously reported visual-patch based attacks, small graph perturbations are far more challenging to detect, making them a more significant threat. In this paper, we present Adversarial-LCD, a novel black-box evasion attack framework that employs an eigencentrality-based perturbation method and an SVM-RBF surrogate model with a Weisfeiler-Lehman feature extractor for attacking graph-based LCD. Our evaluation shows that the attack performance of Adversarial-LCD with the SVM-RBF surrogate model was superior to that of other machine learning surrogate algorithms, including SVM-linear, SVM-polynomial, and Bayesian classifier, demonstrating the effectiveness of our attack framework. Furthermore, we show that our eigencentrality-based perturbation method outperforms other algorithms, such as Random-walk and Shortest-path, highlighting the efficiency of Adversarial-LCD's perturbation selection method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06991v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着机器人技术的进步，大型工厂和仓库越来越普遍地采用视觉SLAM（vSLAM）实现的自动化机器人，这些机器人在人类旁边运行。这使得对vSLAM组件的任何对抗性攻击都可能对与它们一起工作的人类有害。环路闭合检测（LCD）是vSLAM中的一个关键组件，它可以最大限度地减少映射中的漂移积累，因为随着时间的推移，即使是很小的漂移也会积累成显著的漂移。Kim等人先前的一项工作SymbioLCD2将视觉特征和语义对象统一到一个单一的图结构中，用于寻找循环闭包候选者。虽然这比基于视觉特征的LCD提供了性能改进，但它也为潜在的基于图形的对抗性攻击创建了单点漏洞。与之前报道的基于视觉补丁的攻击不同，小的图形扰动更难检测，使其成为更重要的威胁。在本文中，我们提出了对抗性LCD，一种新的黑匣子规避攻击框架，该框架采用了基于特征中心性的扰动方法和具有Weisfeiler-Lehman特征提取器的SVM-RBF代理模型来攻击基于图的LCD。我们的评估表明，SVM-RBF代理模型的对抗性LCD的攻击性能优于其他机器学习代理算法，包括SVM线性、SVM多项式和贝叶斯分类器，证明了我们的攻击框架的有效性。此外，我们还表明，我们的基于特征中心性的扰动方法优于其他算法，如随机游动和最短路径，突出了对抗性LCD的扰动选择方法的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06991v1" target="_blank">2312.06991v1</a>
                              </td>
                              <td>Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection</td>
                              <td>Jonathan J. Y. Kim</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06991v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06991v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06741v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Splatting SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06741v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06741v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们首次将3D高斯散射应用于使用单个移动单目或RGB-D相机的增量3D重建。我们的同步定位和映射（SLAM）方法以3fps实时运行，使用高斯作为唯一的3D表示，统一了所需的表示，以实现准确、高效的跟踪、映射和高质量渲染。需要一些创新来从现场摄像机连续重建具有高保真度的3D场景。首先，为了超越最初的3DGS算法，该算法需要来自离线运动结构（SfM）系统的精确姿态，我们使用针对3D高斯的直接优化来制定3DGS的相机跟踪，并表明这能够实现快速而稳健的跟踪，并具有广泛的收敛范围。其次，通过利用高斯的显式性质，我们引入了几何验证和正则化来处理增量三维密集重建中出现的模糊性。最后，我们介绍了一个完整的SLAM系统，它不仅在新的视图合成和轨迹估计方面取得了最先进的结果，而且还重建了微小甚至透明的物体。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06741v1" target="_blank">2312.06741v1</a>
                              </td>
                              <td>Gaussian Splatting SLAM</td>
                              <td>Hidenobu Matsuki</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06741v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05889v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuperPrimitive: Scene Reconstruction at a Primitive Level</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05889v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05889v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其计算复杂性和固有的视觉模糊性，从一组图像或单目视频中进行联合相机姿态和密集几何估计仍然是一个具有挑战性的问题。大多数密集增量重建系统直接对图像像素进行操作，并使用多视图几何提示来求解其3D位置。这种像素级方法存在模糊性或违反多视图一致性的问题（例如，由无纹理或镜面引起）。我们用一种新的图像表示来解决这个问题，我们称之为超原始。超基元是通过将图像分割成语义相关的局部区域并用估计的表面法线方向对其进行增强来获得的，这两者都是由最先进的单图像神经网络预测的。这提供了每个SuperPrimitive的局部几何体估计，而它们的相对位置是基于多视图观测进行调整的。我们通过解决三个3D重建任务来展示我们新表示的多功能性：深度完成、运动中的少量视图结构和单目密集视觉里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05889v1" target="_blank">2312.05889v1</a>
                              </td>
                              <td>SuperPrimitive: Scene Reconstruction at a Primitive Level</td>
                              <td>Kirill Mazur</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05889v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05889v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07894v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iSLAM: Imperative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07894v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07894v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07894v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization and Mapping (SLAM) stands as one of the critical challenges in robot navigation. A SLAM system often consists of a front-end component for motion estimation and a back-end system for eliminating estimation drift. Recent advancements suggest that data-driven methods are highly effective for front-end tasks, while geometry-based methods continue to be essential in the back-end processes. However, such a decoupled paradigm between the data-driven front-end and geometry-based back-end can lead to sub-optimal performance, consequently reducing system capabilities and generalization potential. To solve this problem, we proposed a novel self-supervised imperative learning framework, named imperative SLAM (iSLAM), which fosters reciprocal correction between the front-end and back-end, thus enhancing performance without necessitating any external supervision. Specifically, we formulate the SLAM problem as a bilevel optimization so that the front-end and back-end are bidirectionally connected. As a result, the front-end model can learn global geometric knowledge obtained through pose graph optimization by back-propagating the residuals from the back-end component. We showcase the effectiveness of this new framework through an application of stereo-inertial SLAM. The experiments show that the iSLAM training strategy achieves an accuracy improvement of 22% on average over a baseline model. To the best of our knowledge, iSLAM is the first SLAM system showing that the front-end and back-end can mutually correct each other in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07894v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人导航中的一个关键挑战。SLAM系统通常包括用于运动估计的前端组件和用于消除估计漂移的后端系统。最近的进展表明，数据驱动的方法对前端任务非常有效，而基于几何结构的方法在后端过程中仍然至关重要。然而，数据驱动的前端和基于几何结构的后端之间的这种解耦模式可能会导致次优性能，从而降低系统能力和泛化潜力。为了解决这个问题，我们提出了一种新的自监督命令式学习框架，称为命令式SLAM（iSLAM），它促进了前端和后端之间的相互校正，从而在不需要任何外部监督的情况下提高了性能。具体来说，我们将SLAM问题公式化为双层优化，使前端和后端双向连接。因此，前端模型可以通过反向传播来自后端组件的残差来学习通过位姿图优化获得的全局几何知识。我们通过立体惯性SLAM的应用展示了这种新框架的有效性。实验表明，与基线模型相比，iSLAM训练策略的准确率平均提高了22%。据我们所知，iSLAM是第一个表明前端和后端可以以自我监督的方式相互校正的SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07894v4" target="_blank">2306.07894v4</a>
                              </td>
                              <td>iSLAM: Imperative SLAM</td>
                              <td>Taimeng Fu</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07894v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07894v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sair-lab/islam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04031v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Importance of Coordinate Frames in Dynamic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04031v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04031v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04031v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. While expressing object points in a local reference frame may seem intuitive, it may not necessarily lead to the most accurate and robust solutions. This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. To this end, we introduce a front-end agnostic framework using GTSAM that can be used to evaluate various Dynamic SLAM formulations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04031v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数同时定位和地图绘制（SLAM）系统传统上假设了一个静态世界，这与现实世界的场景不一致。为了使机器人能够在动态环境中安全地导航和规划，必须使用能够处理移动物体的表示。动态SLAM是SLAM研究中的一个新兴领域，因为它提高了整个系统的精度，同时提供了对物体运动的额外估计。最新的文献提供了动态SLAM的两种主要公式，表示世界或对象坐标系中的动态对象点。虽然在局部参考系中表达对象点可能看起来很直观，但它可能不一定能得出最准确、最稳健的解决方案。本文对各种动态SLAM公式进行了深入分析，确定了解决该问题的最佳方法。为此，我们引入了一个使用GTSAM的前端不可知框架，该框架可用于评估各种动态SLAM公式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04031v1" target="_blank">2312.04031v1</a>
                              </td>
                              <td>The Importance of Coordinate Frames in Dynamic SLAM</td>
                              <td>Jesse Morris</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04031v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04031v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10070v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10070v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10070v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10070v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new dense simultaneous localization and mapping (SLAM) method that uses Gaussian splats as a scene representation. The new representation enables interactive-time reconstruction and photo-realistic rendering of real-world and synthetic scenes. We propose novel strategies for seeding and optimizing Gaussian splats to extend their use from multiview offline scenarios to sequential monocular RGBD input data setups. In addition, we extend Gaussian splats to encode geometry and experiment with tracking against this scene representation. Our method achieves state-of-the-art rendering quality on both real-world and synthetic datasets while being competitive in reconstruction performance and runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10070v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的密集同时定位和映射（SLAM）方法，该方法使用高斯飞溅作为场景表示。新的表示方式实现了真实世界和合成场景的交互式时间重建和照片真实感渲染。我们提出了播种和优化高斯散射的新策略，以将其使用范围从多视点离线场景扩展到顺序单目RGBD输入数据设置。此外，我们扩展了高斯飞溅来编码几何体，并对这种场景表示进行了跟踪实验。我们的方法在真实世界和合成数据集上实现了最先进的渲染质量，同时在重建性能和运行时间方面具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10070v1" target="_blank">2312.10070v1</a>
                              </td>
                              <td>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting</td>
                              <td>Vladimir Yugay</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10070v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10070v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02684v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02684v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02684v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02684v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Point clouds have shown significant potential in various domains, including Simultaneous Localization and Mapping (SLAM). However, existing approaches either rely on dense point clouds to achieve high localization accuracy or use generalized descriptors to reduce map size. Unfortunately, these two aspects seem to conflict with each other. To address this limitation, we propose a unified architecture, DeepPointMap, achieving excellent preference on both aspects. We utilize neural network to extract highly representative and sparse neural descriptors from point clouds, enabling memory-efficient map representation and accurate multi-scale localization tasks (e.g., odometry and loop-closure). Moreover, we showcase the versatility of our framework by extending it to more challenging multi-agent collaborative SLAM. The promising results obtained in these scenarios further emphasize the effectiveness and potential of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02684v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>点云在包括同步定位和映射（SLAM）在内的各个领域都显示出了巨大的潜力。然而，现有的方法要么依靠密集的点云来实现高定位精度，要么使用广义描述符来减小地图大小。不幸的是，这两个方面似乎相互冲突。为了解决这一限制，我们提出了一个统一的体系结构DeepPointMap，在这两个方面都实现了卓越的偏好。我们利用神经网络从点云中提取具有高度代表性和稀疏性的神经描述符，从而实现高效记忆的地图表示和准确的多尺度定位任务（例如里程计和环路闭合）。此外，我们通过将框架扩展到更具挑战性的多智能体协作SLAM，展示了其多功能性。在这些场景中获得的有希望的结果进一步强调了我们方法的有效性和潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02684v1" target="_blank">2312.02684v1</a>
                              </td>
                              <td>DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors</td>
                              <td>Xiaze Zhang</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02684v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02684v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02599v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor Positioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02599v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02599v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02599v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A Magnetic field Aided Inertial Navigation System (MAINS) for indoor navigation is proposed in this paper. MAINS leverages an array of magnetometers to measure spatial variations in the magnetic field, which are then used to estimate the displacement and orientation changes of the system, thereby aiding the inertial navigation system (INS). Experiments show that MAINS significantly outperforms the stand-alone INS, demonstrating a remarkable two orders of magnitude reduction in position error. Furthermore, when compared to the state-of-the-art magnetic-field-aided navigation approach, the proposed method exhibits slightly improved horizontal position accuracy. On the other hand, it has noticeably larger vertical error on datasets with large magnetic field variations. However, one of the main advantages of MAINS compared to the state-of-the-art is that it enables flexible sensor configurations. The experimental results show that the position error after 2 minutes of navigation in most cases is less than 3 meters when using an array of 30 magnetometers. Thus, the proposed navigation solution has the potential to solve one of the key challenges faced with current magnetic-field simultaneous localization and mapping (SLAM) solutions: the very limited allowable length of the exploration phase during which unvisited areas are mapped.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02599v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于室内导航的磁场辅助惯性导航系统。MAINS利用磁强计阵列来测量磁场的空间变化，然后用于估计系统的位移和方向变化，从而帮助惯性导航系统（INS）。实验表明，MAINS明显优于独立的INS，位置误差显著降低了两个数量级。此外，与最先进的磁场辅助导航方法相比，该方法的水平位置精度略有提高。另一方面，在磁场变化较大的数据集上，它具有明显较大的垂直误差。然而，与最先进的技术相比，MAINS的主要优势之一是它能够实现灵活的传感器配置。实验结果表明，当使用30个磁强计的阵列时，导航2分钟后的位置误差在大多数情况下小于3米。因此，所提出的导航解决方案有可能解决当前磁场同步定位和测绘（SLAM）解决方案面临的一个关键挑战：测绘未访问区域的勘探阶段的允许长度非常有限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02599v1" target="_blank">2312.02599v1</a>
                              </td>
                              <td>MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor Positioning</td>
                              <td>Chuan Huang</td>
                              <td>2023-12-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02599v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02599v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/huang-chuan/mainsvsmagekf" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02353v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient 2D Graph SLAM for Sparse Sensing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02353v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02353v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02353v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) plays a vital role in mapping unknown spaces and aiding autonomous navigation. Virtually all state-of-the-art solutions today for 2D SLAM are designed for dense and accurate sensors such as laser range-finders (LiDARs). However, these sensors are not suitable for resource-limited nano robots, which become increasingly capable and ubiquitous nowadays, and these robots tend to mount economical and low-power sensors that can only provide sparse and noisy measurements. This introduces a challenging problem called SLAM with sparse sensing. This work addresses the problem by adopting the form of the state-of-the-art graph-based SLAM pipeline with a novel frontend and an improvement for loop closing in the backend, both of which are designed to work with sparse and uncertain range data. Experiments show that the maps constructed by our algorithm have superior quality compared to prior works on sparse sensing. Furthermore, our method is capable of running in real-time on a modern PC with an average processing time of 1/100th the input interval time.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02353v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位与映射（SLAM）在绘制未知空间和辅助自主导航方面发挥着至关重要的作用。如今，几乎所有最先进的2D SLAM解决方案都是为密集和精确的传感器设计的，如激光测距仪（LiDAR）。然而，这些传感器不适合资源有限的纳米机器人，如今，纳米机器人的能力越来越强，无处不在，而且这些机器人往往安装经济、低功耗的传感器，只能提供稀疏和嘈杂的测量。这引入了一个具有挑战性的问题，称为具有稀疏感测的SLAM。这项工作通过采用最先进的基于图的SLAM流水线的形式来解决这个问题，该流水线具有新颖的前端和对后端闭环的改进，这两种方法都被设计用于处理稀疏和不确定范围的数据。实验表明，与先前的稀疏传感工作相比，我们的算法构建的地图具有更好的质量。此外，我们的方法能够在现代PC上实时运行，平均处理时间为输入间隔时间的1/100。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02353v1" target="_blank">2312.02353v1</a>
                              </td>
                              <td>Efficient 2D Graph SLAM for Sparse Sensing</td>
                              <td>Hanzhi Zhou</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02353v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02353v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/shiftlab-nanodrone/sparse-gslam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02141v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iMatching: Imperative Correspondence Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02141v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02141v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning feature correspondence is a foundational task in computer vision, holding immense importance for downstream applications such as visual odometry and 3D reconstruction. Despite recent progress in data-driven models, feature correspondence learning is still limited by the lack of accurate per-pixel correspondence labels. To overcome this difficulty, we introduce a new self-supervised scheme, imperative learning (IL), for training feature correspondence. It enables correspondence learning on arbitrary uninterrupted videos without any camera pose or depth labels, heralding a new era for self-supervised correspondence learning. Specifically, we formulated the problem of correspondence learning as a bilevel optimization, which takes the reprojection error from bundle adjustment as a supervisory signal for the model. To avoid large memory and computation overhead, we leverage the stationary point to effectively back-propagate the implicit gradients through bundle adjustment. Through extensive experiments, we demonstrate superior performance on tasks including feature matching and pose estimation, in which we obtained an average of 30% accuracy gain over the state-of-the-art matching models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02141v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习特征对应关系是计算机视觉的一项基础任务，对视觉里程计和三维重建等下游应用具有极其重要的意义。尽管最近在数据驱动模型方面取得了进展，但由于缺乏准确的每像素对应标签，特征对应学习仍然受到限制。为了克服这一困难，我们引入了一种新的自监督方案，即命令式学习（IL），用于训练特征对应关系。它可以在没有任何相机姿势或深度标签的情况下，在任意不间断的视频上进行函授学习，预示着自我监督函授学习的新时代。具体来说，我们将对应学习问题公式化为双层优化，将束调整的重投影误差作为模型的监督信号。为了避免大的内存和计算开销，我们利用驻点通过束调整有效地反向传播隐式梯度。通过广泛的实验，我们在包括特征匹配和姿态估计在内的任务上表现出了卓越的性能，在这些任务中，我们获得了比最先进的匹配模型平均30%的精度增益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02141v1" target="_blank">2312.02141v1</a>
                              </td>
                              <td>iMatching: Imperative Correspondence Learning</td>
                              <td>Zitong Zhan</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02141v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02141v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_05236v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05236v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our world is full of identical objects (\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05236v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的世界充满了相同的物体（例如，可乐罐、同一型号的汽车）。当这些重复出现在一起时，为我们有效地推理3D提供了额外而有力的线索。受这一观察结果的启发，我们引入了“重复结构”（SfD），这是一种新颖的逆图形框架，可以从包含多个相同对象的单个图像中重建几何体、材料和照明。SfD首先识别图像中对象的多个实例，然后联合估计所有实例的6DoF姿势。随后使用反向图形管道来联合推理对象的形状、材质和环境光，同时遵守实例之间的共享几何图形和材质约束。我们的主要贡献包括利用对象副本作为单图像逆图形的鲁棒先验，并提出用于联合6-DoF对象姿态估计的平面内旋转鲁棒运动结构（SfM）公式。通过利用来自单个图像的多视图线索，SfD生成了更真实、更详细的3D重建，显著优于具有相似或更多观测值的现有单个图像重建模型和多视图重建方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05236v1" target="_blank">2401.05236v1</a>
                              </td>
                              <td>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</td>
                              <td>Tianhang Cheng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05236v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05236v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tianhang-cheng/sfd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03450v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Classification of Critical Configurations for any Number of Projective Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03450v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03450v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是从一组图像中恢复有关相机和3D场景的信息的过程。通常，在无噪声设置中，如果提供了足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，也不可能进行独特的恢复；这些被称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明了它们形成了众所周知的代数变体，如二次曲面和次数最多为4的曲线。本文还改进了早期的结果，发现了以前未知的关键构型，并表明一些以前认为是关键的构型实际上不是。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03450v1" target="_blank">2401.03450v1</a>
                              </td>
                              <td>A Classification of Critical Configurations for any Number of Projective Views</td>
                              <td>Martin Bråtelund</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03450v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03450v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mabraate/critical-configurations" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_11153v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Research on Multilingual Natural Scene Text Detection Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_11153v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural scene text detection is a significant challenge in computer vision, with tremendous potential applications in multilingual, diverse, and complex text scenarios. We propose a multilingual text detection model to address the issues of low accuracy and high difficulty in detecting multilingual text in natural scenes. In response to the challenges posed by multilingual text images with multiple character sets and various font styles, we introduce the SFM Swin Transformer feature extraction network to enhance the model's robustness in detecting characters and fonts across different languages. Dealing with the considerable variation in text scales and complex arrangements in natural scene text images, we present the AS-HRFPN feature fusion network by incorporating an Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module. The feature fusion network improvements enhance the model's ability to detect text sizes and orientations. Addressing diverse backgrounds and font variations in multilingual scene text images is a challenge for existing methods. Limited local receptive fields hinder detection performance. To overcome this, we propose a Global Semantic Segmentation Branch, extracting and preserving global features for more effective text detection, aligning with the need for comprehensive information. In this study, we collected and built a real-world multilingual natural scene text image dataset and conducted comprehensive experiments and analyses. The experimental results demonstrate that the proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher than the baseline model. We also conducted extensive cross-dataset validation on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of our approach. The code and dataset can be found at https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_11153v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然场景文本检测是计算机视觉中的一个重大挑战，在多语言、多样化和复杂的文本场景中具有巨大的潜在应用。我们提出了一种多语言文本检测模型，以解决在自然场景中检测多语言文本的准确性低和难度高的问题。为了应对具有多个字符集和各种字体样式的多语言文本图像带来的挑战，我们引入了SFM Swin Transformer特征提取网络，以增强模型在检测不同语言的字符和字体时的鲁棒性。针对自然场景文本图像中文本尺度的巨大变化和复杂排列，我们结合自适应空间特征融合模块和空间金字塔池模块，提出了AS-HRFPN特征融合网络。特征融合网络的改进增强了模型检测文本大小和方向的能力。解决多语言场景文本图像中的不同背景和字体变化是对现有方法的挑战。有限的局部感受野阻碍了检测性能。为了克服这一点，我们提出了一个全局语义分割分支，提取并保留全局特征，以实现更有效的文本检测，从而满足对全面信息的需求。在本研究中，我们收集并构建了一个真实世界的多语言自然场景文本图像数据集，并进行了全面的实验和分析。实验结果表明，该算法的F测度为85.02%，比基线模型高4.71%。我们还对MSRA-TD500、ICDAR2017MLT和ICDAR2015数据集进行了广泛的跨数据集验证，以验证我们方法的通用性。代码和数据集位于https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.11153v2" target="_blank">2312.11153v2</a>
                              </td>
                              <td>Research on Multilingual Natural Scene Text Detection Algorithm</td>
                              <td>Tao Wang</td>
                              <td>2023-12-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_11153v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.11153v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code: \url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。代码：\url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v3" target="_blank">2306.09012v3</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v3" target="_blank">2310.03704v3</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15471v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Residual Learning for Image Point Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15471v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local image feature descriptors have had a tremendous impact on the development and application of computer vision methods. It is therefore unsurprising that significant efforts are being made for learning-based image point descriptors. However, the advantage of learned methods over handcrafted methods in real applications is subtle and more nuanced than expected. Moreover, handcrafted descriptors such as SIFT and SURF still perform better point localization in Structure-from-Motion (SfM) compared to many learned counterparts. In this paper, we propose a very simple and effective approach to learning local image descriptors by using a hand-crafted detector and descriptor. Specifically, we choose to learn only the descriptors, supported by handcrafted descriptors while discarding the point localization head. We optimize the final descriptor by leveraging the knowledge already present in the handcrafted descriptor. Such an approach of optimization allows us to discard learning knowledge already present in non-differentiable functions such as the hand-crafted descriptors and only learn the residual knowledge in the main network branch. This offers 50X convergence speed compared to the standard baseline architecture of SuperPoint while at inference the combined descriptor provides superior performance over the learned and hand-crafted descriptors. This is done with minor increase in the computations over the baseline learned descriptor. Our approach has potential applications in ensemble learning and learning with non-differentiable functions. We perform experiments in matching, camera localization and Structure-from-Motion in order to showcase the advantages of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15471v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部图像特征描述符对计算机视觉方法的发展和应用产生了巨大的影响。因此，对基于学习的图像点描述符做出重大努力并不令人惊讶。然而，在实际应用中，学习方法相对于手工制作方法的优势是微妙的，而且比预期的更微妙。此外，与许多学习的描述符相比，手工制作的描述符（如SIFT和SURF）在运动结构（SfM）中仍然执行更好的点定位。在本文中，我们提出了一种非常简单有效的方法，通过使用手工制作的检测器和描述符来学习局部图像描述符。具体来说，我们选择只学习描述符，由手工制作的描述符支持，同时丢弃点定位头。我们通过利用手工制作的描述符中已经存在的知识来优化最终描述符。这种优化方法允许我们丢弃已经存在于不可微函数中的学习知识，例如手工制作的描述符，并且只学习主网络分支中的剩余知识。与SuperPoint的标准基线架构相比，这提供了50倍的收敛速度，而在推理时，组合描述符提供了优于学习和手工制作的描述符的性能。这是在计算量比基线学习描述符略有增加的情况下完成的。我们的方法在集成学习和具有不可微函数的学习中具有潜在的应用。我们在匹配、相机定位和运动结构方面进行了实验，以展示我们方法的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15471v1" target="_blank">2312.15471v1</a>
                              </td>
                              <td>Residual Learning for Image Point Descriptors</td>
                              <td>Rashik Shrestha</td>
                              <td>2023-12-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15471v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15471v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13977v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13977v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13977v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经隐函数在多视图重建领域取得了显著的成果。然而，大多数现有的方法都是为密集视图量身定制的，并且在处理稀疏视图时表现出不令人满意的性能。已经提出了几种最新的方法来推广隐式重建以解决稀疏视图重建任务，但它们仍然存在较高的训练成本，并且仅在精心选择的视角下有效。在本文中，我们提出了一种新的稀疏视图重建框架，该框架利用表面先验来实现高度忠实的表面重建。具体而言，我们设计了全局几何对齐和局部几何细化的几个约束条件，以共同优化粗略形状和精细细节。为了实现这一点，我们训练神经网络从SfM获得的表面点学习全局隐式场，然后将其作为粗略的几何约束。为了利用局部几何一致性，我们将表面上的点投影到可见和不可见的视图上，将投影特征的一致丢失视为精细的几何约束。DTU和BlendedMVS数据集在两种流行的稀疏设置中的实验结果表明，与最先进的方法相比，有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13977v2" target="_blank">2312.13977v2</a>
                              </td>
                              <td>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</td>
                              <td>Han Huang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13977v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13977v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10529v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformers in Unsupervised Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10529v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers have revolutionized deep learning based computer vision with improved performance as well as robustness to natural corruptions and adversarial attacks. Transformers are used predominantly for 2D vision tasks, including image classification, semantic segmentation, and object detection. However, robots and advanced driver assistance systems also require 3D scene understanding for decision making by extracting structure-from-motion (SfM). We propose a robust transformer-based monocular SfM method that learns to predict monocular pixel-wise depth, ego vehicle's translation and rotation, as well as camera's focal length and principal point, simultaneously. With experiments on KITTI and DDAD datasets, we demonstrate how to adapt different vision transformers and compare them against contemporary CNN-based methods. Our study shows that transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust against natural corruptions, as well as untargeted and targeted attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10529v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers通过提高性能以及对自然腐蚀和对抗性攻击的鲁棒性，彻底改变了基于深度学习的计算机视觉。转换器主要用于2D视觉任务，包括图像分类、语义分割和对象检测。然而，机器人和先进的驾驶员辅助系统也需要3D场景理解，以便通过从运动中提取结构（SfM）来进行决策。我们提出了一种基于稳健变换器的单目SfM方法，该方法可以同时学习预测单目像素深度、自我车辆的平移和旋转以及相机的焦距和主点。通过在KITTI和DDAD数据集上的实验，我们展示了如何适应不同的视觉变换器，并将其与当代基于CNN的方法进行比较。我们的研究表明，基于转换器的体系结构虽然运行时效率较低，但可以实现相当的性能，同时对自然损坏以及无目标和有针对性的攻击更具鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10529v1" target="_blank">2312.10529v1</a>
                              </td>
                              <td>Transformers in Unsupervised Structure-from-Motion</td>
                              <td>Hemang Chawla</td>
                              <td>2023-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10529v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10529v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neurai-lab/mt-sfmlearner" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-light image enhancement is a crucial visual task, and many unsupervised methods tend to overlook the degradation of visible information in low-light scenes, which adversely affects the fusion of complementary information and hinders the generation of satisfactory results. To address this, our study introduces ``Enlighten-Your-Voice'', a multimodal enhancement framework that innovatively enriches user interaction through voice and textual commands. This approach does not merely signify a technical leap but also represents a paradigm shift in user engagement. Our model is equipped with a Dual Collaborative Attention Module (DCAM) that meticulously caters to distinct content and color discrepancies, thereby facilitating nuanced enhancements. Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play module that synergizes semantic context with low-light enhancement operations, sharpening the algorithm's efficacy. Crucially, ``Enlighten-Your-Voice'' showcases remarkable generalization in unsupervised zero-shot scenarios. The source code can be accessed from https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱光图像增强是一项至关重要的视觉任务，许多无监督方法往往忽略了弱光场景中可见信息的退化，这对互补信息的融合产生了不利影响，并阻碍了令人满意的结果的产生。为了解决这一问题，我们的研究引入了“启发你的声音”，这是一个多模式增强框架，通过语音和文本命令创新地丰富了用户交互。这种方法不仅意味着技术上的飞跃，而且代表着用户参与度的范式转变。我们的模型配备了双协作注意力模块（DCAM），该模块可精心处理不同的内容和颜色差异，从而促进细微的增强。作为补充，我们引入了一个语义特征融合（SFM）即插即用模块，该模块将语义上下文与弱光增强操作协同，提高了算法的有效性。至关重要的是，“启蒙青年之声”在无监督的零样本场景中表现出了显著的泛化能力。源代码可以从https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10109v1" target="_blank">2312.10109v1</a>
                              </td>
                              <td>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</td>
                              <td>Xiaofeng Zhang</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10109v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhangbaijin/enlighten-your-voice" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08863v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08863v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the reconstruction of high-fidelity 3D head models from static portrait image has made great progress. However, most methods require multi-view or multi-illumination information, which therefore put forward high requirements for data acquisition. In this paper, we study the reconstruction of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid structure from motion (NRSFM) methods have been widely used to solve such problems according to the two-dimensional correspondence between different frames. However, the inaccurate correspondence caused by high-complex hair structures and various facial expression changes would heavily influence the reconstruction accuracy. To tackle these problems, we propose a prior-guided dynamic implicit neural network. Specifically, we design a two-part dynamic deformation field to transform the current frame space to the canonical one. We further model the head geometry in the canonical space with a learnable signed distance field (SDF) and optimize it using the volumetric rendering with the guidance of two-main head priors to improve the reconstruction accuracy and robustness. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate the effectiveness and robustness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08863v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，从静态人像图像重建高保真三维头部模型取得了很大进展。然而，大多数方法都需要多视图或多照明信息，因此对数据采集提出了很高的要求。在本文中，我们研究了从任意单目视频中重建高保真3D头部模型。根据不同框架之间的二维对应关系，非刚性运动结构（NRSFM）方法已被广泛用于解决这些问题。然而，高度复杂的头发结构和各种面部表情变化导致的不准确对应将严重影响重建的准确性。为了解决这些问题，我们提出了一种先验引导的动态隐式神经网络。具体来说，我们设计了一个由两部分组成的动态变形场，将当前帧空间转换为规范帧空间。我们进一步用可学习的符号距离场（SDF）在正则空间中对头部几何结构进行建模，并在两个主要头部先验的指导下使用体积渲染对其进行优化，以提高重建精度和鲁棒性。广泛的消融研究和与最先进方法的比较证明了我们提出的方法的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08863v1" target="_blank">2312.08863v1</a>
                              </td>
                              <td>HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</td>
                              <td>Xueying Wang</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08863v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08760v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08760v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice. To overcome this limitation, we propose a novel \underline{c}amera parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion (SfM). Given a sequence of images, CF-NeRF estimates the camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset NeRFBuster which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to camera rotation and achieves state-of-the-art results without providing prior information and constraints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08760v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）在新的视图合成中表现出了令人印象深刻的性能。然而，NeRF及其大多数变体仍然依赖于传统的复杂管道来提供外在和内在的相机参数，如COLMAP。最近的工作，如NeRFmm、BARF和L2G NeRF，直接将相机参数视为可学习的，并通过差分体绘制进行估计。然而，这些方法适用于具有轻微运动的前瞻性场景，在实践中无法解决旋转场景。为了克服这一限制，我们提出了一种新颖的下划线{c}amera参数\下划线{f}ree神经辐射场（CF NeRF），其增量重建3D表示并从运动中恢复受增量结构启发的相机参数（SfM）。给定一系列图像，CF-NeRF逐个估计图像的相机参数，并通过初始化、隐式定位和隐式优化重建场景。为了评估我们的方法，我们使用了一个具有挑战性的真实世界数据集NeRFBuster，该数据集提供了复杂轨迹下的12个场景。结果表明，CF-NeRF对相机旋转具有鲁棒性，并且在不提供先验信息和约束的情况下实现了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08760v1" target="_blank">2312.08760v1</a>
                              </td>
                              <td>CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</td>
                              <td>Qingsong Yan</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08760v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08760v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COLMAP-Free 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然神经渲染在场景重建和新颖的视图合成方面取得了令人印象深刻的进展，但它在很大程度上依赖于精确预计算的相机姿态。为了放松这一限制，已经做出了多项努力来训练神经辐射场（NeRF），而不需要预处理相机姿势。然而，NeRF的隐式表示为同时优化3D结构和相机姿态提供了额外的挑战。另一方面，鉴于其明确的点云表示，最近提出的3D高斯飞溅提供了新的机会。本文利用输入视频流的显式几何表示和连续性来执行新的视图合成，而无需任何SfM预处理。我们以顺序的方式处理输入帧，并通过一次获取一个输入帧来逐步增长3D高斯集，而无需预先计算相机姿势。在大的运动变化下，我们的方法在视图合成和相机姿态估计方面比以前的方法有了显著的改进。我们的项目页面是https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07504v1" target="_blank">2312.07504v1</a>
                              </td>
                              <td>COLMAP-Free 3D Gaussian Splatting</td>
                              <td>Yang Fu</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06865v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06865v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes the incorporation of techniques from stereophotoclinometry (SPC) into a keypoint-based structure-from-motion (SfM) system to estimate the surface normal and albedo at detected landmarks to improve autonomous surface and shape characterization of small celestial bodies from in-situ imagery. In contrast to the current state-of-the-practice method for small body shape reconstruction, i.e., SPC, which relies on human-in-the-loop verification and high-fidelity a priori information to achieve accurate results, we forego the expensive maplet estimation step and instead leverage dense keypoint measurements and correspondences from an autonomous keypoint detection and matching method based on deep learning to provide the necessary photogrammetric constraints. Moreover, we develop a factor graph-based approach allowing for simultaneous optimization of the spacecraft's pose, landmark positions, Sun-relative direction, and surface normals and albedos via fusion of Sun sensor measurements and image keypoint measurements. The proposed framework is validated on real imagery of the Cornelia crater on Asteroid 4 Vesta, along with pose estimation and mapping comparison against an SPC reconstruction, where we demonstrate precise alignment to the SPC solution without relying on any a priori camera pose and topography information or humans-in-the-loop</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06865v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文建议将立体摄影测斜（SPC）技术纳入基于关键点的运动结构（SfM）系统，以估计探测到的地标的表面法线和反照率，从而从原位图像中改进小型天体的自主表面和形状特征。与小体型重建的实践方法（即SPC）的当前状态相反，SPC依赖于人在环验证和高保真度先验信息来实现准确的结果，我们放弃了昂贵的maplet估计步骤，而是利用基于深度学习的自主关键点检测和匹配方法的密集关键点测量和对应关系来提供必要的摄影测量约束。此外，我们开发了一种基于因子图的方法，通过融合太阳传感器测量和图像关键点测量，可以同时优化航天器的姿态、地标位置、太阳相对方向以及表面法线和反照率。所提出的框架在小行星4灶神星上科妮利亚陨石坑的真实图像上得到了验证，同时还进行了姿态估计和与SPC重建的映射比较，在SPC重建中，我们展示了与SPC解决方案的精确对准，而不依赖于任何先验的相机姿态和地形信息或环中的人类</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06865v1" target="_blank">2312.06865v1</a>
                              </td>
                              <td>Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</td>
                              <td>Travis Driver</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06865v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06865v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06741v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Splatting SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06741v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06741v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们首次将3D高斯散射应用于使用单个移动单目或RGB-D相机的增量3D重建。我们的同步定位和映射（SLAM）方法以3fps实时运行，使用高斯作为唯一的3D表示，统一了所需的表示，以实现准确、高效的跟踪、映射和高质量渲染。需要一些创新来从现场摄像机连续重建具有高保真度的3D场景。首先，为了超越最初的3DGS算法，该算法需要来自离线运动结构（SfM）系统的精确姿态，我们使用针对3D高斯的直接优化来制定3DGS的相机跟踪，并表明这能够实现快速而稳健的跟踪，并具有广泛的收敛范围。其次，通过利用高斯的显式性质，我们引入了几何验证和正则化来处理增量三维密集重建中出现的模糊性。最后，我们介绍了一个完整的SLAM系统，它不仅在新的视图合成和轨迹估计方面取得了最先进的结果，而且还重建了微小甚至透明的物体。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06741v1" target="_blank">2312.06741v1</a>
                              </td>
                              <td>Gaussian Splatting SLAM</td>
                              <td>Hidenobu Matsuki</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06741v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v3" target="_blank">2308.08479v3</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05889v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuperPrimitive: Scene Reconstruction at a Primitive Level</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05889v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05889v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其计算复杂性和固有的视觉模糊性，从一组图像或单目视频中进行联合相机姿态和密集几何估计仍然是一个具有挑战性的问题。大多数密集增量重建系统直接对图像像素进行操作，并使用多视图几何提示来求解其3D位置。这种像素级方法存在模糊性或违反多视图一致性的问题（例如，由无纹理或镜面引起）。我们用一种新的图像表示来解决这个问题，我们称之为超原始。超基元是通过将图像分割成语义相关的局部区域并用估计的表面法线方向对其进行增强来获得的，这两者都是由最先进的单图像神经网络预测的。这提供了每个SuperPrimitive的局部几何体估计，而它们的相对位置是基于多视图观测进行调整的。我们通过解决三个3D重建任务来展示我们新表示的多功能性：深度完成、运动中的少量视图结构和单目密集视觉里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05889v1" target="_blank">2312.05889v1</a>
                              </td>
                              <td>SuperPrimitive: Scene Reconstruction at a Primitive Level</td>
                              <td>Kirill Mazur</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05889v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05889v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v3" target="_blank">2311.17245v3</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04563v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Geometry Grounded Deep Structure From Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04563v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04563v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构（SfM）是计算机视觉界的一个长期问题，其目的是从一组不受约束的2D图像中重建场景的相机姿态和3D结构。经典框架通过检测和匹配关键点、配准图像、三角测量3D点和进行束调整，以增量的方式解决了这个问题。最近的研究工作主要围绕着利用深度学习技术的力量来增强特定元素（例如，关键点匹配），但仍基于原始的、不可微分的管道。相反，我们提出了一种新的深度流水线VGGSfM，其中每个组件都是完全可微的，因此可以以端到端的方式进行训练。为此，我们引入了新的机制和简化。首先，我们在深度2D点跟踪的最新进展的基础上提取可靠的像素精确轨迹，这消除了对成对匹配进行链接的需要。此外，我们根据图像和跟踪特征同时恢复所有相机，而不是逐渐注册相机。最后，我们优化相机，并通过可微分束调整层对3D点进行三角测量。我们在三个流行的数据集上获得了最先进的性能，即CO3D、IMC Phototourism和ETH3D。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04563v1" target="_blank">2312.04563v1</a>
                              </td>
                              <td>Visual Geometry Grounded Deep Structure From Motion</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04563v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04634v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04634v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04634v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一种可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04634v1" target="_blank">2311.04634v1</a>
                              </td>
                              <td>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</td>
                              <td>Linus Franke</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04634v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04634v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lfranke/vet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的照片真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。代码位于：https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从内窥镜视频中生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对自监督鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，并且轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中进行高质量的3D对象重建。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从随意图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们认为NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于单目相机重建的现有技术主要依赖于运动结构（SfM）流水线。然而，这种方法往往会产生缺乏关键尺度信息的重建结果，随着时间的推移，图像的积累会导致不可避免的漂移问题。相比之下，基于激光雷达扫描的地图绘制方法由于其精确的距离测量而在大规模城市场景重建中很受欢迎，而这在基于视觉的方法中根本不具备。研究人员试图利用激光雷达和相机的同时测量，在地图绘制结果中追求精确的缩放和颜色细节。然而，结果受到外部校准和时间同步精度的影响。在本文中，我们提出了一种新的具有成本效益的重建管道，该管道利用预先建立的激光雷达图作为固定约束，以有效解决单目相机重建中存在的固有规模挑战。据我们所知，我们的方法是第一个将图像配准到点云图上，而不需要同步捕获相机和激光雷达数据，这使我们能够灵活地管理各个感兴趣区域的重建细节水平。为了促进该领域的进一步研究，我们发布了Colmap PCD$｛^｛3｝｝$，这是一款利用Colmap算法的开源工具，可以将图像精确地精细配准到点云地图上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05504v1" target="_blank">2310.05504v1</a>
                              </td>
                              <td>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</td>
                              <td>Chunge Bai</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaobaiiiiii/colmap-pcd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05134v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05134v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05134v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a critical task in mobile robotics, and researchers are continuously developing new approaches to enhance its efficiency. In this article, we propose a novel approach to improve the accuracy of visual localization using Structure from Motion (SfM) techniques. We highlight the limitations of global SfM, which suffers from high latency, and the challenges of local SfM, which requires large image databases for accurate reconstruction. To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as opposed to image databases, to cut down on the space required for storage. We suggest that sampling reference images around the prior query position can lead to further improvements. We evaluate the accuracy of our proposed method against ground truth obtained using LIDAR and Advanced Lidar Odometry and Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM with COLMAP in the conducted experiments. Our proposed method achieves an accuracy of 0.068 meters compared to the ground truth, which is slightly lower than the most advanced method COLMAP, which has an accuracy of 0.022 meters. However, the size of the database required for COLMAP is 400 megabytes, whereas the size of our NeRF model is only 160 megabytes. Finally, we perform an ablation study to assess the impact of using reference images from the NeRF reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05134v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是移动机器人的一项关键任务，研究人员正在不断开发新的方法来提高其效率。在本文中，我们提出了一种使用运动结构（SfM）技术提高视觉定位准确性的新方法。我们强调了全局SfM的局限性，它具有高延迟，以及局部SfM面临的挑战，后者需要大型图像数据库才能进行精确重建。为了解决这些问题，我们建议利用神经辐射场（NeRF），而不是图像数据库，来减少存储所需的空间。我们建议，对先前查询位置周围的参考图像进行采样可以带来进一步的改进。我们根据使用激光雷达和高级激光雷达实时测距和测绘（A-LOAM）获得的地面实况评估了我们提出的方法的准确性，并在所进行的实验中比较了其相对于局部SfM和COLMAP的存储使用情况。与地面实况相比，我们提出的方法实现了0.068米的精度，这略低于最先进的方法COLMAP，后者的精度为0.022米。然而，COLMAP所需的数据库大小为400兆字节，而我们的NeRF模型的大小仅为160兆字节。最后，我们进行了消融研究，以评估使用NeRF重建的参考图像的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05134v1" target="_blank">2310.05134v1</a>
                              </td>
                              <td>LocoNeRF: A NeRF-based Approach for Local Structure from Motion for Precise Localization</td>
                              <td>Artem Nenashev</td>
                              <td>2023-10-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05134v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_04145v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_04145v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_04145v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_04145v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用未校准的多摄像机系统从运动中构建结构是一项具有挑战性的任务。本文提出了一种束平差解决方案，该解决方案实现了一个基线约束，即这些相机彼此静止。我们假设这些相机安装在移动平台上，未经校准，并且粗略同步。为此，我们提出了针对相机具有重叠视图的场景制定的基线约束。该约束被纳入束调整解决方案中，以保持不同相机的相对运动静止。实验使用安装在车辆上的两个并置GoPro相机的视频帧进行，无需系统校准。这两台摄像机被放置在拍摄重叠内容的位置。我们使用所提出的约束进行了束调整，然后生成了3D密集点云。通过将这些密集点云与激光雷达参考数据进行比较来进行评估。我们表明，与传统的束平差相比，我们提出的方法实现了29.38%的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.04145v2" target="_blank">2204.04145v2</a>
                              </td>
                              <td>Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems</td>
                              <td>Debao Huang</td>
                              <td>2022-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_04145v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.04145v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01092v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01092v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01092v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the top ranked solution for the AISG-SLA Visual Localisation Challenge benchmark (IJCAI 2023), where the task is to estimate relative motion between images taken in sequence by a camera mounted on a car driving through an urban scene.   For matching images we use our recent deep learning based matcher RoMa. Matching image pairs sequentially and estimating relative motion from point correspondences sampled by RoMa already gives very competitive results -- third rank on the challenge benchmark.   To improve the estimations we extract keypoints in the images, match them using RoMa, and perform structure from motion reconstruction using COLMAP. We choose our recent DeDoDe keypoints for their high repeatability. Further, we address time jumps in the image sequence by matching specific non-consecutive image pairs based on image retrieval with DINOv2. These improvements yield a solution beating all competitors.   We further present a loose upper bound on the accuracy obtainable by the image retrieval approach by also matching hand-picked non-consecutive pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01092v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为AISG-SLA视觉定位挑战基准（IJCAI 2023）提供了排名最高的解决方案，其中的任务是估计安装在汽车上的摄像头在城市场景中依次拍摄的图像之间的相对运动。对于匹配图像，我们使用最近的基于深度学习的匹配器RoMa。按顺序匹配图像对，并根据RoMa采样的点对应关系估计相对运动，已经给出了非常有竞争力的结果——在挑战基准上排名第三。为了改进估计，我们提取图像中的关键点，使用RoMa进行匹配，并使用COLMAP从运动重建中执行结构。我们选择了最近的DeDoDe关键点，因为它们具有很高的可重复性。此外，我们通过将基于图像检索的特定非连续图像对与DINOv2进行匹配来解决图像序列中的时间跳跃问题。这些改进产生了一个击败所有竞争对手的解决方案。我们进一步提出了图像检索方法通过匹配手工挑选的非连续对所获得的精度的宽松上限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01092v1" target="_blank">2310.01092v1</a>
                              </td>
                              <td>Leveraging Cutting Edge Deep Learning Based Image Matching for Reconstructing a Large Scene from Sparse Images</td>
                              <td>Georg Bökman</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01092v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01092v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00783v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Propagating Semantic Labels in Video Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00783v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00783v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00783v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义分割结合了两个子任务：像素级图像掩码的识别和对掩码应用语义标签。最近，引入了所谓的基础模型；在非常大的数据集上训练的通用模型，这些数据集可以专门化并应用于更具体的任务。一个这样的模型，分段任意模型（SAM），执行图像分割。像CLIPSeg和MaskRCNN这样的语义分割系统是在成对片段和语义标签的数据集上训练的。但是，手动标记自定义数据非常耗时。这项工作提出了一种对视频中的对象进行分割的方法。一旦在视频帧中找到对象，则可以将该片段传播到未来的帧；从而减少了手动注释的工作量。该方法将SAM与运动结构（SfM）相结合。首先使用SfM将输入到系统的视频重构为3D几何结构。然后使用SAM对视频帧进行分割。然后将SAM识别的片段投影到重建的3D几何体上。在随后的视频帧中，标记的3D几何体被重新投影到新的透视图中，从而减少SAM的调用次数。评估系统性能，包括SAM和SfM组件的贡献。性能通过三个主要指标进行评估：计算时间、带有手动标签的掩码IOU和跟踪丢失数量。结果表明，该系统在视频帧上跟踪对象的计算时间大大提高了人类的性能，但性能较差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00783v1" target="_blank">2310.00783v1</a>
                              </td>
                              <td>Propagating Semantic Labels in Video Data</td>
                              <td>David Balaban</td>
                              <td>2023-10-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00783v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00783v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16632v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse Submodular Function Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16632v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16632v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we study the problem of minimizing a submodular function $f : 2^V \rightarrow \mathbb{R}$ that is guaranteed to have a $k$-sparse minimizer. We give a deterministic algorithm that computes an additive $\epsilon$-approximate minimizer of such $f$ in $\widetilde{O}(\mathsf{poly}(k) \log(|f|/\epsilon))$ parallel depth using a polynomial number of queries to an evaluation oracle of $f$, where $|f| = \max_{S \subseteq V} |f(S)|$. Further, we give a randomized algorithm that computes an exact minimizer of $f$ with high probability using $\widetilde{O}(|V| \cdot \mathsf{poly}(k))$ queries and polynomial time. When $k = \widetilde{O}(1)$, our algorithms use either nearly-constant parallel depth or a nearly-linear number of evaluation oracle queries. All previous algorithms for this problem either use $\Omega(|V|)$ parallel depth or $\Omega(|V|^2)$ queries.   In contrast to state-of-the-art weakly-polynomial and strongly-polynomial time algorithms for SFM, our algorithms use first-order optimization methods, e.g., mirror descent and follow the regularized leader. We introduce what we call {\em sparse dual certificates}, which encode information on the structure of sparse minimizers, and both our parallel and sequential algorithms provide new algorithmic tools for allowing first-order optimization methods to efficiently compute them. Correspondingly, our algorithm does not invoke fast matrix multiplication or general linear system solvers and in this sense is more combinatorial than previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16632v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们研究了子模函数$f:2^V\rightarrow\mathbb｛R｝$的最小化问题，该子模函数保证具有$k$-稀疏极小值。我们给出了一个确定性算法，该算法使用对$f$的评估预言的多项式查询数来计算$\widetilde｛O｝（\mathsf｛poly｝（k）\log（|f|/\epsilon））$并行深度中的$f$近似极小值，其中$|f|=\max_｛S\substeq V｝|f（S）|$。此外，我们给出了一个随机算法，该算法使用$\widetilde{O}（|V|\cdot\mathsf{poly}（k））$查询和多项式时间以高概率计算$f$的精确极小值。当$k=\widetilde｛O｝（1）$时，我们的算法使用几乎恒定的并行深度或几乎线性数量的评估oracle查询。以前针对此问题的所有算法都使用$\Omega（|V|）$并行深度或$\Omega（|V|^2）$查询。与最先进的SFM弱多项式和强多项式时间算法相比，我们的算法使用一阶优化方法，例如镜像下降和遵循正则化前导。我们介绍了我们所称的｛\em稀疏双证书｝，它对关于稀疏最小化器结构的信息进行编码，并且我们的并行和顺序算法都提供了新的算法工具，允许一阶优化方法有效地计算它们。相应地，我们的算法不调用快速矩阵乘法或一般线性系统求解器，并且在这个意义上比以前最先进的方法更具组合性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16632v1" target="_blank">2309.16632v1</a>
                              </td>
                              <td>Sparse Submodular Function Minimization</td>
                              <td>Andrei Graur</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16632v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16632v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13772v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Segmentation from a Moving Monocular Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13772v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13772v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13772v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当存在未知的摄像机运动、不同类型的物体运动和复杂的场景结构时，从移动的单目摄像机中识别和分割移动物体是困难的。为了应对这些挑战，我们利用了单目运动分割方法的两个流行分支：基于点轨迹的方法和基于光流的方法，通过在对象级别协同融合这两个高度互补的运动线索。通过这样做，我们能够同时对不同场景结构中的各种复杂物体运动进行建模，这是现有方法无法实现的。我们首先利用对象识别、分割和跟踪方面的最新基础模型，获得视频中每个常见对象的特定对象点轨迹和光流掩模。然后，我们使用极线几何和光流提供的运动信息构建了两个稳健的仿射矩阵，表示整个视频中的成对对象运动仿射。最后，使用共正则化多视图谱聚类来融合两个亲和矩阵，得到最终的聚类结果。我们的方法在KT3DMoSeg数据集上显示了最先进的性能，该数据集包含复杂的运动和场景结构。当使用视觉SLAM或SFM时，能够识别移动物体使我们能够将其移除以用于地图构建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13772v1" target="_blank">2309.13772v1</a>
                              </td>
                              <td>Motion Segmentation from a Moving Monocular Camera</td>
                              <td>Yuxiang Huang</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13772v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13772v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12804v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12804v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12804v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12804v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>珊瑚礁是地球上最多样化的生态系统之一，数亿人依赖珊瑚礁。不幸的是，大多数珊瑚礁都受到全球气候变化和当地人为压力的威胁。为了更好地了解珊瑚礁退化背后的动力学，以高空间和时间分辨率进行监测是关键。然而，由于需要大量的体力劳动，量化珊瑚覆盖率和物种丰度的传统监测方法在规模上受到限制。尽管计算机视觉工具已被用于帮助这一过程，特别是用于3D地图绘制的SfM摄影测量和用于图像分割的深度神经网络，但数据产品的分析造成了瓶颈，有效地限制了其可扩展性。本文提出了一种从自我运动视频映射水下环境的新范式，将使用机器学习来适应水下具有挑战性的条件的3D映射系统与图像语义分割的现代方法相结合。该方法以红海亚喀巴湾北部的珊瑚礁为例，展示了前所未有的高精度3D语义映射，大大降低了所需的劳动力成本：用廉价的消费级相机在潜水5分钟内获取的100米视频样带可以在5分钟内全自动分析。我们的方法通过向视频样带的全自动分析迈出了一大步，大大扩大了珊瑚礁监测的规模。该方法通过减少劳动力、设备、物流和计算成本，使珊瑚礁断面民主化。这有助于更有效地为保护政策提供信息。基于运动结构学习的基本计算方法对珊瑚礁以外的水下环境的快速低成本测绘具有广泛的意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12804v1" target="_blank">2309.12804v1</a>
                              </td>
                              <td>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</td>
                              <td>Jonathan Sauder</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12804v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12804v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕捉的同时运行在线SfM，新拍摄的动态图像是用相应的姿势和点进行在线估计的，即，你捕捉到的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以实现在以在线方式拍摄的同时稳健地配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v1" target="_blank">2309.11883v1</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10748v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10748v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10748v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10748v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的手-物体交互数据集显示出有限的真实物体可变性，并依赖于拟合MANO参数模型来获得真实的手形。为了超越这些限制并推动进一步的研究，我们引入了SHOWMe数据集，该数据集由96个视频组成，用真实和详细的手对象3D纹理网格进行注释。根据最近的工作，我们考虑了一个刚性手对象场景，其中手相对于对象的姿势在整个视频序列中保持不变。这一假设使我们能够将亚毫米精度的地面实况3D扫描注册到SHOWMe中的图像序列中。尽管更简单，但这一假设在所需精度和细节水平很重要的应用中是有意义的，例如，人机协作中的对象移交、对象扫描或操作和接触点分析。重要的是，手对象系统的刚性允许使用由刚性配准步骤和多视图重建（MVR）部分组成的两阶段流水线来处理未知手持对象的基于视频的3D重建。我们仔细评估了这两个阶段的一组非平凡基线，并表明使用SfM工具箱或手部姿态估计器来恢复刚性变换和现成的MVR算法，可以实现有前景的对象不可知的3D手部对象重建。然而，这些方法对初始相机姿态估计仍然敏感，由于对象上缺乏纹理或手的严重遮挡，初始相机姿态评估可能不精确，这为重建留下了改进的空间。代码和数据集可在https://europe.naverlabs.com/research/showme</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10748v1" target="_blank">2309.10748v1</a>
                              </td>
                              <td>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</td>
                              <td>Anilkumar Swamy</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10748v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10748v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10269v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10269v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10269v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV with a Humminbird single beam echosounder and Teledyne FLIR camera at Lake ESTI at the Texas A&M Engineering Extension Service Disaster City complex.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10269v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>非通航河流和蓄水池在缓冲社区免受洪水侵袭方面发挥着重要作用，但应急规划者往往没有数据表明它们在淹没周围地区之前可以携带的水量。本文描述了一种实用的方法，即使用未折叠的海洋表面飞行器（USV）收集水深图和浅水堤的数字表面图，并将其合并为一个统一的体积模型。将泊松曲面重建算法应用于水下表面的稀疏声纳深度读数，开发了水线下网格。河岸的密集水线上网格是使用商业运动结构（SfM）包创建的。由于许多原因，合并是具有挑战性的，最重要的是传感器覆盖范围的差距，即USV无法收集声纳深度数据或视觉上看到通向堤岸的沙滩，因此两个网格可能不会相交。该方法在Hydronalix EMILY USV上进行了演示，该V带有Humminbird单波束回声测深仪和Teledyne FLIR相机，位于德克萨斯州农工工程扩展服务灾难城市综合体的ESTI湖。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10269v1" target="_blank">2309.10269v1</a>
                              </td>
                              <td>Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water</td>
                              <td>Jayesh Tripathi</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10269v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10269v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08927v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08927v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08927v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08927v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用神经辐射场（NeRF）进行动态重建需要精确的相机姿态。这些通常很难用现有的运动结构（SfM）管道来检索，因为相机和场景内容都可能发生变化。我们提出了DynaMoN，它利用同步定位和映射（SLAM）与运动掩蔽相结合来处理动态场景内容。我们基于SLAM的稳健跟踪模块显著加快了动态NeRF的训练过程，同时提高了合成视图的质量。对TUM RGB-D、BONN RGB-D Dynamic和DyCheck的iPhone数据集这三个真实世界的数据集进行了广泛的实验验证，显示了DynaMoN在相机姿态估计和新颖视图合成方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08927v1" target="_blank">2309.08927v1</a>
                              </td>
                              <td>DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</td>
                              <td>Mert Asim Karaoglu</td>
                              <td>2023-09-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08927v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08927v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_05319v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Print Debugging to Improve Code Generation in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05319v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05319v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05319v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a "print debugging" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05319v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在代码生成任务方面取得了重大进展，但它们在处理复杂数据结构和算法的编程问题方面的性能仍然不理想。为了解决这个问题，我们提出了一种上下文学习方法，该方法通过使用“打印调试”方法来指导LLM进行调试，该方法包括插入打印语句来跟踪和分析日志，以修复错误。我们收集了一个Leetcode问题数据集，并使用Leetcode在线判断系统对我们的方法进行了评估。GPT-4的实验证明了我们方法的有效性，在简单和中等级别的Leetcode问题上分别比橡皮鸭调试好1.5%和17.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05319v1" target="_blank">2401.05319v1</a>
                              </td>
                              <td>Leveraging Print Debugging to Improve Code Generation in Large Language Models</td>
                              <td>Xueyu Hu</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05319v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05319v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05302v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05302v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05302v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05302v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example "Given a robot's behavior X, would the human observer find it explicable?". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05302v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型在各种自然语言和生成任务中表现出了非凡的生成能力。然而，可能的拟人化和对失败案例的宽容推动了对大语言模型涌现能力的讨论，尤其是对大语言模式中心理理论能力的讨论。虽然存在一些错误信念测试来验证推断和维护另一个实体的心理模型的能力，但我们研究了ToM能力的一个特殊应用，它具有更高的风险和可能不可逆转的后果：人机交互。在这项工作中，我们探索了感知行为识别的任务，其中机器人采用大型语言模型（LLM）以类似于人类观察者的方式评估机器人生成的行为。我们关注四种行为类型，即可解释、可阅读、可预测和模糊行为，这些行为已被广泛用于合成可解释的机器人行为。因此，LLM的目标是成为代理的人类代理，并回答某个代理行为将如何被循环中的人类感知，例如“给定机器人的行为X，人类观察者会发现它是可解释的吗？”。我们进行了一项人类受试者研究，以验证用户能够在五个领域的精心策划的情况下（机器人设置和计划）正确回答这样的问题。信念测试的第一个分析产生了非常积极的结果，夸大了人们对LLM拥有ToM能力的期望。然后，我们提出并执行了一套打破这种错觉的扰动测试，即不一致信念、不一致上下文和信念测试。我们得出的结论是，LLM在香草提示上的高分显示了它在HRI设置中的潜在用途，然而，在LLM缺乏的情况下，拥有ToM要求对琐碎或无关的扰动保持不变。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05302v1" target="_blank">2401.05302v1</a>
                              </td>
                              <td>Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?</td>
                              <td>Mudit Verma</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05302v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05302v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05300v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">I am a Strange Dataset: Metalinguistic Tests for Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05300v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05300v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05300v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Statements involving metalinguistic self-reference ("This paper has six sections.") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present "I am a Strange Dataset", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like "The penultimate word in this sentence is" (where a correct continuation is "is"). In verification, models judge the truth of statements like "The penultimate word in this sentence is sentence." (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05300v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>涉及元语言自指的陈述（“本文共有六个部分。”）在许多领域都很普遍。大型语言模型（LLM）能处理这样的语言吗？在本文中，我们提出了“我是一个奇怪的数据集”，这是一个解决这个问题的新数据集。有两个子任务：生成和验证。在生成中，模型继续诸如“这句话的倒数第二个单词是”（其中正确的延续是“is”）之类的语句。在验证过程中，模型会判断诸如“这句话的倒数第二个单词是句子”（false）之类的陈述的真实性。我们还提供了差异最小的元语言非自指示例，通过探究模型是否能够处理元语言来补充主要数据集。该数据集由专家手工制作，并由非专家注释器进行验证。我们通过API测试了各种开源LLM（7B到70B参数）以及闭源LLM。所有模型在两个子任务中，甚至在非自指元语言控制数据上都表现得近乎偶然，尽管我们发现随着模型规模的增加，这些模型都有一些稳步的改进。GPT 4是唯一一个持续表现明显好于偶然性的模型，它仍然只在60%的范围内，而我们未经训练的人类注释者的得分在89-93%的范围内。数据集和评估工具包可在https://github.com/TristanThrush/i-am-a-strange-dataset.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05300v1" target="_blank">2401.05300v1</a>
                              </td>
                              <td>I am a Strange Dataset: Metalinguistic Tests for Language Models</td>
                              <td>Tristan Thrush</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05300v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05300v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tristanthrush/i-am-a-strange-dataset" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05273v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05273v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05273v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05273v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The paper also discusses potential enhancements and future applications, positioning INACIA as a model for worldwide AI integration in legal domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05273v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了INACIA（Instru\c｛c｝\ao Assistida com Intellig encia Artificial），这是一个开创性的系统，旨在将大型语言模型（LLM）集成到巴西联邦账户法院（TCU）的操作框架中。该系统自动化了案例分析的各个阶段，包括基本信息提取、可采性检查、莫拉和黑烟分析以及建议生成。通过一系列实验，我们展示了INACIA在从案件文件中提取相关信息、评估其法律合理性和产生司法建议方面的潜力。利用验证数据集和LLM，我们的评估方法提供了一种评估系统性能的创新方法，与人类判断高度相关。研究结果突出了INACIA在处理复杂法律任务方面的熟练程度，表明其适合在法律体系中提高效率和司法公正性。本文还讨论了潜在的增强功能和未来的应用，将INACIA定位为全球法律领域人工智能集成的典范。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05273v1" target="_blank">2401.05273v1</a>
                              </td>
                              <td>INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges</td>
                              <td>Jayr Pereira</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05273v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05273v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12520v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12520v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12520v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12520v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from AnghaBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12520v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>反编译是一个研究得很好的领域，有许多高质量的工具可用。这些经常用于安全任务和移植遗留代码。然而，它们经常生成难以阅读的程序，并且需要大量的工程工作来支持新的编程语言和ISA。最近对神经方法的兴趣已经产生了生成可读代码的便携式工具。然而，到目前为止，这种技术通常仅限于没有优化的合成程序，并且没有模型评估其可移植性。此外，虽然生成的代码可能更可读，但通常是不正确的。本文介绍了SLaDe，一种基于在真实代码上训练的序列到序列转换器的小型语言模型分解器。我们开发了一种新颖的标记器，并利用无遗漏训练来生成高质量的代码。我们利用类型推理来生成比标准分析和最近的神经方法更可读、更准确的程序。与标准方法不同，SLaDe可以推断上下文外的类型，与神经方法不同，它生成正确的代码。我们在两个ISA和两个优化级别上对AnghaBench的4000多个函数进行了SLaDe评估。SLaDe的准确度是最先进的工业级反编译器Ghidra的6倍，是大型语言模型ChatGPT的4倍，生成的代码可读性明显高于两者。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12520v2" target="_blank">2305.12520v2</a>
                              </td>
                              <td>SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly</td>
                              <td>Jordi Armengol-Estapé</td>
                              <td>2023-05-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12520v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12520v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02567v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Automatic VQA Evaluation Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02567v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02567v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02567v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task. We plan to release the evaluation code and collected human judgments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02567v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉问答（VQA）任务提出8年后，准确性仍然是自动评估的主要指标。到目前为止，VQA准确性在IID评估环境中是有效的。然而，我们的社区正在向开放式生成模型和OOD评估转变。在这种新的范式中，现有的VQA准确性度量过于严格，低估了VQA系统的性能。因此，需要开发更健壮的自动VQA度量，作为人类判断的代理。在这项工作中，我们建议利用指令调整的大型语言模型（LLM）的上下文学习能力来构建更好的VQA度量。我们将VQA评估公式化为一项答案评级任务，其中LLM被指示在给定一组参考答案的情况下对候选答案的准确性进行评分。我们证明，与几个VQA模型和基准的现有指标相比，所提出的指标更好地与人类判断相关。我们希望我们的指标的广泛采用将有助于更好地估计VQA任务的研究进展。我们计划发布评估代码并收集人类判断。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02567v2" target="_blank">2310.02567v2</a>
                              </td>
                              <td>Improving Automatic VQA Evaluation Using Large Language Models</td>
                              <td>Oscar Mañas</td>
                              <td>2023-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02567v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02567v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05268v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AUTOACT: Automatic Agent Learning from Scratch via Self-Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05268v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05268v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05268v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05268v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言代理在各种复杂任务上都取得了相当大的性能。尽管在这一领域进行了不断的探索，但现有的语言代理系统仍在与成本高昂、不可复制的数据依赖作斗争，并面临着为多个功能强制使用单个模型的挑战。为此，我们引入了AutoAct，这是一种自动代理学习框架，不依赖于来自闭源模型（例如GPT-4）的大规模注释数据和合成轨迹。在使用工具库获得有限数据的情况下，AutoAct首先自动合成规划轨迹，而无需人类或强大的闭源模型的任何帮助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动进行区分，生成子代理组来完成任务。我们用不同的LLM进行了全面的实验，这表明与各种强基线相比，AutoAct产生了更好的或并行的性能。我们甚至注意到，当使用Llama-2-13b模型时，AutoAct可以实现与GPT-3.5-Turbo代理相当的性能。代码将在https://github.com/zjunlp/AutoAct.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05268v1" target="_blank">2401.05268v1</a>
                              </td>
                              <td>AUTOACT: Automatic Agent Learning from Scratch via Self-Planning</td>
                              <td>Shuofei Qiao</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05268v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05268v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zjunlp/autoact" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02384v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02384v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02384v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02384v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Charts play a vital role in data visualization, understanding data patterns, and informed decision-making. However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models. While vision-language models trained on chart data excel in comprehension, they struggle with generalization and require task-specific fine-tuning. To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic and specialized chart types. It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning. This approach enables ChartAssistant to achieve competitive performance across various chart tasks without task-specific fine-tuning. Experimental results demonstrate significant performance gains over the state-of-the-art UniChart method, outperforming OpenAI's GPT-4V(ision) on real-world chart data. The code and data are available at https://github.com/OpenGVLab/ChartAst.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02384v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图表在数据可视化、理解数据模式和知情决策方面发挥着至关重要的作用。然而，图形元素（如条形图、线条）和文本组件（如标签、图例）的独特组合给通用多模式模型带来了挑战。虽然在图表数据上训练的视觉语言模型在理解方面表现出色，但它们难以概括，需要特定任务的微调。为了应对这些挑战，我们提出了ChartAssistant，这是一种用于通用图表理解和推理的基于图表的视觉语言模型。ChartAssistant利用ChartSFT，这是一个全面的数据集，涵盖了基本和专业图表类型的各种图表相关任务。它经历了两个阶段的训练过程，首先是图表到表格解析的预训练，以对齐图表和文本，然后是微调后的多任务指令。这种方法使ChartAssistant能够在各种图表任务中实现有竞争力的性能，而无需特定任务的微调。实验结果表明，与最先进的UniChart方法相比，性能显著提高，在真实世界的图表数据上优于OpenAI的GPT-4V（vision）。代码和数据可在https://github.com/OpenGVLab/ChartAst.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02384v2" target="_blank">2401.02384v2</a>
                              </td>
                              <td>ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</td>
                              <td>Fanqing Meng</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02384v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02384v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/opengvlab/chartast" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05249v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CASA: Causality-driven Argument Sufficiency Assessment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05249v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05249v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05249v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05249v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>论证充分性评估任务旨在确定给定论证的前提是否支持其结论。为了解决这一任务，现有的工作通常在人类注释的数据上训练分类器。然而，对数据进行注释是很费力的，而且由于主观标准的原因，注释往往不一致。受因果文献中充分性概率（PS）定义的启发，我们提出了CASA，这是一个零样本因果驱动的论点充分性评估框架。PS测量在前提事件和结论事件都不存在的情况下，引入前提事件导致结论的可能性。为了估计这种概率，我们建议使用大型语言模型（LLM）生成与前提和结论不一致的上下文，并通过注入前提事件对其进行修正。在两个逻辑谬误检测数据集上的实验表明，CASA准确地识别了不足的论点。我们在写作辅助申请中进一步部署了CASA，并发现CASA产生的建议提高了学生书面论点的充分性。代码和数据可在https://github.com/xxxiaol/CASA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05249v1" target="_blank">2401.05249v1</a>
                              </td>
                              <td>CASA: Causality-driven Argument Sufficiency Assessment</td>
                              <td>Xiao Liu</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05249v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05249v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/xxxiaol/CASA" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xxxiaol/casa" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14115v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A density estimation perspective on learning from pairwise human preferences</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14115v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14115v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14115v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14115v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从人类反馈中学习，特别是从成对偏好中学习，最近已成为训练大型语言模型的关键因素，也是许多研究的主题。最近的工作将其定义为一个强化学习问题，其中从成对的偏好数据中学习奖励函数，并将LLM视为一种策略，该策略通常在额外的正则化约束下适用于最大化奖励。我们提出了一种替代解释，该解释以成对偏好的生成过程为中心，并将LHF视为密度估计问题。我们提供的理论和实证结果表明，对于通过偏好-行为分布方程定义的生成过程家族，在成对偏好上训练奖励函数可以有效地模拟注释者的隐含偏好分布。最后，我们讨论并介绍了关于“注释者错误指定”的研究结果，即对注释者行为做出错误的建模假设，导致模型适应性差的失败案例，这表明从成对的人类偏好中学习的方法可能难以从具有不同观点的注释者群体中学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14115v3" target="_blank">2311.14115v3</a>
                              </td>
                              <td>A density estimation perspective on learning from pairwise human preferences</td>
                              <td>Vincent Dumoulin</td>
                              <td>2023-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14115v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14115v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-deepmind/pbde" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05215v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pre-trained Large Language Models for Financial Sentiment Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05215v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05215v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05215v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05215v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>金融情绪分析是指将金融文本内容分为情绪类别（如积极、消极和中性）。在本文中，我们重点研究财经新闻标题的分类，由于缺乏大量的训练样本，这是一项具有挑战性的任务。为了克服这一困难，我们建议调整预先训练的大型语言模型（LLM）[1，2，3]来解决这个问题。LLM是从大量的文本语料库中训练出来的，在文本理解方面具有优势，并且可以有效地适应特定领域的任务，同时只需要很少的训练样本。特别是，我们将开源的Llama2-7B模型（2023）与监督微调（SFT）技术相结合[4]。实验评估表明，即使使用7B模型（LLM相对较小），我们的方法也显著优于以前最先进的算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05215v1" target="_blank">2401.05215v1</a>
                              </td>
                              <td>Pre-trained Large Language Models for Financial Sentiment Analysis</td>
                              <td>Wei Luo</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05215v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05215v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/shubhamkotal/FinBert" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05200v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05200v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05200v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05200v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBeluga2 trailing closely, presenting an attractive option given its data privacy and customization benefits. Overall, this work offers preliminary insights for factories considering using LLM-tools for knowledge management.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05200v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>有效地管理知识对组织的成功至关重要。在制造业，运营工厂变得越来越知识密集，这给工厂培训和支持新操作员的能力带来了压力。在本文中，我们介绍了一个基于大型语言模型（LLM）的系统，该系统旨在使用工厂文档中包含的广泛知识。该系统旨在有效回答操作员的询问，并促进新知识的共享。为了评估其有效性，我们在工厂环境中进行了评估。这次评估的结果表明了该系统的好处；即实现更快的信息检索和更有效的问题解决。然而，这项研究也强调了当有这样的选择时，人们更倾向于向人类专家学习。此外，我们为该系统测试了几个封闭和开源的LLM。GPT-4的表现一直优于同行，StableBeluga2等开源模型紧随其后，鉴于其数据隐私和定制优势，这是一个有吸引力的选择。总的来说，这项工作为考虑使用LLM工具进行知识管理的工厂提供了初步的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05200v1" target="_blank">2401.05200v1</a>
                              </td>
                              <td>Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking</td>
                              <td>Samuel Kernan Freire</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05200v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05200v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05199v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Monte Carlo Tree Search for Recipe Generation using GPT-2</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05199v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05199v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05199v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic food recipe generation methods provide a creative tool for chefs to explore and to create new, and interesting culinary delights. Given the recent success of large language models (LLMs), they have the potential to create new recipes that can meet individual preferences, dietary constraints, and adapt to what is in your refrigerator. Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes. In this paper, we propose RecipeMC, a text generation method using GPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to define reward functions to put soft constraints on text generation and thus improve the credibility of the generated recipes. Our results show that human evaluators prefer recipes generated with RecipeMC more often than recipes generated with other baseline methods when compared with real recipes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05199v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动食品配方生成方法为厨师探索和创造新的、有趣的烹饪乐趣提供了一个创造性的工具。鉴于大型语言模型（LLM）最近的成功，它们有可能创造出新的食谱，满足个人偏好、饮食限制，并适应冰箱里的食物。关于使用LLM生成配方的现有研究表明，LLM可以进行微调，以生成听起来逼真的配方。然而，仔细检查，这些生成的食谱往往无法满足基本要求，比如将鸡肉作为鸡肉菜肴的配料。在本文中，我们提出了RecipeMC，这是一种使用GPT-2的文本生成方法，它依赖于蒙特卡洛树搜索（MCTS）。RecipeMC允许我们定义奖励函数，对文本生成进行软约束，从而提高生成食谱的可信度。我们的研究结果表明，与真实食谱相比，人类评估者更喜欢使用RecipeMC生成的食谱，而不是使用其他基线方法生成的食谱。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05199v1" target="_blank">2401.05199v1</a>
                              </td>
                              <td>Monte Carlo Tree Search for Recipe Generation using GPT-2</td>
                              <td>Karan Taneja</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05199v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05199v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10638v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HyperPIE: Hyperparameter Information Extraction from Scientific Publications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10638v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10638v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10638v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves an average improvement of 5.5% F1 in entity recognition over using JSON. With our best performing model we extract hyperparameter information from a large number of unannotated papers, and analyze patterns across disciplines. All our data and source code is publicly available at https://github.com/IllDepence/hyperpie</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10638v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从出版物中自动提取信息是使科学知识机器大规模可读的关键。例如，提取的信息可以促进学术搜索、决策和知识图构建。现有方法未涵盖的一种重要信息类型是超参数。在本文中，我们将超参数信息提取（HyperPIE）形式化并处理为实体识别和关系提取任务。我们创建了一个标签数据集，涵盖了各种计算机科学学科的出版物。使用这个数据集，我们训练和评估基于BERT的微调模型以及五个大型语言模型：GPT-3.5、GALACTICA、Falcon、Vicuna和WizardLM。对于微调模型，我们开发了一种关系提取方法，在最先进的基线基础上实现了29%的F1改进。对于大型语言模型，我们开发了一种利用YAML输出进行结构化数据提取的方法，与使用JSON相比，该方法在实体识别方面实现了5.5%F1的平均改进。使用我们性能最好的模型，我们从大量未注释的论文中提取超参数信息，并分析跨学科的模式。我们所有的数据和源代码都可以在https://github.com/IllDepence/hyperpie</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10638v2" target="_blank">2312.10638v2</a>
                              </td>
                              <td>HyperPIE: Hyperparameter Information Extraction from Scientific Publications</td>
                              <td>Tarek Saier</td>
                              <td>2023-12-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10638v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10638v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/IllDepence/hyperpie" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/illdepence/hyperpie" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05190v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Divide and Conquer for Large Language Models Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05190v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05190v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05190v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed strategy significantly boosts the models' reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks. For instance, compared to baseline, we make a striking improvement on low confidence subsets of 8.72\% for AQuA, 15.07\% for ARC Challenge and 7.71\% for RiddleSense. In addition, through extensive analysis on length of rationale and number of options, we verify that longer reasoning paths in PKR could prevent models from referring infer-harmful shortcuts, and also find that removing irrelevant choices in FCR would substantially avoid models' confusion. The code is at \url{https://github.com/AiMijie/Divide-and-Conquer}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05190v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着思维链（CoT）及其衍生方法的出现，大型语言模型（LLM）在各种推理基准中表现出了令人印象深刻的性能，尤其是在涉及多选问题（MCQ）的任务中。然而，当前的工作统一处理所有数据，而不考虑解决问题的难度，这意味着过度关注简单的问题，而不足以解决复杂的问题。为了应对这一挑战，我们受到人类使用启发式策略对任务进行分类并单独处理的启发，建议将分而治之应用于LLM推理。首先，我们根据统计置信度得分（$\mathcal{CS}$）将问题划分为不同的子集，然后用精心设计的方法，包括基于先验知识的推理（PKR）和基于过滤选择的推理（FCR），以及它们的集成变体，固定几乎已解决的集合，并克服要求苛刻的细致入微的过程集合。我们的实验表明，这种提出的策略显著提高了模型在涉及算术、常识和逻辑任务的九个数据集上的推理能力。例如，与基线相比，我们对AQuA的8.72%、ARC Challenge的15.07%和RiddleSense的7.71%的低置信度子集进行了显著改进。此外，通过对基本原理长度和选项数量的广泛分析，我们验证了PKR中较长的推理路径可以防止模型参考推断有害的捷径，并发现在FCR中删除不相关的选择可以大大避免模型的混淆。代码位于\url{https://github.com/AiMijie/Divide-and-Conquer}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05190v1" target="_blank">2401.05190v1</a>
                              </td>
                              <td>Divide and Conquer for Large Language Models Reasoning</td>
                              <td>Zijie Meng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05190v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05190v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/intuit-ai-research/DCR-consistency" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/aimijie/divide-and-conquer" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05176v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can ChatGPT Rival Neural Machine Translation? A Comparative Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05176v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05176v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05176v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings provide valuable insights into the potential of ChatGPT as a capable machine translator, and the influence of prompt engineering on its performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05176v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>受利用大型语言模型进行翻译的兴趣日益增加的启发，本文评估了以ChatGPT为代表的大型语言模型（LLM）与主流神经机器翻译（NMT）引擎在将中国外交文本翻译成英语方面的能力。具体来说，我们检查了ChatGPT和NMT引擎的翻译质量，通过四个自动指标和基于错误类型和六个分析准则的人工评估来衡量。我们的研究结果表明，在不同的提示和NMT系统下，自动度量对ChatGPT产生了相似的结果，而当提供翻译任务的示例或上下文信息时，人类注释者往往会给ChatGPT分配明显更高的分数。自动化指标和人类评估维度之间的成对相关性产生了微弱和不显著的结果，表明两种翻译质量评估方法之间存在差异。这些发现为ChatGPT作为一种有能力的机器翻译器的潜力以及即时工程对其性能的影响提供了宝贵的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05176v1" target="_blank">2401.05176v1</a>
                              </td>
                              <td>Can ChatGPT Rival Neural Machine Translation? A Comparative Study</td>
                              <td>Zhaokun Jiang</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05176v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05176v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13538v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Speak Like a Native: Prompting Large Language Models in a Native Style</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13538v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13538v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13538v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specifically, with AlignedCoT, we observe an average +3.2\% improvement for \texttt{gpt-3.5-turbo} compared to the carefully handcrafted CoT on multi-step reasoning benchmarks.Furthermore, we use AlignedCoT to rewrite the CoT text style in the training set, which improves the performance of Retrieval Augmented Generation by 3.6\%.The source code and dataset is available at https://github.com/yangzhch6/AlignedCoT</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13538v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有大型语言模型的上下文学习（ICL）已成为许多自然语言处理任务的现代选择工具。然而，上下文示例的文本风格如何影响LLM的性能仍有待探索。本文提出了一种新的有效方法，名为\textbf｛AlignedCoT｝，通过将上下文中的示例与LLM的本地风格相结合来提高LLM的推理能力。”本机“”是指LLM的固有特性，可通过零样本方案进行探测。AlignedCoT广泛适用于ICL方法，使其易于与最先进的技术相结合，以进一步提高LLM的性能。我们在数学问答、常识推理和文本理解的几个基准上进行了广泛而全面的实验。实证结果表明，与精心手工制作的演示相比，我们的AlignedCoT显著提高了性能。具体而言，与在多步骤推理基准上精心手工制作的CoT相比，使用AlignedCoT，我们观察到\texttt｛gpt-3.5-turbo｝的平均提高了+3.2\%。此外，我们使用AlignedCoT重写训练集中的CoT文本样式，这将检索增强生成的性能提高了3.6%。源代码和数据集可在https://github.com/yangzhch6/AlignedCoT</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13538v2" target="_blank">2311.13538v2</a>
                              </td>
                              <td>Speak Like a Native: Prompting Large Language Models in a Native Style</td>
                              <td>Zhicheng Yang</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13538v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13538v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/yangzhch6/AlignedCoT" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yangzhch6/alignedcot" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05163v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MISS: A Generative Pretraining and Finetuning Approach for Med-VQA</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05163v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05163v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05163v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models. The code and model weights will be released upon the paper's acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05163v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>医学视觉问答（VQA）是一项具有挑战性的多模式任务，其中视觉语言预训练（VLP）模型可以有效地提高泛化性能。然而，医学领域的大多数方法都将VQA视为一项难以转移到实际应用场景的答案分类任务。此外，由于医学图像的隐私性和昂贵的注释过程，严重缺乏用于预训练的大规模医学图像-文本对数据集。在本文中，我们提出了一种用于医疗VQA任务的大规模多任务自监督学习框架（MISS）。与现有方法不同，我们将医疗VQA视为一项生成性任务。我们统一了文本编码器和多模式编码器，并通过多任务学习对齐图像文本特征。此外，我们提出了一种转移和字幕方法，该方法使用大型语言模型（LLM）扩展了单模态图像数据集的特征空间，使这些传统的医学视野任务数据能够应用于VLP。实验表明，我们的方法用更少的多模式数据集取得了良好的结果，并证明了生成VQA模型的优势。代码和模型权重将在论文通过后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05163v1" target="_blank">2401.05163v1</a>
                              </td>
                              <td>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA</td>
                              <td>Jiawei Chen</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05163v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05163v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_00319v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_00319v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_00319v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_00319v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models, and results indicate that adversarial examples remain a significant threat to large language models. The adversarial examples crafted by LimeAttack are highly transferable and effectively improve model robustness in adversarial training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_00319v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然语言处理模型容易受到对抗性示例的影响。先前的文本对抗性攻击采用梯度或置信度得分来计算单词重要性排名并生成对抗性示例。然而，这些信息在现实世界中是不可用的。因此，我们将重点放在一个更现实、更具挑战性的设置上，称为硬标签攻击，在该设置中，攻击者只能查询模型并获得离散的预测标签。现有的硬标签攻击算法倾向于通过随机替换来初始化对抗性示例，然后利用复杂的启发式算法来优化对抗性扰动。这些方法需要大量的模型查询，并且攻击成功率受到对手初始化的限制。在本文中，我们提出了一种新的硬标签攻击算法LimeAttack，该算法利用局部可解释的方法来近似单词的重要性排序，然后采用波束搜索来找到最优解。大量实验表明，在相同的查询预算下，LimeAttack比现有的硬标签攻击具有更好的攻击性能。此外，我们评估了LimeAttack在大型语言模型上的有效性，结果表明，对抗性示例仍然是对大型语言模型的重大威胁。LimeAttack制作的对抗性示例具有高度的可移植性，并有效地提高了对抗性训练中的模型鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.00319v2" target="_blank">2308.00319v2</a>
                              </td>
                              <td>LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack</td>
                              <td>Hai Zhu</td>
                              <td>2023-08-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_00319v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.00319v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05136v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Code Review Automation: Strengths and Weaknesses of the State of the Art</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05136v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05136v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05136v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques imitating developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, e.g., the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques' capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with ~105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05136v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几位研究人员已经解决了代码审查的自动化问题，目的是降低成本。深度学习在软件工程中的采用将自动化推向了新的边界，在生成任务中模仿开发人员的技术，例如像评审员那样评论代码更改，或者通过修改代码来解决评审员的评论。这些技术的性能通常通过定量指标来评估，例如，测试集中生成正确预测的实例的百分比，这就留下了许多关于技术能力的悬而未决的问题。例如，知道一种方法能够在10%的情况下正确地处理审阅者的评论，而不知道审阅者问了什么，这几乎没有什么价值：如果在所有成功的情况下，处理评论所需的代码更改只是删除了一行空行，该怎么办？在本文中，我们旨在描述三种代码审查自动化技术在上述两项任务中往往成功或失败的情况。这项研究具有很强的定性重点，投入了约105工时的人工检查来手动分析这三种技术产生的正确和错误预测，总共检查了2291个预测。该分析的结果是两个分类法，分别报告两项任务中实验技术成功或失败的代码更改类型，指出未来工作的领域。我们手动分析的结果也是确定了用于训练和测试实验技术的数据集中的几个问题。最后，我们通过将代码审查自动化专用技术的性能与通用大型语言模型ChatGPT进行比较，评估了研究这些技术的重要性，发现ChatGPT在评论代码方面与人类审查人员一样困难。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05136v1" target="_blank">2401.05136v1</a>
                              </td>
                              <td>Code Review Automation: Strengths and Weaknesses of the State of the Art</td>
                              <td>Rosalia Tufano</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05136v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05136v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04620v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Agent Alignment in Evolving Social Norms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04620v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04620v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04620v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04620v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于大型语言模型（LLM）的代理越来越多地渗透到人类生产和生活的各个领域，凸显了将其与人类价值观相一致的重要性。目前人工智能系统的对齐主要集中在通过人工干预被动对齐LLM。然而，代理具有接收环境反馈和自我进化等特性，使得LLM对齐方法不充分。作为回应，我们提出了一个智能体进化和结盟的进化框架，称为EvolutionaryAgent，它将智能体结盟转变为优胜劣汰原则下的进化和选择过程。在社会规范不断演变的环境中，更好地适应当前社会规范的主体将有更高的生存和扩散概率，而那些不完全一致的主体则会随着时间的推移而减少。从多个角度评估Agent与社会规范一致性的实验结果表明，EvolutionaryAgent具有逐渐更好地与不断发展的社会规范一致的能力，同时保持其在一般任务中的熟练度。在作为代理基础的各种开源和闭源LLM上进行的有效性测试也证明了我们的方法的适用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04620v2" target="_blank">2401.04620v2</a>
                              </td>
                              <td>Agent Alignment in Evolving Social Norms</td>
                              <td>Shimin Li</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04620v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04620v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02705v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02705v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02705v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02705v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and parameter selecting, respectively, and two additional modules for state sensing and case rewriting. The agents interact with testing device, make human-like decision and generate action command in a collaborative way. The proposed multi-agent system achieves a close effectiveness to human testers in our experimental studies and gains a significant improvement of Pass@1 accuracy compared with single-agent architecture. More importantly, the proposed system has launched in the formal testing environment of WeChat Pay mobile app, which saves a considerable amount of manpower in the daily development work.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02705v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，我们一直致力于实现微信支付的用户验收测试（UAT）流程自动化，微信支付是中国最具影响力的移动支付应用之一。为此开发了一个名为XUAT的系统。然而，在当前的系统中，仍然存在一个人力密集的阶段，即测试脚本的生成。因此，在本文中，我们专注于提高当前系统自动化水平的方法，特别是测试脚本生成阶段。随着最近的显著成功，大型语言模型（LLM）在获得类人智能方面表现出了巨大的潜力，并且越来越多的研究领域将LLM作为自主主体来获得类人决策能力。受这些工作的启发，我们提出了一个LLM驱动的多智能体协作系统，名为XUAT Copilot，用于自动化UAT。所提出的系统主要由三个基于LLM的代理组成，分别负责动作规划、状态检查和参数选择，以及两个额外的状态感知和案例重写模块。代理与测试设备交互，做出类似人类的决策，并以协作的方式生成动作命令。在我们的实验研究中，所提出的多智能体系统与人类测试人员的效果非常接近，并在Pass@1与单代理体系结构相比的准确性。更重要的是，该系统已在微信支付手机应用程序的正式测试环境中推出，在日常开发工作中节省了大量人力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02705v2" target="_blank">2401.02705v2</a>
                              </td>
                              <td>XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model</td>
                              <td>Zhitao Wang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02705v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02705v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05072v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aligning Translation-Specific Understanding to General Understanding in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05072v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05072v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05072v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation. Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on the self-constructed benchmark ChallengeMT, which includes cases in which multiple SOTA translation systems consistently underperform. Experimental results show the effectiveness of our xIoD, which improves up to +3.85 COMET.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05072v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）已经显示出令人惊讶的语言理解和生成能力，但它们在机器翻译领域尚未取得革命性的进展。有限表现的一个潜在原因是LLM内部翻译特定理解和一般理解之间的错位。为了使翻译的具体理解与一般理解相一致，我们提出了一种新颖的翻译过程xIoD（困难词的跨语言解释），明确地结合了对导致理解不一致的内容的一般理解来指导翻译。具体来说，xIoD对难以翻译的单词进行跨语言翻译，并通过生成的翻译增强翻译。此外，我们重新定义了QE的外部工具，以应对xIoD在检测困难单词和生成有用解释方面的挑战。我们在自行构建的基准ChallengeMT上进行了实验，其中包括多个SOTA翻译系统持续表现不佳的情况。实验结果表明了我们的xIoD的有效性，它将COMET提高到+3.85。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05072v1" target="_blank">2401.05072v1</a>
                              </td>
                              <td>Aligning Translation-Specific Understanding to General Understanding in Large Language Models</td>
                              <td>Yichong Huang</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05072v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05072v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10934v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">APIDocBooster: An Extract-Then-Abstract Framework Leveraging Large Language Models for Augmenting API Documentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10934v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10934v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10934v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>API documentation is often the most trusted resource for programming. Many approaches have been proposed to augment API documentation by summarizing complementary information from external resources such as Stack Overflow. Existing extractive-based summarization approaches excel in producing faithful summaries that accurately represent the source content without input length restrictions. Nevertheless, they suffer from inherent readability limitations. On the other hand, our empirical study on the abstractive-based summarization method, i.e., GPT-4, reveals that GPT-4 can generate coherent and concise summaries but presents limitations in terms of informativeness and faithfulness.   We introduce APIDocBooster, an extract-then-abstract framework that seamlessly fuses the advantages of both extractive (i.e., enabling faithful summaries without length limitation) and abstractive summarization (i.e., producing coherent and concise summaries). APIDocBooster consists of two stages: (1) \textbf{C}ontext-aware \textbf{S}entence \textbf{S}ection \textbf{C}lassification (CSSC) and (2) \textbf{UP}date \textbf{SUM}marization (UPSUM). CSSC classifies API-relevant information collected from multiple sources into API documentation sections. UPSUM first generates extractive summaries distinct from the original API documentation and then generates abstractive summaries guided by extractive summaries through in-context learning.   To enable automatic evaluation of APIDocBooster, we construct the first dataset for API document augmentation. Our automatic evaluation results reveal that each stage in APIDocBooster outperforms its baselines by a large margin. Our human evaluation also demonstrates the superiority of APIDocBooster over GPT-4 and shows that it improves informativeness, relevance, and faithfulness by 13.89\%, 15.15\%, and 30.56\%, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10934v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>API文档通常是最值得信赖的编程资源。已经提出了许多方法来通过总结来自外部资源（如Stack Overflow）的补充信息来扩充API文档。现有的基于提取的摘要方法擅长于生成准确表示源内容的忠实摘要，而不受输入长度限制。然而，它们存在固有的可读性限制。另一方面，我们对基于抽象的摘要方法（即GPT-4）的实证研究表明，GPT-4可以生成连贯简洁的摘要，但在信息性和忠实性方面存在局限性。我们介绍了APIDocBooster，这是一个先提取后抽象的框架，它无缝融合了提取（即无长度限制地实现忠实摘要）和抽象摘要（即生成连贯简洁的摘要）的优势。APIDocBooster由两个阶段组成：（1）\textbf{C}ontext-aware\textbf{S}entence\textbf{S}ection\textbf{C}lassification（CSSC）和（2）\textbf{UP}date\textbf{SUM}marization（上）。CSSC将从多个来源收集的与API相关的信息分类为API文档部分。UPSUM首先生成不同于原始API文档的摘要，然后通过上下文学习生成以摘要为指导的摘要。为了实现APIDocBooster的自动评估，我们构建了API文档扩充的第一个数据集。我们的自动评估结果显示，APIDocBooster中的每个阶段都大大优于其基线。我们的人类评估还证明了APIDocBooster优于GPT-4，并表明它分别提高了13.89%、15.15%和30.56%的信息性、相关性和可信度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10934v2" target="_blank">2312.10934v2</a>
                              </td>
                              <td>APIDocBooster: An Extract-Then-Abstract Framework Leveraging Large Language Models for Augmenting API Documentation</td>
                              <td>Chengran Yang</td>
                              <td>2023-12-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10934v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10934v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15156v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15156v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15156v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15156v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot keyphrase extraction aims to build a keyphrase extractor without training by human-annotated data, which is challenging due to the limited human intervention involved. Challenging but worthwhile, zero-shot setting efficiently reduces the time and effort that data labeling takes. Recent efforts on pre-trained large language models (e.g., ChatGPT and ChatGLM) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this paper, we ask whether strong keyphrase extraction models can be constructed by directly prompting the large language model ChatGPT. Through experimental results, it is found that ChatGPT still has a lot of room for improvement in the keyphrase extraction task compared to existing state-of-the-art unsupervised and supervised models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15156v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本关键词提取旨在建立一个关键词提取器，而无需通过人工标注数据进行训练，由于所涉及的人工干预有限，这是一个挑战。零样本设置具有挑战性，但值得一提，可有效减少数据标记所需的时间和精力。最近在预先训练的大型语言模型（例如，ChatGPT和ChatGLM）上所做的努力在零样本设置上表现出了良好的性能，从而激励我们探索基于提示的方法。在本文中，我们询问是否可以通过直接提示大型语言模型ChatGPT来构建强密钥短语提取模型。通过实验结果发现，与现有最先进的无监督和有监督模型相比，ChatGPT在关键短语提取任务上仍有很大的改进空间。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15156v2" target="_blank">2312.15156v2</a>
                              </td>
                              <td>Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical Study</td>
                              <td>Mingyang Song</td>
                              <td>2023-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15156v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15156v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mysong7nlper/chatgpt_as_keyphrase_extractor" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10744v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10744v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10744v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10744v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been associated with the amygdala, a pivotal cerebral region for emotional learning, in the case of humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10744v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>隐喻和讽刺是我们高度进化的社交技能的宝贵成果。然而，众所周知，患有阿斯伯格综合症的儿童在理解讽刺方面有困难，即使他们拥有一定水平的语言智商，足以理解隐喻。鉴于此，一项对理解隐喻和讽刺的能力进行评分的筛查测试已被用于区分阿斯伯格综合症与其他表现出类似外部行为的症状（如注意力缺陷/多动障碍）。本研究使用标准化测试来检验最近的大型语言模型（LLM）在理解人类细致入微的交流方面的能力。研究结果表明，尽管随着模型参数数量的增加，他们理解隐喻的能力有所提高，但没有观察到讽刺理解的提高。这意味着，必须采用另一种方法来培养LLM掌握讽刺的能力，这与杏仁核有关，杏仁核是人类情绪学习的关键大脑区域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10744v2" target="_blank">2309.10744v2</a>
                              </td>
                              <td>Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome</td>
                              <td>Hiromu Yakura</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10744v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10744v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hiromu/llm-msst" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05054v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05054v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05054v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05054v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05054v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本生成系统中最重要的挑战之一是生成不仅正确而且多样化的输出。最近，最小贝叶斯风险（MBR）解码由于在解码算法中生成最高质量的句子而变得突出。然而，现有的用于生成不同输出的算法主要基于波束搜索或随机采样，因此它们的输出质量受到这些基本方法的限制。在本文中，我们研究了一种替代方法——我们通过对MBR解码实施分集目标来开发分集促进解码算法。我们提出了MBR的两种变体，Diverse MBR（DMBR）和$k$-medioids MBR（KMBR），这两种方法可以生成一组具有高质量和多样性的句子。我们使用编码器-解码器模型和带提示的大型语言模型，在各种定向文本生成任务上评估DMBR和KMBR。实验结果表明，与不同的波束搜索和采样算法相比，该方法实现了更好的折衷。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05054v1" target="_blank">2401.05054v1</a>
                              </td>
                              <td>Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding</td>
                              <td>Yuu Jinnai</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05054v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05054v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04679v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04679v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04679v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04679v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memory- and computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab/RoSA}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04679v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了参数有效微调（PEFT）方法，该方法可以在大型语言模型（LLM）的有限计算和内存预算下提供良好的准确性。我们提出了一种新的PEFT方法，称为鲁棒自适应（RoSA），其灵感来自鲁棒主成分分析（PCA），该方法在一组固定的预训练权重之上联合训练$\textit｛low rank｝$和$\textit｛highly sparse｝$分量，以有效地近似全微调（FFT）解决方案的性能。在一系列具有挑战性的生成任务中，如小学数学和SQL查询生成，这些任务需要进行微调才能获得良好的性能，我们发现在相同的参数预算下，RoSA的性能优于LoRA和纯稀疏微调。我们为RoSA提供系统支持，以补充训练算法，特别是以稀疏GPU内核的形式，实现内存和计算高效的训练。我们的代码将在https://github.com/IST-DASLab/RoSA}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04679v2" target="_blank">2401.04679v2</a>
                              </td>
                              <td>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</td>
                              <td>Mahdi Nikdan</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04679v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04679v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05033v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05033v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05033v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05033v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05033v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）是强大的对话代理，但将它们专门用于实现特定功能可能具有挑战性。指导性调整，即对人类生成的指令和样本响应的模型进行调整（Ouyang et al.，2022），已被证明是一种有效的方法，但需要大量数据样本，a）可能不可用或b）生成成本高昂。此外，当目标是使LLM在对话中遵循特定的工作流程而不是单个指令时，这种成本会增加。受强化学习中的自我游戏技术和LLM模拟人类主体的启发，我们提出了一种更有效的方法，通过LLM参与不同角色的对话来收集数据。这种方法通过LLM的“自对话”生成训练数据，该数据可以被细化并用于监督微调。我们引入了一种自动化的方法来衡量对话的（部分）成功与否。该度量用于过滤生成的会话数据，该会话数据在LLM中反馈用于训练。基于我们对谈话质量的自动化和人工评估，我们证明了这种自言自语的数据可以改善结果。此外，我们还研究了展示生成对话质量的各种特征，以及它们如何与作为训练数据的潜在效用联系起来。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05033v1" target="_blank">2401.05033v1</a>
                              </td>
                              <td>Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk</td>
                              <td>Dennis Ulmer</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05033v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05033v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04889v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">KwaiAgents: Generalized Information-seeking Agent System with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04889v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04889v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04889v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04889v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在好奇心的驱使下，人类不断寻求探索和了解周围的世界，从而发明了各种工具来满足这种好奇心。尽管人类没有能力处理和记忆大脑中的大量信息，但他们擅长批判性思维、计划、反思，以及利用现有工具与世界互动和解释世界，使他们能够高效地找到答案。大型语言模型（LLM）的最新进展表明，机器也可能具有上述类似人类的能力，使它们即使在参数计数有限的情况下也能表现出强大的能力。在本文中，我们介绍了KwaiAgents，一个基于LLM的广义信息搜索代理系统。在KwaiAgents中，我们提出了一个以LLM为认知核心的代理系统，该系统能够理解用户的查询、行为准则和引用外部文档。代理还可以从其内部内存中更新和检索信息，使用时间感知搜索浏览工具包计划和执行操作，并最终提供全面的响应。我们进一步研究了当LLM不如GPT-4先进时系统的性能，并引入了元代理优化（MAT）框架，旨在确保即使是开源的7B或13B模型也能在许多代理系统中表现良好。我们利用基准评估和人工评估来系统地验证这些能力。大量实验表明，与其他自治代理相比，我们的代理系统具有优势，并突出了我们微调LLM增强的广义代理能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04889v3" target="_blank">2312.04889v3</a>
                              </td>
                              <td>KwaiAgents: Generalized Information-seeking Agent System with Large Language Models</td>
                              <td>Haojie Pan</td>
                              <td>2023-12-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04889v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04889v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kwaikeg/kwaiagents" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03385v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grimoire is All You Need for Enhancing Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03385v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03385v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03385v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03385v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>上下文学习（ICL）是通过提供一组少量的例子来提高大型语言模型在特定任务上的性能的关键方法之一。然而，由于模型架构、学习数据量和参数大小等因素，不同类型模型的ICL能力表现出显著差异。通常，模型的参数大小越大，学习数据越广泛，其ICL能力就越强。在本文中，我们提出了一种方法SLEICL，该方法包括使用强语言模型从示例中学习，然后总结并将这些学到的技能转移到弱语言模型中进行推理和应用。这确保了ICL的稳定性和有效性。与直接使弱语言模型能够从提示示例中学习相比，SLEICL降低了ICL对这些模型的难度。我们在多达八个数据集和五个语言模型上进行的实验表明，弱语言模型使用SLEICL方法实现了对其自身零样本或少搜索功能的一致改进。在SLEICL的帮助下，一些弱语言模型甚至超过了GPT4-106-preview（零样本）的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03385v2" target="_blank">2401.03385v2</a>
                              </td>
                              <td>Grimoire is All You Need for Enhancing Large Language Models</td>
                              <td>Ding Chen</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03385v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03385v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/iaar-shanghai/grimoire" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04997v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04997v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04997v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04997v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, \ie task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by experiments to systematically analyze the impact of different factors on two public datasets. Finally, we summarize promising directions to shed lights on future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04997v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，像ChatGPT这样的大型语言模型在解决一般任务方面表现出了非凡的能力，展示了在推荐系统中应用的潜力。为了评估LLM在推荐任务中的有效性，我们的研究主要集中在通过提示工程将LLM用作推荐系统。我们提出了一个在推荐任务中使用LLM的通用框架，重点关注LLM作为推荐者的能力。为了进行我们的分析，我们从两个关键方面将LLM的推荐输入形式化为自然语言提示，并解释如何将我们的框架推广到各种推荐场景。关于LLM作为推荐者的使用，我们基于LLM的分类分析了公共可用性、调整策略、模型架构、参数规模和上下文长度对推荐结果的影响。在提示工程方面，我们进一步分析了提示的四个重要组成部分的影响，即任务描述、用户兴趣建模、候选项构建和提示策略。在每一节中，我们首先根据现有文献对概念进行定义和分类。然后，我们提出了一些启发性的研究问题，然后通过实验系统地分析了不同因素对两个公共数据集的影响。最后，我们总结了有希望的方向，以期对未来的研究有所启发。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04997v1" target="_blank">2401.04997v1</a>
                              </td>
                              <td>Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis</td>
                              <td>Lanling Xu</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04997v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04997v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04952v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can AI Write Classical Chinese Poetry like Humans? An Empirical Study Inspired by Turing Test</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04952v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04952v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04952v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Some argue that the essence of humanity, such as creativity and sentiment, can never be mimicked by machines. This paper casts doubt on this belief by studying a vital question: Can AI compose poetry as well as humans? To answer the question, we propose ProFTAP, a novel evaluation framework inspired by Turing test to assess AI's poetry writing capability. We apply it on current large language models (LLMs) and find that recent LLMs do indeed possess the ability to write classical Chinese poems nearly indistinguishable from those of humans. We also reveal that various open-source LLMs can outperform GPT-4 on this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04952v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>一些人认为，人类的本质，如创造力和情感，永远无法被机器模仿。本文通过研究一个至关重要的问题来质疑这一信念：人工智能能像人类一样写诗吗？为了回答这个问题，我们提出了ProFTAP，这是一个受图灵测试启发的新的评估框架，用于评估人工智能的诗歌写作能力。我们将其应用于当前的大型语言模型（LLM），发现最近的LLM确实具有写中国古典诗歌的能力，与人类几乎没有区别。我们还揭示了各种开源LLM在这项任务上的表现优于GPT-4。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04952v1" target="_blank">2401.04952v1</a>
                              </td>
                              <td>Can AI Write Classical Chinese Poetry like Humans? An Empirical Study Inspired by Turing Test</td>
                              <td>Zekun Deng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04952v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04952v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04666v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pre-training LLMs using human-like development data corpus</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04666v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04666v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04666v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Pre-trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre-training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre-training to human language acquisition, where the number of tokens seen by 13-year-old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures, evaluation of changes in performance across epochs, and reported pre-training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict-small tracks in this report.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04666v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>经过预训练的大型语言模型（LLM）在一系列不同的语言推理和理解任务中取得了成功。LLM的预训练阶段着眼于大量的原始文本数据。BabyLM共享任务将LLM预训练与人类语言习得进行了比较，其中13岁儿童看到的标记数量比LLM看到的标记的数量小很多。在这项工作中，我们对LLM进行了预训练，并评估了他们使用与儿童大致相同数量的标记来学习上下文单词表示的能力。我们提供了一套强有力的基线；对于不同的体系结构，评估跨时期的性能变化，并报告任务的严格小型和严格跟踪的预训练指标。我们还试图松散地复制任务组织者给出的RoBERTa基线，以观察训练对超参数选择的稳健性和可复制性。我们在本报告中提供了严格和严格的小轨道的提交细节。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04666v4" target="_blank">2311.04666v4</a>
                              </td>
                              <td>Pre-training LLMs using human-like development data corpus</td>
                              <td>Khushi Bhardwaj</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04666v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04666v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04925v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Impact of Reasoning Step Length on Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04925v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04925v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04925v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04925v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>思维链（CoT）对提高大型语言模型（LLM）的推理能力具有重要意义。然而，CoT的有效性与提示中推理步骤的长度之间的相关性在很大程度上仍然未知。为了阐明这一点，我们进行了几个实证实验来探索它们之间的关系。具体来说，我们设计了实验，在CoT演示中扩展和压缩基本原理推理步骤，同时保持所有其他因素不变。我们有以下主要发现。首先，结果表明，延长提示中的推理步骤，即使不在提示中添加新信息，也能显著提高LLM在多个数据集上的推理能力。或者，即使在保留关键信息的情况下，缩短推理步骤也会显著削弱模型的推理能力。这一发现强调了CoT提示中步骤数量的重要性，并为更好地利用LLM在复杂问题解决场景中的潜力提供了实用指导。其次，我们还调查了CoT的性能与演示中使用的理由之间的关系。令人惊讶的是，结果表明，即使是不正确的推理，如果它们保持必要的推理长度，也会产生有利的结果。第三，我们观察到增加推理步骤的优势取决于任务：更简单的任务需要更少的步骤，而复杂的任务从更长的推理序列中获得显著的收益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04925v1" target="_blank">2401.04925v1</a>
                              </td>
                              <td>The Impact of Reasoning Step Length on Large Language Models</td>
                              <td>Mingyu Jin</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04925v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04925v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04898v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04898v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04898v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04898v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation result compared to existing benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04898v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，出现了各种大型语言模型（LLM）评估数据集，但其中大多数都存在排名失真和模型能力分析困难的问题。针对这些问题，本文引入了中国多选题评估基准ANGO。ANGO首次提出\textit｛Keypoint｝分类标准，ANGO中的每个问题都可以对应多个关键点，有效提高了评价结果的可解释性。基于真实人类的表现，我们建立了一个可量化的问题难度标准，并将ANGO问题划分为9个难度级别，为模型训练提供了更精确的指导。为了最大限度地减少数据泄露的影响并充分利用ANGO的创新功能，我们设计了独家采样策略和支持快速测试集迭代的新评估框架。我们的实验表明，与现有的基准相比，ANGO对模型提出了更大的挑战，并在评估结果中揭示了更多的细节。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04898v1" target="_blank">2401.04898v1</a>
                              </td>
                              <td>ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain</td>
                              <td>Bingchao Wang</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04898v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04898v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04883v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04883v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04883v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04883v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development, while most existing research has primarily centered on single-user chatbots that focus on deciding "What" to answer after user inputs. In this paper, we identified that multi-user chatbots have more complex 3W design dimensions -- "What" to say, "When" to respond, and "Who" to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an LLM-based framework for chatbots specifically designed for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Utterance Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and the appropriate recipients. To make the optimizing process for MUCA easier, we further propose an LLM-based Multi-User Simulator (MUS) that can mimic real user behavior. This enables faster simulation of a conversation between the chatbot and simulated users, making the early development of the chatbot framework much more efficient. MUCA demonstrates effectiveness, including appropriate chime-in timing, relevant content, and positive user engagement, in goal-oriented conversations with a small to medium number of participants, as evidenced by case studies and experimental results from user studies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04883v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展为聊天机器人的开发提供了一条新途径，而大多数现有研究主要集中在单用户聊天机器人上，这些聊天机器人专注于决定用户输入后要回答的“什么”。在这篇论文中，我们发现多用户聊天机器人有更复杂的3W设计维度——“说什么”、“什么时候”回应和“谁”回答。此外，我们还提出了多用户聊天助手（MUCA），这是一个专门为小组讨论设计的基于LLM的聊天机器人框架。MUCA由三个主要模块组成：子主题生成器、对话分析器和话语策略仲裁器。这些模块共同确定合适的响应内容、时间和合适的接收者。为了使MUCA的优化过程更容易，我们进一步提出了一种基于LLM的多用户模拟器（MUS），它可以模拟真实的用户行为。这使得能够更快地模拟聊天机器人和模拟用户之间的对话，使聊天机器人框架的早期开发更加高效。如案例研究和用户研究的实验结果所证明的，MUCA在与中小型参与者的目标导向对话中表现出了有效性，包括适当的时机、相关内容和积极的用户参与。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04883v1" target="_blank">2401.04883v1</a>
                              </td>
                              <td>Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations</td>
                              <td>Manqing Mao</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04883v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04883v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04881v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04881v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04881v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04881v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As LLMs have become capable of processing more complex types of inputs, researchers have recently studied how to efficiently and affordably process possibly arbitrarily long sequences. One effective approach is to use a FIFO memory to store keys and values of an attention sublayer from past chunks to allow subsequent queries to attend. However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture. In this paper, we propose to use eviction policies, such as LRA and LFA, to reduce the memory size and adapt to various architectures, and we also propose the Attendre layer, a wait-to-attend mechanism by retrieving the key-value memory (K/V memory) with evicted queries in the query memory (Q memory). As a first step, we evaluate this method in the context length extension setup using the TriviaQA reading comprehension task, and show the effectiveness of the approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04881v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着LLM能够处理更复杂类型的输入，研究人员最近研究了如何高效且经济地处理可能任意长的序列。一种有效的方法是使用FIFO存储器来存储来自过去块的注意力子层的键和值，以允许后续查询参与。然而，这种方法需要大的内存和/或考虑特定的LM体系结构。此外，由于先前上下文中的键值和当前查询之间的因果性质，该方法不能扩展到双向关注，例如在编码器-解码器或仅PrefixLM解码器的架构中。在本文中，我们建议使用LRA和LFA等驱逐策略来减少内存大小并适应各种体系结构，我们还提出了Attentre层，这是一种等待参与机制，通过在查询内存（Q内存）中检索具有驱逐查询的键值内存（K/V内存）。作为第一步，我们使用TriviaQA阅读理解任务在上下文长度扩展设置中评估了该方法，并展示了该方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04881v1" target="_blank">2401.04881v1</a>
                              </td>
                              <td>Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing</td>
                              <td>Zi Yang</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04881v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04881v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03653v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An exploratory study on automatic identification of assumptions in the development of deep learning frameworks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03653v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03653v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03653v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i.e., ALBERT), and a large language model (i.e., ChatGPT) of identifying assumptions on the AssuEval dataset. The experiment results show that: ALBERT achieves the best performance (f1-score: 0.9584) of identifying assumptions on the AssuEval dataset, which is much better than the other models (the 2nd best f1-score is 0.6211, achieved by ChatGPT). Though ChatGPT is the most popular large language model, we do not recommend using it to identify assumptions in DL framework development because of its low performance on the task. Fine-tuning ChatGPT specifically for assumption identification could improve the performance. This study provides researchers with the largest dataset of assumptions for further research (e.g., assumption classification, evaluation, and reasoning) and helps practitioners better understand assumptions and how to manage them in their projects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03653v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利益相关者在开发深度学习（DL）框架时不断做出假设。这些假设与各种类型的软件工件（例如，需求、设计决策和技术债务）有关，并且可能被证明是无效的，从而导致系统故障。现有的假设管理方法和工具通常依赖于对假设的手动识别。然而，假设分散在DL框架开发的各种来源（例如，代码注释、提交、拉取请求和问题）中，手动识别假设的成本很高（例如，时间和资源）。为了克服DL框架开发中手动识别假设的问题，我们构建了一个新的、最大的假设数据集（即AssuEval），该数据集是从GitHub上的TensorFlow和Keras存储库中收集的；探讨了在AssuEval数据集上识别假设的七个传统机器学习模型（例如，支持向量机、分类和回归树）、一个流行的DL模型（即ALBERT）和一个大型语言模型（即ChatGPT）的性能。实验结果表明：ALBERT在AssuEval数据集上识别假设的性能最好（f1得分：0.9584），远优于其他模型（第二好f1得分为0.6211，由ChatGPT实现）。尽管ChatGPT是最流行的大型语言模型，但我们不建议在DL框架开发中使用它来确定假设，因为它在任务上的性能很低。专门针对假设识别对ChatGPT进行微调可以提高性能。这项研究为研究人员提供了最大的假设数据集，以供进一步研究（例如，假设分类、评估和推理），并帮助从业者更好地理解假设以及如何在项目中管理这些假设。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03653v2" target="_blank">2401.03653v2</a>
                              </td>
                              <td>An exploratory study on automatic identification of assumptions in the development of deep learning frameworks</td>
                              <td>Chen Yang</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03653v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03653v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_17352v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_17352v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_17352v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_17352v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_17352v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自动音频字幕（AAC）旨在为来自自然和/或人类活动的各种声音生成信息性描述。近年来，AAC迅速吸引了研究兴趣，最先进的系统现在依赖于由变压器等强大模型提供动力的序列到序列（seq2seq）主干。遵循应用机器学习研究的宏观趋势，在这项工作中，我们努力通过广泛利用预训练模型和大型语言模型（LLM）来提高seq2seq AAC模型的性能。具体来说，我们利用BEAT来提取细粒度的音频特征。然后，我们使用讲师LLM来获取字幕的文本嵌入，并通过辅助的InfoNCE损失函数将其语言模态知识注入BEAT音频特征中。此外，我们提出了一种新的数据增强方法，该方法使用ChatGPT来产生字幕混合（即两个字幕的语法和紧凑组合），这与相应的音频混合一起，不仅增加了训练数据的数量，而且增加了训练数据的复杂性和多样性。在推理过程中，我们建议使用核采样和混合重排序算法，这在AAC研究中尚未探索。结合我们的努力，我们的模型在Clotho评估中获得了最先进的32.6 SPIDEr FL分数，并赢得了2023 DCASE AAC挑战赛。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.17352v2" target="_blank">2309.17352v2</a>
                              </td>
                              <td>Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation</td>
                              <td>Shih-Lun Wu</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_17352v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.17352v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04854v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04854v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04854v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04854v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often do generate entirely novel text. We begin by defending bibliotechnism against this challenge, showing how novel text may be meaningful only in a derivative sense, so that the content of this generated text depends in an important sense on the content of original human text. We go on to present a different, novel challenge for bibliotechnism, stemming from examples in which LLMs generate "novel reference", using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well-explained by the hypothesis that it has such states. In line with this view, we argue that cases of novel reference provide evidence that LLMs do in fact have beliefs, desires, and intentions, and thus have a limited form of agency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04854v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LLM是像复印机或印刷机这样的文化技术吗？它们传递信息但不能创造新的内容？这种我们称之为文献技术主义的想法面临的一个挑战是，LLM通常会生成完全新颖的文本。我们首先为文献技术主义辩护，以应对这一挑战，展示小说文本如何只有在衍生意义上才有意义，从而使生成的文本的内容在重要意义上取决于原始人类文本的内容。我们继续为文献技术学提出一个不同的、新颖的挑战，源于LLM生成“小说参考”的例子，使用小说名称来指代小说实体。如果LLM不是文化技术，而是具有有限形式的能动性（信仰、欲望和意图），那么这些例子可以很好地解释。根据心灵哲学中的解释主义，一个系统有信仰、欲望和意图，当且仅当其行为被具有这种状态的假设很好地解释时。根据这一观点，我们认为，新颖引用的案例提供了证据，证明LLM实际上有信仰、欲望和意图，因此具有有限的代理形式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04854v1" target="_blank">2401.04854v1</a>
                              </td>
                              <td>Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs</td>
                              <td>Harvey Lederman</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04854v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04854v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04853v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Entity Recognition from Colloquial Text</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04853v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04853v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04853v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Extraction of concepts and entities of interest from non-formal texts such as social media posts and informal communication is an important capability for decision support systems in many domains, including healthcare, customer relationship management, and others. Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges. In our research, we focus on the healthcare domain and investigate the problem of symptom recognition from colloquial texts by designing and evaluating several training strategies for BERT-based model fine-tuning. These strategies are distinguished by the choice of the base model, the training corpora, and application of term perturbations in the training data. The best-performing models trained using these strategies outperform the state-of-the-art specialized symptom recognizer by a large margin. Through a series of experiments, we have found specific patterns of model behavior associated with the training strategies we designed. We present design principles for training strategies for effective entity recognition in colloquial texts based on our findings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04853v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从社交媒体帖子和非正式沟通等非正式文本中提取感兴趣的概念和实体是许多领域决策支持系统的重要能力，包括医疗保健、客户关系管理等。尽管最近在为各种自然语言处理任务训练大型语言模型方面取得了进展，但所开发的模型和技术主要集中在正式文本上，在口语数据上表现不佳，口语数据具有许多明显的挑战。在我们的研究中，我们专注于医疗保健领域，并通过设计和评估基于BERT的模型微调的几种训练策略来研究口语文本中的症状识别问题。这些策略通过选择基本模型、训练语料库和在训练数据中应用项扰动来区分。使用这些策略训练的性能最好的模型在很大程度上优于最先进的专业症状识别器。通过一系列实验，我们发现了与我们设计的训练策略相关的模型行为的特定模式。基于我们的研究结果，我们提出了口语文本中有效实体识别的训练策略的设计原则。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04853v1" target="_blank">2401.04853v1</a>
                              </td>
                              <td>Entity Recognition from Colloquial Text</td>
                              <td>Tamara Babaian</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04853v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04853v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04842v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04842v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04842v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04842v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the generated answers. In the second, we compare generated answers to the top results retrieved by a diverse set of retrieval models, ranging from traditional approaches to advanced methods, allowing us to measure improvements without human judgments. In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the retrieval benchmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04842v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型现在可以直接生成许多事实问题的答案，而无需参考外部来源。不幸的是，人们对评估这些答案的质量和正确性、将一个模型的性能与另一个模型进行比较或将一个提示与另一提示进行比较的方法关注相对较少。此外，生成答案的质量很少与检索到的答案的质量直接进行比较。随着模型的发展和提示的修改，我们没有系统的方法来衡量改进，而不诉诸于昂贵的人工判断。为了解决这个问题，我们采用标准检索基准来评估大型语言模型生成的答案。受BERTScore总结度量的启发，我们探索了两种方法。首先，我们的评估基于基准相关性判断。我们根据经验进行了关于如何利用信息检索相关性判断作为评估生成答案的锚的实验。在第二部分中，我们将生成的答案与一组不同的检索模型检索到的最高结果进行比较，从传统方法到高级方法，使我们能够在没有人为判断的情况下衡量改进。在这两种情况下，我们都测量生成答案的嵌入表示与检索基准中已知或假设的相关段落的嵌入表示之间的相似性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04842v1" target="_blank">2401.04842v1</a>
                              </td>
                              <td>Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers</td>
                              <td>Negar Arabzadeh</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04842v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04842v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13500v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13500v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13500v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13500v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially advantageous in addressing cold start scenarios characterized by limited graph data. Validation across five real-world datasets sourced from the PeerWise platform underscores our approach's effectiveness. Our method outperforms baselines, showcasing enhanced predictive accuracy and robustness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13500v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习者资源通过学生内容的创建为可扩展的教育提供了巨大的潜力。然而，由于学生生成的数据中固有的噪声，预测学生在学习者源问题上的表现是具有挑战性的，这对个性化学习体验至关重要。此外，尽管传统的基于图的方法可以捕捉学生和问题互动的复杂网络，但在冷启动条件下，学生对问题的参与有限，数据稀少，这些方法往往无法实现。为了应对这两个挑战，我们引入了一种创新策略，该策略协同集成有符号图神经网络（SGNN）和大型语言模型（LLM）嵌入的潜力。我们的方法采用有符号的二分图对学生的答案进行全面建模，并辅以增强噪声抵御能力的对比学习框架。此外，LLM的贡献在于生成基础问题嵌入，这在解决以有限图形数据为特征的冷启动场景方面尤其有利。来自PeerWise平台的五个真实世界数据集的验证强调了我们方法的有效性。我们的方法优于基线，显示出增强的预测准确性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13500v2" target="_blank">2309.13500v2</a>
                              </td>
                              <td>Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy</td>
                              <td>Lin Ni</td>
                              <td>2023-09-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13500v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13500v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04092v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04092v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04092v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04092v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite recent advances in text-to-3D generative methods, there is a notable absence of reliable evaluation metrics. Existing metrics usually focus on a single criterion each, such as how well the asset aligned with the input text. These metrics lack the flexibility to generalize to different evaluation criteria and might not align well with human preferences. Conducting user preference studies is an alternative that offers both adaptability and human-aligned results. User studies, however, can be very expensive to scale. This paper presents an automatic, versatile, and human-aligned evaluation metric for text-to-3D generative models. To this end, we first develop a prompt generator using GPT-4V to generate evaluating prompts, which serve as input to compare text-to-3D models. We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria. Finally, we use these pairwise comparison results to assign these models Elo ratings. Experimental results suggest our metric strongly align with human preference across different evaluation criteria.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04092v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管最近在文本到三维生成方法方面取得了进展，但明显缺乏可靠的评估指标。现有的指标通常只关注一个标准，例如资产与输入文本的对齐程度。这些指标缺乏推广到不同评估标准的灵活性，可能与人类偏好不太一致。进行用户偏好研究是一种既能提供适应性又能提供与人类一致的结果的替代方案。然而，用户研究的规模可能非常昂贵。本文提出了一种用于文本到三维生成模型的自动、通用和人工对齐的评估度量。为此，我们首先开发了一个使用GPT-4V生成评估提示的提示生成器，该提示用作将文本与3D模型进行比较的输入。我们进一步设计了一种方法，指示GPT-4V根据用户定义的标准比较两个3D资产。最后，我们使用这些成对的比较结果来为这些模型分配Elo评级。实验结果表明，在不同的评估标准中，我们的指标与人类的偏好非常一致。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04092v2" target="_blank">2401.04092v2</a>
                              </td>
                              <td>GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</td>
                              <td>Tong Wu</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04092v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04092v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/3DTopia/GPTEval3D" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09996v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09996v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09996v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09996v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we explore effective prompting techniques to enhance zero- and few-shot Visual Question Answering (VQA) performance in contemporary Vision-Language Models (VLMs). Central to our investigation is the role of question templates in guiding VLMs to generate accurate answers. We identify that specific templates significantly influence VQA outcomes, underscoring the need for strategic template selection. Another pivotal aspect of our study is augmenting VLMs with image captions, providing them with additional visual cues alongside direct image features in VQA tasks. Surprisingly, this augmentation significantly improves the VLMs' performance in many cases, even though VLMs "see" the image directly! We explore chain-of-thought (CoT) reasoning and find that while standard CoT reasoning causes drops in performance, advanced methods like self-consistency can help recover it. Furthermore, we find that text-only few-shot examples enhance VLMs' alignment with the task format, particularly benefiting models prone to verbose zero-shot answers. Lastly, to mitigate the challenges associated with evaluating free-form open-ended VQA responses using string-matching based VQA metrics, we introduce a straightforward LLM-guided pre-processing technique to adapt the model responses to the expected ground-truth answer distribution. In summary, our research sheds light on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09996v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们探索了在当代视觉语言模型（VLM）中提高零镜头和少镜头视觉问答（VQA）性能的有效提示技术。我们调查的核心是问题模板在引导VLM生成准确答案方面的作用。我们发现，特定的模板会显著影响VQA的结果，强调了战略模板选择的必要性。我们研究的另一个关键方面是用图像字幕增强VLM，在VQA任务中为它们提供额外的视觉提示和直接的图像特征。令人惊讶的是，这种增强在许多情况下显著提高了VLM的性能，即使VLM可以直接“看到”图像！我们探索了思考链（CoT）推理，发现虽然标准的CoT推理会导致性能下降，但自我意识等高级方法可以帮助恢复它。此外，我们发现，基于文本的较少搜索示例增强了VLM与任务格式的一致性，特别有利于倾向于冗长零样本答案的模型。最后，为了缓解使用基于字符串匹配的VQA度量评估自由形式开放式VQA响应的相关挑战，我们引入了一种直接的LLM引导的预处理技术，以使模型响应适应预期的基本事实答案分布。总之，我们的研究揭示了VQA的VLM中提示策略的复杂性，强调了字幕、模板和预处理的协同使用，以提高模型的功效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09996v2" target="_blank">2306.09996v2</a>
                              </td>
                              <td>Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering</td>
                              <td>Rabiul Awal</td>
                              <td>2023-06-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09996v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09996v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/rabiulcste/vqazero" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11406v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LaMP: When Large Language Models Meet Personalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11406v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11406v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11406v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11406v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文强调了个性化在大型语言模型中的重要性，并介绍了LaMP基准——一种用于训练和评估语言模型以产生个性化输出的新基准。LaMP提供了一个全面的评估框架，包含不同的语言任务和每个用户配置文件的多个条目。它由七个个性化任务组成，涵盖三个文本分类和四个文本生成任务。我们还提出了两种检索增强方法，从每个用户档案中检索个人项目，以个性化语言模型输出。为此，我们研究了各种检索模型，包括术语匹配、语义匹配和时间感知方法。针对零样本和微调语言模型的LaMP的大量实验证明了所提出的检索增强方法的有效性，并强调了个性化在各种自然语言任务中的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11406v3" target="_blank">2304.11406v3</a>
                              </td>
                              <td>LaMP: When Large Language Models Meet Personalization</td>
                              <td>Alireza Salemi</td>
                              <td>2023-04-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11406v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11406v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17100v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17100v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17100v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17100v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, greatly enhancing its utility as a biomedical assistant, similar to ChatGPT. Our method demonstrates effective training with diverse datasets can lead to more practical biomedical AI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17100v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>传统的任务和模态专用人工智能（AI）模型在生物医学的实际部署和维护中是不灵活的。与此同时，生物医学数据的日益可用性，加上现代多模式多任务人工智能技术的进步，为通用生物医学人工智能解决方案的出现铺平了道路。这些解决方案有可能解释不同的医疗模式，并产生富有表现力的输出，如自由文本报告或疾病诊断。在这里，我们提出了BiomedGPT，这是第一个用于各种生物医学任务的开源通用视觉语言人工智能。BiomedGPT在26个数据集上完成了5项具有临床意义的任务，获得了16项最先进的结果。值得注意的是，它在放射学人类评估方面优于OpenAI的GPT-4视力（GPT-4V），在乳腺癌症诊断和医学视觉问答方面超过了谷歌的Med-PaLM M（12B）。此外，BiomedGPT促进了零样本迁移学习，极大地提高了其作为生物医学助手的实用性，类似于ChatGPT。我们的方法证明，使用不同数据集进行有效训练可以带来更实用的生物医学人工智能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17100v2" target="_blank">2305.17100v2</a>
                              </td>
                              <td>BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks</td>
                              <td>Kai Zhang</td>
                              <td>2023-05-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17100v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17100v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/taokz/biomedgpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02987v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02987v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02987v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02987v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The emergence of pretrained models has significantly impacted Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and image models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02987v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预训练模型的出现对关系数据集的自然语言处理（NLP）和计算机视觉产生了重大影响。传统上，这些模型是通过微调下游任务来评估的。然而，这就提出了如何更有效地评估这些模型的问题。在这项研究中，我们探索了一种新的方法，即利用与每个实体相关的元特征作为世界知识的来源，并使用模型中的实体表示。我们建议使用这些表示和元特征之间的一致性作为评估预训练模型的度量。我们的方法在各个领域都得到了验证，包括关系数据集模型、大型语言模型和图像模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02987v2" target="_blank">2401.02987v2</a>
                              </td>
                              <td>Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach</td>
                              <td>Prince Aboagye</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02987v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02987v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01275v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01275v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01275v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01275v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01275v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，大型语言模型（LLM）的出现使生成代理发生了革命性的变化。其中，角色扮演会话代理（RPCA）由于其在情感上吸引用户的能力而吸引了相当多的关注。然而，缺乏一个全面的基准阻碍了这一领域的进展。为了弥补这一差距，我们引入了CharacterEval，这是一个全面的RPCA评估的中国基准，并辅以量身定制的高质量数据集。该数据集包括1785个多回合角色扮演对话，包括23020个例子，77个来自中国小说和剧本的角色。它是经过精心构建的，从最初的GPT-4对话提取开始，然后是严格的人为质量控制，并通过来自百度百科的深入人物简介进行了增强。CharacterEval采用了多方面的评估方法，包括四个维度上的十三个有针对性的指标。在CharacterEval上进行的综合实验表明，与GPT-4相比，中国LLM在中国角色扮演会话中表现出更具前景的能力。源代码、数据源和奖励模型将在https://github.com/morecry/CharacterEval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>49</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01275v2" target="_blank">2401.01275v2</a>
                              </td>
                              <td>CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation</td>
                              <td>Quan Tu</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01275v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01275v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/morecry/charactereval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2206_10668v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_10668v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_10668v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_10668v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-art methods for syntactic and semantic parsing when the model output is constrained to be valid.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_10668v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，当输出被约束为有效的语义表示时，从提示或微调的语言模型生成可以很好地执行语义解析。我们介绍了BenchCLAMP，这是一种评估约束L语言模型解析的基准，它包括七个语义解析数据集和两个具有不同输出表示的句法解析数据集的上下文无关语法，以及一个仅生成这些语法所涵盖的有效输出的约束解码接口。我们为每个数据集提供低、中、高资源划分，允许在不同的数据体系下准确比较各种语言模型。我们的基准测试支持使用基于提示的学习和微调来评估语言模型。我们对八种语言模型进行了基准测试，其中包括两种仅通过API可用的GPT-3变体。我们的实验表明，当模型输出被约束为有效时，编码器-解码器预训练的语言模型可以实现类似的性能，或者超过最先进的句法和语义解析方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.10668v2" target="_blank">2206.10668v2</a>
                              </td>
                              <td>BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing</td>
                              <td>Subhro Roy</td>
                              <td>2022-06-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_10668v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.10668v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/microsoft/semantic_parsing_with_constrained_lm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04651v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning to Prompt Segment Anything Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04651v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04651v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04651v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04651v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>细分任何事物模型（SAM），如SEEM和SAM，在学习细分任何事物方面显示出巨大的潜力。SAM的核心设计在于可提示分割，它以手工制作的提示作为输入，并返回期望的分割掩码。SAM使用两种类型的提示，包括空间提示（如点）和语义提示（如文本），这两种提示共同作用，提示SAM对下游数据集上的任何内容进行分段。尽管提示具有重要作用，但如何为SAM获取合适的提示在很大程度上还没有得到充分的探索。在这项工作中，我们研究了SAM的体系结构，并确定了学习SAM有效提示的两个挑战。为此，我们提出了空间语义提示学习（SSPrompt），它可以学习有效的语义和空间提示，以获得更好的SAM。具体而言，SSPrompt引入了空间提示学习和语义提示学习，它们直接在嵌入空间上优化空间提示和语义提示，并选择性地利用预先训练的提示编码器中编码的知识。大量实验表明，SSPrompt在多个广泛采用的数据集上一致地实现了卓越的图像分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04651v1" target="_blank">2401.04651v1</a>
                              </td>
                              <td>Learning to Prompt Segment Anything Models</td>
                              <td>Jiaxing Huang</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04651v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04651v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15374v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15374v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15374v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15374v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The global generation of renewable energy has rapidly increased, primarily due to the installation of large-scale renewable energy power plants. However, monitoring renewable energy assets in these large plants remains challenging due to environmental factors that could result in reduced power generation, malfunctioning, and degradation of asset life. Therefore, the detection of surface defects on renewable energy assets is crucial for maintaining the performance and efficiency of these plants. This paper proposes an innovative detection framework to achieve an economical surface monitoring system for renewable energy assets. High-resolution images of the assets are captured regularly and inspected to identify surface or structural damages on solar panels and wind turbine blades. {Vision transformer (ViT), one of the latest attention-based deep learning (DL) models in computer vision, is proposed in this work to classify surface defects.} The ViT model outperforms other DL models, including MobileNet, VGG16, Xception, EfficientNetB7, and ResNet50, achieving high accuracy scores above 97\% for both wind and solar plant assets. From the results, our proposed model demonstrates its potential for monitoring and detecting damages in renewable energy assets for efficient and reliable operation of renewable power plants.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15374v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全球可再生能源发电量迅速增加，主要是由于安装了大型可再生能源发电厂。然而，由于环境因素可能导致发电量减少、故障和资产寿命下降，监测这些大型工厂的可再生能源资产仍然具有挑战性。因此，检测可再生能源资产的表面缺陷对于保持这些工厂的性能和效率至关重要。本文提出了一种创新的检测框架，以实现经济的可再生能源资产表面监测系统。定期拍摄并检查资产的高分辨率图像，以确定太阳能电池板和风力涡轮机叶片的表面或结构损坏。｛视觉转换器（ViT）是计算机视觉中最新的基于注意力的深度学习（DL）模型之一，在这项工作中被提出用于对表面缺陷进行分类。｝该ViT模型优于其他DL模型，包括MobileNet、VGG16、Xception、EfficientNetB7和ResNet50，在风能和太阳能发电厂资产中实现了97%以上的高精度分数。从结果来看，我们提出的模型展示了其监测和检测可再生能源资产损害的潜力，以实现可再生发电厂的高效可靠运行。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15374v3" target="_blank">2211.15374v3</a>
                              </td>
                              <td>Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model</td>
                              <td>Divyanshi Dwivedi</td>
                              <td>2022-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15374v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15374v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03907v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03907v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03907v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03907v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal 3D object detectors are dedicated to exploring secure and reliable perception systems for autonomous driving (AD). However, while achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they tend to overlook the complexity and harsh conditions of real-world environments. Meanwhile, with the emergence of visual foundation models (VFMs), opportunities and challenges are presented for improving the robustness and generalization of multi-modal 3D object detection in autonomous driving. Therefore, we propose RoboFusion, a robust framework that leverages VFMs like SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the original SAM for autonomous driving scenarios named SAM-AD. To align SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the image features extracted by SAM. We employ wavelet decomposition to denoise the depth-guided images for further noise reduction and weather interference. Lastly, we employ self-attention mechanisms to adaptively reweight the fused features, enhancing informative features while suppressing excess noise. In summary, our RoboFusion gradually reduces noise by leveraging the generalization and robustness of VFMs, thereby enhancing the resilience of multi-modal 3D object detection. Consequently, our RoboFusion achieves state-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03907v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式3D物体探测器致力于探索安全可靠的自动驾驶感知系统。然而，尽管在干净的基准数据集上实现了最先进的（SOTA）性能，但它们往往忽略了现实世界环境的复杂性和恶劣条件。同时，随着视觉基础模型（VFM）的出现，在自动驾驶中提高多模态三维物体检测的鲁棒性和泛化能力也面临着机遇和挑战。因此，我们提出了RoboFusion，这是一个强大的框架，它利用像SAM这样的VFM来解决分布外（OOD）噪声场景。我们首先将最初的SAM应用于名为SAM-AD的自动驾驶场景。为了将SAM或SAM-AD与多模态方法对准，我们引入AD-FPN对SAM提取的图像特征进行上采样。我们使用小波分解对深度引导图像进行去噪，以进一步降低噪声和天气干扰。最后，我们使用自注意机制来自适应地重新加权融合的特征，增强信息特征，同时抑制过量噪声。总之，我们的RoboFusion通过利用VFM的泛化和鲁棒性逐渐降低噪声，从而增强了多模态3D对象检测的弹性。因此，我们的RoboFusion在噪声场景中实现了最先进的性能，正如KITTI-C和nuScenes-C基准测试所证明的那样。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03907v1" target="_blank">2401.03907v1</a>
                              </td>
                              <td>RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM</td>
                              <td>Ziying Song</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03907v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03907v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02317v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02317v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02317v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02317v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address the challenge of image resolution variation for the Segment Anything Model (SAM). SAM, known for its zero-shot generalizability, exhibits a performance degradation when faced with datasets with varying image sizes. Previous approaches tend to resize the image to a fixed size or adopt structure modifications, hindering the preservation of SAM's rich prior knowledge. Besides, such task-specific tuning necessitates a complete retraining of the model, which is cost-expensive and unacceptable for deployment in the downstream tasks. In this paper, we reformulate this issue as a length extrapolation problem, where token sequence length varies while maintaining a consistent patch size for images of different sizes. To this end, we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's adaptability to varying image resolutions while eliminating the need for structure modifications. Firstly, we introduce a new scaling factor to ensure consistent magnitude in the attention layer's dot product values when the token sequence length changes. Secondly, we present a bias-mode attention mask that allows each token to prioritize neighboring information, mitigating the impact of untrained distant information. Our BA-SAM demonstrates efficacy in two scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets, including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to significantly mitigate performance degradation in the zero-shot setting and achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we propose a generalized model and benchmark, showcasing BA-SAM's generalizability across all four datasets simultaneously.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02317v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了分段任意模型（SAM）的图像分辨率变化的挑战。SAM以其零样本可推广性而闻名，当面对具有不同图像大小的数据集时，表现出性能下降。以前的方法倾向于将图像调整为固定大小或采用结构修改，阻碍了SAM丰富的先验知识的保存。此外，这种特定于任务的调整需要对模型进行完全的重新训练，这是成本高昂的，并且对于在下游任务中的部署来说是不可接受的。在本文中，我们将此问题重新表述为长度外推问题，其中令牌序列长度变化，同时保持不同大小图像的一致补丁大小。为此，我们提出了可伸缩偏置模式注意掩码（BA-SAM），以增强SAM对不同图像分辨率的适应性，同时消除对结构修改的需要。首先，我们引入了一个新的缩放因子，以确保当令牌序列长度变化时，注意力层的点积值的大小一致。其次，我们提出了一种偏置模式注意力掩码，允许每个令牌对相邻信息进行优先级排序，从而减轻未经训练的远距离信息的影响。我们的BA-SAM在两种情况下证明了有效性：零样本和微调。对包括DIS5K、DUTS、ISIC、COD10K和COCO在内的各种数据集进行的广泛评估表明，它能够显著缓解零样本环境下的性能退化，并在最小的微调下实现最先进的性能。此外，我们提出了一个广义模型和基准，同时展示了BA-SAM在所有四个数据集上的可推广性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02317v2" target="_blank">2401.02317v2</a>
                              </td>
                              <td>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model</td>
                              <td>Yiran Song</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02317v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02317v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zongzi13545329/ba-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02955v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02955v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02955v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02955v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The CLIP and Segment Anything Model (SAM) are remarkable vision foundation models (VFMs). SAM excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero-shot recognition capabilities. This paper presents an in-depth exploration of integrating these two models into a unified framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired model designed for simultaneous interactive segmentation and recognition, leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The former adapts SAM's knowledge into the CLIP via distillation and learnable transformer adapters, while the latter transfers CLIP knowledge into SAM, enhancing its recognition capabilities. Extensive experiments on various datasets and detectors show the effectiveness of Open-Vocabulary SAM in both segmentation and recognition tasks, significantly outperforming the naive baselines of simply combining SAM and CLIP. Furthermore, aided with image classification data training, our method can segment and recognize approximately 22,000 classes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02955v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP和Segment Anything模型（SAM）是卓越的愿景基础模型（VFM）。SAM擅长跨不同领域的细分任务，而CLIP以其零样本识别能力而闻名。本文对将这两个模型集成到一个统一的框架中进行了深入的探索。具体来说，我们介绍了开放词汇SAM，这是一个受SAM启发的模型，旨在同时进行交互式分割和识别，利用了两个独特的知识转移模块：SAM2CLIP和CLIP2SAM。前者通过蒸馏和可学习的转换器适配器将SAM的知识改编为CLIP，而后者将CLIP知识转换为SAM，增强其识别能力。在各种数据集和检测器上进行的大量实验表明，开放词汇SAM在分割和识别任务中都是有效的，显著优于简单组合SAM和CLIP的原始基线。此外，在图像分类数据训练的辅助下，我们的方法可以分割和识别大约22000个类别。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02955v1" target="_blank">2401.02955v1</a>
                              </td>
                              <td>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</td>
                              <td>Haobo Yuan</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02955v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02955v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/harboryuan/ovsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08533v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Trustchain -- Trustworthy Decentralised Public Key Infrastructure for Digital Credentials</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08533v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08533v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08533v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The sharing of public key information is central to the digital credential security model, but the existing Web PKI with its opaque Certification Authorities and synthetic attestations serves a very different purpose. We propose a new approach to decentralised public key infrastructure, designed for digital identity, in which connections between legal entities that are represented digitally correspond to genuine, pre-existing relationships between recognisable institutions. In this scenario, users can judge for themselves the level of trust they are willing to place in a given chain of attestations. Our proposal includes a novel mechanism for establishing a root of trust in a decentralised setting via independently-verifiable timestamping. We also present a reference implementation built on open networks, protocols and standards. The system has minimal setup costs and is freely available for any community to adopt as a digital public good.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08533v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>公共密钥信息的共享是数字证书安全模型的核心，但现有的Web PKI及其不透明的证书颁发机构和合成证明有着截然不同的用途。我们提出了一种新的去中心化公钥基础设施方法，旨在实现数字身份，其中以数字方式表示的法律实体之间的联系对应于可识别机构之间真实的、预先存在的关系。在这种情况下，用户可以自己判断他们愿意在给定的证明链中给予的信任程度。我们的提案包括一种新的机制，通过独立可验证的时间戳在去中心化环境中建立信任的根源。我们还提供了一个基于开放网络、协议和标准的参考实现。该系统的安装成本最低，任何社区都可以免费使用该系统作为数字公共产品。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08533v2" target="_blank">2305.08533v2</a>
                              </td>
                              <td>Trustchain -- Trustworthy Decentralised Public Key Infrastructure for Digital Credentials</td>
                              <td>Tim Hobson</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08533v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08533v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alan-turing-institute/trustchain" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02326v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02326v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02326v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02326v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the realm of artificial intelligence, the emergence of foundation models, backed by high computing capabilities and extensive data, has been revolutionary. Segment Anything Model (SAM), built on the Vision Transformer (ViT) model with millions of parameters and vast training dataset SA-1B, excels in various segmentation scenarios relying on its significance of semantic information and generalization ability. Such achievement of visual foundation model stimulates continuous researches on specific downstream tasks in computer vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the high-performing SAM for landcover classification on space-borne Synthetic Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's parameters and incorporates lightweight adapters for parameter efficient fine-tuning, and a classwise mask decoder is designed to achieve semantic segmentation task. This adapt-tuning method allows for efficient landcover classification of SAR images, balancing the accuracy with computational demand. In addition, the task specific input module injects low frequency information of SAR images by MLP-based layers to improve the model performance. Compared to conventional state-of-the-art semantic segmentation algorithms by extensive experiments, CWSAM showcases enhanced performance with fewer computing resources, highlighting the potential of leveraging foundational models like SAM for specific downstream tasks in the SAR domain. The source code is available at: https://github.com/xypu98/CWSAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02326v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在人工智能领域，以高计算能力和广泛数据为支撑的基础模型的出现是革命性的。Segment Anything Model（SAM）建立在具有数百万个参数和庞大训练数据集SA-1B的Vision Transformer（ViT）模型的基础上，凭借其语义信息的重要性和泛化能力，在各种分割场景中表现出色。视觉基础模型的这一成果刺激了对计算机视觉中特定下游任务的不断研究。ClassWise SAM适配器（CWSAM）旨在将高性能SAM应用于星载合成孔径雷达（SAR）图像的陆地覆盖分类。所提出的CWSAM冻结了SAM的大部分参数，并结合了轻量级适配器进行参数高效微调，并设计了一个类掩码解码器来实现语义分割任务。这种自适应调谐方法允许对SAR图像进行有效的陆地覆盖分类，平衡精度与计算需求。此外，特定任务输入模块通过基于MLP的层注入SAR图像的低频信息，以提高模型性能。通过广泛的实验，与传统的最先进的语义分割算法相比，CWSAM以更少的计算资源展示了增强的性能，突出了利用SAM等基础模型执行SAR领域特定下游任务的潜力。源代码位于：https://github.com/xypu98/CWSAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02326v1" target="_blank">2401.02326v1</a>
                              </td>
                              <td>ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation</td>
                              <td>Xinyang Pu</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02326v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02326v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xypu98/cwsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02076v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02076v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02076v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02076v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain Generalization (DG) aims to reduce domain shifts between domains to achieve promising performance on the unseen target domain, which has been widely practiced in medical image segmentation. Single-source domain generalization (SDG) is the most challenging setting that trains on only one source domain. Although existing methods have made considerable progress on SDG of medical image segmentation, the performances are still far from the applicable standards when faced with a relatively large domain shift. In this paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve the ability of generalization. Specifically, we introduce a parallel framework, the source images are sent into the SAM module and normal segmentation module respectively. To reduce the calculation resources, we apply a merging strategy before sending images to the SAM module. We extract the bounding boxes from the segmentation module and send the refined version as prompts to the SAM module. We evaluate our model on a classic DG dataset and achieve competitive results compared to other state-of-the-art DG methods. Furthermore, We conducted a series of ablation experiments to prove the effectiveness of the proposed method. The code is publicly available at https://github.com/SARIHUST/SAMMed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02076v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>域泛化（DG）旨在减少域之间的域偏移，以在看不见的目标域上实现有希望的性能，这已在医学图像分割中广泛应用。单源域泛化（SDG）是仅在一个源域上训练的最具挑战性的设置。尽管现有的方法在医学图像分割的SDG方面取得了长足的进步，但在面临相对较大的域偏移时，其性能仍远未达到适用的标准。在本文中，我们将分段任意模型（SAM）应用于SDG，以大大提高泛化能力。具体来说，我们引入了一个并行框架，将源图像分别发送到SAM模块和正常分割模块。为了减少计算资源，我们在将图像发送到SAM模块之前应用合并策略。我们从分割模块中提取边界框，并将细化版本作为提示发送到SAM模块。我们在经典的DG数据集上评估了我们的模型，并与其他最先进的DG方法相比取得了有竞争力的结果。此外，我们还进行了一系列烧蚀实验来证明所提出方法的有效性。该代码可在https://github.com/SARIHUST/SAMMed.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02076v1" target="_blank">2401.02076v1</a>
                              </td>
                              <td>Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation</td>
                              <td>Hanhui Wang</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02076v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02076v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sarihust/sammed" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00488v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On Memorization and Privacy Risks of Sharpness Aware Minimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00488v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00488v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00488v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00488v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在最近的许多工作中，人们越来越关注设计算法，为神经网络损失优化寻求更平坦的最优值，因为有经验证据表明，它在许多数据集中具有更好的泛化性能。在这项工作中，我们通过过参数化模型中的数据记忆来剖析这些性能增益。我们定义了一个新的指标，帮助我们确定哪些数据点特别适合与普通SGD相比，寻求更平坦最优的算法。我们发现，通过Sharpness Aware Minimization（SAM）实现的泛化增益对于非典型数据点尤其显著，这需要记忆。这一见解有助于我们挖掘与SAM相关的更高隐私风险，我们通过详尽的实证评估来验证这一点。最后，我们提出了缓解策略，以实现更理想的准确性与隐私的权衡。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00488v2" target="_blank">2310.00488v2</a>
                              </td>
                              <td>On Memorization and Privacy Risks of Sharpness Aware Minimization</td>
                              <td>Young In Kim</td>
                              <td>2023-09-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00488v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00488v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01563v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Multi-Objective High-Dimensional Feature Selection via Evolutionary Multitasking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01563v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01563v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01563v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Evolutionary Multitasking (EMT) paradigm, an emerging research topic in evolutionary computation, has been successfully applied in solving high-dimensional feature selection (FS) problems recently. However, existing EMT-based FS methods suffer from several limitations, such as a single mode of multitask generation, conducting the same generic evolutionary search for all tasks, relying on implicit transfer mechanisms through sole solution encodings, and employing single-objective transformation, which result in inadequate knowledge acquisition, exploitation, and transfer. To this end, this paper develops a novel EMT framework for multiobjective high-dimensional feature selection problems, namely MO-FSEMT. In particular, multiple auxiliary tasks are constructed by distinct formulation methods to provide diverse search spaces and information representations and then simultaneously addressed with the original task through a multi-slover-based multitask optimization scheme. Each task has an independent population with task-specific representations and is solved using separate evolutionary solvers with different biases and search preferences. A task-specific knowledge transfer mechanism is designed to leverage the advantage information of each task, enabling the discovery and effective transmission of high-quality solutions during the search process. Comprehensive experimental results demonstrate that our MO-FSEMT framework can achieve overall superior performance compared to the state-of-the-art FS methods on 26 datasets. Moreover, the ablation studies verify the contributions of different components of the proposed MO-FSEMT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01563v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>进化多任务（EMT）范式是进化计算中一个新兴的研究课题，近年来已成功应用于解决高维特征选择（FS）问题。然而，现有的基于EMT的FS方法存在一些局限性，例如单一的多任务生成模式、对所有任务进行相同的通用进化搜索、通过唯一解编码依赖于隐式转移机制以及采用单目标转换，这些都导致了知识获取、利用和转移不足。为此，本文开发了一种新的多目标高维特征选择问题的EMT框架，即MO-FSEMT。特别地，通过不同的公式化方法构建多个辅助任务，以提供不同的搜索空间和信息表示，然后通过基于多任务的多任务优化方案与原始任务同时处理。每个任务都有一个具有特定任务表示的独立群体，并使用具有不同偏见和搜索偏好的独立进化求解器进行求解。特定任务的知识转移机制旨在利用每个任务的优势信息，从而在搜索过程中发现并有效传输高质量的解决方案。综合实验结果表明，在26个数据集上，与最先进的FS方法相比，我们的MO-FSEMT框架可以实现总体优越的性能。此外，消融研究验证了所提出的MO-FSEMT的不同成分的贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01563v1" target="_blank">2401.01563v1</a>
                              </td>
                              <td>Towards Multi-Objective High-Dimensional Feature Selection via Evolutionary Multitasking</td>
                              <td>Yinglan Feng</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01563v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11715v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11715v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11715v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11715v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything (SAM), an advanced universal image segmentation model trained on an expansive visual dataset, has set a new benchmark in image segmentation and computer vision. However, it faced challenges when it came to distinguishing between shadows and their backgrounds. To address this, we developed Deshadow-Anything, considering the generalization of large-scale datasets, and we performed Fine-tuning on large-scale datasets to achieve image shadow removal. The diffusion model can diffuse along the edges and textures of an image, helping to remove shadows while preserving the details of the image. Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input perturbation (DDPM-AIP) to accelerate the iterative training speed of diffusion. Experiments on shadow removal tasks demonstrate that these methods can effectively improve image restoration performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11715v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything（SAM）是一种在扩展的视觉数据集上训练的高级通用图像分割模型，为图像分割和计算机视觉树立了新的基准。然而，在区分阴影及其背景方面，它面临着挑战。为了解决这一问题，考虑到大规模数据集的泛化，我们开发了Deshadow Anything，并对大规模数据集进行了微调，以实现图像阴影去除。扩散模型可以沿着图像的边缘和纹理进行扩散，有助于去除阴影，同时保留图像的细节。此外，我们设计了多自注意制导（MSAG）和自适应输入扰动（DDPM-AIP）来加快扩散的迭代训练速度。对阴影去除任务的实验表明，这些方法可以有效地提高图像的恢复性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11715v3" target="_blank">2309.11715v3</a>
                              </td>
                              <td>Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal</td>
                              <td>Xiao Feng Zhang</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11715v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11715v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09383v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Soundly Handling Linearity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09383v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09383v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09383v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to soundly combining linear types with effect handlers. Linear type systems statically ensure that resources such as file handles are used exactly once. Effect handlers provide a modular programming abstraction for implementing features ranging from exceptions to concurrency. Whereas linear type systems bake in the assumption that continuations are invoked exactly once, effect handlers allow continuations to be discarded or invoked more than once. This mismatch leads to soundness bugs in existing systems such as the programming language Links, which combines linearity (for session types) with effect handlers. We introduce control flow linearity as a means to ensure that continuations are used in accordance with the linearity of any resources they capture, ruling out such soundness bugs.   We formalise control flow linearity in a System F-style core calculus Feffpop equipped with linear types, effect types, and effect handlers. We define a linearity-aware semantics to formally prove that Feffpop preserves the integrity of linear values in the sense that no linear value is discarded or duplicated. In order to show that control flow linearity can be made practical, we adapt Links based on the design of Feffpop, in doing so fixing a long-standing soundness bug.   Finally, to better expose the potential of control flow linearity, we define an ML-style core calculus Qeffpop, based on qualified types, which requires no programmer provided annotations, and instead relies entirely on type inference to infer control flow linearity. Both linearity and effects are captured by qualified types. Qeffpop overcomes a number of practical limitations of Feffpop, supporting abstraction over linearity, linearity dependencies between type variables, and a much more fine-grained notion of control flow linearity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09383v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种将线性类型与效果处理程序完美结合的新方法。线性类型系统静态地确保文件句柄等资源只使用一次。效果处理程序提供了一个模块化编程抽象，用于实现从异常到并发的各种功能。线性类型系统在假设连续性只被调用一次的情况下烘焙，而效果处理程序允许连续性被丢弃或多次调用。这种不匹配导致了现有系统中的健全性错误，例如编程语言Links，它将线性（针对会话类型）与效果处理程序相结合。我们引入控制流线性作为一种手段，以确保连续性根据其捕获的任何资源的线性使用，从而排除此类健全性错误。我们在配备了线性类型、效果类型和效果处理程序的系统F型核心演算Feffpop中正式化控制流线性。我们定义了一个线性感知语义，以正式证明Feffpop在没有线性值被丢弃或重复的意义上保持了线性值的完整性。为了证明控制流线性是可行的，我们在Feffpop的设计基础上对Links进行了调整，从而修复了一个长期存在的健全性缺陷。最后，为了更好地揭示控制流线性的潜力，我们基于限定类型定义了ML风格的核心演算Qeffpop，它不需要程序员提供的注释，而是完全依赖于类型推理来推断控制流线性。线性和效果都由合格的类型捕获。Qeffpop克服了Feffpop的许多实际限制，支持对线性的抽象、类型变量之间的线性依赖性，以及更细粒度的控制流线性概念。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09383v2" target="_blank">2307.09383v2</a>
                              </td>
                              <td>Soundly Handling Linearity</td>
                              <td>Wenhao Tang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09383v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09383v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01010v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01010v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01010v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01010v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised Anomaly Detection (UAD) with incremental training is crucial in industrial manufacturing, as unpredictable defects make obtaining sufficient labeled data infeasible. However, continual learning methods primarily rely on supervised annotations, while the application in UAD is limited due to the absence of supervision. Current UAD methods train separate models for different classes sequentially, leading to catastrophic forgetting and a heavy computational burden. To address this issue, we introduce a novel Unsupervised Continual Anomaly Detection framework called UCAD, which equips the UAD with continual learning capability through contrastively-learned prompts. In the proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a concise key-prompt-knowledge memory bank to guide task-invariant `anomaly' model predictions using task-specific `normal' knowledge. Moreover, Structure-based Contrastive Learning (SCL) is designed with the Segment Anything Model (SAM) to improve prompt learning and anomaly segmentation results. Specifically, by treating SAM's masks as structure, we draw features within the same mask closer and push others apart for general feature representations. We conduct comprehensive experiments and set the benchmark on unsupervised continual anomaly detection and segmentation, demonstrating that our method is significantly better than anomaly detection methods, even with rehearsal training. The code will be available at https://github.com/shirowalker/UCAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01010v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有增量训练的无监督异常检测（UAD）在工业制造中至关重要，因为不可预测的缺陷使获得足够的标记数据变得不可行。然而，连续学习方法主要依赖于有监督的注释，而由于缺乏监督，在UAD中的应用受到限制。当前的UAD方法顺序地为不同的类训练单独的模型，导致灾难性的遗忘和沉重的计算负担。为了解决这个问题，我们引入了一种新的无监督连续异常检测框架，称为UCAD，它通过对比学习的提示为UAD提供了连续学习能力。在所提出的UCAD中，我们利用简明的关键提示知识记忆库设计了一个连续提示模块（CPM），以使用特定于任务的“正常”知识来指导任务不变的“异常”模型预测。此外，基于结构的对比学习（SCL）与分段任意模型（SAM）一起设计，以提高即时学习和异常分割的效果。具体来说，通过将SAM的掩码视为结构，我们将同一掩码内的特征画得更近，并将其他掩码推开，以获得一般特征表示。我们进行了全面的实验，并在无监督的连续异常检测和分割方面设定了基准，证明我们的方法明显优于异常检测方法，即使进行了排练训练。代码将在https://github.com/shirowalker/UCAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01010v1" target="_blank">2401.01010v1</a>
                              </td>
                              <td>Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt</td>
                              <td>Jiaqi Liu</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01010v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01010v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/shirowalker/ucad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16754v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16754v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16754v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16754v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative perception has recently gained significant attention in autonomous driving, improving perception quality by enabling the exchange of additional information among vehicles. However, deploying collaborative perception systems can lead to domain shifts due to diverse environmental conditions and data heterogeneity among connected and autonomous vehicles (CAVs). To address these challenges, we propose a unified domain generalization framework applicable in both training and inference stages of collaborative perception. In the training phase, we introduce an Amplitude Augmentation (AmpAug) method to augment low-frequency image variations, broadening the model's ability to learn across various domains. We also employ a meta-consistency training scheme to simulate domain shifts, optimizing the model with a carefully designed consistency loss to encourage domain-invariant representations. In the inference phase, we introduce an intra-system domain alignment mechanism to reduce or potentially eliminate the domain discrepancy among CAVs prior to inference. Comprehensive experiments substantiate the effectiveness of our method in comparison with the existing state-of-the-art works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16754v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作感知最近在自动驾驶中获得了极大的关注，通过实现车辆之间的附加信息交换来提高感知质量。然而，由于联网和自动驾驶汽车（CAV）之间的不同环境条件和数据异构性，部署协同感知系统可能会导致领域转变。为了应对这些挑战，我们提出了一个统一的领域泛化框架，适用于协同感知的训练和推理阶段。在训练阶段，我们引入了一种幅度增强（AmpAug）方法来增强低频图像的变化，从而拓宽了模型在各个领域的学习能力。我们还使用元一致性训练方案来模拟域移动，用精心设计的一致性损失来优化模型，以鼓励域不变表示。在推理阶段，我们引入了一种系统内域对齐机制，以在推理之前减少或潜在地消除CAV之间的域差异。与现有的最先进的工作相比，综合实验证明了我们的方法的有效性。代码将在发布https://github.com/DG-CAVs/DG-CoPerception.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16754v2" target="_blank">2311.16754v2</a>
                              </td>
                              <td>Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving</td>
                              <td>Senkang Hu</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16754v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16754v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dg-cavs/dg-coperception" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01004v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01004v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01004v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01004v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01004v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着多模态和大型语言模型的发展，基于深度学习的医学图像字幕技术有可能提供有价值的诊断建议。然而，当涉及到描述医学图像中的复杂细节时，当前的通用文本和图像预训练模型不能产生令人满意的结果。在本文中，我们提出了一种新的医学图像字幕方法，该方法以分段任意模型（SAM）为指导，通过通用和详细特征提取实现增强编码。此外，我们的方法采用了一种独特的预训练策略和混合语义学习，以同时捕捉医学图像中的整体信息和更精细的细节。我们证明了这种方法的有效性，因为它在生成医学图像描述的各种评估指标上优于预先训练的BLIP2模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01004v2" target="_blank">2311.01004v2</a>
                              </td>
                              <td>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning</td>
                              <td>Zhenyu Zhang</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01004v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01004v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00248v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00248v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00248v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00248v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segmenting any object represents a crucial step towards achieving artificial general intelligence, and the "Segment Anything Model" (SAM) has significantly advanced the development of foundational models in computer vision. We have high expectations regarding whether SAM can enhance highly accurate dichotomous image segmentation. In fact, the evidence presented in this article demonstrates that by inputting SAM with simple prompt boxes and utilizing the results output by SAM as input for IS5Net, we can greatly improve the effectiveness of highly accurate dichotomous image segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00248v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分割任何对象都是实现通用人工智能的关键一步，“分割任何对象模型”（SAM）极大地推动了计算机视觉基础模型的发展。我们对SAM是否能够增强高精度的二分图像分割抱有很高的期望。事实上，本文提供的证据表明，通过使用简单的提示框输入SAM，并利用SAM输出的结果作为IS5Net的输入，我们可以大大提高高精度二分图像分割的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00248v1" target="_blank">2401.00248v1</a>
                              </td>
                              <td>Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation</td>
                              <td>Xianjie Liu</td>
                              <td>2023-12-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00248v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00248v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17482v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17482v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17482v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17482v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed Pareto curves and show that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large. This empirical speed up in pretraining enables researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models. We open source our model weights and code.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17482v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管BERT风格的编码器模型在NLP研究中被大量使用，但由于训练成本高，许多研究人员并没有从头开始预训练自己的BERT。自BERT首次崭露头角以来的过去五年中，其他变压器架构和训练配置取得了许多进展，但这些架构和配置尚未系统地纳入BERT。在这里，我们介绍MosaicBERT，这是一种BERT风格的编码器架构和训练配方，根据经验进行了快速预训练优化。这种高效的架构将FlashAttention、Attention with Linear Biases（ALiBi）、Gated Linear Units（GLU）、一个动态移除填充令牌的模块和低精度LayerNorm集成到经典的转换器编码器块中。除了RoBERTa和其他编码器模型的最佳实践外，训练配方还包括掩蔽语言建模（MLM）目标的30%掩蔽率、bfloat16精度和针对GPU吞吐量优化的词汇大小。当在C4数据集上从头开始进行预训练时，该基础模型在8个A100 80 GB GPU上的1.13小时内实现了79.6的下游平均GLUE（dev）得分，成本约为20美元。我们绘制了广泛的精度与预训练速度的Pareto曲线，并表明与竞争性的BERT基数和基数相比，MosaicBERT基数和基数始终是Pareto最优的。这种经验上的预训练速度使研究人员和工程师能够以低成本预训练定制的BERT风格的模型，而不是对现有的通用模型进行微调。我们开源我们的模型权重和代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17482v1" target="_blank">2312.17482v1</a>
                              </td>
                              <td>MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining</td>
                              <td>Jacob Portes</td>
                              <td>2023-12-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17482v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17482v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mosaicml/examples" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_12620v7_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_12620v7_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_12620v7_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_12620v7_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has recently gained popularity in the field of image segmentation due to its impressive capabilities in various segmentation tasks and its prompt-based interface. However, recent studies and individual experiments have shown that SAM underperforms in medical image segmentation, since the lack of the medical specific knowledge. This raises the question of how to enhance SAM's segmentation capability for medical images. In this paper, instead of fine-tuning the SAM model, we propose the Medical SAM Adapter (Med-SA), which incorporates domain-specific medical knowledge into the segmentation model using a light yet effective adaptation technique. In Med-SA, we propose Space-Depth Transpose (SD-Trans) to adapt 2D SAM to 3D medical images and Hyper-Prompting Adapter (HyP-Adpt) to achieve prompt-conditioned adaptation. We conduct comprehensive evaluation experiments on 17 medical image segmentation tasks across various image modalities. Med-SA outperforms several state-of-the-art (SOTA) medical image segmentation methods, while updating only 2\% of the parameters. Our code is released at https://github.com/KidsWithTokens/Medical-SAM-Adapter.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_12620v7_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其在各种分割任务中令人印象深刻的能力及其基于提示的界面，分段任何模型（SAM）最近在图像分割领域广受欢迎。然而，最近的研究和个人实验表明，由于缺乏医学特定知识，SAM在医学图像分割中表现不佳。这就提出了如何增强SAM对医学图像的分割能力的问题。在本文中，我们提出了医学SAM适配器（Med SA），而不是微调SAM模型，它使用一种轻便而有效的自适应技术将特定领域的医学知识结合到分割模型中。在Med SA中，我们提出了空间深度转换酶（SD Trans）以使2D SAM适应3D医学图像，并提出了超提示适配器（HyP-Adpt）以实现即时条件适应。我们对17个不同图像模式的医学图像分割任务进行了综合评估实验。Med-SA优于几种最先进的（SOTA）医学图像分割方法，同时仅更新了2%的参数。我们的代码发布于https://github.com/KidsWithTokens/Medical-SAM-Adapter.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.12620v7" target="_blank">2304.12620v7</a>
                              </td>
                              <td>Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</td>
                              <td>Junde Wu</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_12620v7_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.12620v7" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kidswithtokens/medical-sam-adapter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17141v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Probabilistic Programming with Exact Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17141v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17141v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17141v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We spell out the paradigm of exact conditioning as an intuitive and powerful way of conditioning on observations in probabilistic programs. This is contrasted with likelihood-based scoring known from languages such as Stan. We study exact conditioning in the cases of discrete and Gaussian probability, presenting prototypical languages for each case and giving semantics to them. We make use of categorical probability (namely Markov and CD categories) to give a general account of exact conditioning which avoids limits and measure theory, instead focusing on restructuring dataflow and program equations. The correspondence between such categories and a class of programming languages is made precise by defining the internal language of a CD category.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17141v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们阐述了精确条件反射的范式，作为概率程序中对观测进行条件反射的一种直观而强大的方式。这与Stan等语言中已知的基于可能性的评分形成了对比。我们研究了离散概率和高斯概率情况下的精确条件，为每种情况提供了原型语言，并给出了它们的语义。我们利用范畴概率（即马尔可夫和CD范畴）来给出精确条件的一般说明，避免了极限和测度理论，而专注于重构数据流和程序方程。通过定义CD类别的内部语言，可以精确地确定这些类别与一类编程语言之间的对应关系。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17141v1" target="_blank">2312.17141v1</a>
                              </td>
                              <td>Probabilistic Programming with Exact Conditions</td>
                              <td>Dario Stein</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17141v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17141v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17127v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Probabilistic programming interfaces for random graphs: Markov categories, graphons, and nominal sets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17127v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17127v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17127v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\H{o}s-R\'enyi graphons. In this way, we build new models of graph probabilistic programming from graphons.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17127v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了图上概率编程语言的语义模型，并从图论和组合学中建立了与图的联系。我们证明了我们的图概率编程语言的每个表现良好的等式理论都对应于一个图，相反，每个图都是这样产生的。我们提供了三种结构来证明每一个图都源于一个等式理论。第一个是一个抽象的构造，使用马尔可夫范畴和单次不确定性。第二个和第三个更具体。第二种是根据传统的测度论概率，它涵盖了“黑白”图。第三个是在Gabbay和Pitts的标称集上的概率单元。具体地说，我们使用了由图理论引起的标称集的变化，它涵盖了Erd\H{o}s-R\恩伊图。通过这种方式，我们从图中建立了新的图概率规划模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17127v1" target="_blank">2312.17127v1</a>
                              </td>
                              <td>Probabilistic programming interfaces for random graphs: Markov categories, graphons, and nominal sets</td>
                              <td>Nathanael L. Ackerman</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17127v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17127v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17116v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Visual Reinforcement Learning with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17116v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17116v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够推广到看不见的环境中的学习策略是视觉强化学习（RL）中的一个基本挑战。虽然目前的大多数方法都侧重于通过辅助监督、预训练或数据增强来获取稳健的视觉表示，但现代视觉基础模型的潜力仍然不足。在这项工作中，我们介绍了可泛化视觉RL的分段任意模型（SAM-G），这是一个新的框架，利用分段任意模型的可提示分割能力来增强视觉RL代理的泛化能力。我们利用DINOv2和SAM的图像特征来寻找与SAM的点提示对应关系，然后SAM直接为代理生成高质量的掩码图像。在8个DMControl任务和3个Adroit任务中进行评估后，SAM-G显著提高了视觉泛化能力，而不会改变RL代理的架构，而只是改变他们的观察结果。值得注意的是，与最先进的方法相比，SAM-G在DMControl和Adroit上具有挑战性的视频硬设置上分别实现了44%和29%的相对改进。视频和代码：https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17116v1" target="_blank">2312.17116v1</a>
                              </td>
                              <td>Generalizable Visual Reinforcement Learning with Segment Anything Model</td>
                              <td>Ziyu Wang</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17116v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wadiuvatzy/sam-g" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16694v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denotational semantics for languages for inference: semirings, monads, and tensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16694v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16694v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16694v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Computational effects are commonly modelled by monads, but often a monad can be presented by an algebraic theory of operations and equations. This talk is about monads and algebraic theories for languages for inference, and their connections to semirings and tensors.   A basic class of examples of algebraic theories comes from considering the theory of modules for a semiring, e.g. the theory of unnormalized distributions, where the semiring is that of the non-negative real numbers. We propose that an interesting perspective is given by studying theories via semirings, and to this end explore several examples of subtheories of module theories, mostly relating to probability. Our main contribution concerns the commutative combination of effects, as studied by Hyland, Plotkin and Power: we observe that while the semiring tensor does not in general determine the tensor of subtheories of module theories, it still does in several fundamental probabilistic examples.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16694v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算效果通常由一元模型来建模，但一元通常可以由运算和方程的代数理论来表示。这篇演讲是关于推理语言的单子和代数理论，以及它们与半环和张量的联系。代数理论的一类基本例子来自于考虑半环的模理论，例如非规范化分布理论，其中半环是非负实数的半环。我们提出，通过半环研究理论提供了一个有趣的视角，并为此探索了模理论子理论的几个例子，主要与概率有关。我们的主要贡献涉及Hyland、Plotkin和Power研究的效应的交换组合：我们观察到，虽然半环张量通常不能确定模理论子理论的张量，但它在几个基本的概率例子中仍然确定。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16694v1" target="_blank">2312.16694v1</a>
                              </td>
                              <td>Denotational semantics for languages for inference: semirings, monads, and tensors</td>
                              <td>Cristina Matache</td>
                              <td>2023-12-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16694v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16694v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16410v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16410v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16410v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16410v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of Remote Sensing (RS) widely employs Change Detection (CD) on very-high-resolution (VHR) images. A majority of extant deep-learning-based methods hinge on annotated samples to complete the CD process. Recently, the emergence of Vision Foundation Model (VFM) enables zero-shot predictions in particular vision tasks. In this work, we propose an unsupervised CD method named Segment Change Model (SCM), built upon the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates features extracted at different scales and integrates them in a top-down manner to enhance discriminative change edges. We further design an innovative Piecewise Semantic Attention (PSA) scheme, which can offer semantic representation without training, thereby minimize pseudo change phenomenon. Through conducting experiments on two public datasets, the proposed SCM increases the mIoU from 46.09% to 53.67% on the LEVIR-CD dataset, and from 47.56% to 52.14% on the WHU-CD dataset. Our codes are available at https://github.com/StephenApX/UCD-SCM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16410v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>遥感（RS）领域广泛地在甚高分辨率（VHR）图像上使用变化检测（CD）。大多数现存的基于深度学习的方法都依赖于注释样本来完成CD过程。最近，视觉基础模型（VFM）的出现使得零样本预测能够在特定的视觉任务中实现。在这项工作中，我们提出了一种无监督的CD方法，称为分段变化模型（SCM），建立在分段任意模型（SAM）和对比语言图像预训练（CLIP）的基础上。我们的方法重新校准在不同尺度上提取的特征，并以自上而下的方式对其进行集成，以增强判别变化边缘。我们进一步设计了一种创新的分段语义注意力（PSA）方案，该方案可以在不经过训练的情况下提供语义表示，从而最大限度地减少伪变化现象。通过在两个公共数据集上进行实验，所提出的SCM在LEVIR-CD数据集上将mIoU从46.09%提高到53.67%，在WHU-CD数据集上从47.56%提高到52.14%。我们的代码可在https://github.com/StephenApX/UCD-SCM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16410v1" target="_blank">2312.16410v1</a>
                              </td>
                              <td>Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings</td>
                              <td>Xiaoliang Tan</td>
                              <td>2023-12-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16410v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16410v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/stephenapx/ucd-scm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_04668v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deciding Equations in the Time Warp Algebra</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_04668v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_04668v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_04668v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Join-preserving maps on the discrete time scale $\omega^+$, referred to as time warps, have been proposed as graded modalities that can be used to quantify the growth of information in the course of program execution. The set of time warps forms a simple distributive involutive residuated lattice -- called the time warp algebra -- that is equipped with residual operations relevant to potential applications. In this paper, we show that although the time warp algebra generates a variety that lacks the finite model property, it nevertheless has a decidable equational theory. We also describe an implementation of a procedure for deciding equations in this algebra, written in the OCaml programming language, that makes use of the Z3 theorem prover.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_04668v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>离散时间尺度$\omega^+$上的连接保留映射，称为时间扭曲，已被提议作为分级模式，可用于量化程序执行过程中的信息增长。这组时间扭曲形成了一个简单的分配对合残差格，称为时间扭曲代数，它配备了与潜在应用相关的残差运算。在本文中，我们证明了尽管时间扭曲代数生成了一个缺乏有限模型性质的变种，但它仍然具有可判定的方程理论。我们还描述了用OCaml编程语言编写的、使用Z3定理证明器的、用于判定该代数中的方程的过程的实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.04668v3" target="_blank">2302.04668v3</a>
                              </td>
                              <td>Deciding Equations in the Time Warp Algebra</td>
                              <td>Sam van Gool</td>
                              <td>2023-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_04668v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.04668v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16084v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LangSplat: 3D Language Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16084v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16084v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类生活在3D世界中，通常使用自然语言与3D场景进行交互。最近，对3D语言字段进行建模以支持3D中的开放式语言查询越来越受到关注。本文介绍了LangSplat，它构建了一个三维语言字段，可以在三维空间中进行精确高效的开放式词汇查询。与现有的将CLIP语言嵌入NeRF模型的方法不同，LangSplat通过利用3D高斯集合来表示语言领域，从而推进了这一领域的发展，每个高斯集合都对从CLIP中提取的语言特征进行编码。通过使用基于瓦片的飞溅技术来渲染语言特征，我们避免了NeRF中固有的昂贵的渲染过程。LangSplat不是直接学习CLIP嵌入，而是首先训练场景式语言自动编码器，然后在特定场景的潜在空间上学习语言特征，从而减轻显式建模带来的大量内存需求。现有的方法难以处理不精确和模糊的3D语言字段，这些字段无法辨别对象之间的清晰边界。我们深入研究了这个问题，并建议使用SAM学习分层语义，从而消除了在各种规模上广泛查询语言字段和DINO特征正则化的需要。在开放词汇三维对象定位和语义分割方面的大量实验表明，LangSplat显著优于先前最先进的方法LERF。值得注意的是，LangSplat非常高效，与分辨率为1440$\times$1080的LERF相比，它实现了｛\speed｝$\times$的加速。我们强烈建议读者在上查看我们的视频结果https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16084v1" target="_blank">2312.16084v1</a>
                              </td>
                              <td>LangSplat: 3D Language Gaussian Splatting</td>
                              <td>Minghan Qin</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16084v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16084v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/minghanqin/LangSplat" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2401_05336v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Online Sign Language Recognition and Translation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05336v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05336v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05336v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The objective of sign language recognition is to bridge the communication gap between the deaf and the hearing. Numerous previous works train their models using the well-established connectionist temporal classification (CTC) loss. During the inference stage, the CTC-based models typically take the entire sign video as input to make predictions. This type of inference scheme is referred to as offline recognition. In contrast, while mature speech recognition systems can efficiently recognize spoken words on the fly, sign language recognition still falls short due to the lack of practical online solutions. In this work, we take the first step towards filling this gap. Our approach comprises three phases: 1) developing a sign language dictionary encompassing all glosses present in a target sign language dataset; 2) training an isolated sign language recognition model on augmented signs using both conventional classification loss and our novel saliency loss; 3) employing a sliding window approach on the input sign sequence and feeding each sign clip to the well-optimized model for online recognition. Furthermore, our online recognition model can be extended to boost the performance of any offline model, and to support online translation by appending a gloss-to-text network onto the recognition model. By integrating our online framework with the previously best-performing offline model, TwoStream-SLR, we achieve new state-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T, and CSL-Daily. Code and models will be available at https://github.com/FangyunWei/SLRT</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05336v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手语识别的目标是弥合聋人和听力之间的沟通差距。许多先前的工作使用公认的连接主义时间分类（CTC）损失来训练他们的模型。在推理阶段，基于CTC的模型通常将整个符号视频作为输入来进行预测。这种类型的推理方案被称为离线识别。相比之下，尽管成熟的语音识别系统可以快速有效地识别口语，但由于缺乏实用的在线解决方案，手语识别仍然不足。在这项工作中，我们迈出了填补这一空白的第一步。我们的方法包括三个阶段：1）开发一个包含目标手语数据集中所有注释的手语词典；2） 使用传统的分类损失和我们的新显著性损失在增强符号上训练孤立的手语识别模型；3） 在输入符号序列上采用滑动窗口方法，并将每个符号片段馈送到优化良好的模型用于在线识别。此外，我们的在线识别模型可以扩展，以提高任何离线模型的性能，并通过在识别模型上添加文本网络来支持在线翻译。通过将我们的在线框架与之前性能最好的离线模型TwoStream SLR集成，我们在三个基准上实现了最先进的性能：Phoenix-2014、Phoenix-2014T和CSL Daily。代码和型号将在https://github.com/FangyunWei/SLRT</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05336v1" target="_blank">2401.05336v1</a>
                              </td>
                              <td>Towards Online Sign Language Recognition and Translation</td>
                              <td>Ronglai Zuo</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05336v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05336v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/FangyunWei/SLRT" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05224v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Do Vision and Language Encoders Represent the World Similarly?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05224v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05224v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05224v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demonstrate the effectiveness of this on several downstream tasks including cross-lingual, cross-domain caption matching and image classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05224v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的对齐文本图像编码器已经成为视觉语言任务的事实模型。此外，特定于模态的编码器在其各自的领域中实现了令人印象深刻的性能。这提出了一个核心问题：单模态视觉和语言编码器之间是否存在一致性，因为它们从根本上代表了相同的物理世界？使用中心核对齐（CKA）分析图像字幕基准上视觉和语言模型的潜在空间结构，我们发现未对齐和对齐编码器的表示空间在语义上相似。在像CLIP这样的对齐编码器中缺乏统计相似性的情况下，我们证明了在没有任何训练的情况下存在未对齐编码器的可能匹配。我们将其定义为一个利用图之间语义相似性的种子图匹配问题，并提出了两种方法——快速二次分配问题优化和一种新的基于局部CKA度量的匹配/检索。我们证明了这在几个下游任务中的有效性，包括跨语言、跨域字幕匹配和图像分类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05224v1" target="_blank">2401.05224v1</a>
                              </td>
                              <td>Do Vision and Language Encoders Represent the World Similarly?</td>
                              <td>Mayug Maniparambil</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05224v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05224v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05168v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-guided Source-free Object Detection in Aerial Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05168v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05168v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05168v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain adaptation is crucial in aerial imagery, as the visual representation of these images can significantly vary based on factors such as geographic location, time, and weather conditions. Additionally, high-resolution aerial images often require substantial storage space and may not be readily accessible to the public. To address these challenges, we propose a novel Source-Free Object Detection (SFOD) method. Specifically, our approach is built upon a self-training framework; however, self-training can lead to inaccurate learning in the absence of labeled training data. To address this issue, we further integrate Contrastive Language-Image Pre-training (CLIP) to guide the generation of pseudo-labels, termed CLIP-guided Aggregation. By leveraging CLIP's zero-shot classification capability, we use it to aggregate scores with the original predicted bounding boxes, enabling us to obtain refined scores for the pseudo-labels. To validate the effectiveness of our method, we constructed two new datasets from different domains based on the DIOR dataset, named DIOR-C and DIOR-Cloudy. Experiments demonstrate that our method outperforms other comparative algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05168v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>领域自适应在航空图像中至关重要，因为这些图像的视觉表现可能会因地理位置、时间和天气条件等因素而发生显著变化。此外，高分辨率航空图像通常需要大量的存储空间，公众可能无法轻易获取。为了应对这些挑战，我们提出了一种新的无源对象检测（SFOD）方法。具体而言，我们的方法建立在自我培训框架之上；然而，在缺乏标记的训练数据的情况下，自我训练可能导致学习不准确。为了解决这个问题，我们进一步集成了对比语言图像预训练（CLIP）来指导伪标签的生成，称为CLIP引导的聚合。通过利用CLIP的零样本分类功能，我们使用它将分数与原始预测的边界框聚合在一起，使我们能够获得伪拉贝尔的精细分数。为了验证我们方法的有效性，我们在DIOR数据集的基础上，从不同的领域构建了两个新的数据集，分别命名为DIOR-C和DIOR-Cloudy。实验表明，我们的方法优于其他比较算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05168v1" target="_blank">2401.05168v1</a>
                              </td>
                              <td>CLIP-guided Source-free Object Detection in Aerial Images</td>
                              <td>Nanqing Liu</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05168v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05168v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05166v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05166v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05166v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05166v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In dyadic interactions, humans communicate their intentions and state of mind using verbal and non-verbal cues, where multiple different facial reactions might be appropriate in response to a specific speaker behaviour. Then, how to develop a machine learning (ML) model that can automatically generate multiple appropriate, diverse, realistic and synchronised human facial reactions from an previously unseen speaker behaviour is a challenging task. Following the successful organisation of the first REACT challenge (REACT 2023), this edition of the challenge (REACT 2024) employs a subset used by the previous challenge, which contains segmented 30-secs dyadic interaction clips originally recorded as part of the NOXI and RECOLA datasets, encouraging participants to develop and benchmark Machine Learning (ML) models that can generate multiple appropriate facial reactions (including facial image sequences and their attributes) given an input conversational partner's stimulus under various dyadic video conference scenarios. This paper presents: (i) the guidelines of the REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii) the performance of the baseline systems on the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation, respectively. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2024.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05166v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在二元互动中，人类使用语言和非语言线索来传达他们的意图和精神状态，其中多种不同的面部反应可能适合于对特定说话者行为的反应。然后，如何开发一种机器学习（ML）模型，从以前看不见的说话者行为中自动生成多个适当、多样、逼真和同步的人类面部反应，是一项具有挑战性的任务。继第一次REACT挑战（REACT 2023）成功组织后，本次挑战（REACT 2024）采用了前一次挑战使用的子集，其中包含最初记录为NOXI和RECOLA数据集一部分的分段30秒二元交互片段，鼓励参与者开发和基准机器学习（ML）模型，该模型可以在各种二元视频会议场景下，根据输入对话伙伴的刺激生成多个适当的面部反应（包括面部图像序列及其属性）。本文介绍：（i）REACT 2024挑战的指导方针；（ii）在挑战中使用的数据集；以及（iii）基线系统对两个子挑战的性能：分别为离线多重适当面部反应生成和在线多重适当面部响应生成。挑战基线代码可在https://github.com/reactmultimodalchallenge/baseline_react2024.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05166v1" target="_blank">2401.05166v1</a>
                              </td>
                              <td>REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge</td>
                              <td>Siyang Song</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05166v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05166v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/reactmultimodalchallenge/baseline_react2024" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_14865v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt-aligned Gradient for Prompt Tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_14865v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_14865v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_14865v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we can craft a zero-shot classifier by "prompt", e.g., the confidence score of an image being "[CLASS]" can be obtained by using the VLM provided similarity measure between the image and the prompt sentence "a photo of a [CLASS]". Therefore, prompt shows a great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the prompt-based similarity measure. However, we find a common failure that improper fine-tuning may not only undermine the prompt's inherent prediction for the task-related classes, but also for other classes in the VLM vocabulary. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompt. We present Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from forgetting the the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the "general direction", which is represented as the gradient of the KL loss of the pre-defined prompt prediction. Extensive experiments demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods. Codes are available at https://github.com/BeierZhu/Prompt-align.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_14865v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于像CLIP这样的大型预先训练的视觉语言模型（VLM），我们可以通过“提示”来制作零样本分类器，例如，可以通过使用VLM提供的图像与提示句“[CLASS]的照片”之间的相似性测量来获得“[CLASP]”的图像的置信度得分。因此，如果我们微调基于提示的相似性度量，提示显示出VLM快速适应下游任务的巨大潜力。然而，我们发现了一个常见的失败，即不适当的微调不仅可能破坏提示对任务相关类的固有预测，而且可能破坏VLM词汇中其他类的内在预测。现有的方法仍然通过使用传统的反过拟合技术来解决这个问题，如早期停止和数据扩充，这些技术缺乏专门针对提示的原则性解决方案。我们提出了被称为ProGrad的“提示对齐梯度”，以防止快速调谐忘记从VLM中学到的一般知识。特别是，ProGrad只更新梯度与“一般方向”对齐（或不冲突）的提示，该方向表示为预定义提示预测的KL损失的梯度。大量实验表明，与最先进的即时调谐方法相比，ProGrad的少镜头泛化能力更强。代码可在https://github.com/BeierZhu/Prompt-align.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.14865v3" target="_blank">2205.14865v3</a>
                              </td>
                              <td>Prompt-aligned Gradient for Prompt Tuning</td>
                              <td>Beier Zhu</td>
                              <td>2022-05-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_14865v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.14865v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/beierzhu/prompt-align" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08106v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08106v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08106v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08106v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08106v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的基础模型允许在没有额外训练数据的情况下对各种任务进行零样本传输。然而，零样本的表现不如完全监督的表现具有竞争力。因此，为了提高性能，通常还采用微调和集成来更好地适应下游任务。然而，我们认为，这种先前的工作忽视了基础模型中固有的偏见。由于网络规模的训练集高度不平衡，这些基础模型不可避免地向频繁语义倾斜，因此后续的微调或组合仍然存在偏差。在这项研究中，我们系统地检查了基础模型中的偏差，并证明了我们提出的广义Logit平差（GLA）方法的有效性。请注意，基础模型中的偏差估计是具有挑战性的，因为大多数预训练数据不能像传统的长尾分类任务中那样被明确访问。为此，GLA提供了一种基于优化的基础模型去偏估计方法。由于我们的工作解决了预训练中的一个基本缺陷，所提出的GLA在各种任务中都表现出了显著的改进：它在ImageNet上实现了1.5 pp的精度提高，在11个少镜头数据集上实现了较大的平均提高（1.4-4.6 pp），在长尾分类上实现了2.4 pp的提高。代码在\url中{https://github.com/BeierZhu/GLA}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08106v2" target="_blank">2310.08106v2</a>
                              </td>
                              <td>Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models</td>
                              <td>Beier Zhu</td>
                              <td>2023-10-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08106v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08106v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/BeierZhu/GLA" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04903v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SnapCap: Efficient Snapshot Compressive Video Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04903v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04903v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04903v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Captioning (VC) is a challenging multi-modal task since it requires describing the scene in language by understanding various and complex videos. For machines, the traditional VC follows the "imaging-compression-decoding-and-then-captioning" pipeline, where compression is pivot for storage and transmission. However, in such a pipeline, some potential shortcomings are inevitable, i.e., information redundancy resulting in low efficiency and information loss during the sampling process for captioning. To address these problems, in this paper, we propose a novel VC pipeline to generate captions directly from the compressed measurement, which can be captured by a snapshot compressive sensing camera and we dub our model SnapCap. To be more specific, benefiting from the signal simulation, we have access to obtain abundant measurement-video-annotation data pairs for our model. Besides, to better extract language-related visual representations from the compressed measurement, we propose to distill the knowledge from videos via a pre-trained CLIP with plentiful language-vision associations to guide the learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we conduct experiments on two widely-used VC datasets. Both the qualitative and quantitative results verify the superiority of our pipeline over conventional VC pipelines. In particular, compared to the "caption-after-reconstruction" methods, our SnapCap can run at least 3$\times$ faster, and achieve better caption results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04903v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频字幕（VC）是一项具有挑战性的多模式任务，因为它需要通过理解各种复杂的视频来用语言描述场景。对于机器，传统的VC遵循“图像压缩解码，然后加字幕”的流水线，其中压缩是存储和传输的中心。然而，在这样的流水线中，一些潜在的缺点是不可避免的，即信息冗余导致字幕采样过程中的低效率和信息丢失。为了解决这些问题，在本文中，我们提出了一种新的VC管道来直接从压缩的测量中生成字幕，该字幕可以由快照压缩传感相机捕获，我们将我们的模型命名为SnapCap。更具体地说，得益于信号模拟，我们可以为我们的模型获得丰富的测量视频注释数据对。此外，为了更好地从压缩测量中提取与语言相关的视觉表示，我们建议通过预先训练的具有丰富语言视觉关联的CLIP从视频中提取知识，以指导我们的SnapCap的学习。为了证明SnapCap的有效性，我们在两个广泛使用的VC数据集上进行了实验。定性和定量结果都验证了我们的管道相对于传统VC管道的优越性。特别是，与“重建后的字幕”方法相比，我们的SnapCap可以运行至少快3$\times$，并获得更好的字幕效果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04903v1" target="_blank">2401.04903v1</a>
                              </td>
                              <td>SnapCap: Efficient Snapshot Compressive Video Captioning</td>
                              <td>Jianqiao Sun</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04903v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04903v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练健壮和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识有帮助，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v4" target="_blank">2305.14093v4</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kunhao-liu/3d-ovs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04608v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04608v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04608v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04608v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. However, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. In this work, we introduce Emotional Image Content Generation (EICG), a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically, we propose an emotion space and construct a mapping network to align it with the powerful Contrastive Language-Image Pre-training (CLIP) space, providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. In addition to generation, our method can help emotion understanding and inspire emotional art design.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04608v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，图像生成任务取得了显著进展，用户可以创建高质量的视觉惊人的图像。然而，现有的文本到图像的扩散模型精通于生成具体的概念（狗），但遇到了更抽象的概念（情感）的挑战。已经做出了一些努力来通过颜色和风格的调整来修改图像情绪，但在用固定的图像内容有效地传达情绪方面面临限制。在这项工作中，我们介绍了情感图像内容生成（EICG），这是一项在给定情感类别的情况下生成语义清晰、情感忠实的图像的新任务。具体来说，我们提出了一个情绪空间，并构建了一个映射网络，将其与强大的对比语言图像预训练（CLIP）空间对齐，提供对抽象情绪的具体解释。进一步提出了属性损失和情感置信度，以确保生成图像的语义多样性和情感保真度。我们的方法在数量和质量上都优于最先进的文本到图像方法，其中我们推导了三个自定义指标，即情感准确性、语义清晰度和语义多样性。除了生成，我们的方法还可以帮助理解情感，启发情感艺术设计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04608v1" target="_blank">2401.04608v1</a>
                              </td>
                              <td>EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</td>
                              <td>Jingyuan Yang</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04608v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04608v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04578v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Effective pruning of web-scale datasets based on complexity of concept clusters</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04578v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04578v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04578v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training CLIP-style models. Today's most effective pruning method on ImageNet clusters data samples into separate concepts according to their embedding and prunes away the most prototypical samples. We scale this approach to LAION and improve it by noting that the pruning rate should be concept-specific and adapted to the complexity of the concept. Using a simple and intuitive complexity measure, we are able to reduce the training cost to a quarter of regular training. By filtering from the LAION dataset, we find that training on a smaller set of high-quality data can lead to higher performance with significantly lower training costs. More specifically, we are able to outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot accuracy by 1.1p.p. while only using 27.7% of the data and training compute. Despite a strong reduction in training cost, we also see improvements on ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a competitive average zero-shot accuracy on 38 evaluation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04578v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用大规模的网络规模数据集在机器学习模型中带来了前所未有的性能提升，但也对其训练提出了奇怪的计算要求。为了提高训练和数据效率，我们在这里突破了修剪大规模多模式数据集以训练CLIP风格模型的极限。今天ImageNet上最有效的修剪方法根据数据样本的嵌入将其聚类为单独的概念，并修剪掉最典型的样本。我们将这种方法扩展到LAION，并通过注意修剪率应该是特定于概念的，并适应概念的复杂性来改进它。使用简单直观的复杂性度量，我们能够将培训成本降低到常规培训的四分之一。通过从LAION数据集进行过滤，我们发现在较小的一组高质量数据上进行训练可以带来更高的性能，同时显著降低训练成本。更具体地说，我们仅使用27.7%的数据和训练计算，就能够在ImageNet零样本上以1.1p.p.的精度优于LAION训练的OpenCLIP-ViT-B32模型。尽管训练成本大大降低，但我们也看到了ImageNet dist.shift、检索任务和VTAB的改进。在DataComp Medium基准测试中，我们在38项评估任务中实现了最先进的ImageNet零样本精度和具有竞争力的平均零样本精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04578v1" target="_blank">2401.04578v1</a>
                              </td>
                              <td>Effective pruning of web-scale datasets based on complexity of concept clusters</td>
                              <td>Amro Abbas</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04578v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04578v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16741v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16741v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16741v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16741v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16741v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型在各种应用中取得了显著的成功，如疾病诊断和文本报告生成。到目前为止，内窥镜视频分析的基础模型仍然缺乏。在本文中，我们提出了Endo FM，这是一个专门使用大量内窥镜视频数据开发的基础模型。首先，我们构建了一个视频转换器，它可以捕获跨空间和时间维度的局部和全局长程依赖关系。其次，我们通过自监督的方式使用全局和局部视图预训练我们的变换器模型，旨在使其对时空变化具有鲁棒性，并在不同场景中具有判别力。为了开发基础模型，我们将中国上海仁济医院宝山分院的9个公开可用的数据集和一个私人收集的数据集相结合，构建了一个大规模的内镜视频数据集。我们的数据集总体上由超过33K个视频片段组成，高达500万帧，包括各种协议、靶器官和疾病类型。我们经过预训练的Endo FM可以作为骨干，通过微调，轻松用于特定的下游任务。通过对3种不同类型的下游任务（包括分类、分割和检测）进行实验，我们的Endo FM显著超过了当前最先进的（SOTA）自监督预训练和基于适配器的迁移学习方法，例如VCL（用于分类、分割和检测的3.1%F1、4.8%Dice和5.5%F1）和ST适配器（用于分类和检测的5.9%F1、9.6%Dice和9.9%F1）。代码、数据集和模型发布于https://github.com/med-air/Endo-FM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16741v4" target="_blank">2306.16741v4</a>
                              </td>
                              <td>Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</td>
                              <td>Zhao Wang</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16741v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16741v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/med-air/endo-fm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04350v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04350v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04350v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04350v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks, and exhibit remarkable zero-shot generalization capability, while they are also vulnerable to imperceptible adversarial examples. Existing works typically employ adversarial training (fine-tuning) as a defense method against adversarial examples. However, direct application to the CLIP model may result in overfitting, compromising the model's capacity for generalization. In this paper, we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch, to enhance the model's zero-shot adversarial robustness. Specifically, PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model, aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outperforms the state-of-the-art method, improving the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach consistently improves clean accuracy by an average of 8.72%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04350v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的大规模预先训练的视觉语言模型在各种任务中表现出了令人印象深刻的性能，并表现出显著的零样本泛化能力，同时它们也容易受到难以察觉的对抗性例子的影响。现有的工作通常采用对抗性训练（微调）作为对抗性示例的防御方法。然而，直接应用于CLIP模型可能会导致过拟合，损害模型的泛化能力。在本文中，我们提出了预训练模型引导的对抗微调（PMG-AFT）方法，该方法通过仔细设计辅助分支来利用来自原始预训练模型的监督，以增强模型的零样本对抗鲁棒性。具体而言，PMG-AFT最小化了目标模型中对抗性示例的特征与预训练模型中的特征之间的距离，旨在保留预训练模型已经捕获的泛化特征。在15个零样本数据集上进行的大量实验表明，PMG-AFT显著优于最先进的方法，将前1名的鲁棒精度平均提高了4.99%。此外，我们的方法始终将清洁精度平均提高8.72%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04350v1" target="_blank">2401.04350v1</a>
                              </td>
                              <td>Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness</td>
                              <td>Sibo Wang</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04350v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04350v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_00829v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online Sensitivity Optimization in Differentially Private Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_00829v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_00829v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_00829v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former with gradient descent, with minimal repercussions on the overall privacy analysis. Our method is thoroughly assessed against alternative fixed and adaptive strategies across diverse datasets, tasks, model dimensions, and privacy levels. Our results indicate that it performs comparably or better in the evaluated scenarios, given the same privacy requirements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_00829v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练不同的私有机器学习模型需要约束个人对优化过程的贡献。这是通过在平均和批量消毒之前将其梯度的$2$-范数裁剪在预定阈值来实现的。这种选择以两种相反的方式对优化产生不利影响：它要么在较低值下由于过度削波而加剧偏差，要么在较高值下增加净化噪声。选择在很大程度上取决于数据集、模型架构等因素，甚至在同一优化中也有所不同，需要进行细致的调整，通常通过网格搜索来完成。为了避免在超参数调整中产生的隐私费用，我们提出了一种动态优化裁剪阈值的新方法。我们将该阈值视为一个额外的可学习参数，在阈值和成本函数之间建立一个干净的关系。这使我们能够通过梯度下降优化前者，对整体隐私分析的影响最小。我们的方法针对不同数据集、任务、模型维度和隐私级别的替代固定和自适应策略进行了全面评估。我们的结果表明，在相同的隐私要求下，它在评估的场景中表现得相当或更好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.00829v2" target="_blank">2310.00829v2</a>
                              </td>
                              <td>Online Sensitivity Optimization in Differentially Private Learning</td>
                              <td>Filippo Galli</td>
                              <td>2023-10-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_00829v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.00829v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03851v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03851v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03851v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03851v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, there has been a surge in the popularity of pre trained large language models (LLMs) (such as GPT-4), sweeping across the entire Natural Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have demonstrated advanced multi-modal understanding capabilities and showcased strong performance across various benchmarks. The LLM has started to embody traits of artificial general intelligence, which holds vital guidance for enhancing brain-like characteristics within visual encoding models. Hence, This paper proposes a new multi-modal training paradigm, aligning with LLM, for encoding fMRI activity in visual cortex. Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM). Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all stimulus images, forming a high-quality textual description set. Moreover, we use the pre-trained text encoder (CLIP) to process these detailed descriptions, obtaining the text embedding features. Next, we use the contrast loss function to minimize the distance between the image embedding features and the text embedding features to complete the alignment operation of the stimulus image and text information. With the assistance of the pre-trained LLM, this alignment process facilitates better learning of the visual encoding model, resulting in higher precision. The final experimental results indicate that our training paradigm has significantly aided in enhancing the performance of the visual encoding model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03851v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，预训练的大型语言模型（如GPT-4）的普及率激增，席卷了整个自然语言处理（NLP）和计算机视觉（CV）社区。这些LLM展示了先进的多模态理解能力，并在各种基准测试中表现出强大的性能。LLM已经开始体现通用人工智能的特征，这为增强视觉编码模型中的类脑特征提供了重要指导。因此，本文提出了一种新的多模式训练范式，与LLM相一致，用于编码视觉皮层的fMRI活动。基于这一范式，我们在fMRI数据中训练了一个编码模型，称为LLM视觉编码模型（LLM-VEM）。具体来说，我们利用LLM（miniGPT4）为所有刺激图像生成描述性文本，形成高质量的文本描述集。此外，我们使用预训练的文本编码器（CLIP）来处理这些详细的描述，获得文本嵌入特征。接下来，我们使用对比度损失函数来最小化图像嵌入特征和文本嵌入特征之间的距离，以完成刺激图像和文本信息的对齐操作。在预先训练的LLM的帮助下，这种对齐过程有助于更好地学习视觉编码模型，从而获得更高的精度。最终的实验结果表明，我们的训练范式在提高视觉编码模型的性能方面有显著的帮助。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03851v1" target="_blank">2401.03851v1</a>
                              </td>
                              <td>Aligned with LLM: a new multi-modal training paradigm for encoding fMRI activity in visual cortex</td>
                              <td>Shuxiao Ma</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03851v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03851v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_04751v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Parameter-Efficient Few-Shot Class Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_04751v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_04751v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_04751v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Few-Shot Class Incremental Learning (FSCIL) is a challenging continual learning task, where limited training examples are available during several learning sessions. To succeed in this task, it is necessary to avoid over-fitting new classes caused by biased distributions in the few-shot training sets. The general approach to address this issue involves enhancing the representational capability of a pre-defined backbone architecture by adding special modules for backward compatibility with older classes. However, this approach has not yet solved the dilemma of ensuring high classification accuracy over time while reducing the gap between the performance obtained on larger training sets and the smaller ones. In this work, we propose an alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to reduce the loss of information between different learning sessions. Instead of adapting additional modules to address information loss, we leverage the vast knowledge acquired by CLIP in large-scale pre-training and its effectiveness in generalizing to new concepts. Our approach is multimodal and parameter-efficient, relying on learnable prompts for both the language and vision encoders to enable transfer learning across sessions. We also introduce prompt regularization to improve performance and prevent forgetting. Our experimental results demonstrate that CPE-CLIP significantly improves FSCIL performance compared to state-of-the-art proposals while also drastically reducing the number of learnable parameters and training costs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_04751v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>少镜头课堂增量学习（FSCIL）是一项具有挑战性的持续学习任务，在几次学习过程中可以获得有限的培训示例。为了成功完成这项任务，有必要避免由于少数射击训练集中的偏差分布而导致的对新类的过度拟合。解决这个问题的一般方法包括通过添加与旧类向后兼容的特殊模块来增强预定义主干架构的表示能力。然而，这种方法还没有解决随着时间的推移确保高分类精度，同时减少在较大训练集和较小训练集上获得的性能之间的差距的难题。在这项工作中，我们提出了一种称为连续参数高效CLIP（CPE-CLIP）的替代方法，以减少不同学习会话之间的信息损失。我们没有调整额外的模块来解决信息丢失问题，而是利用CLIP在大规模预培训中获得的大量知识及其推广到新概念的有效性。我们的方法是多模式和参数高效的，依赖于语言和视觉编码器的可学习提示，实现跨会话的迁移学习。我们还引入了即时正则化来提高性能和防止遗忘。我们的实验结果表明，与最先进的方案相比，CPE-CLIP显著提高了FSCIL的性能，同时也大幅减少了可学习参数的数量和训练成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.04751v2" target="_blank">2303.04751v2</a>
                              </td>
                              <td>Multimodal Parameter-Efficient Few-Shot Class Incremental Learning</td>
                              <td>Marco D'Alessandro</td>
                              <td>2023-03-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_04751v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.04751v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03788v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03788v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03788v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03788v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-light image enhancement techniques have significantly progressed, but unstable image quality recovery and unsatisfactory visual perception are still significant challenges. To solve these problems, we propose a novel and robust low-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion, abbreviated as CFWD. Specifically, we design a guided network with a multiscale visual language in the frequency domain based on the wavelet transform to achieve effective image enhancement iteratively. In addition, we combine the advantages of Fourier transform in detail perception to construct a hybrid frequency domain space with significant perceptual capabilities(HFDPM). This operation guides wavelet diffusion to recover the fine-grained structure of the image and avoid diversity confusion. Extensive quantitative and qualitative experiments on publicly available real-world benchmarks show that our method outperforms existing state-of-the-art methods and better reproduces images similar to normal images. Code is available at https://github.com/He-Jinhong/CFWD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03788v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>微光图像增强技术已经取得了显著进展，但不稳定的图像质量恢复和不令人满意的视觉感知仍然是重大挑战。为了解决这些问题，我们提出了一种新的、稳健的基于CLIP傅立叶引导小波扩散的微光图像增强方法，简称CFWD。具体来说，我们设计了一个基于小波变换的频域多尺度视觉语言的引导网络，以迭代实现有效的图像增强。此外，我们结合傅立叶变换在细节感知方面的优势，构建了一个具有显著感知能力的混合频域空间（HFDPM）。该操作引导小波扩散来恢复图像的细粒度结构，避免多样性混淆。在公开的真实世界基准上进行的大量定量和定性实验表明，我们的方法优于现有的最先进的方法，更好地再现了与正常图像相似的图像。代码位于https://github.com/He-Jinhong/CFWD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03788v1" target="_blank">2401.03788v1</a>
                              </td>
                              <td>Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion</td>
                              <td>Minglong Xue</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03788v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03788v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/He-Jinhong/CFWD" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_16781v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kiki or Bouba? Sound Symbolism in Vision-and-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_16781v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_16781v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_16781v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_16781v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管人们认为人类语言中声音和意义之间的映射在很大程度上是任意的，但认知科学的研究表明，不同语言和人口群体之间的特定声音和含义之间存在着不可忽视的相关性，这一现象被称为声音象征。在意义的许多维度中，声音象征主义在语言和视觉领域之间的跨模态关联方面尤为突出和充分体现。在这项工作中，我们解决了声音象征主义是否反映在视觉和语言模型中的问题，如CLIP和稳定扩散。使用零样本知识探究来研究这些模型的内在知识，我们发现有力的证据表明它们确实显示了这种模式，与心理语言学中众所周知的kiki-bouba效应相似。我们的工作提供了一种新颖的方法来展示声音象征主义，并使用计算工具理解其本质。我们的代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.16781v2" target="_blank">2310.16781v2</a>
                              </td>
                              <td>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models</td>
                              <td>Morris Alper</td>
                              <td>2023-10-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_16781v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.16781v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03522v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Text-Driven Traffic Anomaly Detection with Temporal High-Frequency Modeling in Driving Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03522v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03522v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03522v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Traffic anomaly detection (TAD) in driving videos is critical for ensuring the safety of autonomous driving and advanced driver assistance systems. Previous single-stage TAD methods primarily rely on frame prediction, making them vulnerable to interference from dynamic backgrounds induced by the rapid movement of the dashboard camera. While two-stage TAD methods appear to be a natural solution to mitigate such interference by pre-extracting background-independent features (such as bounding boxes and optical flow) using perceptual algorithms, they are susceptible to the performance of first-stage perceptual algorithms and may result in error propagation. In this paper, we introduce TTHF, a novel single-stage method aligning video clips with text prompts, offering a new perspective on traffic anomaly detection. Unlike previous approaches, the supervised signal of our method is derived from languages rather than orthogonal one-hot vectors, providing a more comprehensive representation. Further, concerning visual representation, we propose to model the high frequency of driving videos in the temporal domain. This modeling captures the dynamic changes of driving scenes, enhances the perception of driving behavior, and significantly improves the detection of traffic anomalies. In addition, to better perceive various types of traffic anomalies, we carefully design an attentive anomaly focusing mechanism that visually and linguistically guides the model to adaptively focus on the visual context of interest, thereby facilitating the detection of traffic anomalies. It is shown that our proposed TTHF achieves promising performance, outperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and achieving high generalization on the DADA dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03522v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>驾驶视频中的交通异常检测（TAD）对于确保自动驾驶和高级驾驶员辅助系统的安全至关重要。以前的单级TAD方法主要依赖于帧预测，这使得它们容易受到仪表板摄像头快速移动引起的动态背景的干扰。虽然两阶段TAD方法似乎是通过使用感知算法预提取与背景无关的特征（如边界框和光流）来减轻这种干扰的自然解决方案，但它们容易受到第一阶段感知算法性能的影响，并可能导致误差传播。在本文中，我们介绍了TTHF，这是一种新的将视频片段与文本提示对齐的单阶段方法，为交通异常检测提供了一个新的视角。与以前的方法不同，我们方法的监督信号是从语言中导出的，而不是正交的单热向量，提供了更全面的表示。此外，关于视觉表示，我们建议在时域中对驾驶视频的高频进行建模。该建模捕捉了驾驶场景的动态变化，增强了对驾驶行为的感知，并显著提高了对交通异常的检测。此外，为了更好地感知各种类型的交通异常，我们精心设计了一种专注的异常聚焦机制，该机制在视觉和语言上引导模型自适应地聚焦于感兴趣的视觉上下文，从而有助于检测交通异常。结果表明，我们提出的TTHF实现了很好的性能，在DoTA数据集上的AUC超过了最先进的竞争对手+5.4%，并在DADA数据集上实现了高度泛化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03522v1" target="_blank">2401.03522v1</a>
                              </td>
                              <td>Text-Driven Traffic Anomaly Detection with Temporal High-Frequency Modeling in Driving Videos</td>
                              <td>Rongqin Liang</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03522v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03522v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_12075v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_12075v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_12075v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_12075v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Studying backdoor attacks is valuable for model copyright protection and enhancing defenses. While existing backdoor attacks have successfully infected multimodal contrastive learning models such as CLIP, they can be easily countered by specialized backdoor defenses for MCL models. This paper reveals the threats in this practical scenario that backdoor attacks can remain effective even after defenses and introduces the \emph{\toolns} attack, which is resistant to backdoor detection and model fine-tuning defenses. To achieve this, we draw motivations from the perspective of the Bayesian rule and propose a dual-embedding guided framework for backdoor attacks. Specifically, we ensure that visual trigger patterns approximate the textual target semantics in the embedding space, making it challenging to detect the subtle parameter variations induced by backdoor learning on such natural trigger patterns. Additionally, we optimize the visual trigger patterns to align the poisoned samples with target vision features in order to hinder the backdoor unlearning through clean fine-tuning. Extensive experiments demonstrate that our attack significantly outperforms state-of-the-art baselines (+45.3% ASR) in the presence of SoTA backdoor defenses, rendering these mitigation and detection strategies virtually ineffective. Furthermore, our approach effectively attacks some more rigorous scenarios like downstream tasks. We believe that this paper raises awareness regarding the potential threats associated with the practical application of multimodal contrastive learning and encourages the development of more robust defense mechanisms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_12075v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>研究后门攻击对模型版权保护和增强防御具有重要意义。虽然现有的后门攻击已经成功感染了CLIP等多模式对比学习模型，但它们可以很容易地通过MCL模型的专门后门防御来对抗。本文揭示了这种实际场景中的威胁，即即使在防御之后，后门攻击也可以保持有效，并介绍了能够抵抗后门检测和模型微调防御的\emph｛\toolns｝攻击。为了实现这一点，我们从贝叶斯规则的角度出发，提出了一个用于后门攻击的双重嵌入引导框架。具体来说，我们确保视觉触发模式在嵌入空间中近似于文本目标语义，这使得检测后门学习对这种自然触发模式引起的细微参数变化具有挑战性。此外，我们优化了视觉触发模式，以使中毒样本与目标视觉特征对齐，从而通过干净的微调阻碍后门遗忘。大量实验表明，在存在SoTA后门防御的情况下，我们的攻击显著优于最先进的基线（+45.3%ASR），使这些缓解和检测策略实际上无效。此外，我们的方法有效地攻击了一些更严格的场景，如下游任务。我们认为，本文提高了人们对多模式对比学习实际应用相关潜在威胁的认识，并鼓励开发更强大的防御机制。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.12075v2" target="_blank">2311.12075v2</a>
                              </td>
                              <td>BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning</td>
                              <td>Siyuan Liang</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_12075v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.12075v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03476v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03476v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03476v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03476v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current talking avatars mostly generate co-speech gestures based on audio and text of the utterance, without considering the non-speaking motion of the speaker. Furthermore, previous works on co-speech gesture generation have designed network structures based on individual gesture datasets, which results in limited data volume, compromised generalizability, and restricted speaker movements. To tackle these issues, we introduce FreeTalker, which, to the best of our knowledge, is the first framework for the generation of both spontaneous (e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium) speaker motions. Specifically, we train a diffusion-based model for speaker motion generation that employs unified representations of both speech-driven gestures and text-driven motions, utilizing heterogeneous data sourced from various motion datasets. During inference, we utilize classifier-free guidance to highly control the style in the clips. Additionally, to create smooth transitions between clips, we utilize DoubleTake, a method that leverages a generative prior and ensures seamless motion blending. Extensive experiments show that our method generates natural and controllable speaker movements. Our code, model, and demo are are available at \url{https://youngseng.github.io/FreeTalker/}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03476v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的说话化身大多基于说话的音频和文本生成共同说话手势，而不考虑说话者的非说话动作。此外，先前关于协同语音手势生成的工作已经基于单个手势数据集设计了网络结构，这导致数据量有限、可推广性受损和说话者运动受限。为了解决这些问题，我们引入了FreeTalker，据我们所知，它是第一个生成自发（例如，共同发言手势）和非自发（例如在讲台上移动）说话者动作的框架。具体来说，我们训练了一个基于扩散的说话人运动生成模型，该模型利用来自各种运动数据集的异构数据，采用语音驱动手势和文本驱动运动的统一表示。在推理过程中，我们利用无分类器引导来高度控制剪辑中的风格。此外，为了在片段之间创建平滑的过渡，我们使用DoubleTake，这是一种利用生成先验并确保无缝运动混合的方法。大量实验表明，我们的方法可以产生自然可控的扬声器运动。我们的代码、模型和演示可在\url上获得{https://youngseng.github.io/FreeTalker/}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03476v1" target="_blank">2401.03476v1</a>
                              </td>
                              <td>Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness</td>
                              <td>Sicheng Yang</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03476v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03476v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_10428v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Region-Prompted Adapter Tuning for Visual Abductive Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_10428v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_10428v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_10428v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Abductive Reasoning is an emerging vision-language (VL) topic where the model needs to retrieve/generate a likely textual hypothesis from a visual input (image or its part) using backward reasoning based on commonsense. Unlike in conventional VL retrieval or captioning tasks, where entities of texts appear in the image, in abductive inferences, the relevant facts about inferences are not readily apparent in the input images. Besides, these inferences are causally linked to specific regional visual cues and would change as cues change. Existing works highlight cues utilizing a specific prompt (e.g., colorful prompt). Then, a full fine-tuning of a VL foundation model is launched to tweak its function from perception to deduction. However, the colorful prompt uniformly patchify ``regional hints'' and ``global context'' at the same granularity level and may lose fine-grained visual details crucial for VAR. Meanwhile, full fine-tuning of VLF on limited data would easily be overfitted.   To tackle this, we propose a simple yet effective Region-Prompted Adapter (RPA), a hybrid parameter-efficient fine-tuning method that leverages the strengths of detailed cues and efficient training for the VAR task. RPA~consists of two novel modules: Regional Prompt Generator (RPG) and Adapter$^\textbf{+}$. The prior encodes ``regional visual hints'' and ``global contexts'' into visual prompts separately at fine and coarse-grained levels. The latter extends the vanilla adapters with a new Map Adapter, which modifies the attention map using a trainable low-dim query/key projection. Additionally, we propose a new Dual-Contrastive Loss to regress the visual feature toward features of factual description and plausible hypothesis. Experiments on the Sherlock demonstrate that RPA outperforms previous SOTAs, achieving the 1st rank on leaderboards (Comparison to Human Accuracy: RPA~31.74 vs CPT-CLIP 29.58).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_10428v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉推理是一个新兴的视觉语言（VL）主题，其中模型需要使用基于常识的反向推理从视觉输入（图像或其部分）中检索/生成可能的文本假设。与传统的VL检索或字幕任务中文本实体出现在图像中不同，在溯因推理中，关于推理的相关事实在输入图像中并不明显。此外，这些推断与特定的区域视觉线索有因果关系，并会随着线索的变化而变化。现有作品利用特定提示（如彩色提示）突出提示。然后，对VL基础模型进行全面微调，将其功能从感知调整为推导。然而，彩色提示将“区域提示”和“全局上下文”统一拼凑在同一粒度级别，并可能丢失对VAR至关重要的细粒度视觉细节。同时，在有限的数据上对VLF进行完全微调很容易被过度拟合。为了解决这一问题，我们提出了一种简单而有效的区域提示适配器（RPA），这是一种混合参数高效的微调方法，利用了VAR任务的详细提示和有效训练的优势。RPA~由两个新颖的模块组成：区域提示生成器（RPG）和适配器$^\textbf｛+｝$。先验将“区域视觉提示”和“全局上下文”分别编码为细粒度和粗粒度的视觉提示。后者用一个新的Map Adapter扩展了vanilla适配器，该适配器使用可训练的低调查询/键投影来修改注意力图。此外，我们提出了一种新的双重对比损失，以使视觉特征回归到事实描述和可信假设的特征。在Sherlock上的实验表明，RPA优于以前的SOTA，在排行榜上排名第一（与人类精度相比：RPA~31.74 vs CPT-CLIP 29.58）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.10428v3" target="_blank">2303.10428v3</a>
                              </td>
                              <td>A Region-Prompted Adapter Tuning for Visual Abductive Reasoning</td>
                              <td>Hao Zhang</td>
                              <td>2023-03-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_10428v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.10428v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00260v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00260v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00260v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00260v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the past decade, visual gaze estimation has garnered growing attention within the research community, thanks to its wide-ranging application scenarios. While existing estimation approaches have achieved remarkable success in enhancing prediction accuracy, they primarily infer gaze directions from single-image signals and discard the huge potentials of the currently dominant text guidance. Notably, visual-language collaboration has been extensively explored across a range of visual tasks, such as image synthesis and manipulation, leveraging the remarkable transferability of large-scale Contrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing gaze estimation approaches ignore the rich semantic cues conveyed by linguistic signals and priors in CLIP feature space, thereby yielding performance setbacks. In pursuit of making up this gap, we delve deeply into the text-eye collaboration protocol and introduce a novel gaze estimation framework in this paper, referred to as GazeCLIP. Specifically, we intricately design a linguistic description generator to produce text signals with coarse directional cues. Additionally, a CLIP-based backbone that excels in characterizing text-eye pairs for gaze estimation is presented. This is followed by the implementation of a fine-grained multi-modal fusion module aimed at modeling the interrelationships between heterogeneous inputs. Extensive experiments on three challenging datasets demonstrate the superiority of the proposed GazeCLIP which surpasses the previous approaches and achieves the state-of-the-art estimation accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00260v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的十年里，视觉凝视估计由于其广泛的应用场景，在研究界引起了越来越多的关注。虽然现有的估计方法在提高预测精度方面取得了显著的成功，但它们主要从单个图像信号推断视线方向，并放弃了当前占主导地位的文本引导的巨大潜力。值得注意的是，视觉语言协作已经在一系列视觉任务中得到了广泛的探索，如图像合成和操作，利用了大规模对比语言图像预训练（CLIP）模型的显著可移植性。然而，现有的凝视估计方法忽略了CLIP特征空间中语言信号和先验所传达的丰富语义线索，从而导致性能受挫。为了弥补这一差距，我们深入研究了文本眼协作协议，并在本文中引入了一种新的凝视估计框架，称为GazeCLIP。具体来说，我们复杂地设计了一个语言描述生成器，以产生具有粗略方向线索的文本信号。此外，还提出了一种基于CLIP的主干，该主干擅长于表征用于凝视估计的文本-眼睛对。随后实现了一个细粒度的多模式融合模块，旨在对异构输入之间的相互关系进行建模。在三个具有挑战性的数据集上进行的大量实验证明了所提出的GazeCLIP的优越性，它超越了以前的方法，并实现了最先进的估计精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00260v2" target="_blank">2401.00260v2</a>
                              </td>
                              <td>GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance</td>
                              <td>Jun Wang</td>
                              <td>2023-12-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00260v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00260v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03177v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03177v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03177v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03177v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content. Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals. Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents. In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos. We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation modeling. By representing textual units and video frames as nodes and using hyperedges to depict their relationships, a multi-modal hypergraph is constructed. In this way, the query and the video can be aligned in a high-order semantic space. In addition, to enhance the model's generalization ability, the extracted features are fed into a variational inference component for computation, obtaining the variational representation under the Gaussian distribution. The incorporation of hypergraphs and variational inference allows our model to capture complex, n-ary interactions among textual and visual contents. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on the text-video retrieval task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03177v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本视频检索是一项具有挑战性的任务，旨在识别给定文本查询的相关视频。与传统的文本检索相比，文本视频检索的主要障碍是查询的文本性质与视频内容的视觉丰富性之间的语义差距。以前的工作主要集中在通过精细地聚合字帧匹配信号来对齐查询和视频。受人类模块化判断文本和视频之间相关性的认知过程的启发，由于视频内容的连续性和复杂性，判断需要高阶匹配信号。在本文中，我们提出了块级文本视频匹配，其中提取查询块来描述特定的检索单元，并将视频块分割成与视频不同的片段。我们将块级匹配公式化为查询单词和视频帧之间的n元相关性建模，并引入了一个用于n元相关性模型的多模态超图。通过将文本单元和视频帧表示为节点，并使用超边来描述它们的关系，构造了一个多模态超图。通过这种方式，查询和视频可以在高阶语义空间中对齐。此外，为了增强模型的泛化能力，将提取的特征输入变分推理组件进行计算，得到高斯分布下的变分表示。超图和变分推理的结合使我们的模型能够捕捉文本和视觉内容之间复杂的n元交互。实验结果表明，我们提出的方法在文本视频检索任务上取得了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03177v1" target="_blank">2401.03177v1</a>
                              </td>
                              <td>Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks</td>
                              <td>Qian Li</td>
                              <td>2024-01-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03177v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03177v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03105v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03105v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03105v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03105v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal Large Language Models (MLLMs) are experiencing rapid growth, yielding a plethora of noteworthy contributions in recent months. The prevailing trend involves adopting data-driven methodologies, wherein diverse instruction-following datasets are collected. However, a prevailing challenge persists in these approaches, specifically in relation to the limited visual perception ability, as CLIP-like encoders employed for extracting visual information from inputs. Though these encoders are pre-trained on billions of image-text pairs, they still grapple with the information loss dilemma, given that textual captions only partially capture the contents depicted in images. To address this limitation, this paper proposes to improve the visual perception ability of MLLMs through a mixture-of-experts knowledge enhancement mechanism. Specifically, we introduce a novel method that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs. Extensive experiments have evaluated its effectiveness of advancing MLLMs, showcasing improved visual perception achieved through the integration of visual experts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03105v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式大型语言模型（MLLM）正在经历快速增长，近几个月来产生了大量值得注意的贡献。主流趋势包括采用数据驱动的方法，其中收集不同的指令遵循数据集。然而，在这些方法中仍然存在一个普遍的挑战，特别是与有限的视觉感知能力有关，因为采用了类似CLIP的编码器来从输入中提取视觉信息。尽管这些编码器是在数十亿对图像-文本上进行预训练的，但它们仍然面临信息丢失的困境，因为文本字幕只部分捕捉到图像中描绘的内容。为了解决这一局限性，本文提出通过混合专家知识增强机制来提高MLLMs的视觉感知能力。具体而言，我们引入了一种新方法，该方法将多任务编码器和视觉工具结合到现有的MLLMs训练和推理管道中，旨在提供更全面、更准确的视觉输入摘要。广泛的实验评估了其推进MLLMs的有效性，展示了通过整合视觉专家实现的视觉感知的改善。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03105v1" target="_blank">2401.03105v1</a>
                              </td>
                              <td>Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models</td>
                              <td>Xin He</td>
                              <td>2024-01-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03105v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03105v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03048v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Latte: Latent Diffusion Transformer for Video Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03048v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03048v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03048v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to text-to-video generation (T2V) task, where Latte achieves comparable results compared to recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03048v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的潜在扩散转换器，即Latte，用于视频生成。Latte首先从输入视频中提取时空标记，然后采用一系列Transformer块对潜在空间中的视频分布进行建模。为了对从视频中提取的大量令牌进行建模，从分解输入视频的空间和时间维度的角度引入了四种有效的变体。为了提高生成视频的质量，我们通过严格的实验分析确定了Latte的最佳实践，包括视频片段补丁嵌入、模型变体、时间步长类信息注入、时间位置嵌入和学习策略。我们的综合评估表明，Latte在四个标准视频生成数据集（即FaceForensics、SkyTimelapse、UCF101和Taichi HD）中实现了最先进的性能。此外，我们将Latte扩展到文本到视频生成（T2V）任务，其中与最近的T2V模型相比，Latte实现了可比的结果。我们坚信，Latte为未来将变压器纳入视频生成的扩散模型的研究提供了宝贵的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03048v1" target="_blank">2401.03048v1</a>
                              </td>
                              <td>Latte: Latent Diffusion Transformer for Video Generation</td>
                              <td>Xin Ma</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03048v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03048v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/maxin-cn/Latte" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02957v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02957v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We delve into a nuanced but significant challenge inherent to Vision Transformers (ViTs): feature maps of these models exhibit grid-like artifacts, which detrimentally hurt the performance of ViTs in downstream tasks. Our investigations trace this fundamental issue down to the positional embeddings at the input stage. To address this, we propose a novel noise model, which is universally applicable to all ViTs. Specifically, the noise model dissects ViT outputs into three components: a semantics term free from noise artifacts and two artifact-related terms that are conditioned on pixel locations. Such a decomposition is achieved by enforcing cross-view feature consistency with neural fields in a per-image basis. This per-image optimization process extracts artifact-free features from raw ViT outputs, providing clean features for offline applications. Expanding the scope of our solution to support online functionality, we introduce a learnable denoiser to predict artifact-free features directly from unprocessed ViT outputs, which shows remarkable generalization capabilities to novel data without the need for per-image optimization. Our two-stage approach, termed Denoising Vision Transformers (DVT), does not require re-training existing pre-trained ViTs and is immediately applicable to any Transformer-based architecture. We evaluate our method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP, DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT consistently and significantly improves existing state-of-the-art general-purpose models in semantic and geometric tasks across multiple datasets (e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT design, especially regarding the naive use of positional embeddings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02957v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02957v1" target="_blank">2401.02957v1</a>
                              </td>
                              <td>Denoising Vision Transformers</td>
                              <td>Jiawei Yang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02957v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02955v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02955v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02955v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02955v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The CLIP and Segment Anything Model (SAM) are remarkable vision foundation models (VFMs). SAM excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero-shot recognition capabilities. This paper presents an in-depth exploration of integrating these two models into a unified framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired model designed for simultaneous interactive segmentation and recognition, leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The former adapts SAM's knowledge into the CLIP via distillation and learnable transformer adapters, while the latter transfers CLIP knowledge into SAM, enhancing its recognition capabilities. Extensive experiments on various datasets and detectors show the effectiveness of Open-Vocabulary SAM in both segmentation and recognition tasks, significantly outperforming the naive baselines of simply combining SAM and CLIP. Furthermore, aided with image classification data training, our method can segment and recognize approximately 22,000 classes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02955v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP和Segment Anything模型（SAM）是卓越的愿景基础模型（VFM）。SAM擅长跨不同领域的细分任务，而CLIP以其零样本识别能力而闻名。本文对将这两个模型集成到一个统一的框架中进行了深入的探索。具体来说，我们介绍了开放词汇SAM，这是一个受SAM启发的模型，旨在同时进行交互式分割和识别，利用了两个独特的知识转移模块：SAM2CLIP和CLIP2SAM。前者通过蒸馏和可学习的转换器适配器将SAM的知识改编为CLIP，而后者将CLIP知识转换为SAM，增强其识别能力。在各种数据集和检测器上进行的大量实验表明，开放词汇SAM在分割和识别任务中都是有效的，显著优于简单组合SAM和CLIP的原始基线。此外，在图像分类数据训练的辅助下，我们的方法可以分割和识别大约22000个类别。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02955v1" target="_blank">2401.02955v1</a>
                              </td>
                              <td>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</td>
                              <td>Haobo Yuan</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02955v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02955v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/harboryuan/ovsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09215v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09215v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09215v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09215v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our code is available at https://github.com/kirill-vish/Beyond-INet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09215v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代计算机视觉为从业者提供了各种各样的模型，从多个选项中选择一个模型用于特定应用可能具有挑战性。传统上，在ImageNet上通过它们的分类精度来比较竞争模型架构和训练协议。然而，这一单一指标并不能完全捕捉到对专门任务至关重要的性能细微差别。在这项工作中，我们对ConvNet和Vision Transformer架构中超出ImageNet准确性的模型行为进行了深入的比较分析，每种架构都跨越了监督和CLIP训练范式。尽管我们选择的模型具有相似的ImageNet精度和计算要求，但我们发现它们在许多其他方面有所不同：错误类型、输出校准、可转移性和特征不变性等。这种传统指标无法捕捉到的模型特征的多样性，凸显了在不同模型之间进行选择时需要进行更细致的分析。我们的代码可在https://github.com/kirill-vish/Beyond-INet.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09215v2" target="_blank">2311.09215v2</a>
                              </td>
                              <td>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</td>
                              <td>Kirill Vishniakov</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09215v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09215v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kirill-vish/beyond-inet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02651v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking PathCLIP for Pathology Image Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02651v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02651v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02651v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate image classification and retrieval are of importance for clinical diagnosis and treatment decision-making. The recent contrastive language-image pretraining (CLIP) model has shown remarkable proficiency in understanding natural images. Drawing inspiration from CLIP, PathCLIP is specifically designed for pathology image analysis, utilizing over 200,000 image and text pairs in training. While the performance the PathCLIP is impressive, its robustness under a wide range of image corruptions remains unknown. Therefore, we conduct an extensive evaluation to analyze the performance of PathCLIP on various corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In our experiments, we introduce seven corruption types including brightness, contrast, Gaussian blur, resolution, saturation, hue, and markup at four severity levels. Through experiments, we find that PathCLIP is relatively robustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot classification. Among the seven corruptions, blur and resolution can cause server performance degradation of the PathCLIP. This indicates that ensuring the quality of images is crucial before conducting a clinical test. Additionally, we assess the robustness of PathCLIP in the task of image-image retrieval, revealing that PathCLIP performs less effectively than PLIP on Osteosarcoma but performs better on WSSS4LUAD under diverse corruptions. Overall, PathCLIP presents impressive zero-shot classification and retrieval performance for pathology images, but appropriate care needs to be taken when using it. We hope this study provides a qualitative impression of PathCLIP and helps understand its differences from other CLIP models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02651v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确的图像分类和检索对临床诊断和治疗决策具有重要意义。最近的对比语言图像预训练（CLIP）模型在理解自然图像方面表现出了非凡的熟练度。PathCLIP的灵感来源于CLIP，专门用于病理学图像分析，在训练中使用了超过200000对图像和文本。虽然PathCLIP的性能令人印象深刻，但它在各种图像损坏情况下的稳健性仍然未知。因此，我们进行了广泛的评估，以分析PathCLIP在骨肉瘤和WSSS4LUAD数据集的各种损坏图像上的性能。在我们的实验中，我们引入了七种损坏类型，包括亮度、对比度、高斯模糊、分辨率、饱和度、色调和四个严重级别的标记。通过实验，我们发现PathCLIP对图像破坏具有相对的鲁棒性，并且在零样本分类中超过了OpenAI-CLIP和PLIP。在这七种损坏中，模糊和分辨率会导致PathCLIP的服务器性能下降。这表明，在进行临床测试之前，确保图像质量至关重要。此外，我们评估了PathCLIP在图像检索任务中的稳健性，表明PathCLIP对骨肉瘤的效果不如PLIP，但在不同的腐蚀下对WSSS4LUAD的效果更好。总体而言，PathCLIP为病理学图像提供了令人印象深刻的零样本分类和检索性能，但在使用时需要采取适当的谨慎措施。我们希望这项研究能提供PathCLIP的定性印象，并有助于理解其与其他CLIP模型的差异。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02651v1" target="_blank">2401.02651v1</a>
                              </td>
                              <td>Benchmarking PathCLIP for Pathology Image Analysis</td>
                              <td>Sunyi Zheng</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02651v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02651v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01129v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ControlDreamer: Stylized 3D Generation with Multi-View ControlNet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01129v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01129v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01129v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in text-to-3D generation have significantly contributed to the automation and democratization of 3D content creation. Building upon these developments, we aim to address the limitations of current methods in generating 3D models with creative geometry and styles. We introduce multi-view ControlNet, a novel depth-aware multi-view diffusion model trained on generated datasets from a carefully curated text corpus. Our multi-view ControlNet is then integrated into our two-stage pipeline, ControlDreamer, enabling text-guided generation of stylized 3D models. Additionally, we present a comprehensive benchmark for 3D style editing, encompassing a broad range of subjects, including objects, animals, and characters, to further facilitate research on diverse 3D generation. Our comparative analysis reveals that this new pipeline outperforms existing text-to-3D methods as evidenced by human evaluations and CLIP score metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01129v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到3D生成的最新进展对3D内容创建的自动化和民主化做出了重大贡献。在这些发展的基础上，我们旨在解决当前方法在生成具有创造性几何形状和样式的三维模型方面的局限性。我们介绍了多视图ControlNet，这是一种新颖的深度感知多视图扩散模型，在精心策划的文本语料库中生成的数据集上进行训练。然后，我们的多视图ControlNet集成到我们的两阶段管道ControlDreamer中，实现了以文本为导向的风格化3D模型生成。此外，我们为3D风格编辑提供了一个全面的基准，涵盖了广泛的主题，包括物体、动物和角色，以进一步促进对多样化3D生成的研究。我们的比较分析表明，这种新的管道优于现有的文本到3D方法，这可以通过人工评估和CLIP得分指标来证明。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01129v2" target="_blank">2312.01129v2</a>
                              </td>
                              <td>ControlDreamer: Stylized 3D Generation with Multi-View ControlNet</td>
                              <td>Yeongtak Oh</td>
                              <td>2023-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01129v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01129v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02309v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02309v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02309v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02309v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip. Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly. These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance. Nevertheless, these approaches underutilize the reciprocal relationship between two tasks. In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD. Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space. Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction. Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD. Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods. Codes are available at \url{https://github.com/mingyao1120/TR-DETR}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02309v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于自然语言查询的视频瞬间检索（MR）和高光检测（HD）是两个高度相关的任务，旨在获取视频中的相关瞬间和每个视频片段的高光分数。最近，有几种方法致力于构建基于DETR的网络，以联合解决MR和HD问题。这些方法在进行多模态特征提取和特征交互后，只需添加两个独立的任务头，即可获得良好的性能。然而，这些方法没有充分利用两项任务之间的相互关系。在本文中，我们提出了一种基于DETR的任务互易变换器（TR-DETR），该变换器专注于探索MR和HD之间固有的互易性。具体而言，首先构建局部全局多模态对齐模块，以将来自不同模态的特征对齐到共享的潜在空间中。随后，设计了一种视觉特征精化，以从视觉特征中消除与查询无关的信息，用于模态交互。最后，构建了一个任务协作模块，利用MR和HD之间的互易性来细化检索流水线和亮点分数预测过程。在QVHighlights、Charades STA和TVSum数据集上的综合实验表明，TR-DETR优于现有的最先进的方法。代码位于\url{https://github.com/mingyao1120/TR-DETR}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02309v2" target="_blank">2401.02309v2</a>
                              </td>
                              <td>TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection</td>
                              <td>Hao Sun</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02309v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02309v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mingyao1120/tr-detr" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02584v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Weakly Supervised Text-to-Audio Grounding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02584v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02584v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02584v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-audio grounding (TAG) task aims to predict the onsets and offsets of sound events described by natural language. This task can facilitate applications such as multimodal information retrieval. This paper focuses on weakly-supervised text-to-audio grounding (WSTAG), where frame-level annotations of sound events are unavailable, and only the caption of a whole audio clip can be utilized for training. WSTAG is superior to strongly-supervised approaches in its scalability to large audio-text datasets. Two WSTAG frameworks are studied in this paper: sentence-level and phrase-level. First, we analyze the limitations of mean pooling used in the previous WSTAG approach and investigate the effects of different pooling strategies. We then propose phrase-level WSTAG to use matching labels between audio clips and phrases for training. Advanced negative sampling strategies and self-supervision are proposed to enhance the accuracy of the weak labels and provide pseudo strong labels. Experimental results show that our system significantly outperforms the previous WSTAG SOTA. Finally, we conduct extensive experiments to analyze the effects of several factors on phrase-level WSTAG. The code and model is available at https://github.com/wsntxxn/TextToAudioGrounding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02584v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到音频接地（TAG）任务旨在预测自然语言描述的声音事件的开始和偏移。该任务可以促进诸如多模式信息检索之类的应用。本文重点研究弱监督文本到音频基础（WSTAG），其中声音事件的帧级注释不可用，并且只能使用整个音频片段的字幕进行训练。WSTAG在对大型音频文本数据集的可扩展性方面优于强监督方法。本文研究了两个WSTAG框架：句子层次和短语层次。首先，我们分析了以前的WSTAG方法中使用的平均池的局限性，并研究了不同池策略的影响。然后，我们提出短语级别的WSTAG来使用音频片段和短语之间的匹配标签进行训练。提出了先进的负采样策略和自我监督，以提高弱标签的准确性，并提供伪强标签。实验结果表明，我们的系统明显优于之前的WSTAG SOTA。最后，我们进行了广泛的实验来分析几个因素对短语水平WSTAG的影响。代码和型号可在https://github.com/wsntxxn/TextToAudioGrounding.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02584v1" target="_blank">2401.02584v1</a>
                              </td>
                              <td>Towards Weakly Supervised Text-to-Audio Grounding</td>
                              <td>Xuenan Xu</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02584v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02584v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wsntxxn/TextToAudioGrounding" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_06485v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Communication-Efficient Triangle Counting under Local Differential Privacy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_06485v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_06485v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_06485v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangle counting in networks under LDP (Local Differential Privacy) is a fundamental task for analyzing connection patterns or calculating a clustering coefficient while strongly protecting sensitive friendships from a central server. In particular, a recent study proposes an algorithm for this task that uses two rounds of interaction between users and the server to significantly reduce estimation error. However, this algorithm suffers from a prohibitively high communication cost due to a large noisy graph each user needs to download.   In this work, we propose triangle counting algorithms under LDP with a small estimation error and communication cost. We first propose two-rounds algorithms consisting of edge sampling and carefully selecting edges each user downloads so that the estimation error is small. Then we propose a double clipping technique, which clips the number of edges and then the number of noisy triangles, to significantly reduce the sensitivity of each user's query. Through comprehensive evaluation, we show that our algorithms dramatically reduce the communication cost of the existing algorithm, e.g., from 6 hours to 8 seconds or less at a 20 Mbps download rate, while keeping a small estimation error.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_06485v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LDP（Local Differential Privacy，本地差分隐私）下网络中的三角计数是分析连接模式或计算聚类系数的基本任务，同时强烈保护敏感友谊免受中央服务器的攻击。特别是，最近的一项研究提出了一种用于该任务的算法，该算法使用用户和服务器之间的两轮交互来显著降低估计误差。然而，由于每个用户需要下载大的噪声图，该算法的通信成本高得令人望而却步。在这项工作中，我们提出了LDP下的三角形计数算法，该算法具有较小的估计误差和通信成本。我们首先提出了两轮算法，包括边缘采样和仔细选择每个用户下载的边缘，以使估计误差较小。然后，我们提出了一种双裁剪技术，该技术先裁剪边的数量，然后裁剪有噪三角形的数量，以显著降低每个用户查询的灵敏度。通过综合评估，我们发现我们的算法显著降低了现有算法的通信成本，例如，在20Mbps的下载速率下，从6小时降低到8秒或更短，同时保持了较小的估计误差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.06485v3" target="_blank">2110.06485v3</a>
                              </td>
                              <td>Communication-Efficient Triangle Counting under Local Differential Privacy</td>
                              <td>Jacob Imola</td>
                              <td>2021-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_06485v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.06485v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/triangleldp/triangleldp" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10159v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10159v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10159v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10159v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recognizing the activities causing distraction in real-world driving scenarios is critical for ensuring the safety and reliability of both drivers and pedestrians on the roadways. Conventional computer vision techniques are typically data-intensive and require a large volume of annotated training data to detect and classify various distracted driving behaviors, thereby limiting their efficiency and scalability. We aim to develop a generalized framework that showcases robust performance with access to limited or no annotated training data. Recently, vision-language models have offered large-scale visual-textual pretraining that can be adapted to task-specific learning like distracted driving activity recognition. Vision-language pretraining models, such as CLIP, have shown significant promise in learning natural language-guided visual representations. This paper proposes a CLIP-based driver activity recognition approach that identifies driver distraction from naturalistic driving images and videos. CLIP's vision embedding offers zero-shot transfer and task-based finetuning, which can classify distracted activities from driving video data. Our results show that this framework offers state-of-the-art performance on zero-shot transfer and video-based CLIP for predicting the driver's state on two public datasets. We propose both frame-based and video-based frameworks developed on top of the CLIP's visual representation for distracted driving detection and classification tasks and report the results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10159v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>认识到现实驾驶场景中造成分心的活动对于确保道路上驾驶员和行人的安全性和可靠性至关重要。传统的计算机视觉技术通常是数据密集型的，需要大量的注释训练数据来检测和分类各种分心的驾驶行为，从而限制了其效率和可扩展性。我们的目标是开发一个通用框架，通过访问有限或无注释的训练数据来展示稳健的性能。最近，视觉语言模型提供了大规模的视觉文本预训练，可以适应特定任务的学习，如分心驾驶活动识别。视觉语言预训练模型，如CLIP，在学习自然语言引导的视觉表示方面显示出了巨大的前景。本文提出了一种基于CLIP的驾驶员活动识别方法，该方法可以识别驾驶员对自然驾驶图像和视频的分心。CLIP的视觉嵌入提供了零样本转移和基于任务的微调，可以从驾驶视频数据中分类分心的活动。我们的结果表明，该框架在零样本传输和基于视频的CLIP上提供了最先进的性能，用于在两个公共数据集上预测驾驶员的状态。我们提出了在CLIP的视觉表示基础上开发的基于帧和基于视频的框架，用于分心驾驶检测和分类任务，并报告了结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10159v3" target="_blank">2306.10159v3</a>
                              </td>
                              <td>Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos</td>
                              <td>Md Zahid Hasan</td>
                              <td>2023-06-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10159v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10159v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zahid-isu/driveclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02418v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning to Prompt with Text Only Supervision for Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02418v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02418v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02418v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundational vision-language models such as CLIP are becoming a new paradigm in vision, due to their excellent generalization abilities. However, adapting these models for downstream tasks while maintaining their generalization remains a challenge. In literature, one branch of methods adapts CLIP by learning prompts using visual information. While effective, most of these works require labeled data which is not practical, and often struggle to generalize towards new datasets due to over-fitting on the source data. An alternative approach resorts to training-free methods by generating class descriptions from large language models (LLMs) and perform prompt ensembling. However, these methods often generate class specific prompts that cannot be transferred to other classes, which incur higher costs by generating LLM descriptions for each class separately. In this work, we propose to combine the strengths of these both streams of methods by learning prompts using only text data derived from LLMs. As supervised training of prompts is not trivial due to absence of images, we develop a training approach that allows prompts to extract rich contextual knowledge from LLM data. Moreover, with LLM contextual data mapped within the learned prompts, it enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost. To the best of our knowledge, this is the first work that learns generalized prompts using text only data. We perform extensive evaluations on 4 benchmarks where our method improves over prior ensembling works while being competitive to those utilizing labeled images. Our code and pre-trained models are available at https://github.com/muzairkhattak/ProText.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02418v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP等基础视觉语言模型由于其卓越的泛化能力，正在成为视觉领域的一种新范式。然而，使这些模型适应下游任务，同时保持其通用性仍然是一个挑战。在文献中，方法的一个分支通过使用视觉信息学习提示来适应CLIP。虽然有效，但这些工作中的大多数都需要标记数据，这是不实际的，并且由于对源数据的过度拟合，通常难以推广到新的数据集。另一种方法是通过从大型语言模型（LLM）中生成类描述并执行即时组合来求助于无训练方法。然而，这些方法通常会生成特定于类的提示，这些提示无法传递到其他类，这会因为分别为每个类生成LLM描述而产生更高的成本。在这项工作中，我们建议通过仅使用LLM导出的文本数据来学习提示，将这两种方法的优势结合起来。由于没有图像，提示的监督训练并非微不足道，我们开发了一种训练方法，允许提示从LLM数据中提取丰富的上下文知识。此外，通过在学习的提示中映射LLM上下文数据，它能够将提示零样本转移到新的类和数据集，这可能会降低LLM提示的工程成本。据我们所知，这是第一部使用纯文本数据学习通用提示的作品。我们对4个基准进行了广泛的评估，其中我们的方法比先前的集成工作有所改进，同时与使用标记图像的方法相比具有竞争力。我们的代码和预训练模型可在https://github.com/muzairkhattak/ProText.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02418v1" target="_blank">2401.02418v1</a>
                              </td>
                              <td>Learning to Prompt with Text Only Supervision for Vision-Language Models</td>
                              <td>Muhammad Uzair Khattak</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02418v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02418v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/muzairkhattak/protext" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02402v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02402v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02402v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02402v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D panoptic segmentation is a challenging perception task, which aims to predict both semantic and instance annotations for 3D points in a scene. Although prior 3D panoptic segmentation approaches have achieved great performance on closed-set benchmarks, generalizing to novel categories remains an open problem. For unseen object categories, 2D open-vocabulary segmentation has achieved promising results that solely rely on frozen CLIP backbones and ensembling multiple classification outputs. However, we find that simply extending these 2D models to 3D does not achieve good performance due to poor per-mask classification quality on novel categories. In this paper, we propose the first method to tackle 3D open-vocabulary panoptic segmentation. Our model takes advantage of the fusion between learnable LiDAR features and dense frozen vision CLIP features, using a single classification head to make predictions for both base and novel classes. To further improve the classification performance on novel classes and leverage the CLIP model, we propose two novel loss functions: object-level distillation loss and voxel-level distillation loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our method outperforms strong baselines by a large margin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02402v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维全景分割是一项具有挑战性的感知任务，旨在预测场景中三维点的语义和实例注释。尽管先前的3D全景分割方法在闭集基准上取得了很好的性能，但推广到新的类别仍然是一个悬而未决的问题。对于看不见的对象类别，2D开放词汇分割已经取得了很有希望的结果，该结果仅依赖于冻结的CLIP主干和集合多个分类输出。然而，我们发现，简单地将这些2D模型扩展到3D并不能获得良好的性能，因为在新的类别上每个掩码的分类质量较差。在本文中，我们提出了第一种解决三维开放词汇全景分割的方法。我们的模型利用了可学习的激光雷达特征和密集冻结视觉CLIP特征之间的融合，使用单个分类头对基本类和新类进行预测。为了进一步提高新类的分类性能并利用CLIP模型，我们提出了两种新的损失函数：对象级蒸馏损失和体素级蒸馏损失。我们在nuScenes和SemanticKITTI数据集上的实验表明，我们的方法在很大程度上优于强基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02402v1" target="_blank">2401.02402v1</a>
                              </td>
                              <td>3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation</td>
                              <td>Zihao Xiao</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02402v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02402v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02347v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02347v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02347v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02347v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we propose a novel zero-shot image captioning framework with text-only training to reduce the modality gap. In particular, we introduce a subregion feature aggregation to leverage local region information, which produces a compact visual representation for matching text representation. Moreover, we incorporate a noise injection and CLIP reranking strategy to boost captioning performance. We also extend our framework to build a zero-shot VQA pipeline, demonstrating its generality. Through extensive experiments on common captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that our method achieves remarkable performance improvements. Code is available at https://github.com/Artanic30/MacCap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02347v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像字幕旨在生成图像的描述性和有意义的文本描述，从而实现广泛的视觉语言应用。先前的工作已经证明，利用对比图像语言预训练（CLIP）的力量为实现零样本字幕提供了一种很有前途的方法，从而消除了对昂贵字幕注释的需求。然而，在CLIP的潜在空间中广泛观察到的模态间隙破坏了成对图像-文本特征之间的对齐，从而损害了零样本字幕的性能。为了解决这个问题，我们对CLIP潜在空间进行了分析，得出了两个发现。首先，我们观察到，由于文本描述中固有的信息丢失，CLIP的图像子区域的视觉特征可以实现更接近配对字幕。此外，我们还表明，配对图像文本之间的模态间隙可以根据经验建模为零均值高斯分布。受这些发现的启发，我们提出了一种新颖的零样本图像字幕框架，该框架通过文本训练来减少模态间隙。特别地，我们引入了子区域特征聚合来利用局部区域信息，这为匹配文本表示产生了紧凑的视觉表示。此外，我们结合了噪声注入和CLIP重新排序策略，以提高字幕性能。我们还扩展了我们的框架来构建零样本VQA管道，展示了它的通用性。通过在常见的字幕和VQA数据集（如MSCOCO、Flickr30k和VQAV2）上进行广泛的实验，我们表明我们的方法实现了显著的性能改进。代码位于https://github.com/Artanic30/MacCap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02347v1" target="_blank">2401.02347v1</a>
                              </td>
                              <td>Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training</td>
                              <td>Longtian Qiu</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02347v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02347v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/artanic30/maccap" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01736v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Few-shot Adaptation of Multi-modal Foundation Models: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01736v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01736v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01736v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01736v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态（视觉语言）模型，如CLIP，正在取代传统的监督预训练模型（例如，基于ImageNet的预训练），成为新一代的视觉基础模型。这些模型具有从数十亿互联网图像-文本对中学习的鲁棒和对齐的语义表示，可以以零样本的方式应用于各种下游任务。然而，在医学成像和遥感等一些细粒度领域，多模态基础模型的性能往往还有很多不足之处。因此，许多研究人员已经开始探索这些模型的少量镜头自适应方法，逐渐得出三种主要的技术方法：1）基于提示的方法，2）基于适配器的方法，以及3）基于外部知识的方法。然而，这一快速发展的领域在没有进行全面调查以系统地组织研究进展的情况下产生了许多结果。因此，在本次调查中，我们介绍并分析了多模态模型的少镜头自适应方法的研究进展，总结了常用的数据集和实验装置，并比较了不同方法的结果。此外，由于现有方法缺乏可靠的理论支持，我们推导了多模态模型的少镜头自适应泛化误差界。该定理揭示了多模态基础模型的泛化误差受三个因素的约束：域间隙、模型容量和样本量。基于此，我们从以下几个方面提出了三种可能的解决方案：1）自适应领域泛化，2）自适应模型选择，以及3）自适应知识利用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01736v2" target="_blank">2401.01736v2</a>
                              </td>
                              <td>Few-shot Adaptation of Multi-modal Foundation Models: A Survey</td>
                              <td>Fan Liu</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01736v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01736v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02173v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompt Decoupling for Text-to-Image Person Re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02173v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02173v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02173v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image person re-identification (TIReID) aims to retrieve the target person from an image gallery via a textual description query. Recently, pre-trained vision-language models like CLIP have attracted significant attention and have been widely utilized for this task due to their robust capacity for semantic concept learning and rich multi-modal knowledge. However, recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the entire network to adapt the CLIP model for the TIReID task. Although these methods show competitive performance on this topic, they are suboptimal as they necessitate simultaneous domain adaptation and task adaptation. To address this issue, we attempt to decouple these two processes during the training stage. Specifically, we introduce the prompt tuning strategy to enable domain adaptation and propose a two-stage training approach to disentangle domain adaptation from task adaptation. In the first stage, we freeze the two encoders from CLIP and solely focus on optimizing the prompts to alleviate domain gap between the original training data of CLIP and downstream tasks. In the second stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize capturing fine-grained information, which is more suitable for TIReID task. Finally, we evaluate the effectiveness of our method on three widely used datasets. Compared to the directly fine-tuned approach, our method achieves significant improvements.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02173v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像人物重新识别（TIReID）旨在通过文本描述查询从图像库中检索目标人物。最近，像CLIP这样的预训练视觉语言模型引起了人们的极大关注，并因其强大的语义概念学习能力和丰富的多模态知识而被广泛用于这项任务。然而，最近基于CLIP的TIReID方法通常依赖于整个网络的直接微调，以使CLIP模型适应TIReID任务。尽管这些方法在这个主题上表现出了竞争力，但它们是次优的，因为它们需要同时进行领域自适应和任务自适应。为了解决这个问题，我们试图在训练阶段将这两个过程脱钩。具体来说，我们引入了即时调整策略来实现领域自适应，并提出了一种两阶段训练方法来区分领域自适应和任务自适应。在第一阶段，我们冻结了CLIP中的两个编码器，并仅专注于优化提示，以缓解CLIP的原始训练数据与下游任务之间的域差距。在第二阶段，我们维护固定的提示，并微调CLIP模型，以优先捕获细粒度信息，这更适合TIReID任务。最后，我们在三个广泛使用的数据集上评估了我们的方法的有效性。与直接微调的方法相比，我们的方法取得了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02173v1" target="_blank">2401.02173v1</a>
                              </td>
                              <td>Prompt Decoupling for Text-to-Image Person Re-identification</td>
                              <td>Weihao Li</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02173v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02173v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09024v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09024v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09024v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09024v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09024v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的深层视觉语言模型的一个关键好处是它们能够实现零样本开放式词汇分类；用户有能力在推理时通过自然语言提示来定义新颖的类标签。然而，尽管基于CLIP的零样本分类器在一系列领域转移中表现出了具有竞争力的性能，但它们仍然非常容易受到对抗性攻击。因此，确保此类模型的稳健性对于其在野外的可靠部署至关重要。在这项工作中，我们介绍了开放词汇认证（OVC），这是一种通过随机平滑技术为CLIP等开放词汇模型设计的快速认证方法。给定提示的基本“训练”集及其相应的认证CLIP分类器，OVC依赖于这样的观察，即具有新提示的分类器可以被视为基本训练集中附近分类器的扰动版本。因此，OVC可以使用增量随机平滑的变体来快速验证新的分类器。通过使用缓存技巧，我们在新提示的认证过程中实现了大约两个数量级的加速。为了实现进一步的（启发式）加速，OVC使用多元正态分布来近似给定输入处的嵌入空间，绕过了通过视觉主干的前向传递进行采样的需要。我们在CIFAR-10和ImageNet测试数据集上使用多个视觉语言主干进行实验评估，证明了OVC的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09024v2" target="_blank">2311.09024v2</a>
                              </td>
                              <td>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</td>
                              <td>A K Nirala</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09024v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09024v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02137v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02137v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02137v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02137v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal alignment between language and vision is the fundamental topic in current vision-language model research. Contrastive Captioners (CoCa), as a representative method, integrates Contrastive Language-Image Pretraining (CLIP) and Image Caption (IC) into a unified framework, resulting in impressive results. CLIP imposes a bidirectional constraints on global representation of entire images and sentences. Although IC conducts an unidirectional image-to-text generation on local representation, it lacks any constraint on local text-to-image reconstruction, which limits the ability to understand images at a fine-grained level when aligned with texts. To achieve multimodal alignment from both global and local perspectives, this paper proposes Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional interactions on images and texts across the global and local representation levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM) head based on ITC and IC heads. The improved SyCoCa can further leverage textual cues to reconstruct contextual images and visual cues to predict textual contents. When implementing bidirectional local interactions, the local contents of images tend to be cluttered or unrelated to their textual descriptions. Thus, we employ an attentive masking strategy to select effective image patches for interaction. Extensive experiments on five vision-language tasks, including image-text retrieval, image-captioning, visual question answering, and zero-shot/finetuned image classification, validate the effectiveness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02137v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言和视觉之间的多模式对齐是当前视觉语言模型研究的基本课题。对比字幕（CoCa）作为一种具有代表性的方法，将对比语言图像预训练（CLIP）和图像字幕（IC）整合到一个统一的框架中，取得了令人印象深刻的效果。CLIP对整个图像和句子的全局表示施加了双向约束。尽管IC在局部表示上进行单向的图像到文本生成，但它对局部文本到图像重建缺乏任何约束，这限制了在与文本对齐时以细粒度水平理解图像的能力。为了从全局和局部两个角度实现多模式对齐，本文提出了对称对比字幕（SyCoCa），它引入了图像和文本在全局和局部表示水平上的双向交互。具体来说，我们在ITC和IC头的基础上扩展了文本引导掩模图像建模（TG-MIM）头。改进的SyCoCa可以进一步利用文本线索来重建上下文图像和视觉线索来预测文本内容。当实现双向局部交互时，图像的局部内容往往是杂乱的，或者与它们的文本描述无关。因此，我们采用了一种专注的掩蔽策略来选择有效的图像补丁进行交互。在五个视觉语言任务上的大量实验，包括图像文本检索、图像自适应、视觉问答和零样本/微调图像分类，验证了我们提出的方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02137v1" target="_blank">2401.02137v1</a>
                              </td>
                              <td>SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment</td>
                              <td>Ziping Ma</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02137v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02460v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02460v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02460v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02460v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale, aligned image and text datasets in specific domains. In this work, we leverage two complementary sources of information -- descriptions of categories generated by large language models (LLMs) and abundant, fine-grained image classification datasets -- to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side, we develop methods to train VLMs with this "bag-level" image-text supervision. We find that simply using these attributes at test-time does not improve performance, but our training strategy, for example, on the iNaturalist dataset, leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds and flowers. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways, we generate descriptions that capture visual appearance, habitat, and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We plan to release the benchmark, consisting of 7 datasets, which will contribute to future research in zero-shot recognition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02460v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有视觉语言模型（VLM）（如CLIP）的零样本性能受到特定领域中大规模对齐图像和文本数据集可用性的限制。在这项工作中，我们利用两个互补的信息来源——大型语言模型（LLM）生成的类别描述和丰富的细粒度图像分类数据集——来提高VLM在细粒度领域的零样本分类性能。在技术方面，我们开发了使用这种“袋级”图像文本监督来训练VLM的方法。我们发现，在测试时简单地使用这些属性并不能提高性能，但我们的训练策略，例如，在iNaturalist数据集上，导致新类别的鸟类和花卉的零样本分类准确率平均提高4-5%。在使用类别的子集来微调模型的领域中也观察到了类似的改进。通过以各种方式提示LLM，我们生成捕捉视觉外观、栖息地和地理区域的描述，并将其与现有属性（如类别的分类结构）配对。我们系统地评估了它们在自然域中改进零样本分类的能力。我们的研究结果表明，地理先验同样有效，并且与视觉外观互补。我们的方法也优于先前在VLM的基于提示的调整方面的工作。我们计划发布由7个数据集组成的基准，这将有助于零样本识别的未来研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02460v1" target="_blank">2401.02460v1</a>
                              </td>
                              <td>Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions</td>
                              <td>Oindrila Saha</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02460v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02460v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_13505v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_13505v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_13505v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_13505v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-Language Pre-training has demonstrated its remarkable zero-shot recognition ability and potential to learn generalizable visual representations from language supervision. Taking a step ahead, language-supervised semantic segmentation enables spatial localization of textual inputs by learning pixel grouping solely from image-text pairs. Nevertheless, the state-of-the-art suffers from clear semantic gaps between visual and textual modality: plenty of visual concepts appeared in images are missing in their paired captions. Such semantic misalignment circulates in pre-training, leading to inferior zero-shot performance in dense predictions due to insufficient visual concepts captured in textual representations. To close such semantic gap, we propose Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, we establish a concept archive that maintains potential visually-matched concepts with our proposed vision-driven expansion and text-to-vision-guided ranking. Relevant concepts can thus be identified via cluster-guided sampling and fed into pre-training, thereby bridging the gap between visual and textual semantics. Extensive experiments over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb zero-shot transfer performance and greatly boosts language-supervised segmentation baseline by a large margin, suggesting the value of bridging semantic gap in pre-training data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_13505v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言预训练已经证明了其显著的零样本识别能力和从语言监督中学习通用视觉表示的潜力。更进一步，语言监督语义分割通过仅从图像-文本对中学习像素分组，实现了文本输入的空间定位。然而，最先进的技术在视觉和文本模态之间存在明显的语义差距：图像中出现的大量视觉概念在其配对字幕中缺失。这种语义错位在预训练中循环，由于文本表示中捕获的视觉概念不足，导致密集预测中的零样本性能较差。为了缩小这种语义差距，我们提出了概念库（CoCu），这是一种利用CLIP来补偿缺失语义的管道。对于每个图像-文本对，我们建立了一个概念档案，通过我们提出的视觉驱动的扩展和文本到视觉引导的排名来维护潜在的视觉匹配概念。因此，可以通过聚类引导的采样来识别相关概念，并将其输入到预训练中，从而弥合视觉语义和文本语义之间的差距。在一套广泛的8个分割基准上进行的大量实验表明，CoCu实现了卓越的零样本传递性能，并在很大程度上大大提高了语言监督的分割基线，这表明了在预训练数据中弥合语义差距的价值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.13505v4" target="_blank">2309.13505v4</a>
                              </td>
                              <td>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</td>
                              <td>Yun Xing</td>
                              <td>2023-09-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_13505v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.13505v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xing0047/rewrite" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06942v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06942v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06942v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06942v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06942v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了InternVid，这是一个以视频为中心的大规模多模式数据集，能够学习强大且可转移的视频文本表示，用于多模式理解和生成。InternVid数据集包含700多万个视频，持续时间近760K小时，产生2.34亿个视频片段，并附有总计410万个单词的详细描述。我们的核心贡献是开发一种可扩展的方法，用大型语言模型（LLM）自主构建高质量的视频文本数据集，从而展示其在大规模学习视频语言表示方面的功效。具体来说，我们使用多尺度方法来生成视频相关描述。此外，我们还介绍了基于ViT-L的视频文本表示学习模型ViCLIP。该模型通过对比学习在InternVid上学习，展示了领先的零样本动作识别和有竞争力的视频检索性能。除了识别和检索等基本的视频理解任务外，我们的数据集和模型还有广泛的应用。它们特别有利于生成交错的视频文本数据，用于学习以视频为中心的对话系统，推进视频到文本和文本到视频生成研究。这些拟议资源为对多模式视频理解和生成感兴趣的研究人员和从业者提供了一个工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06942v2" target="_blank">2307.06942v2</a>
                              </td>
                              <td>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</td>
                              <td>Yi Wang</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06942v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06942v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/opengvlab/internvideo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2305_14093v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练健壮和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识有帮助，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v4" target="_blank">2305.14093v4</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kunhao-liu/3d-ovs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02957v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02957v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We delve into a nuanced but significant challenge inherent to Vision Transformers (ViTs): feature maps of these models exhibit grid-like artifacts, which detrimentally hurt the performance of ViTs in downstream tasks. Our investigations trace this fundamental issue down to the positional embeddings at the input stage. To address this, we propose a novel noise model, which is universally applicable to all ViTs. Specifically, the noise model dissects ViT outputs into three components: a semantics term free from noise artifacts and two artifact-related terms that are conditioned on pixel locations. Such a decomposition is achieved by enforcing cross-view feature consistency with neural fields in a per-image basis. This per-image optimization process extracts artifact-free features from raw ViT outputs, providing clean features for offline applications. Expanding the scope of our solution to support online functionality, we introduce a learnable denoiser to predict artifact-free features directly from unprocessed ViT outputs, which shows remarkable generalization capabilities to novel data without the need for per-image optimization. Our two-stage approach, termed Denoising Vision Transformers (DVT), does not require re-training existing pre-trained ViTs and is immediately applicable to any Transformer-based architecture. We evaluate our method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP, DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT consistently and significantly improves existing state-of-the-art general-purpose models in semantic and geometric tasks across multiple datasets (e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT design, especially regarding the naive use of positional embeddings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02957v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02957v1" target="_blank">2401.02957v1</a>
                              </td>
                              <td>Denoising Vision Transformers</td>
                              <td>Jiawei Yang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02957v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02361v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02361v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release all our models to the research community. Codes and trained models are released at https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02361v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding DINO是一种最先进的开放集检测模型，可处理多种视觉任务，包括开放词汇检测（OVD）、短语基础（PG）和参考表达理解（REC）。它的有效性导致它被广泛采用为各种下游应用程序的主流架构。然而，尽管其意义重大，但由于其培训代码的不可用，最初的Grounding DINO模型缺乏全面的公共技术细节。为了弥补这一差距，我们推出了MM Grounding DINO，这是一个开源、全面、用户友好的基线，它是用MMDetection工具箱构建的。它采用丰富的视觉数据集进行预训练，并采用各种检测和基础数据集进行微调。我们对每一个报告的结果进行了全面的分析，并对复制进行了详细的设置。对上述基准的广泛实验表明，我们的MM Grounding DINO Tiny优于Grounding DINO Tiny基线。我们向研究界发布所有模型。代码和经过训练的模型发布于https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02361v2" target="_blank">2401.02361v2</a>
                              </td>
                              <td>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</td>
                              <td>Xiangyu Zhao</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02361v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02361v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/open-mmlab/mmdetection" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12735v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12735v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with negligible performance loss, demonstrating the potential to be a powerful backbone for downstream vision tasks. The code is available at: github.com/sunsmarterjie/iTPN.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12735v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了整体预训练的变换金字塔网络（iTPN），以联合优化网络主干和瓶颈，从而使表示模型和下游任务之间的传输间隙最小。iTPN诞生于两个精心设计：1）第一个预先训练的视觉转换器上的特征金字塔（ViT）。2） 使用掩蔽特征建模（MFM）对特征金字塔进行多阶段监督。iTPN更新为Fast iTPN，通过两种灵活的设计减少了计算内存开销并加速了推理。1） 令牌迁移：丢弃主干的冗余令牌，同时在功能金字塔中补充它们，而无需注意操作。2） 代币采集：通过引入少量采集代币，降低全球关注带来的计算成本。基本/大级别Fast iTPN在ImageNet-1K上实现了88.75%/89.5%的前1级精度。在使用DINO的1x训练计划的情况下，基本/大级别Fast iTPN在COCO对象检测上实现了58.4%/58.8%的box AP，在使用MaskDINO的ADE20K语义分割上实现了57.5%/58.7%的mIoU。快速iTPN可以将推理过程加速70%，而性能损失可以忽略不计，这表明它有潜力成为下游视觉任务的强大支柱。该代码位于：github.com/sunsmarterjie/iTPN。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12735v2" target="_blank">2211.12735v2</a>
                              </td>
                              <td>Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</td>
                              <td>Yunjie Tian</td>
                              <td>2022-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12735v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunsmarterjie/itpn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01013v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01013v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired by InfoNCE, we introduce a novel contrastive loss function that facilitates smoother training and better convergence, thereby enhancing performance in artifact classification. In summary, this study establishes the efficacy of SSL in leveraging unlabeled data, particularly in enhancing the capabilities of the Transformer model. This approach holds promise for broader applications in PICU environments, where annotated data is often limited.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01013v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CHU Sainte Justine儿科重症监护室（PICU）最近的研究表明，传统的机器学习方法，如半监督标签传播和K近邻，在PPG信号的伪影检测方面，主要是在数据有限的情况下，优于基于Transformer的模型。这项研究通过使用自监督学习（SSL）从这些数据中提取潜在特征，然后对标记数据进行微调，解决了大量未标记数据利用不足的问题。我们的实验表明，SSL显著增强了Transformer模型学习表示的能力，提高了其在工件分类任务中的稳健性。在各种SSL技术中，包括掩蔽、对比学习和DINO（无标签的自蒸馏），对比学习在小型PPG数据集中表现出最稳定和优越的性能。此外，我们深入研究了优化对比损失函数，这对对比SSL至关重要。受InfoNCE的启发，我们引入了一种新的对比损失函数，该函数有助于更平滑的训练和更好的收敛，从而提高伪像分类的性能。总之，本研究确定了SSL在利用未标记数据方面的有效性，特别是在增强Transformer模型的能力方面。这种方法有望在PICU环境中获得更广泛的应用，在PICU中，注释数据通常是有限的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01013v1" target="_blank">2401.01013v1</a>
                              </td>
                              <td>Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</td>
                              <td>Thanh-Dung Le</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01013v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00463v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Local Representations of Self-supervised Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00463v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design an evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval, and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Furthermore, we demonstrate that removing these high-variance features enhances k-NN by providing an analysis of the benchmarks for this work and for Scale-MAE, a recent extension of masked autoencoders. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute-intensive counterpart DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00463v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对各种自监督视觉变压器（ViT）进行了比较分析，重点分析了它们的局部代表性。受大型语言模型的启发，我们研究了ViT在几乎没有微调的情况下执行各种计算机视觉任务的能力。我们设计了一个评估框架来分析局部（即补丁级别）表示在少镜头语义分割、实例识别、对象检索和跟踪背景下的质量。我们发现，与掩蔽图像建模相比，基于对比学习的方法（如DINO）产生了更通用的补丁表示，可以立即应用于下游任务，而无需参数调整。使用后一种方法学习的嵌入，例如在掩码自动编码器中，具有高方差特征，这会损害基于距离的算法，例如k-NN，并且不包含用于大多数下游任务的有用信息。此外，我们证明，通过为这项工作和Scale MAE（掩蔽自动编码器的最近扩展）提供基准分析，去除这些高方差特征可以增强k-NN。最后，我们找到了一个对象实例检索设置，其中DINOv2，一个在两个数量级以上的数据上预训练的模型，其性能比计算密集度较低的对应DINO差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00463v1" target="_blank">2401.00463v1</a>
                              </td>
                              <td>Analyzing Local Representations of Self-supervised Vision Transformers</td>
                              <td>Ani Vanyan</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00463v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v4" target="_blank">2310.03940v4</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18628v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18628v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised semantic segmentation aims to categorize each pixel in an image into a corresponding class without the use of annotated data. It is a widely researched area as obtaining labeled datasets is expensive. While previous works in the field have demonstrated a gradual improvement in model accuracy, most required neural network training. This made segmentation equally expensive, especially when dealing with large-scale datasets. We thus propose a lightweight clustering framework for unsupervised semantic segmentation. We discovered that attention features of the self-supervised Vision Transformer exhibit strong foreground-background differentiability. Therefore, clustering can be employed to effectively separate foreground and background image patches. In our framework, we first perform multilevel clustering across the Dataset-level, Category-level, and Image-level, and maintain consistency throughout. Then, the binary patch-level pseudo-masks extracted are upsampled, refined and finally labeled. Furthermore, we provide a comprehensive analysis of the self-supervised Vision Transformer features and a detailed comparison between DINO and DINOv2 to justify our claims. Our framework demonstrates great promise in unsupervised semantic segmentation and achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18628v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督语义分割旨在将图像中的每个像素分类到相应的类别中，而不使用注释数据。这是一个广泛研究的领域，因为获得标记的数据集是昂贵的。虽然该领域先前的工作已经证明模型精度逐渐提高，但大多数都需要神经网络训练。这使得分割同样昂贵，尤其是在处理大规模数据集时。因此，我们提出了一种用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉转换器的注意力特征表现出很强的前景-背景可微性。因此，可以采用聚类来有效地分离前景和背景图像块。在我们的框架中，我们首先在数据集级别、类别级别和图像级别执行多级聚类，并始终保持一致性。然后，对提取的二进制补丁级伪掩码进行上采样、细化和最终标记。此外，我们对自监督视觉转换器的功能进行了全面分析，并对DINO和DINOv2进行了详细比较，以证明我们的说法是正确的。我们的框架在无监督语义分割方面表现出了巨大的前景，并在PASCAL VOC和MS COCO数据集上取得了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18628v2" target="_blank">2311.18628v2</a>
                              </td>
                              <td>A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</td>
                              <td>Yau Shing Jonathan Cheung</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18628v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18628v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17742v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Vision from Models Rivals Learning Vision from Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17742v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data. We synthesize a large dataset of image captions using LLMs, then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning, treating images sharing the same caption as positive pairs. The resulting representations transfer well to many downstream tasks, competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR outperforms previous self-supervised methods by a significant margin, e.g., improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17742v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SynCLR，这是一种专门从合成图像和合成字幕中学习视觉表示的新方法，无需任何真实数据。我们使用LLM合成了一个大型的图像字幕数据集，然后使用现成的文本到图像模型来生成与每个合成字幕相对应的多个图像。我们通过对比学习对这些合成图像进行视觉表征学习，将共享同一字幕的图像视为正对。由此产生的表示很好地转移到许多下游任务，在图像分类任务中与其他通用视觉表示学习器（如CLIP和DINO v2）竞争。此外，在语义分割等密集预测任务中，SynCLR显著优于以前的自监督方法，例如，在ViT-B/16的ADE20k上比MAE和iBOT提高了6.2和4.3mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17742v1" target="_blank">2312.17742v1</a>
                              </td>
                              <td>Learning Vision from Models Rivals Learning Vision from Data</td>
                              <td>Yonglong Tian</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17742v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17742v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/syn-rep-learn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02366v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02366v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02366v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据注释的资源密集型过程以及这些系统无法推广到不同的数据分布，阻碍了深度学习系统与医疗保健的集成。基础模型是在大型数据集上预先训练的模型，已成为减少对注释数据的依赖并增强模型可推广性和稳健性的解决方案。DINOv2是一个开源的基础模型，通过对1.42亿张策划的自然图像进行自我监督学习进行预训练，在各种视觉任务中表现出有希望的能力。然而，关于DINOv2对放射学成像的适应性，以及其特征是否足够通用以有利于放射学图像分析，一个关键问题仍未得到解答。因此，本研究全面评估了DINOv2的放射学，在不同的模式（X射线、CT和MRI）下进行了100多项实验。为了衡量DINOv2特征表示的有效性和可推广性，我们分析了医学图像分析任务中的模型，包括2D和3D图像上的疾病分类和器官分割，以及在不同的设置下，如kNN、少镜头学习、线性探测、端到端微调和参数有效微调。与已建立的监督、自监督和弱监督模型的比较分析揭示了DINOv2的优越性能和跨任务可推广性。这些发现有助于深入了解优化医学成像预训练策略的潜在途径，并增进对DINOv2在弥合自然图像分析和放射学图像分析之间差距方面的作用的更广泛理解。我们的代码可在https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02366v3" target="_blank">2312.02366v3</a>
                              </td>
                              <td>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</td>
                              <td>Mohammed Baharoon</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02366v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02366v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mohammedsb/dinov2forradiology" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Visual Reinforcement Learning with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够推广到看不见的环境中的学习策略是视觉强化学习（RL）中的一个基本挑战。虽然目前的大多数方法都侧重于通过辅助监督、预训练或数据增强来获取稳健的视觉表示，但现代视觉基础模型的潜力仍然不足。在这项工作中，我们介绍了可泛化视觉RL的分段任意模型（SAM-G），这是一个新的框架，利用分段任意模型的可提示分割能力来增强视觉RL代理的泛化能力。我们利用DINOv2和SAM的图像特征来寻找与SAM的点提示对应关系，然后SAM直接为代理生成高质量的掩码图像。在8个DMControl任务和3个Adroit任务中进行评估后，SAM-G显著提高了视觉泛化能力，而不会改变RL代理的架构，而只是改变他们的观察结果。值得注意的是，与最先进的方法相比，SAM-G在DMControl和Adroit上具有挑战性的视频硬设置上分别实现了44%和29%的相对改进。视频和代码：https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17116v1" target="_blank">2312.17116v1</a>
                              </td>
                              <td>Generalizable Visual Reinforcement Learning with Segment Anything Model</td>
                              <td>Ziyu Wang</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wadiuvatzy/sam-g" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16084v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LangSplat: 3D Language Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16084v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16084v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类生活在3D世界中，通常使用自然语言与3D场景进行交互。最近，对3D语言字段进行建模以支持3D中的开放式语言查询越来越受到关注。本文介绍了LangSplat，它构建了一个三维语言字段，可以在三维空间中进行精确高效的开放式词汇查询。与现有的将CLIP语言嵌入NeRF模型的方法不同，LangSplat通过利用3D高斯集合来表示语言领域，从而推进了这一领域的发展，每个高斯集合都对从CLIP中提取的语言特征进行编码。通过使用基于瓦片的飞溅技术来渲染语言特征，我们避免了NeRF中固有的昂贵的渲染过程。LangSplat不是直接学习CLIP嵌入，而是首先训练场景式语言自动编码器，然后在特定场景的潜在空间上学习语言特征，从而减轻显式建模带来的大量内存需求。现有的方法难以处理不精确和模糊的3D语言字段，这些字段无法辨别对象之间的清晰边界。我们深入研究了这个问题，并建议使用SAM学习分层语义，从而消除了在各种规模上广泛查询语言字段和DINO特征正则化的需要。在开放词汇三维对象定位和语义分割方面的大量实验表明，LangSplat显著优于先前最先进的方法LERF。值得注意的是，LangSplat非常高效，与分辨率为1440$\times$1080的LERF相比，它实现了｛\speed｝$\times$的加速。我们强烈建议读者在上查看我们的视频结果https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16084v1" target="_blank">2312.16084v1</a>
                              </td>
                              <td>LangSplat: 3D Language Gaussian Splatting</td>
                              <td>Minghan Qin</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16084v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16084v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/minghanqin/LangSplat" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06709v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06709v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework.   Code: https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06709v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近出现了一些视觉基础模型（VFM）作为许多下游任务的骨干。像CLIP、DINOv2、SAM这样的VFM都是以不同的目标进行训练的，在各种下游任务中表现出独特的特征。我们发现，尽管这些模型在概念上存在差异，但通过多教师提炼，它们可以有效地合并为一个统一的模型。我们将这种方法命名为AM-RADIO（聚集模型——将所有域归一）。这种综合方法不仅超越了个别教师模型的性能，而且融合了它们的独特特征，如零样本视觉语言理解、详细的像素级理解和开放的词汇分割能力。为了追求硬件效率最高的主干，我们使用相同的培训方法评估了多教师蒸馏管道中的许多架构。这导致了一种新型架构（E-RADIO）的开发，其性能超过了其前身，并且至少比教师模型快7倍。我们的全面基准测试过程涵盖下游任务，包括ImageNet分类、ADE20k语义分割、COCO对象检测和LLaVa-1.5框架。代码：https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06709v3" target="_blank">2312.06709v3</a>
                              </td>
                              <td>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</td>
                              <td>Mike Ranzinger</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06709v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06709v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nvlabs/radio" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16211v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16211v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT with a causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16211v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果网络被广泛应用于许多领域，包括流行病学、社会科学、医学和工程，以对变量之间的复杂关系进行建模。虽然直接从观测数据中用算法推断这些模型很方便，但由此产生的网络往往存在错误边缘。审核和纠正这些网络可能需要分析员经常无法获得的领域专业知识。我们建议使用大型语言模型（如ChatGPT）作为因果网络的审计员。我们的方法为ChatGPT提供了一个因果网络，一次一个边缘，以产生关于边缘方向性、可能的混杂因素和中介变量的见解。我们要求ChatGPT反思每个因果关系的各个方面，然后我们生成可视化结果，总结这些观点，供人类分析师指导边缘、收集更多数据或测试进一步的假设。我们设想一个系统，其中大型语言模型、自动因果推理以及人类分析师和领域专家作为一个团队携手合作，为任何给定的案例场景推导出整体和全面的因果模型。本文介绍了一个新兴原型获得的第一个结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16211v1" target="_blank">2312.16211v1</a>
                              </td>
                              <td>An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</td>
                              <td>Yanming Zhang</td>
                              <td>2023-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16211v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16211v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14810v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14810v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider optimal experimental design (OED) for nonlinear Bayesian inverse problems governed by large-scale partial differential equations (PDEs). For the optimality criteria of Bayesian OED, we consider both expected information gain and summary statistics including the trace and determinant of the information matrix that involves the evaluation of the parameter-to-observable (PtO) map and its derivatives. However, it is prohibitive to compute and optimize these criteria when the PDEs are very expensive to solve, the parameters to estimate are high-dimensional, and the optimization problem is combinatorial, high-dimensional, and non-convex. To address these challenges, we develop an accurate, scalable, and efficient computational framework to accelerate the solution of Bayesian OED. In particular, the framework is developed based on derivative-informed neural operator (DINO) surrogates with proper dimension reduction techniques and a modified swapping greedy algorithm. We demonstrate the high accuracy of the DINO surrogates in the computation of the PtO map and the optimality criteria compared to high-fidelity finite element approximations. We also show that the proposed method is scalable with increasing parameter dimensions. Moreover, we demonstrate that it achieves high efficiency with over 1000X speedup compared to a high-fidelity Bayesian OED solution for a three-dimensional PDE example with tens of thousands of parameters, including both online evaluation and offline construction costs of the surrogates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14810v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑由大规模偏微分方程（PDE）控制的非线性贝叶斯反问题的最优实验设计（OED）。对于贝叶斯OED的最优性标准，我们考虑了预期信息增益和汇总统计，包括信息矩阵的迹和行列式，该信息矩阵涉及对参数-可观测（PtO）图及其导数的评估。然而，当偏微分方程的求解非常昂贵，要估计的参数是高维的，并且优化问题是组合的、高维的和非凸的时，计算和优化这些准则是禁止的。为了应对这些挑战，我们开发了一个准确、可扩展和高效的计算框架来加速贝叶斯OED的解决方案。特别是，该框架是基于导数知情神经算子（DINO）代理，采用适当的降维技术和改进的交换贪婪算法开发的。与高保真有限元近似相比，我们证明了DINO替代物在PtO映射计算中的高精度和最优性标准。我们还证明了所提出的方法随着参数维数的增加是可扩展的。此外，我们证明，对于具有数万个参数的三维PDE示例，与高保真度贝叶斯OED解决方案相比，它实现了超过1000倍的加速，包括代理的在线评估和离线构建成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14810v1" target="_blank">2312.14810v1</a>
                              </td>
                              <td>Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</td>
                              <td>Jinwoo Go</td>
                              <td>2023-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14810v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14810v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12359v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-DINOiser: Teaching CLIP a few DINO tricks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12359v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, self-supervised representation methods have demonstrated good localization properties without human-made annotations nor explicit supervision. In this work, we take the best of both worlds and propose a zero-shot open-vocabulary semantic segmentation method, which does not require any annotations. We propose to locally improve dense MaskCLIP features, computed with a simple modification of CLIP's last pooling layer, by integrating localization priors extracted from self-supervised features. By doing so, we greatly improve the performance of MaskCLIP and produce smooth outputs. Moreover, we show that the used self-supervised feature properties can directly be learnt from CLIP features therefore allowing us to obtain the best results with a single pass through CLIP model. Our method CLIP-DINOiser needs only a single forward pass of CLIP and two light convolutional layers at inference, no extra supervision nor extra memory and reaches state-of-the-art results on challenging and fine-grained benchmarks such as COCO, Pascal Context, Cityscapes and ADE20k. The code to reproduce our results is available at https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12359v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其与任意文本提示的无缝交互，流行的CLIP模型显示了令人印象深刻的零样本功能。然而，它缺乏空间意识，不适合于密集的计算机视觉任务，例如语义分割，而不需要额外的微调步骤，该步骤通常使用注释，并可能抑制其原始的开放词汇特性。同时，自监督表示方法在没有人为注释和明确监督的情况下表现出良好的定位特性。在这项工作中，我们两全其美，提出了一种零样本开放词汇语义分割方法，该方法不需要任何注释。我们建议通过集成从自监督特征中提取的定位先验，局部改进密集的MaskCLIP特征，该特征通过对CLIP的最后一个池化层进行简单修改来计算。通过这样做，我们大大提高了MaskCLIP的性能，并产生了平滑的输出。此外，我们证明了所使用的自监督特征属性可以直接从CLIP特征中学习，因此允许我们使用单次通过的CLIP模型获得最佳结果。我们的方法CLIP DINOiser在推理时只需要一次CLIP的前向传递和两个轻卷积层，无需额外的监督和额外的内存，并且在具有挑战性的细粒度基准（如COCO、Pascal Context、Cityscapes和ADE20k）上达到了最先进的结果。重现我们结果的代码可在https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12359v1" target="_blank">2312.12359v1</a>
                              </td>
                              <td>CLIP-DINOiser: Teaching CLIP a few DINO tricks</td>
                              <td>Monika Wysoczańska</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12359v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12359v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wysoczanska/clip_dinoiser" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10912v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10912v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompt techniques. Specifically, IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10912v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，计算机视觉领域发生了范式转变，反映了大型语言模型在自然语言处理领域的变革性影响。本文深入探讨了开放世界分割的探索，提出了一种称为图像提示分割（IPSeg）的新方法，该方法利用了视觉基础模型的力量。IPSeg是无训练范式的原则，它利用了图像提示技术。具体来说，IPSeg利用包含主观视觉概念的单个图像作为灵活的提示来查询视觉基础模型，如DINOv2和Stable Diffusion。我们的方法提取提示图像和输入图像的鲁棒特征，然后通过一个新颖的特征交互模块将输入表示与提示表示进行匹配，以生成突出显示输入图像中目标对象的点提示。生成的点提示进一步用于引导Segment Anything Model对输入图像中的目标对象进行分割。所提出的方法通过消除对详尽培训课程的需求而脱颖而出，从而提供了更高效和可扩展的解决方案。在COCO、PASCAL VOC和其他数据集上的实验证明了IPSeg使用直观的图像提示进行灵活的开放世界分割的有效性。这项工作开创了通过图像中传达的视觉概念来挖掘开放世界理解的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10912v2" target="_blank">2310.10912v2</a>
                              </td>
                              <td>Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</td>
                              <td>Lv Tang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10912v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10912v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11125v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11125v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11125v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类别级物体姿态估计旨在从已知类别中预测物体的6D姿态和3D尺寸，通常难以应对大的类内形状变化。利用平均形状的现有作品往往无法捕捉到这种变化。为了解决这个问题，我们提出了SecondPose，这是一种将DINOv2中特定于对象的几何特征与语义类别先验相结合的新方法。利用DINOv2在提供SE（3）一致语义特征方面的优势，我们分层提取了两种类型的SE（3（3）不变几何特征，以进一步封装局部到全局的特定对象信息。然后，这些几何特征与DINOv2特征点对齐，以在SE（3）变换下建立一致的对象表示，促进从相机空间到预定义规范空间的映射，从而进一步增强姿态估计。在NOCS-REAL275上进行的大量实验表明，SecondPose比最先进的技术进步了12.4%。此外，在更复杂的数据集HouseCat6D上，SecondPose仍然以很大的优势超过了其他竞争对手，该数据集提供了具有光度挑战性的物体。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11125v2" target="_blank">2311.11125v2</a>
                              </td>
                              <td>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</td>
                              <td>Yamei Chen</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11125v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11125v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/NOrangeeroli/SecondPose" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08825v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guided Diffusion from Self-Supervised Diffusion Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08825v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Guidance serves as a key concept in diffusion models, yet its effectiveness is often limited by the need for extra data annotation or classifier pretraining. That is why guidance was harnessed from self-supervised learning backbones, like DINO. However, recent studies have revealed that the feature representation derived from diffusion model itself is discriminative for numerous downstream tasks as well, which prompts us to propose a framework to extract guidance from, and specifically for, diffusion models. Our research has yielded several significant contributions. Firstly, the guidance signals from diffusion models are on par with those from class-conditioned diffusion models. Secondly, feature regularization, when based on the Sinkhorn-Knopp algorithm, can further enhance feature discriminability in comparison to unconditional diffusion models. Thirdly, we have constructed an online training approach that can concurrently derive guidance from diffusion models for diffusion models. Lastly, we have extended the application of diffusion models along the constant velocity path of ODE to achieve a more favorable balance between sampling steps and fidelity. The performance of our methods has been outstanding, outperforming related baseline comparisons in large-resolution datasets, such as ImageNet256, ImageNet256-100 and LSUN-Churches. Our code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08825v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指导是扩散模型中的一个关键概念，但其有效性往往受到额外数据注释或分类器预训练需求的限制。这就是为什么指导是由自我监督的学习骨干，如DINO来利用的。然而，最近的研究表明，从扩散模型本身导出的特征表示对许多下游任务也是有区别的，这促使我们提出一个框架来从扩散模型中提取指导，特别是针对扩散模型。我们的研究取得了一些重大贡献。首先，来自扩散模型的引导信号与来自类条件扩散模型的指导信号是一致的。其次，与无条件扩散模型相比，基于Sinkhorn-Knopp算法的特征正则化可以进一步提高特征的可分辨性。第三，我们构建了一种在线培训方法，可以同时从扩散模型中获得对扩散模型的指导。最后，我们扩展了扩散模型在ODE等速路径上的应用，以在采样步骤和保真度之间实现更有利的平衡。我们的方法的性能非常出色，在大分辨率数据集（如ImageNet256、ImageNet256-100和LSUN Churches）中优于相关的基线比较。我们的代码将会发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08825v1" target="_blank">2312.08825v1</a>
                              </td>
                              <td>Guided Diffusion from Self-Supervised Diffusion Features</td>
                              <td>Vincent Tao Hu</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08825v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08825v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09118v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WildlifeDatasets: An open-source toolkit for animal re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09118v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09118v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了WildlifeDataset(https://github.com/WildlifeDatasets/wildlife-datasets)-主要面向生态学家和计算机视觉/机器学习研究人员的开源工具包。WildlifeDataset是用Python编写的，允许直接访问公开可用的野生动物数据集，并为数据集预处理、性能分析和模型微调提供了多种方法。我们在各种场景和基线实验中展示了该工具包，据我们所知，包括对野生动物重新识别的数据集和方法进行最全面的实验比较，包括局部描述符和深度学习方法。此外，我们提供了第一个用于广泛物种内个体重新识别的基础模型MegaDescriptor，该模型在动物重新识别数据集上提供了最先进的性能，并显著优于其他预先训练的模型，如CLIP和DINOv2。为了向公众提供该模型，并允许与任何现有的野生动物监测应用程序轻松集成，我们通过HuggingFace中心提供多种MegaDescriptor风格（即小型、中型和大型）(https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09118v2" target="_blank">2311.09118v2</a>
                              </td>
                              <td>WildlifeDatasets: An open-source toolkit for animal re-identification</td>
                              <td>Vojtěch Čermák</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09118v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09118v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wildlifedatasets/wildlife-datasets" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03999v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Self-Supervised Representations to Multi-Domain Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03999v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03999v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前最先进的自监督方法在单个领域上训练时是有效的，但在看不见的领域上表现出有限的泛化能力。我们观察到，即使在混合域上训练，这些模型的泛化能力也很差，这使得它们不适合在不同的现实世界设置下部署。因此，我们提出了一种通用的、轻量级的域解纠缠模块（DDM），该模块可以插入任何自监督编码器，以在具有或不具有共享类的多个不同域上有效地执行表示学习。在根据自监督损失进行预训练期间，DDM通过将表示空间拆分为域变体和域不变部分，在表示空间中强制解纠缠。当域标签不可用时，DDM使用稳健的集群方法来发现伪域。我们发现，在包括PACS、DomainNet和WILDS在内的多领域基准测试上，在包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins在内的最先进的自监督模型上，使用DDM的预训练可以显示高达3.5%的线性探测精度提高。与基线相比，用DDM训练的模型对看不见的领域的泛化能力显著提高（7.4%）。因此，DDM可以有效地调整自监督编码器，为不同的多域数据提供高质量、可推广的表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03999v2" target="_blank">2309.03999v2</a>
                              </td>
                              <td>Adapting Self-Supervised Representations to Multi-Domain Setups</td>
                              <td>Neha Kalibhat</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03999v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03999v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v6_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v6_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to 40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v6_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们的习得表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作预训练编码器上的正则化术语，以补救低质量表示。与基线相比，使用Q-Score正则化进行微调可以在ImageNet-100上将SSL模型的线性探测精度提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性密切相关，通过Q分数正则化增强这些特征使SSL表示更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v6" target="_blank">2203.01881v6</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v6_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v6" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07006v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixed Pseudo Labels for Semi-Supervised Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07006v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While the pseudo-label method has demonstrated considerable success in semi-supervised object detection tasks, this paper uncovers notable limitations within this approach. Specifically, the pseudo-label method tends to amplify the inherent strengths of the detector while accentuating its weaknesses, which is manifested in the missed detection of pseudo-labels, particularly for small and tail category objects. To overcome these challenges, this paper proposes Mixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled data, to mitigate the negative impact of missed detections and balance the model's learning across different object scales. Additionally, the model's detection performance on tail categories is improved by resampling labeled data with relevant instances. Notably, MixPL consistently improves the performance of various detectors and obtains new state-of-the-art results with Faster R-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore, MixPL also exhibits good scalability on large models, improving DINO Swin-L by 2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017 benchmark without extra annotations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07006v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然伪标签方法在半监督对象检测任务中取得了相当大的成功，但本文揭示了该方法的显著局限性。具体而言，伪标签方法倾向于放大检测器的固有优势，同时强调其弱点，这表现在伪标签的遗漏检测，特别是对于小型和尾部类别的对象。为了克服这些挑战，本文提出了混合伪标签（MixPL），由伪标签数据的Mixup和Mosaic组成，以减轻遗漏检测的负面影响，并平衡模型在不同对象尺度上的学习。此外，通过对带有相关实例的标记数据进行重新采样，提高了模型在尾部类别上的检测性能。值得注意的是，MixPL持续改进了各种探测器的性能，并在COCO标准和COCO完整基准上使用Faster R-CNN、FCOS和DINO获得了最先进的新结果。此外，MixPL在大型模型上也表现出良好的可扩展性，在没有额外注释的情况下，将DINO Swin-L提高了2.5%的mAP，并在COCO val2017基准上实现了非平凡的新记录（60.2%的mAP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07006v1" target="_blank">2312.07006v1</a>
                              </td>
                              <td>Mixed Pseudo Labels for Semi-Supervised Object Detection</td>
                              <td>Zeming Chen</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07006v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07006v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/czm369/mixpl" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15404v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoMa: Robust Dense Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15404v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature matching is an important computer vision task that involves estimating correspondences between two images of a 3D scene, and dense methods estimate all such correspondences. The aim is to learn a robust model, i.e., a model able to match under challenging real-world changes. In this work, we propose such a model, leveraging frozen pretrained features from the foundation model DINOv2. Although these features are significantly more robust than local features trained from scratch, they are inherently coarse. We therefore combine them with specialized ConvNet fine features, creating a precisely localizable feature pyramid. To further improve robustness, we propose a tailored transformer match decoder that predicts anchor probabilities, which enables it to express multimodality. Finally, we propose an improved loss formulation through regression-by-classification with subsequent robust regression. We conduct a comprehensive set of experiments that show that our method, RoMa, achieves significant gains, setting a new state-of-the-art. In particular, we achieve a 36% improvement on the extremely challenging WxBS benchmark. Code is provided at https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15404v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征匹配是一项重要的计算机视觉任务，涉及估计3D场景的两个图像之间的对应关系，密集方法估计所有这些对应关系。其目的是学习一个稳健的模型，即能够在具有挑战性的现实世界变化下进行匹配的模型。在这项工作中，我们提出了这样一个模型，利用来自基础模型DINOv2的冻结预训练特征。尽管这些特征明显比从头开始训练的局部特征更健壮，但它们本质上是粗糙的。因此，我们将它们与专门的ConvNet精细特征相结合，创建了一个可精确定位的特征金字塔。为了进一步提高鲁棒性，我们提出了一种定制的变换器匹配解码器，该解码器预测锚概率，使其能够表达多模态。最后，我们通过分类回归和随后的稳健回归，提出了一个改进的损失公式。我们进行了一系列全面的实验，结果表明我们的RoMa方法取得了显著的成果，创造了新的最先进水平。特别是，我们在极具挑战性的WxBS基准上实现了36%的改进。代码提供于https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15404v2" target="_blank">2305.15404v2</a>
                              </td>
                              <td>RoMa: Robust Dense Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15404v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/roma" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析主要的MVSSL族。通过这个ER界，我们证明了基于聚类的方法，如DeepCluster和SwAV，最大化了MI。我们还重新解释了基于蒸馏的方法（如BYOL和DINO）的机制，表明它们明确地最大化了重建项，隐含地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v2" target="_blank">2307.10907v2</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/apple/ml-entropy-reconstruction" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05464v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05464v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models can encounter unexpected failures, especially when dealing with challenging sub-populations. One common reason for these failures is the occurrence of objects in backgrounds that are rarely seen during training. To gain a better understanding of these failure modes, human-interpretable descriptions are crucial for further analysis and improvement which is expensive. In this study, we propose an end-to-end framework that utilizes the capabilities of large language models (ChatGPT) and vision-language deep models (CLIP) to generate text descriptions of failure modes associated with spurious correlations (e.g. rarely seen backgrounds) without human-in-the-loop intervention. These descriptions can be used to generate synthetic data using generative models, such as diffusion models. The model can now use this generated data to learn from its weaknesses and enhance its performance on backgrounds that are uncommon for each class of data. Our approach serves as a broad solution, promising progress in comprehending model failure modes and strengthening deep learning models across a wide range of failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot manner. Our experiments have shown remarkable \textbf{improvements in accuracy ($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong background association) across $40$ different models, such as ResNets, EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05464v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型可能会遇到意想不到的失败，尤其是在处理具有挑战性的子群体时。这些失败的一个常见原因是在训练过程中很少看到背景中的物体。为了更好地理解这些故障模式，人类可解释的描述对于进一步的分析和改进至关重要，这是昂贵的。在这项研究中，我们提出了一个端到端的框架，该框架利用大型语言模型（ChatGPT）和视觉语言深度模型（CLIP）的能力，在没有人工干预的情况下，生成与虚假相关性（如罕见背景）相关的故障模式的文本描述。这些描述可以用于使用生成模型（例如扩散模型）生成合成数据。该模型现在可以使用这些生成的数据来学习其弱点，并提高其在每类数据中都不常见的背景下的性能。我们的方法是一个广泛的解决方案，有望在理解模型故障模式和以少量方式自动加强各种故障场景（如百家乐、颜色）的深度学习模型方面取得进展。我们的实验表明，在ImageNet-1000、CIFAR-10和CIFAR-100等各种数据集上，在40美元的不同模型（如ResNets、EfficientNets、DenseNets、Vision Transformer（ViT）、SwAVs、MoCos、DINO和CLIP）中，硬子种群（特别是错误的背景关联）的准确度显著提高（$\sim\textbf｛21%｝$）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05464v1" target="_blank">2312.05464v1</a>
                              </td>
                              <td>Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</td>
                              <td>Atoosa Chegini</td>
                              <td>2023-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05464v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05464v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05189v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Autonomous Organizations as Public Services Supplying Platform</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05189v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA is a public company owned by Municipality of L Aquila, it supplies the institution with network services and software applications for distributing services to citizens. The future policy of the company is to enlarge the offer of its services to nearby communities that are unable to set up and maintain their own network and software structures. This paper presents thus a possible architecture model to support small municipalities in supplying public services to citizens, with the aid of SED Spa. Through second level platforms based on Blockchain networks and Multi-agents Systems running on smart contracts, the system will focus on Waste Tax (Ta.Ri) management system in the Fascicolo del Cittadino environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05189v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA是拉奎拉市拥有的一家上市公司，为该机构提供网络服务和软件应用程序，用于向公民分发服务。该公司未来的政策是扩大向附近社区提供的服务，这些社区无法建立和维护自己的网络和软件结构。因此，本文提出了一种可能的建筑模型，以支持小城市在SED Spa的帮助下向公民提供公共服务。通过基于区块链网络的二级平台和运行在智能合约上的多代理系统，该系统将专注于Cittadino Fascolo环境中的废物税（Ta.Ri）管理系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05189v1" target="_blank">2312.05189v1</a>
                              </td>
                              <td>Distributed Autonomous Organizations as Public Services Supplying Platform</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-12-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05189v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-View Unsupervised Image Generation with Cross Attention Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing interest in novel view synthesis, driven by Neural Radiance Field (NeRF) models, is hindered by scalability issues due to their reliance on precisely annotated multi-view images. Recent models address this by fine-tuning large text2image diffusion models on synthetic multi-view data. Despite robust zero-shot generalization, they may need post-processing and can face quality issues due to the synthetic-real domain gap. This paper introduces a novel pipeline for unsupervised training of a pose-conditioned diffusion model on single-category datasets. With the help of pretrained self-supervised Vision Transformers (DINOv2), we identify object poses by clustering the dataset through comparing visibility and locations of specific object parts. The pose-conditioned diffusion model, trained on pose labels, and equipped with cross-frame attention at inference time ensures cross-view consistency, that is further aided by our novel hard-attention guidance. Our model, MIRAGE, surpasses prior work in novel view synthesis on real images. Furthermore, MIRAGE is robust to diverse textures and geometries, as demonstrated with our experiments on synthetic images generated with pretrained Stable Diffusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRF）模型的驱动下，人们对新视图合成越来越感兴趣，但由于它们依赖于精确注释的多视图图像，因此受到可扩展性问题的阻碍。最近的模型通过在合成多视图数据上微调大文本2图像扩散模型来解决这一问题。尽管有强大的零样本泛化，但它们可能需要后处理，并可能由于合成域间隙而面临质量问题。本文介绍了一种新的流水线，用于在单类别数据集上对姿态条件扩散模型进行无监督训练。在预训练的自监督视觉变换器（DINOv2）的帮助下，我们通过比较特定对象部分的可见性和位置来对数据集进行聚类，从而识别对象姿态。在姿势标签上训练并在推理时配备跨帧注意力的姿势条件扩散模型确保了跨视图的一致性，这进一步得益于我们新颖的硬注意力引导。我们的模型，MIRAGE，在真实图像的新颖视图合成方面超越了以往的工作。此外，正如我们在使用预训练的稳定扩散生成的合成图像上的实验所证明的那样，MIRAGE对不同的纹理和几何形状是鲁棒的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04337v1" target="_blank">2312.04337v1</a>
                              </td>
                              <td>Multi-View Unsupervised Image Generation with Cross Attention Guidance</td>
                              <td>Llukman Cerkezi</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别优于DINO和OpenCLIP 19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v2" target="_blank">2306.03881v2</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01677v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-task Image Restoration Guided By Robust DINO Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01677v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01677v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task image restoration has gained significant interest due to its inherent versatility and efficiency compared to its single-task counterpart. Despite its potential, performance degradation is observed with an increase in the number of tasks, primarily attributed to the distinct nature of each restoration task. Addressing this challenge, we introduce \mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approach leveraging robust features extracted from DINOv2. Our empirical analysis shows that while shallow features of DINOv2 capture rich low-level image characteristics, the deep features ensure a robust semantic representation insensitive to degradations while preserving high-frequency contour details. Building on these features, we devise specialized components, including multi-layer semantic fusion module, DINO-Restore adaption and fusion module, and DINO perception contrastive loss, to integrate DINOv2 features into the restoration paradigm. Equipped with the aforementioned components, our DINO-IR performs favorably against existing multi-task image restoration approaches in various tasks by a large margin, indicating the superiority and necessity of reinforcing the robust features for multi-task image restoration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01677v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与单任务图像恢复相比，多任务图像恢复由于其固有的多功能性和效率而引起了人们的极大兴趣。尽管有其潜力，但随着任务数量的增加，性能会下降，这主要归因于每个恢复任务的不同性质。为了应对这一挑战，我们引入了\box｛\textbf｛DINO-IR｝｝，这是一种利用从DINOv2中提取的鲁棒特征的新的多任务图像恢复方法。我们的实证分析表明，虽然DINOv2的浅层特征捕捉到了丰富的低层图像特征，但深层特征确保了对退化不敏感的鲁棒语义表示，同时保留了高频轮廓细节。基于这些特征，我们设计了专门的组件，包括多层语义融合模块、DINO恢复适应和融合模块以及DINO感知对比损失，以将DINOv2特征整合到恢复范式中。配备了上述组件，我们的DINO-IR在各种任务中与现有的多任务图像恢复方法相比表现良好，这表明了增强多任务图像修复的鲁棒性特征的优越性和必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01677v2" target="_blank">2312.01677v2</a>
                              </td>
                              <td>Multi-task Image Restoration Guided By Robust DINO Features</td>
                              <td>Xin Lin</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01677v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01677v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00230v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00230v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00230v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00230v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用视觉位置识别（VPR）技术来确定公开可用图像的地理位置是现实世界VPR应用的一个紧迫问题。尽管目前大多数VPR方法在理想条件下都能获得良好的结果，但它们在复杂环境中的性能通常不令人满意，这些环境的特点是光照变化、季节变化和移动物体引起的遮挡。在这项研究中，我们利用DINOv2模型作为骨干网络进行修剪和微调，以提取稳健的图像特征。我们提出了一种新的VPR架构，称为DINO Mix，它将基础视觉模型与特征聚合相结合。该架构依赖于基础视觉模型强大的图像特征提取能力。我们使用基于MLP Mixer的混合模块来聚合图像特征，从而产生全局鲁棒和可推广的描述符，从而实现高精度的VPR。我们通过实验证明，所提出的DINO-Mix架构显著优于当前最先进的（SOTA）方法。在具有光照变化、季节变化和遮挡的测试集（Tokyo24/7，Nordland，SF-XL-Testv1）中，我们提出的DINO Mix架构分别实现了91.75%、80.18%和82%的Top-1准确率。与SOTA方法相比，我们的体系结构的平均准确度提高了5.14%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00230v2" target="_blank">2311.00230v2</a>
                              </td>
                              <td>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</td>
                              <td>Gaoshuang Huang</td>
                              <td>2023-11-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00230v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00230v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/GaoShuang98/DINO-Mix" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01576v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01576v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01576v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing Building Damage Detection (BDD) methods always require labour-intensive pixel-level annotations of buildings and their conditions, hence largely limiting their applications. In this paper, we investigate a challenging yet practical scenario of BDD, Unsupervised Building Damage Detection (U-BDD), where only unlabelled pre- and post-disaster satellite image pairs are provided. As a pilot study, we have first proposed an advanced U-BDD baseline that leverages pre-trained vision-language foundation models (i.e., Grounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent domain gap between satellite and generic images causes low confidence in the foundation models used to identify buildings and their damages. In response, we further present a novel self-supervised framework, U-BDD++, which improves upon the U-BDD baseline by addressing domain-specific issues associated with satellite imagery. Furthermore, the new Building Proposal Generation (BPG) module and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module in U-BDD++ ensure high-quality self-training. Extensive experiments on the widely used building damage assessment benchmark demonstrate the effectiveness of the proposed method for unsupervised building damage detection. The presented annotation-free and foundation model-based paradigm ensures an efficient learning phase. This study opens a new direction for real-world BDD and sets a strong baseline for future research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01576v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的建筑物损坏检测（BDD）方法总是需要对建筑物及其条件进行劳动密集型像素级注释，因此在很大程度上限制了其应用。在本文中，我们研究了BDD的一个具有挑战性但实用的场景，即无监督的建筑物损坏检测（U-BDD），其中只提供未标记的灾前和灾后卫星图像对。作为一项试点研究，我们首先提出了一种先进的U-BDD基线，该基线利用预先训练的视觉语言基础模型（即基础DINO、SAM和CLIP）来解决U-BDD任务。然而，卫星图像和普通图像之间明显的领域差距导致用于识别建筑物及其损坏的基础模型的置信度较低。作为回应，我们进一步提出了一个新的自我监督框架U-BDD++，该框架通过解决与卫星图像相关的特定领域问题，改进了U-BDD基线。此外，U-BDD++中新的建筑方案生成（BPG）模块和启用CLIP的嘈杂建筑方案选择（CLIP-BPS）模块确保了高质量的自我培训。在广泛使用的建筑损伤评估基准上进行的大量实验证明了所提出的无监督建筑损伤检测方法的有效性。所提出的无注释和基于基础的范式确保了有效的学习阶段。这项研究为现实世界的BDD开辟了一个新的方向，并为未来的研究奠定了坚实的基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01576v1" target="_blank">2312.01576v1</a>
                              </td>
                              <td>Learning Efficient Unsupervised Satellite Image-based Building Damage Detection</td>
                              <td>Yiyun Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01576v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01576v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fzmi/ubdd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03513v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03513v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03513v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and later fine-tune the model to predict high-resolution land cover maps. We rigorously evaluate the utility of attention maps generated by the ViT backbone and compare them with the model's token embedding space. We observe a small improvement in model performance with pre-training compared to training from scratch and discuss the limitations and opportunities of SSL for remote sensing and land cover segmentation. Beyond small performance increases, we show that ViT attention maps hold great intrinsic value for remote sensing, and could provide useful inputs to other algorithms. With this, our work lays the groundwork for bigger and better SSL models for Earth Observation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03513v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）模型最近在包括图像分割在内的各种任务中表现出了显著的性能。本研究探讨了无标签自蒸馏（DINO）算法的涌现特性及其在合成孔径雷达（SAR）图像中的应用。我们使用未标记的SAR数据预训练基于视觉变换器（ViT）的DINO模型，然后对模型进行微调，以预测高分辨率的土地覆盖图。我们严格评估了ViT主干生成的注意力图的效用，并将其与模型的令牌嵌入空间进行了比较。我们观察到，与从头开始的训练相比，预训练的模型性能略有提高，并讨论了SSL在遥感和土地覆盖分割方面的局限性和机会。除了小幅的性能提高外，我们还表明，ViT注意力图对遥感具有巨大的内在价值，并可以为其他算法提供有用的输入。有了这一点，我们的工作为更大更好的地球观测SSL模型奠定了基础。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03513v2" target="_blank">2310.03513v2</a>
                              </td>
                              <td>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery</td>
                              <td>Joseph A. Gallego-Mejia</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03513v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03513v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02048v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02048v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02048v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02048v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们使用三个地区（中国、美国和欧洲）的两个合成孔径雷达数据集（S1GRD或GSSIC）预训练了一个基于DINO-ViT的模型。我们在较小的标记数据集上微调模型，以预测植被百分比，并实证研究模型的嵌入空间与其在不同地理区域和看不见的数据之间的联系。对于S1GRD，不同区域的嵌入空间明显分离，而GSSIC的嵌入空间重叠。在微调过程中，位置模式仍然存在，嵌入中距离越大，通常会导致不熟悉区域的误差越大。通过这一点，我们的工作增加了我们对应用于遥感的自监督模型的可推广性的理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02048v2" target="_blank">2310.02048v2</a>
                              </td>
                              <td>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</td>
                              <td>Laura Martínez-Ferrer</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02048v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18809v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FoundPose: Unseen Object Pose Estimation with Foundation Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18809v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18809v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose FoundPose, a method for 6D pose estimation of unseen rigid objects from a single RGB image. The method assumes that 3D models of the objects are available but does not require any object-specific training. This is achieved by building upon DINOv2, a recent vision foundation model with impressive generalization capabilities. An online pose estimation stage is supported by a minimal object representation that is built during a short onboarding stage from DINOv2 patch features extracted from rendered object templates. Given a query image with an object segmentation mask, FoundPose first rapidly retrieves a handful of similarly looking templates by a DINOv2-based bag-of-words approach. Pose hypotheses are then generated from 2D-3D correspondences established by matching DINOv2 patch features between the query image and a retrieved template, and finally optimized by featuremetric refinement. The method can handle diverse objects, including challenging ones with symmetries and without any texture, and noticeably outperforms existing RGB methods for coarse pose estimation in both accuracy and speed on the standard BOP benchmark. With the featuremetric and additional MegaPose refinement, which are demonstrated complementary, the method outperforms all RGB competitors. Source code is at: evinpinar.github.io/foundpose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18809v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了FoundPose，这是一种从单个RGB图像中对看不见的刚性物体进行6D姿态估计的方法。该方法假设对象的3D模型是可用的，但不需要任何特定于对象的训练。这是在DINOv2的基础上实现的，DINOv2是一个最近的视觉基础模型，具有令人印象深刻的泛化能力。在线姿态估计阶段由最小对象表示支持，该最小对象表示是在短期入职阶段根据从渲染对象模板中提取的DINOv2补丁特征构建的。给定一个带有对象分割掩码的查询图像，FoundPose首先通过基于DINOv2的单词袋方法快速检索一些外观相似的模板。然后，从通过匹配查询图像和检索到的模板之间的DINOv2补丁特征而建立的2D-3D对应关系中生成姿势假设，并最终通过特征度量细化进行优化。该方法可以处理不同的对象，包括具有对称性且没有任何纹理的具有挑战性的对象，并且在标准BOP基准上，在精度和速度方面明显优于现有的RGB方法进行粗略姿态估计。该方法的特征度量和额外的MegaPose细化被证明是互补的，优于所有RGB竞争对手。源代码位于：evinpinar.github.io/foundpose。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18809v1" target="_blank">2311.18809v1</a>
                              </td>
                              <td>FoundPose: Unseen Object Pose Estimation with Foundation Features</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18809v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18809v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00079v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00079v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00079v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores advancements in high-fidelity personalized image generation through the utilization of pre-trained text-to-image diffusion models. While previous approaches have made significant strides in generating versatile scenes based on text descriptions and a few input images, challenges persist in maintaining the subject fidelity within the generated images. In this work, we introduce an innovative algorithm named HiFi Tuner to enhance the appearance preservation of objects during personalized image generation. Our proposed method employs a parameter-efficient fine-tuning framework, comprising a denoising process and a pivotal inversion process. Key enhancements include the utilization of mask guidance, a novel parameter regularization technique, and the incorporation of step-wise subject representations to elevate the sample fidelity. Additionally, we propose a reference-guided generation approach that leverages the pivotal inversion of a reference image to mitigate unwanted subject variations and artifacts. We further extend our method to a novel image editing task: substituting the subject in an image through textual manipulations. Experimental evaluations conducted on the DreamBooth dataset using the Stable Diffusion model showcase promising results. Fine-tuning solely on textual embeddings improves CLIP-T score by 3.6 points and improves DINO score by 9.6 points over Textual Inversion. When fine-tuning all parameters, HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2 points over DreamBooth, establishing a new state of the art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00079v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文通过利用预先训练的文本到图像扩散模型，探索了高保真个性化图像生成的进展。虽然以前的方法在基于文本描述和一些输入图像生成多功能场景方面取得了重大进展，但在保持生成图像中的主题保真度方面仍然存在挑战。在这项工作中，我们引入了一种名为HiFi Tuner的创新算法，以在个性化图像生成过程中增强对象的外观保护。我们提出的方法采用了一个参数有效的微调框架，包括去噪过程和关键反演过程。关键的增强包括利用掩模引导、一种新的参数正则化技术，以及结合逐步主题表示来提高样本保真度。此外，我们提出了一种参考引导生成方法，该方法利用参考图像的关键反转来减轻不必要的受试者变化和伪影。我们进一步将我们的方法扩展到一个新颖的图像编辑任务：通过文本操作替换图像中的主体。使用稳定扩散模型在DreamBooth数据集上进行的实验评估显示了有希望的结果。与文本反转相比，仅对文本嵌入进行微调可将CLIP-T分数提高3.6分，将DINO分数提高9.6分。当对所有参数进行微调时，HiFi Tuner将CLIP-T分数提高了1.2分，并将DINO分数提高了比DreamBooth高1.2分，建立了新的技术水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00079v1" target="_blank">2312.00079v1</a>
                              </td>
                              <td>HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models</td>
                              <td>Zhonghao Wang</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00079v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00079v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17893v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17893v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17893v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective approach for self-supervised video object segmentation (VOS). Our key insight is that the inherent structural dependencies present in DINO-pretrained Transformers can be leveraged to establish robust spatio-temporal correspondences in videos. Furthermore, simple clustering on this correspondence cue is sufficient to yield competitive segmentation results. Previous self-supervised VOS techniques majorly resort to auxiliary modalities or utilize iterative slot attention to assist in object discovery, which restricts their general applicability and imposes higher computational requirements. To deal with these challenges, we develop a simplified architecture that capitalizes on the emerging objectness from DINO-pretrained Transformers, bypassing the need for additional modalities or slot attention. Specifically, we first introduce a single spatio-temporal Transformer block to process the frame-wise DINO features and establish spatio-temporal dependencies in the form of self-attention. Subsequently, utilizing these attention maps, we implement hierarchical clustering to generate object segmentation masks. To train the spatio-temporal block in a fully self-supervised manner, we employ semantic and dynamic motion consistency coupled with entropy normalization. Our method demonstrates state-of-the-art performance across multiple unsupervised VOS benchmarks and particularly excels in complex real-world multi-object video segmentation tasks such as DAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will be released at https://github.com/shvdiwnkozbw/SSL-UVOS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17893v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种简单而有效的自监督视频对象分割（VOS）方法。我们的关键见解是，DINO预训练的变形金刚中存在的固有结构依赖性可以用来在视频中建立强大的时空对应关系。此外，在该对应线索上的简单聚类足以产生有竞争力的分割结果。以前的自监督VOS技术主要采用辅助模态或利用迭代槽注意力来辅助对象发现，这限制了它们的普遍适用性，并提出了更高的计算要求。为了应对这些挑战，我们开发了一种简化的架构，该架构利用了DINO预训练变压器中出现的对象性，绕过了对额外模式或插槽关注的需求。具体来说，我们首先引入单个时空变换器块来处理逐帧的DINO特征，并以自注意的形式建立时空依赖关系。随后，利用这些注意力图，我们实现了分层聚类来生成对象分割掩码。为了以完全自监督的方式训练时空块，我们使用语义和动态运动一致性以及熵归一化。我们的方法在多个无监督VOS基准测试中展示了最先进的性能，尤其擅长复杂的真实世界多对象视频分割任务，如DAVIS-17-无监督和YouTube-VIS-19。代码和模型检查点将在发布https://github.com/shvdiwnkozbw/SSL-UVOS.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17893v1" target="_blank">2311.17893v1</a>
                              </td>
                              <td>Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation</td>
                              <td>Shuangrui Ding</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17893v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17893v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/shvdiwnkozbw/ssl-uvos" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像的扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上与SOTA表示相似。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果令人惊讶地好，并且在这些融合特征上使用最近邻居的零样本评估在基准数据集（例如，SPair-71k、PF-Pascal和TSS）上提供了比现有技术方法显著的性能增益。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v2" target="_blank">2305.15347v2</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15937v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimal Transport Aggregation for Visual Place Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15937v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15937v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The task of Visual Place Recognition (VPR) aims to match a query image against references from an extensive database of images from different places, relying solely on visual cues. State-of-the-art pipelines focus on the aggregation of features extracted from a deep backbone, in order to form a global descriptor for each image. In this context, we introduce SALAD (Sinkhorn Algorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's soft-assignment of local features to clusters as an optimal transport problem. In SALAD, we consider both feature-to-cluster and cluster-to-feature relations and we also introduce a 'dustbin' cluster, designed to selectively discard features deemed non-informative, enhancing the overall descriptor quality. Additionally, we leverage and fine-tune DINOv2 as a backbone, which provides enhanced description power for the local features, and dramatically reduces the required training time. As a result, our single-stage method not only surpasses single-stage baselines in public VPR datasets, but also surpasses two-stage methods that add a re-ranking with significantly higher cost. Code and models are available at https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15937v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉位置识别（VPR）的任务旨在仅依靠视觉提示，将查询图像与来自不同位置的图像的广泛数据库中的参考进行匹配。最先进的管道专注于从深层主干提取的特征的聚合，以便为每个图像形成全局描述符。在这种情况下，我们引入了SALAD（局部聚合描述符的Sinkhorn算法），它将NetVLAD的局部特征到簇的软分配重新表述为最优传输问题。在SALAD中，我们考虑了特征到聚类和聚类到特征的关系，我们还引入了一个“垃圾箱”聚类，旨在选择性地丢弃被视为非信息性的特征，提高整体描述符质量。此外，我们利用并微调DINOv2作为主干，它为局部特征提供了增强的描述能力，并显著减少了所需的训练时间。因此，我们的单阶段方法不仅超过了公共VPR数据集中的单阶段基线，而且还超过了以显著更高的成本添加重新排序的两阶段方法。代码和型号可在https://github.com/serizba/salad.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15937v1" target="_blank">2311.15937v1</a>
                              </td>
                              <td>Optimal Transport Aggregation for Visual Place Recognition</td>
                              <td>Sergio Izquierdo</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15937v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/serizba/salad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14665v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14665v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14665v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) can be used to solve complex visual tasks without human labels. Self-supervised representations encode useful semantic information about images, and as a result, they have already been used for tasks such as unsupervised semantic segmentation. In this paper, we investigate self-supervised representations for instance segmentation without any manual annotations. We find that the features of different SSL methods vary in their level of instance-awareness. In particular, DINO features, which are known to be excellent semantic descriptors, lack behind MAE features in their sensitivity for separating instances.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14665v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）可以用于解决没有人为标签的复杂视觉任务。自监督表示对图像的有用语义信息进行编码，因此，它们已经被用于无监督语义分割等任务。在本文中，我们研究了在没有任何手动注释的情况下进行实例分割的自监督表示。我们发现，不同SSL方法的特性在实例感知级别上有所不同。特别是，众所周知，DINO特征是优秀的语义描述符，但在分离实例的敏感性方面却落后于MAE特征。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14665v1" target="_blank">2311.14665v1</a>
                              </td>
                              <td>Understanding Self-Supervised Features for Learning Unsupervised Instance Segmentation</td>
                              <td>Paul Engstler</td>
                              <td>2023-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14665v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14665v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13110v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13110v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13110v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13110v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们认为表示学习的一个自然目标是将数据的分布（比如令牌集）压缩和转换为非相干子空间上支持的低维高斯混合。这种表示的优度可以通过一种称为稀疏率降低的原则性度量来评估，该度量同时最大化学习表示的内在信息增益和外在稀疏性。从这个角度来看，包括transformer在内的流行的深度网络架构可以被视为实现了优化这一措施的迭代方案。特别地，我们从该目标部分的交替优化中推导出一个变换器块：多头自注意算子通过对特征的编码率实现近似梯度下降步骤来压缩表示，随后的多层感知器对特征进行稀疏化。这导致了一系列类似白盒变压器的深度网络架构，称为CRATE，在数学上是完全可解释的。我们通过去噪和压缩之间的新连接表明，上述压缩编码的逆编码可以通过同一类CRATE架构来实现。因此，如此导出的白盒架构对于编码器和解码器都是通用的。实验表明，尽管这些网络很简单，但它们确实学会了压缩和稀疏大规模真实世界图像和文本数据集的表示，并实现了非常接近高度工程化的基于转换器的模型的性能：ViT、MAE、DINO、BERT和GPT2。我们认为，从数据压缩的统一角度来看，所提出的计算框架在弥合深度学习理论和实践之间的差距方面显示出巨大的潜力。代码位于：https://ma-lab-berkeley.github.io/CRATE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13110v2" target="_blank">2311.13110v2</a>
                              </td>
                              <td>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?</td>
                              <td>Yaodong Yu</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13110v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13110v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_13717v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_13717v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_13717v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr\'echet Inception Distance is a widely used metric for evaluating synthetic image quality that utilizes an ImageNet-trained InceptionV3 network as a feature extractor. However, its application in medical imaging lacks a standard feature extractor, leading to biased and inconsistent comparisons. This study aimed to compare state-of-the-art feature extractors for computing Fr\'echet Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data augmentation techniques tailored for limited data domains on datasets comprising three medical imaging modalities and four anatomical locations. Human evaluation of generative quality (via a visual Turing test) was compared to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer architectures, in addition to an InceptionV3 network trained on a large medical dataset, RadImageNet. All ImageNet-based extractors were consistent with each other, but only SwAV was significantly correlated with medical expert judgment. The RadImageNet-based FD showed volatility and lacked correlation with human judgment. Caution is advised when using medical image-trained extraction networks in the FD calculation. These networks should be rigorously evaluated on the imaging modality under consideration and publicly released. ImageNet-based extractors, while imperfect, are consistent and widely understood. Training extraction networks with SwAV is a promising approach for synthetic medical image evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_13717v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Fr’echet Inception Distance是一种广泛用于评估合成图像质量的指标，它利用ImageNet训练的InceptionV3网络作为特征提取器。然而，它在医学成像中的应用缺乏标准的特征提取器，导致了有偏差和不一致的比较。本研究旨在比较用于计算医学成像中Fr’chet距离（FD）的最先进的特征提取器。StyleGAN2网络使用针对数据集上的有限数据域量身定制的数据增强技术进行训练，数据集包括三种医学成像模式和四个解剖位置。将人类对生成质量的评估（通过视觉图灵测试）与使用ImageNet训练的InceptionV3、ResNet50、SwAV、DINO和Swin Transformer架构以及在大型医学数据集RadImageNet上训练的InceptV3网络计算的FD进行比较。所有基于ImageNet的提取器彼此一致，但只有SwAV与医学专家的判断显著相关。基于RadImageNet的FD显示出波动性，并且缺乏与人类判断的相关性。当在FD计算中使用医学图像训练的提取网络时，建议谨慎。这些网络应根据正在考虑的成像模式进行严格评估并公开发布。基于ImageNet的提取器虽然不完善，但却是一致的，并得到了广泛的理解。用SwAV训练提取网络是一种很有前途的合成医学图像评估方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.13717v1" target="_blank">2311.13717v1</a>
                              </td>
                              <td>Importance of Feature Extraction in the Calculation of Fréchet Distance for Medical Imaging</td>
                              <td>McKell Woodland</td>
                              <td>2023-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_13717v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.13717v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mckellwoodland/fid-med-eval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12969v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detect Every Thing with Few Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12969v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12969v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12969v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开集对象检测的目的是检测训练中发现的任意类别之外的任意类别。最近的进展采用了开放词汇范式，利用视觉语言主干来用语言表示类别。在本文中，我们介绍了DE ViT，这是一种开放集对象检测器，它使用仅视觉的DINOv2主干，并通过示例图像而不是语言来学习新的类别。为了提高一般检测能力，我们将多分类任务转换为二进制分类任务，同时绕过每类推理，并提出了一种新的区域传播定位技术。我们使用COCO和LVIS在开放词汇、少镜头和单镜头物体检测基准上评估了DE-ViT。对于COCO，DE ViT比开放词汇SoTA高6.9 AP50，并在新类中达到50 AP50。DE ViT在10次发射时以15毫安时的容量超过了少数发射的SoTA，在30次发射时则以7.2毫安时的体积超过了SoTA，而在一次发射时仅以2.8 AP50的容量超过SoTA。对于LVIS，DE ViT比开放词汇表SoTA高2.2掩码AP，达到34.3掩码AP。代码可在https://github.com/mlzxy/devit.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12969v2" target="_blank">2309.12969v2</a>
                              </td>
                              <td>Detect Every Thing with Few Examples</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12969v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12969v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mlzxy/devit" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2006_01236v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2006_01236v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2006_01236v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A classic result in formal language theory is the equivalence among non-counting, or aperiodic, regular languages, and languages defined through star-free regular expressions, or first-order logic. Past attempts to extend this result beyond the realm of regular languages have met with difficulties: for instance it is known that star-free tree languages may violate the non-counting property and there are aperiodic tree languages that cannot be defined through first-order logic. We extend such classic equivalence results to a significant family of deterministic context-free languages, the operator-precedence languages (OPL), which strictly includes the widely investigated visibly pushdown, alias input-driven, family and other structured context-free languages. The OP model originated in the '60s for defining programming languages and is still used by high performance compilers; its rich algebraic properties have been investigated initially in connection with grammar learning and recently completed with further closure properties and with monadic second order logic definition. We introduce an extension of regular expressions, the OP-expressions (OPE) which define the OPLs and, under the star-free hypothesis, define first-order definable and non-counting OPLs. Then, we prove, through a fairly articulated grammar transformation, that aperiodic OPLs are first-order definable. Thus, the classic equivalence of star-freeness, aperiodicity, and first-order definability is established for the large and powerful class of OPLs. We argue that the same approach can be exploited to obtain analogous results for visibly pushdown languages too.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2006_01236v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>形式语言理论中的一个经典结果是不计数或非周期的正则语言与通过无星正则表达式或一阶逻辑定义的语言之间的等价性。过去试图将这一结果扩展到正则语言之外的尝试遇到了困难：例如，已知无星树语言可能违反不计数性质，并且存在无法通过一阶逻辑定义的非周期树语言。我们将这种经典等价结果扩展到一个重要的确定上下文无关语言族，即算子优先语言（OPL），它严格包括广泛研究的可见下推、别名输入驱动、族和其他结构化上下文无关语言。OP模型起源于60年代，用于定义编程语言，目前仍被高性能编译器使用；它丰富的代数性质最初与语法学习有关，最近又与进一步的闭包性质和一元二阶逻辑定义有关。我们引入了正则表达式的一个扩展，即OP表达式（OPE），它定义了OPL，并在无星假设下定义了一阶可定义和不计数的OPL。然后，我们通过一个相当清晰的语法转换证明了非周期OPL是一阶可定义的。因此，对于大而有力的OPL类，建立了星自由度、非周期性和一阶可定义性的经典等价性。我们认为，同样的方法也可以用于获得明显下推语言的类似结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2006.01236v7" target="_blank">2006.01236v7</a>
                              </td>
                              <td>Aperiodicity, Star-freeness, and First-order Logic Definability of Operator Precedence Languages</td>
                              <td>Dino Mandrioli</td>
                              <td>2020-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2006_01236v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2006.01236v7" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11754v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11754v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11754v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automotive related datasets have previously been used for training autonomous driving systems or vehicle classification tasks. However, there is a lack of datasets in the field of automotive AI for car parts detection, and most available datasets are limited in size and scope, struggling to cover diverse scenarios. To address this gap, this paper presents a large-scale and fine-grained automotive dataset consisting of 84,162 images for detecting 12 different types of car parts. This dataset was collected from natural cameras and online websites which covers various car brands, scenarios, and shooting angles. To alleviate the burden of manual annotation, we propose a novel semi-supervised auto-labeling method that leverages state-of-the-art pre-trained detectors. Moreover, we study the limitations of the Grounding DINO approach for zero-shot labeling. Finally, we evaluate the effectiveness of our proposed dataset through fine-grained car parts detection by training several lightweight YOLO-series detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11754v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>汽车相关数据集先前已用于训练自动驾驶系统或车辆分类任务。然而，用于汽车零部件检测的汽车人工智能领域缺乏数据集，大多数可用数据集的大小和范围都有限，难以覆盖各种场景。为了解决这一差距，本文提出了一个由84162张图像组成的大规模细粒度汽车数据集，用于检测12种不同类型的汽车零件。该数据集是从自然相机和在线网站收集的，涵盖了各种汽车品牌、场景和拍摄角度。为了减轻手动注释的负担，我们提出了一种新的半监督自动标记方法，该方法利用了最先进的预训练检测器。此外，我们还研究了用于零样本标记的Grounding DINO方法的局限性。最后，我们通过训练几个轻量级YOLO系列检测器，通过细粒度的汽车零件检测来评估我们提出的数据集的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11754v1" target="_blank">2311.11754v1</a>
                              </td>
                              <td>A Large-Scale Car Parts (LSCP) Dataset for Lightweight Fine-Grained Detection</td>
                              <td>Wang Jie</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11754v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11754v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>