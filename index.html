<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2024-02-06</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_03246v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03246v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03246v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03246v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03246v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义理解在密集同时定位和映射（SLAM）中起着至关重要的作用，有助于全面的场景解释。将高斯散射集成到SLAM系统中的最新进展已经证明了其通过使用显式3D高斯表示生成高质量渲染的有效性。在这一进展的基础上，我们提出了SGS-SLAM，这是第一个基于3D高斯的语义密集视觉SLAM系统，它提供了精确的3D语义分割和高保真度重建。具体而言，我们建议在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。大量实验表明，SGS-SLAM在相机姿态估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保持了实时渲染能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03246v1" target="_blank">2402.03246v1</a>
                              </td>
                              <td>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</td>
                              <td>Mingrui Li</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03246v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03246v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02020v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02020v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02020v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02020v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping pipeline based on neural multiresolution voxels, characterized by ultra-fast convergence and incremental expansion capabilities. This pipeline utilizes RGBD images as input to construct multiresolution neural voxels, achieving rapid convergence while maintaining robust incremental scene reconstruction and camera tracking. Central to our methodology is to propose a novel implicit representation, termed VDF that combines the implementation of neural signed distance field (SDF) voxels with an SDF activation strategy. This approach entails the direct optimization of color features and SDF values anchored within the voxels, substantially enhancing the rate of scene convergence. To ensure the acquisition of clear edge delineation, SDF activation is designed, which maintains exemplary scene representation fidelity even under constraints of voxel resolution. Furthermore, in pursuit of advancing rapid incremental expansion with low computational overhead, we developed hashMV, a novel hash-based multiresolution voxel management structure. This architecture is complemented by a strategically designed voxel generation technique that synergizes with a two-dimensional scene prior. Our empirical evaluations, conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's exceptional efficacy in terms of convergence speed, tracking accuracy, scene reconstruction, and rendering quality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02020v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuV SLAM，这是一种基于神经多分辨率体素的新型密集同时定位和映射管道，具有超快收敛和增量扩展能力。该流水线利用RGBD图像作为输入来构建多分辨率神经体素，实现快速收敛，同时保持稳健的增量场景重建和相机跟踪。我们方法的核心是提出一种新的隐式表示，称为VDF，它将神经符号距离场（SDF）体素的实现与SDF激活策略相结合。这种方法需要对锚定在体素内的颜色特征和SDF值进行直接优化，从而显著提高场景收敛速度。为了确保获得清晰的边缘描绘，设计了SDF激活，即使在体素分辨率的约束下，它也能保持示例场景表示的保真度。此外，为了追求以低计算开销推进快速增量扩展，我们开发了hashMV，这是一种新的基于哈希的多分辨率体素管理结构。该架构由战略性设计的体素生成技术补充，该技术与二维场景先验协同。我们在Replica和ScanNet数据集上进行的经验评估证实了NeuV SLAM在收敛速度、跟踪精度、场景重建和渲染质量方面的卓越功效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02020v1" target="_blank">2402.02020v1</a>
                              </td>
                              <td>NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM</td>
                              <td>Wenzhi Guo</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02020v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02020v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01967v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01967v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01967v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01967v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In autonomous robotics, a critical challenge lies in developing robust solutions for Active Collaborative SLAM, wherein multiple robots collaboratively explore and map an unknown environment while intelligently coordinating their movements and sensor data acquisitions. In this article, we present two approaches for coordinating a system consisting of multiple robots to perform Active Collaborative SLAM (AC-SLAM) for environmental exploration. Our two coordination approaches, synchronous and asynchronous implement a methodology to prioritize robot goal assignments by the central server. We also present a method to efficiently spread the robots for maximum exploration while keeping SLAM uncertainty low. Both coordination approaches were evaluated through simulation and experiments on publicly available datasets, rendering promising results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01967v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在自主机器人中，一个关键的挑战在于为主动协作SLAM开发强大的解决方案，其中多个机器人协同探索和绘制未知环境的地图，同时智能地协调它们的运动和传感器数据采集。在本文中，我们提出了两种方法来协调由多个机器人组成的系统，以执行用于环境勘探的主动协作SLAM（AC-SLAM）。我们的两种协调方法，同步和异步，实现了一种由中央服务器对机器人目标分配进行优先级排序的方法。我们还提出了一种方法，在保持低SLAM不确定性的同时，有效地分散机器人进行最大限度的探索。通过在公开数据集上进行模拟和实验，对这两种协调方法进行了评估，得出了有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01967v2" target="_blank">2310.01967v2</a>
                              </td>
                              <td>Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</td>
                              <td>Matteo Maragliano</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01967v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01967v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00588v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BrainSLAM: SLAM on Neural Population Activity Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00588v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00588v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00588v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00588v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）算法通常用于机器人系统中，用于学习新环境的地图。大脑似乎也会学习地图，但其机制尚不清楚，也不清楚如何从神经活动数据中推断出这些地图。我们推出BrainSLAM；一种仅使用从大鼠的三个大脑区域（海马体、前额叶皮层和顶叶皮层）同时记录的群体活动（局部场电位，LFP）数据进行SLAM的方法。该系统使用卷积神经网络（CNN）从大鼠在2D迷宫中导航时记录的神经局部场电位数据的小波标度图中解码速度和熟悉度信息。CNN的输出驱动了一个受RatSLAM启发的架构，为执行路径集成的吸引器网络和执行“环路闭合”（检测先前访问的位置并校正地图混叠错误）的独立系统提供动力。这三个组成部分加在一起可以构建环境的忠实表示，同时跟踪动物的位置。这是从大脑记录中推断空间图的首次演示。我们的发现将SLAM扩展到了一种新的模式，实现了一种绘制环境的新方法，并有助于更好地理解认知地图在导航和决策中的作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00588v1" target="_blank">2402.00588v1</a>
                              </td>
                              <td>BrainSLAM: SLAM on Neural Population Activity Data</td>
                              <td>Kipp Freud</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00588v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00588v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00438v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The GREENBOT dataset: Multimodal mobile robotic dataset for a typical Mediterranean greenhouse</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00438v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00438v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00438v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an innovative dataset specifically crafted for challenging agricultural settings (a greenhouse), where achieving precise localization is of paramount importance. The dataset was gathered using a mobile platform equipped with a set of sensors typically used in mobile robots, as it was moved through all the corridors of a typical Mediterranean greenhouse featuring tomato crop. This dataset presents a unique opportunity for constructing detailed 3D models of plants in such indoor-like space, with potential applications such as robotized spraying. For the first time to the best knowledge of authors, a dataset suitable to put at test Simultaneous Localization and Mapping (SLAM) methods is presented in a greenhouse environment, which poses unique challenges. The suitability of the dataset for such goal is assessed by presenting SLAM results with state-of-the-art algorithms. The dataset is available online in \url{https://arm.ual.es/arm-group/dataset-greenhouse-2024/}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00438v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一个专门为具有挑战性的农业环境（温室）设计的创新数据集，在那里实现精确定位至关重要。数据集是使用一个移动平台收集的，该平台配备了一组移动机器人通常使用的传感器，数据集在以番茄作物为特色的典型地中海温室的所有走廊中移动。该数据集为在这种类似室内的空间中构建植物的详细3D模型提供了一个独特的机会，具有机器人喷涂等潜在应用。据作者所知，首次在温室环境中提出了一个适合测试同步定位和映射（SLAM）方法的数据集，这带来了独特的挑战。通过使用最先进的算法呈现SLAM结果来评估数据集对该目标的适用性。数据集可在线获取，网址为\url{https://arm.ual.es/arm-group/dataset-greenhouse-2024/}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00438v1" target="_blank">2402.00438v1</a>
                              </td>
                              <td>The GREENBOT dataset: Multimodal mobile robotic dataset for a typical Mediterranean greenhouse</td>
                              <td>Fernando Cañadas-Aránega</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00438v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00438v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_03256v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OROS: Online Operation and Orchestration of Collaborative Robots using 5G</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_03256v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_03256v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_03256v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The 5G mobile networks extend the capability for supporting collaborative robot operations in outdoor scenarios. However, the restricted battery life of robots still poses a major obstacle to their effective implementation and utilization in real scenarios. One of the most challenging situations is the execution of mission-critical tasks that require the use of various onboard sensors to perform simultaneous localization and mapping (SLAM) of unexplored environments. Given the time-sensitive nature of these tasks, completing them in the shortest possible time is of the highest importance. In this paper, we analyze the benefits of 5G-enabled collaborative robots by enhancing the intelligence of the robot operation through joint orchestration of Robot Operating System (ROS) and 5G resources for energysaving goals, addressing the problem from both offline and online manners. We propose OROS, a novel orchestration approach that minimizes mission-critical task completion times as well as overall energy consumption of 5G-connected robots by jointly optimizing robotic navigation and sensing together with infrastructure resources. We validate our 5G-enabled collaborative framework by means of Matlab/Simulink, ROS software and Gazebo simulator. Our results show an improvement between 3.65% and 11.98% in exploration task by exploiting 5G orchestration features for battery savings when using 3 robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_03256v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>5G移动网络扩展了在户外场景中支持机器人协同操作的能力。然而，机器人电池寿命有限仍然是其在现实场景中有效实施和利用的主要障碍。最具挑战性的情况之一是执行任务关键型任务，这些任务需要使用各种机载传感器对未探索的环境进行同时定位和测绘（SLAM）。鉴于这些任务的时间敏感性，在尽可能短的时间内完成任务是最重要的。在本文中，我们分析了支持5G的协作机器人的好处，通过机器人操作系统（ROS）和5G资源的联合协调来提高机器人操作的智能性，以实现节能目标，从离线和在线两种方式解决问题。我们提出了OROS，这是一种新的协调方法，通过联合优化机器人导航和传感以及基础设施资源，最大限度地减少任务关键任务的完成时间以及5G连接机器人的总体能耗。我们通过Matlab/Simulink、ROS软件和Gazebo模拟器验证了我们的5G协作框架。我们的结果显示，当使用3个机器人时，通过利用5G协调功能节省电池，探索任务的效率提高了3.65%至11.98%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.03256v3" target="_blank">2205.03256v3</a>
                              </td>
                              <td>OROS: Online Operation and Orchestration of Collaborative Robots using 5G</td>
                              <td>Arnau Romero</td>
                              <td>2022-05-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_03256v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.03256v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16719v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16719v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16719v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16719v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16719v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于腿式机器人的高度动态运动和传感器精度的限制，其状态估计具有挑战性。通过集成卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，该解决方案结合本体感觉和外部感觉信息来估计机器人躯干的状态。利用联合编码器和IMU测量，我们的卡尔曼滤波器通过单个刚体模型得到增强，该模型结合了凸模型预测控制优化的地面反作用力控制输出。通过门控递归单元进一步细化了估计，该单元还考虑了应用于深度图像的视觉转换器自动编码器的语义见解和机器人高度。该框架不仅提供了准确的机器人状态估计，包括不确定性评估，而且可以通过学习将传感器测量和模型简化产生的非线性误差降至最低。使用四足机器人在各种地形上对所提出的方法进行了硬件评估，与我们的VIO SLAM基线相比，均方根误差提高了65%。代码示例：https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16719v2" target="_blank">2401.16719v2</a>
                              </td>
                              <td>OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</td>
                              <td>Alexander Schperberg</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16719v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16719v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alexs28/optistate" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17907v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17907v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17907v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17907v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation. SubPipe has been recorded using a \gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors. The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand. The AUV's pose ground truth is estimated from the navigation sensors. The side-scan sonar and RGB images include object detection and segmentation annotations, respectively. State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms. To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario. The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17907v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了用于SLAM、目标检测和图像分割的水下数据集SubPipe。SubPipe是使用由OceanScan MST操作的LAUV进行记录的，并携带一个传感器套件，包括两个摄像头、一个侧扫声纳和一个惯性导航系统以及其他传感器。AUV已经部署在管道检查环境中，海底管道部分被沙子覆盖。AUV的姿态地面实况是由导航传感器估计的。侧扫声纳和RGB图像分别包括物体检测和分割注释。在SubPipe上对最先进的分割、对象检测和SLAM方法进行了基准测试，以展示数据集在利用计算机视觉算法方面的挑战和机遇。据作者所知，这是第一个提供真实管道检查场景的注释水下数据集。该数据集和实验可在线公开获取，网址为https://github.com/remaro-network/SubPipe-dataset</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17907v1" target="_blank">2401.17907v1</a>
                              </td>
                              <td>SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization</td>
                              <td>Olaya Álvarez-Tuñón</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17907v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17907v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/remaro-network/subpipe-dataset" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17826v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17826v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17826v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17826v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately generating ground truth (GT) trajectories is essential for Simultaneous Localization and Mapping (SLAM) evaluation, particularly under varying environmental conditions. This study introduces a systematic approach employing a prior map-assisted framework for generating dense six-degree-of-freedom (6-DoF) GT poses for the first time, enhancing the fidelity of both indoor and outdoor SLAM datasets. Our method excels in handling degenerate and stationary conditions frequently encountered in SLAM datasets, thereby increasing robustness and precision. A significant aspect of our approach is the detailed derivation of covariances within the factor graph, enabling an in-depth analysis of pose uncertainty propagation. This analysis crucially contributes to demonstrating specific pose uncertainties and enhancing trajectory reliability from both theoretical and empirical perspectives. Additionally, we provide an open-source toolbox (https://github.com/JokerJohn/Cloud_Map_Evaluation) for map evaluation criteria, facilitating the indirect assessment of overall trajectory precision. Experimental results show at least a 30\% improvement in map accuracy and a 20\% increase in direct trajectory accuracy compared to the Iterative Closest Point (ICP) \cite{sharp2002icp} algorithm across diverse campus environments, with substantially enhanced robustness. Our open-source solution (https://github.com/JokerJohn/PALoc), extensively applied in the FusionPortable\cite{Jiao2022Mar} dataset, is geared towards SLAM benchmark dataset augmentation and represents a significant advancement in SLAM evaluations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17826v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确生成地面实况（GT）轨迹对于同时定位和测绘（SLAM）评估至关重要，尤其是在不同的环境条件下。本研究首次引入了一种系统方法，采用先验地图辅助框架生成密集的六自由度（6-DoF）GT姿态，提高了室内和室外SLAM数据集的保真度。我们的方法擅长处理SLAM数据集中经常遇到的退化和平稳条件，从而提高了鲁棒性和精度。我们方法的一个重要方面是在因子图中详细推导协变量，从而能够深入分析姿态不确定性的传播。这一分析从理论和实证角度对证明特定姿态的不确定性和提高轨迹可靠性至关重要。此外，我们还提供了一个开源工具箱(https://github.com/JokerJohn/Cloud_Map_Evaluation)用于地图评估标准，便于间接评估总体轨迹精度。实验结果表明，在不同的校园环境中，与迭代最近点算法相比，地图精度至少提高了30%，直接轨迹精度提高了20%，鲁棒性显著增强。我们的开源解决方案(https://github.com/JokerJohn/PALoc)，广泛应用于FusionPortable\cite{Jia2022Mar}数据集，旨在增强SLAM基准数据集，代表着SLAM评估的重大进步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17826v1" target="_blank">2401.17826v1</a>
                              </td>
                              <td>PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation</td>
                              <td>Xiangcheng Hu</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17826v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17826v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/JokerJohn/PALoc" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jokerjohn/paloc" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10896v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10896v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10896v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10896v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This document presents PLVS: a real-time system that leverages sparse SLAM, volumetric mapping, and 3D unsupervised incremental segmentation. PLVS stands for Points, Lines, Volumetric mapping, and Segmentation. It supports RGB-D and Stereo cameras, which may be optionally equipped with IMUs. The SLAM module is keyframe-based, and extracts and tracks sparse points and line segments as features. Volumetric mapping runs in parallel with respect to the SLAM front-end and generates a 3D reconstruction of the explored environment by fusing point clouds backprojected from keyframes. Different volumetric mapping methods are supported and integrated in PLVS. We use a novel reprojection error to bundle-adjust line segments. This error exploits available depth information to stabilize the position estimates of line segment endpoints. An incremental and geometric-based segmentation method is implemented and integrated for RGB-D cameras in the PLVS framework. We present qualitative and quantitative evaluations of the PLVS framework on some publicly available datasets. The appendix details the adopted stereo line triangulation method and provides a derivation of the Jacobians we used for line error terms. The software is available as open-source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10896v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了PLVS：一个利用稀疏SLAM、体积映射和3D无监督增量分割的实时系统。PLVS代表点、线、体积映射和分割。它支持RGB-D和立体声相机，这些相机可以选择配备IMU。SLAM模块基于关键帧，提取并跟踪稀疏点和线段作为特征。体积映射相对于SLAM前端并行运行，并通过融合从关键帧反向投影的点云来生成探索环境的3D重建。PLVS支持并集成了不同的体积映射方法。我们使用一种新的重投影误差来对调整线段进行集束。该误差利用可用的深度信息来稳定线段端点的位置估计。在PLVS框架中，实现并集成了RGB-D相机的增量和基于几何的分割方法。我们在一些公开的数据集上对PLVS框架进行了定性和定量评估。附录详细介绍了所采用的立体线三角测量方法，并提供了我们用于线误差项的雅可比矩阵的推导。该软件是开源的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10896v2" target="_blank">2309.10896v2</a>
                              </td>
                              <td>PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</td>
                              <td>Luigi Freda</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10896v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10896v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luigifreda/plvs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17741v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17741v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17741v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17741v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition. The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency. In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data. This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility. The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums. The demo of the robot can be found on https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17741v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了Haris，一种先进的自主移动机器人系统，用于使用车牌识别来跟踪拥挤停车场中的车辆位置。该系统采用同步定位和地图绘制（SLAM）实现停车区的自主导航和精确地图绘制，消除了对GPS的依赖。此外，该系统利用复杂的框架，使用计算机视觉技术进行物体检测和自动车牌识别（ALPR）来读取车牌号并将其与位置数据相关联。这些信息随后与后端服务同步，并通过用户友好的移动应用程序向用户提供访问权限，轻松定位车辆，缓解停车设施内的拥堵。拟议的系统有可能改善体育场馆等拥挤场所的短期大型室外停车场的管理。机器人的演示可以在上找到https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17741v1" target="_blank">2401.17741v1</a>
                              </td>
                              <td>Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance</td>
                              <td>Layth Hamad</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17741v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17463v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev Interpolation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17463v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17463v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17463v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new metric for robot state estimation based on the recently introduced $\text{SE}_2(3)$ Lie group definition. Our metric is related to prior metrics for SLAM but explicitly takes into account the linear velocity of the state estimate, improving over current pose-based trajectory analysis. This has the benefit of providing a single, quantitative metric to evaluate state estimation algorithms against, while being compatible with existing tools and libraries. Since ground truth data generally consists of pose data from motion capture systems, we also propose an approach to compute the ground truth linear velocity based on polynomial interpolation. Using Chebyshev interpolation and a pseudospectral parameterization, we can accurately estimate the ground truth linear velocity of the trajectory in an optimal fashion with best approximation error. We demonstrate how this approach performs on multiple robotic platforms where accurate state estimation is vital, and compare it to alternative approaches such as finite differences. The pseudospectral parameterization also provides a means of trajectory data compression as an additional benefit. Experimental results show our method provides a valid and accurate means of comparing state estimation systems, which is also easy to interpret and report.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17463v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于最近引入的$\text，我们提出了一种新的机器人状态估计度量{SE}_2（3） $Lie群定义。我们的度量与SLAM的先前度量有关，但明确考虑了状态估计的线速度，改进了当前基于姿态的轨迹分析。这样做的好处是提供了一个单一的定量指标来评估状态估计算法，同时与现有的工具和库兼容。由于地面实况数据通常由运动捕捉系统的姿态数据组成，我们还提出了一种基于多项式插值的地面实况线速度计算方法。使用切比雪夫插值和伪谱参数化，我们可以以最佳的方式以最佳的近似误差准确地估计轨迹的真实线速度。我们展示了这种方法在多个机器人平台上的表现，在这些平台上，准确的状态估计至关重要，并将其与有限差分等替代方法进行了比较。伪谱参数化还提供了一种轨迹数据压缩方法作为额外的好处。实验结果表明，我们的方法为比较状态估计系统提供了一种有效而准确的方法，而且易于解释和报告。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17463v1" target="_blank">2401.17463v1</a>
                              </td>
                              <td>A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev Interpolation</td>
                              <td>Varun Agrawal</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17463v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17061v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17061v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Omnidirectional and 360{\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17061v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全方位和360度图像在工业和消费社会中越来越普遍，引起了全方位计算机视觉的关注。它们的宽视场允许仅从图像中收集大量关于环境的信息。然而，这些图像的失真需要开发特定的算法来进行处理和解释。此外，大量的图像对于基于学习的计算机视觉算法的正确训练是必不可少的。在本文中，我们提出了一种用于生成具有语义和深度信息的全向图像数据集的工具。这些图像是从在虚幻引擎4的真实虚拟环境中通过接口插件获取的一组捕获中合成的。我们收集了各种著名的投影模型，如等矩形和圆柱形全景图、不同的鱼眼透镜、折反射系统和经验模型。此外，在我们的工具中，我们将真实感非中心投影系统包括为非中心全景和非中心折反射系统。据我们所知，这是文献中第一个报道的生成真实感非中心图像的工具。此外，由于全向图像是虚拟制作的，我们提供了关于语义和深度的像素信息，以及相机校准参数的完美知识。这允许创建具有像素精度的地面实况信息，用于训练学习算法和测试3D视觉方法。为了验证所提出的工具，测试了不同的计算机视觉算法，如从屈光和折反射中心图像中提取线，使用等矩形全景进行3D布局恢复和SLAM，以及从非中心全景进行3D重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17061v1" target="_blank">2401.17061v1</a>
                              </td>
                              <td>OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</td>
                              <td>Bruno Berenguel-Baeta</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17061v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17061v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sbrunoberenguel/omniscv" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion.   This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements.   Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.   We release our software and hardware and self-collected datasets on Github\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种集成的精确激光雷达、惯性和视觉（LIV）多模态传感器融合测绘系统，该系统建立在可微分表面飞溅的基础上，以提高测绘保真度、质量和结构精度。值得注意的是，这也是一种用于激光雷达视觉惯性传感器融合的新型紧耦合映射。该系统利用激光雷达和视觉数据的互补特性来捕捉大规模3D场景的几何结构，并高保真地恢复其视觉表面信息。表面高斯场景的初始姿态是使用具有尺寸自适应体素的激光雷达惯性系统获得的。然后，我们通过视觉推导的光度梯度优化和细化高斯，以优化激光雷达测量的质量和密度。我们的方法与各种类型的激光雷达兼容，包括固态和机械激光雷达，支持重复和非重复扫描模式。通过激光雷达支持结构构建，并促进在不同的LIV数据集上实时生成真实感渲染图。它在为数字双胞胎和虚拟现实生成实时真实感场景方面表现出了显著的弹性和多功能性，同时在实时SLAM和机器人领域也具有潜在的适用性。我们在Github上发布了我们的软件和硬件以及自行收集的数据集\脚注[3]{https://github.com/sheng00125/LIV-GaussMap}以造福社会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14857v1" target="_blank">2401.14857v1</a>
                              </td>
                              <td>LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</td>
                              <td>Sheng Hong</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13877v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AscDAMs: Advanced SLAM-based channel detection and mapping system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13877v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13877v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>获得高分辨率、准确的河道地形和沉积条件是渠化泥石流研究的首要挑战。目前，包括卫星成像和无人机摄影测量在内的广泛使用的测绘技术难以精确观测山区长深沟的河道内部条件，特别是汶川地震地区的深沟。SLAM是一种新兴的3D地图技术；然而，即使对最先进的SLAM来说，长深沟中极其崎岖的环境也带来了两大挑战：（1）非典型特征；（2） 传感器剧烈摆动和振荡。这些问题导致SLAM结果的大偏差和大量噪声。为了改进这种环境中的SLAM映射，我们提出了一种先进的基于SLAM的信道检测和映射系统，即AscDAM。它对后处理SLAM结果有三个主要改进：（1）数字正射影像图辅助偏差校正算法大大消除了系统误差；（2） 点云平滑算法大大减少了噪声；（3） 横截面提取算法能够对河道沉积物及其变化进行定量评估。2023年2月和11月，在中国汶川县楚头沟进行了两次野外实验，代表了雨季前后的观测结果。我们展示了AscDAM极大地提高SLAM结果的能力，促进了SLAM在特殊挑战环境中的映射。所提出的方法弥补了现有技术在检测泥石流通道内部方面的不足，包括详细的通道形态、侵蚀模式、沉积物区分、体积估计和变化检测。它有助于加强对全面泥石流机制、地震后长期演变和灾害评估的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13877v1" target="_blank">2401.13877v1</a>
                              </td>
                              <td>AscDAMs: Advanced SLAM-based channel detection and mapping system</td>
                              <td>Tengfei Wang</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13877v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13877v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Object Navigation in real environments using hybrid policies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>导航已经通过SLAM和规划的结合在机器人技术中得到了经典的解决。最近，除了航路点规划之外，还探索了在模拟环境中涉及（视觉）高级推理的重要组成部分的问题，主要通过大规模机器学习来解决，特别是RL、离线RL或模仿学习。这些方法需要代理学习各种技能，如局部规划、映射对象和查询所学习的空间表示。与航路点规划（PointGoal）等更简单的任务相比，对于这些更复杂的任务，目前最先进的模型已经在模拟中进行了全面评估，但据我们所知，还没有在真实环境中进行评估。在这项工作中，我们重点关注sim2real转移。我们针对具有挑战性的多对象导航（Multi-ON）任务，并将其移植到包含原始虚拟Multi-ON对象的真实副本的物理环境中。我们介绍了一种混合导航方法，该方法将问题分解为两种不同的技能：（1）航路点导航是用经典的SLAM与符号规划器相结合来解决的，而（2）探索、语义映射和目标检索是用监督学习和RL相结合来训练的深度神经网络来解决的。我们展示了与端到端方法相比，这种方法在模拟和真实环境中的优势，并在该任务中优于SOTA。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13800v1" target="_blank">2401.13800v1</a>
                              </td>
                              <td>Multi-Object Navigation in real environments using hybrid policies</td>
                              <td>Assem Sadek</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14590v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14590v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Place recognition is crucial for robot localization and loop closure in simultaneous localization and mapping (SLAM). Light Detection and Ranging (LiDAR), known for its robust sensing capabilities and measurement consistency even in varying illumination conditions, has become pivotal in various fields, surpassing traditional imaging sensors in certain applications. Among various types of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning patterns have recently been utilized in robotics applications. Some LiDARs provide additional measurements such as reflectivity, Near Infrared (NIR), and velocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these advances, there is a lack of comprehensive datasets reflecting the broad spectrum of LiDAR configurations for place recognition. To tackle this issue, our paper proposes the HeLiPR dataset, curated especially for place recognition with heterogeneous LiDARs, embodying spatiotemporal variations. To the best of our knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset supporting inter-LiDAR place recognition with both non-repetitive and spinning LiDARs, accommodating different field of view (FOV)s and varying numbers of rays. The dataset covers diverse environments, from urban cityscapes to high-dynamic freeways, over a month, enhancing adaptability and robustness across scenarios. Notably, HeLiPR includes trajectories parallel to MulRan sequences, making it valuable for research in heterogeneous LiDAR place recognition and long-term studies. The dataset is accessible at https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14590v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在同步定位与映射（SLAM）中，位置识别对于机器人定位和闭环至关重要。光探测和测距（LiDAR）以其强大的传感能力和测量一致性而闻名，即使在不同的光照条件下也是如此，在各个领域已经成为关键，在某些应用中超过了传统的成像传感器。在各种类型的激光雷达中，旋转激光雷达被广泛使用，而非重复扫描模式最近被用于机器人应用。一些激光雷达提供额外的测量，如反射率、近红外（NIR）和调频连续波（FMCW）激光雷达的速度。尽管取得了这些进展，但缺乏全面的数据集来反映用于位置识别的激光雷达配置的广谱性。为了解决这个问题，我们的论文提出了HeLiPR数据集，该数据集专门用于异构激光雷达的位置识别，体现了时空变化。据我们所知，HeLiPR数据集是第一个支持非重复和旋转激光雷达的激光雷达位置识别的异构激光雷达数据集，可容纳不同的视场和不同数量的光线。该数据集在一个多月的时间里涵盖了从城市景观到高动态高速公路的各种环境，增强了跨场景的适应性和稳健性。值得注意的是，HeLiPR包括与MulRan序列平行的轨迹，这对于异质激光雷达位置识别和长期研究的研究具有价值。数据集可在访问https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14590v2" target="_blank">2309.14590v2</a>
                              </td>
                              <td>HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</td>
                              <td>Minwoo Jung</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14590v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14590v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转的激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展我们之前仅专注于全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v2" target="_blank">2311.00928v2</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning algorithms have driven expressive progress in many complex tasks. The loss function is a core component of deep learning techniques, guiding the learning process of neural networks. This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches. The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips. Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习算法推动了许多复杂任务的表达进度。损失函数是深度学习技术的核心组成部分，指导神经网络的学习过程。本文通过引入基于深度学习的视觉里程计方法的一致性损失来做出贡献。运动一致性损失探索出现在连续重叠视频剪辑中的重复运动。实验结果表明，我们的方法提高了模型在KITTI里程计基准上的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10857v1" target="_blank">2401.10857v1</a>
                              </td>
                              <td>Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</td>
                              <td>André O. Françani</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10560v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10560v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To enhance the performance and effect of AR/VR applications and visual assistance and inspection systems, visual simultaneous localization and mapping (vSLAM) is a fundamental task in computer vision and robotics. However, traditional vSLAM systems are limited by the camera's narrow field-of-view, resulting in challenges such as sparse feature distribution and lack of dense depth information. To overcome these limitations, this paper proposes a 360ORB-SLAM system for panoramic images that combines with a depth completion network. The system extracts feature points from the panoramic image, utilizes a panoramic triangulation module to generate sparse depth information, and employs a depth completion network to obtain a dense panoramic depth map. Experimental results on our novel panoramic dataset constructed based on Carla demonstrate that the proposed method achieves superior scale accuracy compared to existing monocular SLAM methods and effectively addresses the challenges of feature association and scale ambiguity. The integration of the depth completion network enhances system stability and mitigates the impact of dynamic elements on SLAM performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10560v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了提高AR/VR应用程序以及视觉辅助和检测系统的性能和效果，视觉同步定位和映射（vSLAM）是计算机视觉和机器人技术中的一项基本任务。然而，传统的vSLAM系统受到相机狭窄视场的限制，导致了特征分布稀疏和缺乏密集深度信息等挑战。为了克服这些限制，本文提出了一种360ORB-SLAM全景图像系统，该系统与深度完成网络相结合。该系统从全景图像中提取特征点，利用全景三角测量模块生成稀疏的深度信息，并利用深度完成网络获得密集的全景深度图。在基于Carla构建的新全景数据集上的实验结果表明，与现有的单目SLAM方法相比，该方法实现了优越的尺度精度，并有效地解决了特征关联和尺度模糊的挑战。深度完井网络的集成增强了系统稳定性，并减轻了动态元素对SLAM性能的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10560v1" target="_blank">2401.10560v1</a>
                              </td>
                              <td>360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</td>
                              <td>Yichen Chen</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10560v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10560v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09388v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09388v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces CognitiveDog, a pioneering development of quadruped robot with Large Multi-modal Model (LMM) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. The system was realized on Unitree Go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. These tasks do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. The paper delves into the intricacies of this system, dataset characteristics, and the software architecture. Key to this development is the robot's proficiency in navigating space using Visual-SLAM, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. Experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in real-world applications. The dataset used to fine-tune the robot-dog behavior generation model is provided at the following link: huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09388v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了CognitiveDog，这是一种具有大型多模态模型（LMM）的四足机器人的开创性发展，它不仅能够与人类进行言语交流，而且能够通过物体操纵与环境进行物理交互。该系统是在配备了自定义夹持器的Unitree Go1机器狗上实现的，并展示了自主决策能力，独立确定最合适的动作以及与各种对象的交互，以完成用户定义的任务。这些任务不一定包括直接指令，挑战机器人根据自然语言输入和环境线索理解和执行指令。本文深入研究了该系统的复杂性、数据集特性和软件体系结构。这一开发的关键是机器人熟练地使用Visual SLAM在空间中导航，有效地操纵和运输物体，并在任务执行过程中提供富有洞察力的自然语言评论。实验结果突出了机器人先进的任务理解能力和适应性，突出了其在现实世界应用中的潜力。用于微调机器狗行为生成模型的数据集在以下链接中提供：huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09388v1" target="_blank">2401.09388v1</a>
                              </td>
                              <td>CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</td>
                              <td>Artem Lykov</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09388v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09388v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09331v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09331v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios. The code is available at \url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09331v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在具有挑战性的条件下有望获得卓越的性能，但由于难以从事件流中提取和跟踪稳定特征，基于事件的运动估计仍然是一个难题。为了使估计具有鲁棒性，通常认为需要与其他传感器进行融合。在这项工作中，我们通过使用阿克曼转向平台的约束非完整运动模型，在平面地面车辆上演示了可靠的、纯基于事件的视觉里程计。我们将基于常规帧的相机的单特征n线性扩展到准时间连续事件轨迹的情况，并通过变阶泰勒展开实现多项式形式。通过直方图投票可以简单地实现对多个事件轨迹的稳健平均。如模拟数据和真实数据所示，我们的算法实现了对车辆瞬时转速的准确和稳健估计，从而获得了与正常条件下基于帧的传感器获得的德尔塔旋转相当的结果。此外，在具有挑战性的照明场景中，我们显著优于更传统的替代方案。代码位于\url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09331v1" target="_blank">2401.09331v1</a>
                              </td>
                              <td>Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</td>
                              <td>Wanting Xu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09331v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09331v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gowanting/nhevo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09322v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09322v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active visual SLAM finds a wide array of applications in GNSS-Denied sub-terrain environments and outdoor environments for ground robots. To achieve robust localization and mapping accuracy, it is imperative to incorporate the perception considerations in the goal selection and path planning towards the goal during an exploration mission. Through this work, we propose FIT-SLAM (Fisher Information and Traversability estimation-based Active SLAM), a new exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D environments. This approach is devised with the dual objectives of sustaining an efficient exploration rate while optimizing SLAM accuracy. Initially, an estimation of a global traversability map is conducted, which accounts for the environmental constraints pertaining to traversability. Subsequently, we propose a goal candidate selection approach along with a path planning method towards this goal that takes into account the information provided by the landmarks used by the SLAM backend to achieve robust localization and successful path execution . The entire algorithm is tested and evaluated first in a simulated 3D world, followed by a real-world environment and is compared to pre-existing exploration methods. The results obtained during this evaluation demonstrate a significant increase in the exploration rate while effectively minimizing the localization covariance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09322v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动视觉SLAM在拒绝全球导航卫星系统的地下环境和地面机器人的户外环境中有着广泛的应用。为了实现稳健的定位和测绘精度，在探索任务期间，必须将感知考虑因素纳入目标选择和目标路径规划中。通过这项工作，我们提出了FIT-SLAM（基于Fisher信息和遍历性估计的主动SLAM），这是一种为无人地面飞行器（UGV）探索3D环境量身定制的新探索方法。这种方法的设计具有双重目标，即在优化SLAM精度的同时保持有效的勘探速率。最初，对全球可穿越性地图进行估计，该地图考虑了与可穿越性相关的环境约束。随后，我们提出了一种目标候选选择方法以及实现该目标的路径规划方法，该方法考虑了SLAM后端使用的地标提供的信息，以实现稳健的定位和成功的路径执行。首先在模拟的3D世界中测试和评估整个算法，然后在真实世界环境中进行测试和评估，并将其与预先存在的探索方法进行比较。在该评估过程中获得的结果表明，在有效地最小化定位协方差的同时，勘探率显著提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09322v1" target="_blank">2401.09322v1</a>
                              </td>
                              <td>FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</td>
                              <td>Suchetan Saravanan</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09322v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09322v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09160v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09160v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unreliable feature extraction and matching in handcrafted features undermine the performance of visual SLAM in complex real-world scenarios. While learned local features, leveraging CNNs, demonstrate proficiency in capturing high-level information and excel in matching benchmarks, they encounter challenges in continuous motion scenes, resulting in poor generalization and impacting loop detection accuracy. To address these issues, we present DK-SLAM, a monocular visual SLAM system with adaptive deep local features. MAML optimizes the training of these features, and we introduce a coarse-to-fine feature tracking approach. Initially, a direct method approximates the relative pose between consecutive frames, followed by a feature matching method for refined pose estimation. To counter cumulative positioning errors, a novel online learning binary feature-based online loop closure module identifies loop nodes within a sequence. Experimental results underscore DK-SLAM's efficacy, outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly available datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09160v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手工特征中不可靠的特征提取和匹配会破坏视觉SLAM在复杂现实世界场景中的性能。虽然利用细胞神经网络学习局部特征，证明了它们在捕捉高级信息方面的熟练程度和在匹配基准方面的出色表现，但它们在连续运动场景中遇到了挑战，导致泛化能力差，并影响环路检测精度。为了解决这些问题，我们提出了DK-SLAM，一种具有自适应深度局部特征的单目视觉SLAM系统。MAML优化了这些特征的训练，并引入了一种从粗到细的特征跟踪方法。首先，直接方法近似连续帧之间的相对姿态，然后是用于精细姿态估计的特征匹配方法。为了应对累积的定位误差，一种新颖的基于二进制特征的在线学习回路闭合模块识别序列中的回路节点。实验结果强调了DK-SLAM的功效，在公开可用的数据集上优于代表性的SLAM解决方案，如ORB-SLAM3。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09160v1" target="_blank">2401.09160v1</a>
                              </td>
                              <td>DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</td>
                              <td>Hao Qu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09160v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09160v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09101v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09101v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09101v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09101v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a SLAM system for building globally consistent maps, called PIN-SLAM, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that PIN-SLAM is robust to various environments and versatile to different range sensors such as LiDAR and RGB-D cameras. PIN-SLAM achieves pose estimation accuracy better or on par with the state-of-the-art LiDAR odometry or SLAM systems and outperforms the recent neural implicit SLAM approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, PIN-SLAM can run at the sensor frame rate on a moderate GPU. Codes will be available at: https://github.com/PRBonn/PIN_SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09101v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>精确和稳健的定位和映射是大多数自主机器人的重要组成部分。在本文中，我们提出了一种用于构建全局一致映射的SLAM系统，称为PIN-SLAM，它基于弹性和紧凑的基于点的隐式神经映射表示。以距离测量作为输入，我们的方法在局部隐式符号距离场的增量学习和使用无对应的点到隐式模型配准的给定当前局部地图的姿态估计之间交替。我们的隐式映射基于稀疏的可优化神经点，这些神经点在闭合回路时具有固有的弹性和可变形性，并可通过全局姿态调整进行调整。还使用神经点特征来检测循环。大量实验验证了PIN-SLAM对各种环境具有鲁棒性，并适用于不同的距离传感器，如激光雷达和RGB-D相机。PIN-SLAM实现了更好或与最先进的LiDAR里程计或SLAM系统不相上下的姿态估计精度，并优于最近的神经隐式SLAM方法，同时保持了更一致、高度紧凑的隐式映射，可以重建为准确和完整的网格。最后，得益于用于高效神经点索引的体素哈希和无最近点关联的快速隐式基于映射的配准，PIN-SLAM可以在中等GPU上以传感器帧速率运行。代码将在以下位置提供：https://github.com/PRBonn/PIN_SLAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09101v1" target="_blank">2401.09101v1</a>
                              </td>
                              <td>PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency</td>
                              <td>Yue Pan</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09101v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09101v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/prbonn/pin_slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2204_10993v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2204_10993v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2204_10993v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2204_10993v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper considers outdoor terrain mapping using RGB images obtained from an aerial vehicle. While feature-based localization and mapping techniques deliver real-time vehicle odometry and sparse keypoint depth reconstruction, a dense model of the environment geometry and semantics (vegetation, buildings, etc.) is usually recovered offline with significant computation and storage. This paper develops a joint 2D-3D learning approach to reconstruct a local metric-semantic mesh at each camera keyframe maintained by a visual odometry algorithm. Given the estimated camera trajectory, the local meshes can be assembled into a global environment model to capture the terrain topology and semantics during online operation. A local mesh is reconstructed using an initialization and refinement stage. In the initialization stage, we estimate the mesh vertex elevation by solving a least squares problem relating the vertex barycentric coordinates to the sparse keypoint depth measurements. In the refinement stage, we associate 2D image and semantic features with the 3D mesh vertices using camera projection and apply graph convolution to refine the mesh vertex spatial coordinates and semantic features based on joint 2D and 3D supervision. Quantitative and qualitative evaluation using real aerial images show the potential of our method to support environmental monitoring and surveillance applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2204_10993v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文考虑使用从飞行器获得的RGB图像进行户外地形测绘。虽然基于特征的定位和映射技术提供了实时车辆里程测量和稀疏关键点深度重建，但环境几何和语义（植被、建筑物等）的密集模型通常通过大量的计算和存储离线恢复。本文开发了一种2D-3D联合学习方法，以重建由视觉里程计算法维护的每个相机关键帧处的局部度量语义网格。给定估计的相机轨迹，可以将局部网格组装到全局环境模型中，以在在线操作期间捕捉地形拓扑和语义。使用初始化和细化阶段来重建局部网格。在初始化阶段，我们通过求解将顶点重心坐标与稀疏关键点深度测量值相关的最小二乘问题来估计网格顶点高程。在细化阶段，我们使用相机投影将二维图像和语义特征与三维网格顶点相关联，并基于二维和三维联合监督应用图卷积来细化网格顶点的空间坐标和语义特征。使用真实航空图像进行的定量和定性评估显示了我们的方法支持环境监测和监视应用的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2204.10993v2" target="_blank">2204.10993v2</a>
                              </td>
                              <td>TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning</td>
                              <td>Qiaojun Feng</td>
                              <td>2022-04-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2204_10993v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2204.10993v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08134v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">S3M: Semantic Segmentation Sparse Mapping for UAVs with RGB-D Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08134v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08134v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08134v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unmanned Aerial Vehicles (UAVs) hold immense potential for critical applications, such as search and rescue operations, where accurate perception of indoor environments is paramount. However, the concurrent amalgamation of localization, 3D reconstruction, and semantic segmentation presents a notable hurdle, especially in the context of UAVs equipped with constrained power and computational resources. This paper presents a novel approach to address challenges in semantic information extraction and utilization within UAV operations. Our system integrates state-of-the-art visual SLAM to estimate a comprehensive 6-DoF pose and advanced object segmentation methods at the back end. To improve the computational and storage efficiency of the framework, we adopt a streamlined voxel-based 3D map representation - OctoMap to build a working system. Furthermore, the fusion algorithm is incorporated to obtain the semantic information of each frame from the front-end SLAM task, and the corresponding point. By leveraging semantic information, our framework enhances the UAV's ability to perceive and navigate through indoor spaces, addressing challenges in pose estimation accuracy and uncertainty reduction. Through Gazebo simulations, we validate the efficacy of our proposed system and successfully embed our approach into a Jetson Xavier AGX unit for real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08134v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无人机在搜索和救援行动等关键应用中具有巨大潜力，在这些应用中，准确感知室内环境至关重要。然而，定位、3D重建和语义分割的同时融合是一个显著的障碍，尤其是在配备有限功率和计算资源的无人机的情况下。本文提出了一种新的方法来解决无人机作战中语义信息提取和利用方面的挑战。我们的系统集成了最先进的视觉SLAM来估计全面的6-DoF姿态，并在后端集成了先进的对象分割方法。为了提高框架的计算和存储效率，我们采用了一种简化的基于体素的三维地图表示——OctoMap来构建一个工作系统。此外，融合算法用于从前端SLAM任务中获得每个帧的语义信息和对应点。通过利用语义信息，我们的框架增强了无人机在室内空间感知和导航的能力，解决了姿态估计准确性和减少不确定性方面的挑战。通过Gazebo模拟，我们验证了我们提出的系统的有效性，并成功地将我们的方法嵌入到Jetson Xavier AGX单元中，用于真实世界的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08134v1" target="_blank">2401.08134v1</a>
                              </td>
                              <td>S3M: Semantic Segmentation Sparse Mapping for UAVs with RGB-D Camera</td>
                              <td>Thanh Nguyen Canh</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08134v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08134v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08132v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Object-Oriented Semantic Mapping for Reliable UAVs Navigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08132v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08132v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08132v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To autonomously navigate in real-world environments, special in search and rescue operations, Unmanned Aerial Vehicles (UAVs) necessitate comprehensive maps to ensure safety. However, the prevalent metric map often lacks semantic information crucial for holistic scene comprehension. In this paper, we proposed a system to construct a probabilistic metric map enriched with object information extracted from the environment from RGB-D images. Our approach combines a state-of-the-art YOLOv8-based object detection framework at the front end and a 2D SLAM method - CartoGrapher at the back end. To effectively track and position semantic object classes extracted from the front-end interface, we employ the innovative BoT-SORT methodology. A novel association method is introduced to extract the position of objects and then project it with the metric map. Unlike previous research, our approach takes into reliable navigating in the environment with various hollow bottom objects. The output of our system is a probabilistic map, which significantly enhances the map's representation by incorporating object-specific attributes, encompassing class distinctions, accurate positioning, and object heights. A number of experiments have been conducted to evaluate our proposed approach. The results show that the robot can effectively produce augmented semantic maps containing several objects (notably chairs and desks). Furthermore, our system is evaluated within an embedded computer - Jetson Xavier AGX unit to demonstrate the use case in real-world applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08132v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了在现实世界环境中自主导航，特别是在搜救行动中，无人机需要全面的地图来确保安全。然而，流行的度量图往往缺乏对整体场景理解至关重要的语义信息。在本文中，我们提出了一种构建概率度量图的系统，该系统富含从RGB-D图像的环境中提取的对象信息。我们的方法在前端结合了最先进的基于YOLOv8的对象检测框架，在后端结合了2D SLAM方法CartoGraper。为了有效地跟踪和定位从前端接口提取的语义对象类，我们采用了创新的BoT SORT方法。引入了一种新的关联方法来提取物体的位置，然后将其与度量图进行投影。与之前的研究不同，我们的方法考虑了在各种中空底部物体的环境中进行可靠导航。我们系统的输出是一张概率地图，它通过结合特定对象的属性，包括类别区分、准确定位和对象高度，显著增强了地图的表示。已经进行了大量实验来评估我们提出的方法。结果表明，该机器人可以有效地生成包含多个对象（尤其是椅子和桌子）的增强语义图。此外，我们的系统在嵌入式计算机Jetson Xavier AGX单元中进行了评估，以展示真实应用中的用例。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08132v1" target="_blank">2401.08132v1</a>
                              </td>
                              <td>Object-Oriented Semantic Mapping for Reliable UAVs Navigation</td>
                              <td>Thanh Nguyen Canh</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08132v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08132v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08043v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08043v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08043v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于视觉的定位是一种具有成本效益的解决方案，因此对许多智能移动平台具有吸引力。然而，其准确性和特别是鲁棒性仍然受到低照明条件、照明变化和侵略性运动的影响。基于事件的相机是受生物启发的视觉传感器，在HDR条件下表现良好，具有高时间分辨率，因此在这种具有挑战性的场景中提供了一种有趣的替代方案。虽然纯基于事件的解决方案目前还没有产生令人满意的映射结果，但目前的工作证明了如果允许替代传感器进行映射，则纯基于事件跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关场景由深度相机支持的跟踪或基于地图的定位给出，其中半密集地图由基于常规图像的视觉SLAM或来自运动系统的结构预先创建。传统的基于边缘的3D-2D对准通过利用从事件流获得的带符号时间表面图（STSM）的新颖的极性感知配准来扩展。此外，我们还介绍了一种新的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08043v1" target="_blank">2401.08043v1</a>
                              </td>
                              <td>Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</td>
                              <td>Yi-Fan Zuo</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08043v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyfff/canny-evt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07962v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cesium Tiles for High-realism Simulation and Comparing SLAM Results in Corresponding Virtual and Real-world Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07962v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07962v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07962v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article discusses the use of a simulated environment to predict algorithm results in the real world. Simulators are crucial in allowing researchers to test algorithms, sensor integration, and navigation systems without deploying expensive hardware. This article examines how the AirSim simulator, Unreal Engine, and Cesium plugin can be used to generate simulated digital twin models of real-world locations. Several technical challenges in completing the analysis are discussed and the technical solutions are detailed in this article. Work investigates how to assess mapping results for a real-life experiment using Cesium Tiles provided by digital twins of the experimental location. This is accompanied by a description of a process for duplicating real-world flights in simulation. The performance of these methods is evaluated by analyzing real-life and experimental image telemetry with the Direct Sparse Odometry (DSO) mapping algorithm. Results indicate that Cesium Tiles environments can provide highly accurate models of ground truth geometry after careful alignment. Further, results from real-life and simulated telemetry analysis indicate that the virtual simulation results accurately predict real-life results. Findings indicate that the algorithm results in real life and in the simulated duplicate exhibited a high degree of similarity. This indicates that the use of Cesium Tiles environments as a virtual digital twin for real-life experiments will provide representative results for such algorithms. The impact of this can be significant, potentially allowing expansive virtual testing of robotic systems at specific deployment locations to develop solutions that are tailored to the environment and potentially outperforming solutions meant to work in completely generic environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07962v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文讨论了在现实世界中使用模拟环境来预测算法结果。模拟器对于研究人员在不部署昂贵硬件的情况下测试算法、传感器集成和导航系统至关重要。本文研究了如何使用AirSim模拟器、虚幻引擎和Cesium插件来生成真实世界位置的模拟数字孪生模型。本文讨论了完成分析的几个技术挑战，并详细介绍了技术解决方案。这项工作调查了如何使用实验地点的数字双胞胎提供的铯砖来评估真实实验的映射结果。这还附带了一个在模拟中复制真实世界飞行的过程的描述。通过使用直接稀疏测距（DSO）映射算法分析真实和实验图像遥测，评估了这些方法的性能。结果表明，Cesium Tiles环境在仔细对准后可以提供高度准确的地面实况几何模型。此外，来自真实和模拟遥测分析的结果表明，虚拟模拟结果准确地预测了真实结果。研究结果表明，该算法在现实生活中的结果与模拟副本中的结果具有高度相似性。这表明，使用Cesium Tiles环境作为真实实验的虚拟数字孪生将为此类算法提供代表性结果。这可能会产生重大影响，有可能允许在特定部署位置对机器人系统进行广泛的虚拟测试，以开发适合环境的解决方案，并有可能优于在完全通用环境中工作的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07962v1" target="_blank">2401.07962v1</a>
                              </td>
                              <td>Cesium Tiles for High-realism Simulation and Comparing SLAM Results in Corresponding Virtual and Real-world Environments</td>
                              <td>Chris Beam</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07962v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07962v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14972v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14972v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14972v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14972v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine both the quality and the performance characteristics of modern SLMs relative to an existing customer-facing OpenAI-based implementation. We find that across 9 SLMs and 29 variants, we observe competitive quality-of-results for our use case, significant performance consistency improvement, and a cost reduction of 5x-29x when compared to OpenAI GPT-4.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14972v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多公司依靠OpenAI的GPT-4等托管人工智能模型的API在其产品中创建人工智能体验。除了易用性和缩短生产时间的好处外，这种对专有API的依赖在模型控制、性能可靠性、运行时间可预测性和成本方面也有缺点。与此同时，出现了一系列可供商业使用的开源小语言模型（SLM）。然而，它们取代现有能力的准备情况尚不清楚，而且还没有现成的系统方法来测试这些模型。在本文中，我们提出了现代开源SLM的系统评估方法和特征，以及在将专有LLM API替换为真实世界的产品功能时的权衡。我们设计了SLaM，这是一种自动化分析工具，能够利用任意SLM对产品特征进行定量和定性测试。使用SLaM，我们检查了现代SLM相对于现有面向客户的基于OpenAI的实现的质量和性能特征。我们发现，在9个SLM和29个变体中，与OpenAI GPT-4相比，我们观察到用例的结果质量具有竞争力，性能一致性显著提高，成本降低了5x-29x。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14972v2" target="_blank">2312.14972v2</a>
                              </td>
                              <td>A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production</td>
                              <td>Chandra Irugalbandara</td>
                              <td>2023-12-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14972v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14972v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07658v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robustness Evaluation of Localization Techniques for Autonomous Racing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07658v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07658v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07658v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work introduces SynPF, an MCL-based algorithm tailored for high-speed racing environments. Benchmarked against Cartographer, a state-of-the-art pose-graph SLAM algorithm, SynPF leverages synergies from previous particle-filtering methods and synthesizes them for the high-performance racing domain. Our extensive in-field evaluations reveal that while Cartographer excels under nominal conditions, it struggles when subjected to wheel-slip, a common phenomenon in a racing scenario due to varying grip levels and aggressive driving behaviour. Conversely, SynPF demonstrates robustness in these challenging conditions and a low-latency computation time of 1.25 ms on on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled autonomous racing vehicle, this work not only highlights the vulnerabilities of existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also emphasizes the potential of SynPF as a viable alternative, especially in deteriorating odometry conditions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07658v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作介绍了SynPF，一种基于MCL的算法，适用于高速比赛环境。SynPF以最先进的姿态图SLAM算法“制图器”为基准，利用了以前粒子滤波方法的协同作用，并将其合成为高性能的比赛领域。我们广泛的现场评估表明，虽然制图师在标称条件下表现出色，但在车轮打滑时却很吃力，这是比赛场景中的一种常见现象，原因是不同的抓地力水平和激进的驾驶行为。相反，SynPF在这些具有挑战性的条件下表现出了鲁棒性，并且在没有GPU的板载计算机上具有1.25ms的低延迟计算时间。这项工作使用F1TENTH平台，一种1:10比例的自动驾驶赛车，不仅突出了现有算法在高速场景中的漏洞，测试速度高达7.6米/秒，还强调了SynPF作为一种可行的替代方案的潜力，尤其是在里程计条件恶化的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07658v1" target="_blank">2401.07658v1</a>
                              </td>
                              <td>Robustness Evaluation of Localization Techniques for Autonomous Racing</td>
                              <td>Tian Yi Lim</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07658v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07658v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15169v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimotion Visual Odometry (MVO)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15169v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15169v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15169v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual motion estimation is a well-studied challenge in autonomous navigation. Recent work has focused on addressing multimotion estimation in highly dynamic environments. These environments not only comprise multiple, complex motions but also tend to exhibit significant occlusion.   Estimating third-party motions simultaneously with the sensor egomotion is difficult because an object's observed motion consists of both its true motion and the sensor motion. Most previous works in multimotion estimation simplify this problem by relying on appearance-based object detection or application-specific motion constraints. These approaches are effective in specific applications and environments but do not generalize well to the full multimotion estimation problem (MEP).   This paper presents Multimotion Visual Odometry (MVO), a multimotion estimation pipeline that estimates the full SE(3) trajectory of every motion in the scene, including the sensor egomotion, without relying on appearance-based information. MVO extends the traditional visual odometry (VO) pipeline with multimotion segmentation and tracking techniques. It uses physically founded motion priors to extrapolate motions through temporary occlusions and identify the reappearance of motions through motion closure. Evaluations on real-world data from the Oxford Multimotion Dataset (OMD) and the KITTI Vision Benchmark Suite demonstrate that MVO achieves good estimation accuracy compared to similar approaches and is applicable to a variety of multimotion estimation challenges.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15169v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉运动估计是自主导航中一个研究得很好的挑战。最近的工作集中于解决高度动态环境中的多运动估计问题。这些环境不仅包括多个复杂的运动，而且往往表现出显著的遮挡。很难同时估计第三方运动和传感器自运动，因为物体的观测运动包括其真实运动和传感器运动。先前在多运动估计中的大多数工作通过依赖于基于外观的对象检测或特定于应用程序的运动约束来简化这个问题。这些方法在特定的应用程序和环境中是有效的，但不能很好地推广到完整的多运动估计问题（MEP）。本文介绍了Multimotion Visual Odometry（MVO），这是一种多运动估计管道，它估计场景中每个运动的完整SE（3）轨迹，包括传感器自身运动，而不依赖于基于外观的信息。MVO通过多运动分割和跟踪技术扩展了传统的视觉里程计（VO）管道。它使用物理建立的运动先验来推断通过临时遮挡的运动，并通过运动闭合来识别运动的再现。对牛津多运动数据集（OMD）和KITTI Vision Benchmark Suite的真实世界数据的评估表明，与类似方法相比，MVO实现了良好的估计精度，并适用于各种多运动估计挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15169v3" target="_blank">2110.15169v3</a>
                              </td>
                              <td>Multimotion Visual Odometry (MVO)</td>
                              <td>Kevin M. Judd</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15169v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15169v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_06230v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_06230v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_06230v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_06230v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization And Mapping (C-SLAM) is a vital component for successful multi-robot operations in environments without an external positioning system, such as indoors, underground or underwater. In this paper, we introduce Swarm-SLAM, an open-source C-SLAM system that is designed to be scalable, flexible, decentralized, and sparse, which are all key properties in swarm robotics. Our system supports inertial, lidar, stereo, and RGB-D sensing, and it includes a novel inter-robot loop closure prioritization technique that reduces communication and accelerates convergence. We evaluated our ROS-2 implementation on five different datasets, and in a real-world experiment with three robots communicating through an ad-hoc network. Our code is publicly available: https://github.com/MISTLab/Swarm-SLAM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_06230v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在没有外部定位系统的环境中，如室内、地下或水下，协作式同步定位和测绘（C-SLAM）是多机器人成功操作的重要组成部分。在本文中，我们介绍了Swarm SLAM，这是一个开源的C-SLAM系统，旨在实现可扩展、灵活、分散和稀疏，这些都是群体机器人的关键特性。我们的系统支持惯性、激光雷达、立体声和RGB-D传感，并包括一种新颖的机器人间环路闭合优先级技术，该技术可以减少通信并加速收敛。我们在五个不同的数据集上评估了我们的ROS-2实现，并在三个机器人通过自组织网络通信的真实世界实验中进行了评估。我们的代码是公开的：https://github.com/MISTLab/Swarm-SLAM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.06230v3" target="_blank">2301.06230v3</a>
                              </td>
                              <td>Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems</td>
                              <td>Pierre-Yves Lajoie</td>
                              <td>2023-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_06230v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.06230v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mistlab/swarm-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06323v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06323v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06323v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06323v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present improvements to Kimera, an open-source metric-semantic visual-inertial SLAM library. In particular, we enhance Kimera-VIO, the visual-inertial odometry pipeline powering Kimera, to support better feature tracking, more efficient keyframe selection, and various input modalities (eg monocular, stereo, and RGB-D images, as well as wheel odometry). Additionally, Kimera-RPGO and Kimera-PGMO, Kimera's pose-graph optimization backends, are updated to support modern outlier rejection methods - specifically, Graduated-Non-Convexity - for improved robustness to spurious loop closures. These new features are evaluated extensively on a variety of simulated and real robotic platforms, including drones, quadrupeds, wheeled robots, and simulated self-driving cars. We present comparisons against several state-of-the-art visual-inertial SLAM pipelines and discuss strengths and weaknesses of the new release of Kimera. The newly added features have been released open-source at https://github.com/MIT-SPARK/Kimera.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06323v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了对Kimera的改进，Kimera是一个开源的度量语义视觉惯性SLAM库。特别是，我们增强了Kimera VIO，这是为Kimera提供动力的视觉惯性里程计管道，以支持更好的特征跟踪、更高效的关键帧选择和各种输入模式（如单眼、立体和RGB-D图像，以及车轮里程计）。此外，Kimera的姿势图优化后端Kimera RPGO和Kimera PGMO也进行了更新，以支持现代异常值拒绝方法，特别是分级非凸性，从而提高对虚假环路闭合的鲁棒性。这些新功能在各种模拟和真实的机器人平台上进行了广泛评估，包括无人机、四足动物、轮式机器人和模拟自动驾驶汽车。我们将与几种最先进的视觉惯性SLAM管道进行比较，并讨论新版本Kimera的优势和劣势。新添加的功能已在上开源发布https://github.com/MIT-SPARK/Kimera.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06323v1" target="_blank">2401.06323v1</a>
                              </td>
                              <td>Kimera2: Robust and Accurate Metric-Semantic SLAM in the Real World</td>
                              <td>Marcus Abate</td>
                              <td>2024-01-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06323v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13182v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Camera Visual-Inertial Simultaneous Localization and Mapping for Autonomous Valet Parking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13182v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13182v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13182v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Localization and mapping are key capabilities for self-driving vehicles. In this paper, we build on Kimera and extend it to use multiple cameras as well as external (eg wheel) odometry sensors, to obtain accurate and robust odometry estimates in real-world problems. Additionally, we propose an effective scheme for closing loops that circumvents the drawbacks of common alternatives based on the Perspective-n-Point method and also works with a single monocular camera. Finally, we develop a method for dense 3D mapping of the free space that combines a segmentation network for free-space detection with a homography-based dense mapping technique. We test our system on photo-realistic simulations and on several real datasets collected on a car prototype developed by the Ford Motor Company, spanning both indoor and outdoor parking scenarios. Our multi-camera system is shown to outperform state-of-the art open-source visual-inertial-SLAM pipelines (Vins-Fusion, ORB-SLAM3), and exhibits an average trajectory error under 1% of the trajectory length across more than 8km of distance traveled (combined across all datasets). A video showcasing the system is available at: youtu.be/H8CpzDpXOI8.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13182v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>定位和地图绘制是自动驾驶汽车的关键功能。在本文中，我们建立在Kimera的基础上，并将其扩展到使用多个摄像头以及外部（如车轮）里程计传感器，以在现实世界的问题中获得准确和稳健的里程计估计。此外，我们提出了一种有效的闭环方案，该方案绕过了基于透视n-Point方法的常见替代方案的缺点，也适用于单个单眼相机。最后，我们开发了一种用于自由空间的密集3D映射的方法，该方法将用于自由空间检测的分割网络与基于单应性的密集映射技术相结合。我们在逼真的照片模拟和福特汽车公司开发的汽车原型上收集的几个真实数据集上测试了我们的系统，涵盖了室内和室外停车场景。我们的多摄像头系统显示出优于最先进的开源视觉惯性SLAM管道（Vins Fusion，ORB-SLAM3），并且在超过8km的行进距离上（在所有数据集上组合），平均轨迹误差低于轨迹长度的1%。展示该系统的视频可在以下网址获取：youtu.be/H8CpzDpXOI8。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13182v3" target="_blank">2304.13182v3</a>
                              </td>
                              <td>Multi-Camera Visual-Inertial Simultaneous Localization and Mapping for Autonomous Valet Parking</td>
                              <td>Marcus Abate</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13182v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13182v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05836v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On State Estimation in Multi-Sensor Fusion Navigation: Optimization and Filtering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05836v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05836v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05836v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The essential of navigation, perception, and decision-making which are basic tasks for intelligent robots, is to estimate necessary system states. Among them, navigation is fundamental for other upper applications, providing precise position and orientation, by integrating measurements from multiple sensors. With observations of each sensor appropriately modelled, multi-sensor fusion tasks for navigation are reduced to the state estimation problem which can be solved by two approaches: optimization and filtering. Recent research has shown that optimization-based frameworks outperform filtering-based ones in terms of accuracy. However, both methods are based on maximum likelihood estimation (MLE) and should be theoretically equivalent with the same linearization points, observation model, measurements, and Gaussian noise assumption. In this paper, we deeply dig into the theories and existing strategies utilized in both optimization-based and filtering-based approaches. It is demonstrated that the two methods are equal theoretically, but this equivalence corrupts due to different strategies applied in real-time operation. By adjusting existing strategies of the filtering-based approaches, the Monte-Carlo simulation and vehicular ablation experiments based on visual odometry (VO) indicate that the strategy adjusted filtering strictly equals to optimization. Therefore, future research on sensor-fusion problems should concentrate on their own algorithms and strategies rather than state estimation approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05836v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>导航、感知和决策是智能机器人的基本任务，其本质是估计必要的系统状态。其中，导航是其他上层应用程序的基础，通过集成来自多个传感器的测量，提供精确的位置和方向。通过对每个传感器的观测值进行适当的建模，将导航的多传感器融合任务简化为状态估计问题，该问题可以通过两种方法解决：优化和滤波。最近的研究表明，基于优化的框架在准确性方面优于基于过滤的框架。然而，这两种方法都是基于最大似然估计（MLE）的，并且在理论上应该与相同的线性化点、观测模型、测量和高斯噪声假设等效。在本文中，我们深入挖掘了基于优化和基于过滤的方法中使用的理论和现有策略。结果表明，这两种方法在理论上是相等的，但由于在实时操作中应用的策略不同，这种等价性会破坏。通过调整现有的基于滤波的方法的策略，基于视觉里程计（VO）的蒙特卡洛模拟和车载消融实验表明，策略调整后的滤波严格等于优化。因此，未来对传感器融合问题的研究应该集中在它们自己的算法和策略上，而不是状态估计方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05836v1" target="_blank">2401.05836v1</a>
                              </td>
                              <td>On State Estimation in Multi-Sensor Fusion Navigation: Optimization and Filtering</td>
                              <td>Feng Zhu</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05836v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05836v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05152v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05152v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05152v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative Simultaneous Localization and Mapping (CSLAM) is critical to enable multiple robots to operate in complex environments. Most CSLAM techniques rely on raw sensor measurement or low-level features such as keyframe descriptors, which can lead to wrong loop closures due to the lack of deep understanding of the environment. Moreover, the exchange of these measurements and low-level features among the robots requires the transmission of a significant amount of data, which limits the scalability of the system. To overcome these limitations, we present Multi S-Graphs, a decentralized CSLAM system that utilizes high-level semantic-relational information embedded in the four-layered hierarchical and optimizable situational graphs for cooperative map generation and localization while minimizing the information exchanged between the robots. To support this, we present a novel room-based descriptor which, along with its connected walls, is used to perform inter-robot loop closures, addressing the challenges of multi-robot kidnapped problem initialization. Multiple experiments in simulated and real environments validate the improvement in accuracy and robustness of the proposed approach while reducing the amount of data exchanged between robots compared to other state-of-the-art approaches.   Software available within a docker image: https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05152v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>协作式同时定位和映射（CSLAM）对于使多个机器人能够在复杂环境中操作至关重要。大多数CSLAM技术依赖于原始传感器测量或关键帧描述符等低级特征，由于缺乏对环境的深入了解，这可能导致错误的环路闭合。此外，在机器人之间交换这些测量值和低级特征需要传输大量数据，这限制了系统的可扩展性。为了克服这些限制，我们提出了Multi-S-Graphs，这是一种去中心化的CSLAM系统，它利用嵌入四层分层和可优化的态势图中的高级语义关系信息进行协作地图生成和定位，同时最大限度地减少机器人之间的信息交换。为了支持这一点，我们提出了一种新的基于房间的描述符，该描述符及其连接的墙用于执行机器人间环路闭合，解决了多机器人绑架问题初始化的挑战。在模拟和真实环境中进行的多次实验验证了所提出的方法在准确性和稳健性方面的改进，同时与其他最先进的方法相比，减少了机器人之间交换的数据量。docker映像中可用的软件：https://github.com/snt-arg/multi_s_graphs_docker</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05152v1" target="_blank">2401.05152v1</a>
                              </td>
                              <td>Multi S-Graphs: an Efficient Real-time Distributed Semantic-Relational Collaborative SLAM</td>
                              <td>Miguel Fernandez-Cortizas</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05152v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05152v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/snt-arg/multi_s_graphs_docker" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01657v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01657v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01657v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The back-end module of Distributed Collaborative Simultaneous Localization and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO) under a distributed setting, also known as SE(d)-synchronization. Most existing distributed graph optimization algorithms employ a simple sequential partitioning scheme, which may result in unbalanced subgraph dimensions due to the different geographic locations of each robot, and hence imposes extra communication load. Moreover, the performance of current Riemannian optimization algorithms can be further accelerated. In this letter, we propose a novel distributed pose graph optimization algorithm combining multi-level partitioning with an accelerated Riemannian optimization method. Firstly, we employ the multi-level graph partitioning algorithm to preprocess the naive pose graph to formulate a balanced optimization problem. In addition, inspired by the accelerated coordinate descent method, we devise an Improved Riemannian Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is globally optimal. Finally, we evaluate the effects of four common graph partitioning approaches on the correlation of the inter-subgraphs, and discover that the Highest scheme has the best partitioning performance. Also, we implement simulations to quantitatively demonstrate that our proposed algorithm outperforms the state-of-the-art distributed pose graph optimization protocols.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01657v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布式协同同步定位与映射（DCSLAM）的后端模块需要在分布式环境下求解非线性姿态图优化（PGO），也称为SE（d）-同步。大多数现有的分布式图优化算法都采用了简单的顺序划分方案，由于每个机器人的地理位置不同，这可能会导致子图维度不平衡，从而增加额外的通信负载。此外，当前黎曼优化算法的性能可以进一步提高。在这封信中，我们提出了一种新的分布式位姿图优化算法，该算法将多级划分与加速黎曼优化方法相结合。首先，我们采用多级图分割算法对初始姿态图进行预处理，以形成一个平衡优化问题。此外，受加速坐标下降法的启发，我们设计了一种改进的黎曼块坐标下降（IRBCD）算法，得到的临界点是全局最优的。最后，我们评估了四种常见的图划分方法对子图间相关性的影响，发现Highest方案具有最好的划分性能。此外，我们还进行了仿真，定量地证明了我们提出的算法优于最先进的分布式姿态图优化协议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01657v2" target="_blank">2401.01657v2</a>
                              </td>
                              <td>Distributed Pose-graph Optimization with Multi-level Partitioning for Collaborative SLAM</td>
                              <td>Cunhao Li</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01657v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01657v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tjcunhao/distributed-pose-graph" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04791v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04791v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04791v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel framework for open-set Simultaneous Localization and Mapping (SLAM) in unstructured environments that uses segmentation to create a map of objects and geometric relationships between objects for localization. Our system consists of 1) a front-end mapping pipeline using a zero-shot segmentation model to extract object masks from images and track them across frames to generate an object-based map and 2) a frame alignment pipeline that uses the geometric consistency of objects to efficiently localize within maps taken in a variety of conditions. This approach is shown to be more robust to changes in lighting and appearance than traditional feature-based SLAM systems or global descriptor methods. This is established by evaluating SOS-SLAM on the Batvik seasonal dataset which includes drone flights collected over a coastal plot of southern Finland during different seasons and lighting conditions. Across flights during varying environmental conditions, our approach achieves higher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes within a reference map up to 14x faster than other feature based approaches and has a map size less than 0.4% the size of the most compact other maps. When considering localization performance from varying viewpoints, our approach outperforms all benchmarks from the same viewpoint and most benchmarks from different viewpoints. SOS-SLAM is a promising new approach for SLAM in unstructured environments that is robust to changes in lighting and appearance and is more computationally efficient than other approaches. We release our code and datasets: https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04791v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种在非结构化环境中用于开放集同时定位和映射（SLAM）的新框架，该框架使用分割来创建用于定位的对象和对象之间的几何关系的映射。我们的系统由1）前端映射管道和2）帧对齐管道组成，前者使用零样本分割模型从图像中提取对象掩码，并在帧间跟踪它们以生成基于对象的地图，后者使用对象的几何一致性在各种条件下拍摄的地图内有效定位。与传统的基于特征的SLAM系统或全局描述符方法相比，这种方法对照明和外观的变化更具鲁棒性。这是通过评估巴特维克季节数据集上的SOS-SLAM来建立的，该数据集包括在不同季节和光照条件下在芬兰南部沿海地区收集的无人机飞行。在不同环境条件下的飞行中，我们的方法实现了比基准方法更高的召回率，精度为1.0。SOS-SLAM在参考地图内的定位速度比其他基于特征的方法快14倍，并且地图大小小于最紧凑的其他地图大小的0.4%。当从不同的角度考虑本地化性能时，我们的方法优于来自同一角度的所有基准测试和来自不同角度的大多数基准测试。SOS-SLAM是非结构化环境中SLAM的一种很有前途的新方法，它对光照和外观的变化具有鲁棒性，并且在计算上比其他方法更高效。我们发布我们的代码和数据集：https://acl.mit.edu/SOS-SLAM/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04791v1" target="_blank">2401.04791v1</a>
                              </td>
                              <td>SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments</td>
                              <td>Jouko Kinnari</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04791v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04791v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03398v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03398v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03398v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>AI and robotics technologies have witnessed remarkable advancements in the past decade, revolutionizing work patterns and opportunities in various domains. The application of these technologies has propelled society towards an era of symbiosis between humans and machines. To facilitate efficient communication between humans and intelligent robots, we propose the "Avatar" system, an immersive low-latency panoramic human-robot interaction platform. We have designed and tested a prototype of a rugged mobile platform integrated with edge computing units, panoramic video capture devices, power batteries, robot arms, and network communication equipment. Under favorable network conditions, we achieved a low-latency high-definition panoramic visual experience with a delay of 357ms. Operators can utilize VR headsets and controllers for real-time immersive control of robots and devices. The system enables remote control over vast physical distances, spanning campuses, provinces, countries, and even continents (New York to Shenzhen). Additionally, the system incorporates visual SLAM technology for map and trajectory recording, providing autonomous navigation capabilities. We believe that this intuitive system platform can enhance efficiency and situational experience in human-robot collaboration, and with further advancements in related technologies, it will become a versatile tool for efficient and symbiotic cooperation between AI and humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03398v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能和机器人技术在过去十年中取得了显著进步，改变了各个领域的工作模式和机会。这些技术的应用将社会推向了一个人与机器共生的时代。为了促进人类与智能机器人之间的高效通信，我们提出了“阿凡达”系统，这是一个沉浸式低延迟全景人机交互平台。我们设计并测试了一个坚固的移动平台原型，该平台集成了边缘计算单元、全景视频捕获设备、动力电池、机械臂和网络通信设备。在良好的网络条件下，我们实现了延迟357ms的低延迟高清全景视觉体验。操作员可以利用VR耳机和控制器对机器人和设备进行实时沉浸式控制。该系统能够实现跨越校园、省份、国家甚至大洲（纽约到深圳）的远距离远程控制。此外，该系统结合了用于地图和轨迹记录的视觉SLAM技术，提供了自主导航功能。我们相信，这个直观的系统平台可以提高人机协作的效率和情景体验，随着相关技术的进一步进步，它将成为人工智能与人类高效共生合作的通用工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03398v2" target="_blank">2401.03398v2</a>
                              </td>
                              <td>Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system</td>
                              <td>Junjie Li</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03398v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03398v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_11598v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_11598v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_11598v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_11598v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习神经隐式表示在多视图图像的三维重建中取得了显著的性能。当前的方法使用体积渲染将隐式表示渲染为RGB或深度图像，这些图像由多视图地面实况监督。然而，每次渲染视图都会遇到孔的深度不完整以及深度监督对遮挡结构的不了解，这严重影响了通过体绘制进行几何推断的准确性。为了解决这个问题，我们建议通过具有注意深度融合先验的体绘制，从多视图RGBD图像中学习神经隐式表示。我们的先验允许神经网络从从可用于渲染的所有深度图像中融合的截断有符号距离函数（TSDF）中感知粗略的3D结构。TSDF能够访问一个深度图像上孔的缺失深度以及当前视图中不可见的遮挡部分。通过引入一种新的注意力机制，我们允许神经网络直接使用具有推断占用率的深度融合先验作为学习的隐函数。我们的注意力机制与表示整个场景的一次性融合TSDF或表示同步定位和映射（SLAM）上下文中的部分场景的增量融合TSDF一起工作。我们对广泛使用的基准（包括合成扫描和真实世界扫描）的评估表明，我们优于最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.11598v2" target="_blank">2310.11598v2</a>
                              </td>
                              <td>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</td>
                              <td>Pengchong Hu</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_11598v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.11598v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03604v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03604v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03604v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Inertial Odometry (VIO) algorithms estimate the accurate camera trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The applications of VIO span a diverse range, including augmented reality and indoor navigation. VIO algorithms hold the potential to facilitate navigation for visually impaired individuals in both indoor and outdoor settings. Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges in dynamic environments, particularly in densely populated corridors. Existing VIO datasets, e.g., ADVIO, typically fail to effectively exploit these challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI) to address the mentioned problem and improve the navigation systems. AUT-VI is a novel and super-challenging dataset with 126 diverse sequences in 17 different locations. This dataset contains dynamic objects, challenging loop-closure/map-reuse, different lighting conditions, reflections, and sudden camera movements to cover all extreme navigation scenarios. Moreover, in support of ongoing development efforts, we have released the Android application for data capture to the public. This allows fellow researchers to easily capture their customized VIO dataset variations. In addition, we evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry (VO) methods on our dataset, emphasizing the essential need for this challenging dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03604v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性里程计（VIO）算法通过使用相机和惯性测量单元（IMU）传感器来估计精确的相机轨迹。VIO的应用范围广泛，包括增强现实和室内导航。VIO算法有可能促进视障人士在室内和室外环境中的导航。然而，最先进的VIO算法在动态环境中，特别是在人口稠密的走廊中，遇到了巨大的挑战。现有的VIO数据集，例如ADVIO，通常无法有效利用这些挑战。在本文中，我们引入了Amirkabir校园数据集（AUT-VI）来解决上述问题并改进导航系统。AUT-VI是一个新颖且极具挑战性的数据集，包含17个不同位置的126个不同序列。该数据集包含动态对象、具有挑战性的回路闭合/地图重用、不同的照明条件、反射和相机突然移动，以覆盖所有极端导航场景。此外，为了支持正在进行的开发工作，我们向公众发布了用于数据捕获的Android应用程序。这使得其他研究人员能够轻松地捕捉他们定制的VIO数据集变体。此外，我们在数据集上评估了最先进的视觉惯性里程计（VIO）和视觉里程计（VO）方法，强调了对这一具有挑战性的数据集的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03604v1" target="_blank">2401.03604v1</a>
                              </td>
                              <td>Amirkabir campus dataset: Real-world challenges and scenarios of Visual Inertial Odometry (VIO) for visually impaired people</td>
                              <td>Ali Samadzadeh</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03604v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03604v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/A3DV/VIRec" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02816v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02816v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02816v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we conducted a comparative evaluation of three RGB-D SLAM (Simultaneous Localization and Mapping) algorithms: RTAB-Map, ORB-SLAM3, and OpenVSLAM for SURENA-V humanoid robot localization and mapping. Our test involves the robot to follow a full circular pattern, with an Intel RealSense D435 RGB-D camera installed on its head. In assessing localization accuracy, ORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map at 0.1641 and OpenVSLAM at 0.1847. However, it should be noted that both ORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when the robot encountered a wall with limited feature points. Nevertheless, OpenVSLAM demonstrated the ability to detect loop closures and successfully relocalize itself within the map when the robot approached its initial location. The investigation also extended to mapping capabilities, where RTAB-Map excelled by offering diverse mapping outputs, including dense, OctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM provided only sparse maps.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02816v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对用于SURENA-V人形机器人定位和映射的三种RGB-D SLAM（同时定位和映射）算法：RTAB-Map、ORB-SLAM3和OpenVSLAM进行了比较评估。我们的测试涉及机器人遵循全圆形模式，其头部安装了Intel RealSense D435 RGB-D相机。在评估定位精度方面，ORB-SLAM3的ATE为0.1073，优于其他公司，其次是RTAB-Map，为0.1641，OpenVSLAM为0.1847。然而，应该注意的是，当机器人遇到具有有限特征点的墙壁时，ORB-SLAM3和OpenVSLAM在保持准确的里程测量方面都面临挑战。尽管如此，OpenVSLAM展示了检测环路闭合的能力，并在机器人接近其初始位置时成功地在地图内重新定位自己。调查还扩展到了地图功能，RTAB Map在地图功能方面表现出色，提供了多种地图输出，包括密集、OctoMap和占用网格地图。相比之下，ORB-SLAM3和OpenVSLAM都只提供了稀疏映射。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02816v1" target="_blank">2401.02816v1</a>
                              </td>
                              <td>Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot Localization and Mapping</td>
                              <td>Amirhosein Vedadi</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02816v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02816v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01081v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01081v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01081v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual-inertial SLAM is crucial in various fields, such as aerial vehicles, industrial robots, and autonomous driving. The fusion of camera and inertial measurement unit (IMU) makes up for the shortcomings of a signal sensor, which significantly improves the accuracy and robustness of localization in challenging environments. This article presents PLE-SLAM, an accurate and real-time visual-inertial SLAM algorithm based on point-line features and efficient IMU initialization. First, we use parallel computing methods to extract features and compute descriptors to ensure real-time performance. Adjacent short line segments are merged into long line segments, and isolated short line segments are directly deleted. Second, a rotation-translation-decoupled initialization method is extended to use both points and lines. Gyroscope bias is optimized by tightly coupling IMU measurements and image observations. Accelerometer bias and gravity direction are solved by an analytical method for efficiency. To improve the system's intelligence in handling complex environments, a scheme of leveraging semantic information and geometric constraints to eliminate dynamic features and A solution for loop detection and closed-loop frame pose estimation using CNN and GNN are integrated into the system. All networks are accelerated to ensure real-time performance. The experiment results on public datasets illustrate that PLE-SLAM is one of the state-of-the-art visual-inertial SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01081v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉惯性SLAM在飞行器、工业机器人和自动驾驶等各个领域都至关重要。相机和惯性测量单元（IMU）的融合弥补了信号传感器的不足，显著提高了在具有挑战性的环境中定位的准确性和鲁棒性。本文提出了一种基于点线特征和有效IMU初始化的精确实时视觉惯性SLAM算法PLE-SLAM。首先，我们使用并行计算方法来提取特征并计算描述符，以确保实时性能。相邻的短线段合并为长线段，孤立的短线段直接删除。其次，将旋转-平移解耦初始化方法扩展为同时使用点和线。陀螺仪偏置通过紧密耦合IMU测量和图像观测进行优化。加速度计偏置和重力方向通过效率分析方法求解。为了提高系统在处理复杂环境中的智能性，将一种利用语义信息和几何约束来消除动态特征的方案以及一种使用CNN和GNN的环路检测和闭环帧姿态估计的解决方案集成到系统中。所有网络都被加速以确保实时性能。在公共数据集上的实验结果表明，PLE-SLAM是最先进的视觉惯性SLAM系统之一。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01081v2" target="_blank">2401.01081v2</a>
                              </td>
                              <td>PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and Efficient IMU Initialization</td>
                              <td>Jiaming He</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01081v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01081v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hjmgarmin/ple-slam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11700v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11700v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11700v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11700v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了$\textbf{GS-SLAM}$，它首先在同步定位和映射（SLAM）系统中使用3D高斯表示。它有助于更好地平衡效率和准确性。与最近使用神经隐式表示的SLAM方法相比，我们的方法使用了实时可微分的飞溅渲染管道，大大加快了地图优化和RGB-D重新渲染的速度。具体而言，我们提出了一种自适应扩展策略，该策略添加新的或删除有噪声的3D高斯，以有效地重建新的观测场景几何结构并改进先前观测区域的映射。该策略对于扩展3D高斯表示以重建整个场景而不是在现有方法中合成静态对象至关重要。此外，在姿态跟踪过程中，设计了一种有效的从粗到细的技术来选择可靠的3D高斯表示来优化相机姿态，从而减少了运行时间并实现了稳健的估计。与Replica、TUM-RGBD数据集上现有的最先进的实时方法相比，我们的方法实现了具有竞争力的性能。源代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11700v3" target="_blank">2311.11700v3</a>
                              </td>
                              <td>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</td>
                              <td>Chi Yan</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11700v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11700v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01887v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01887v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01887v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01887v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉里程计基于视觉输入来估计移动的相机的运动。现有的方法大多侧重于双视点跟踪，往往忽略了图像序列中丰富的时间上下文，从而忽略了全局运动模式，并且没有提供对完整轨迹可靠性的评估。这些缺点阻碍了在具有遮挡、动态对象和低纹理区域的场景中的性能。为了应对这些挑战，我们提出了长期有效的任意点跟踪（LEAP）模块。LEAP创新地将视觉、轨迹间和时间线索与精心选择的锚点相结合，用于动态轨迹估计。此外，LEAP的时间概率公式将分布更新集成到可学习的迭代精化模块中，以推理逐点不确定性。基于这些特点，我们开发了LEAP-VO，这是一种强大的视觉里程计系统，擅长处理遮挡和动态场景。我们的专注集成展示了一种新颖的做法，将长期点跟踪作为前端。大量实验表明，在各种视觉里程计基准测试中，所提出的管道显著优于现有基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01887v1" target="_blank">2401.01887v1</a>
                              </td>
                              <td>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</td>
                              <td>Weirong Chen</td>
                              <td>2024-01-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01887v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01887v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2312_10109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-light image enhancement is a crucial visual task, and many unsupervised methods tend to overlook the degradation of visible information in low-light scenes, which adversely affects the fusion of complementary information and hinders the generation of satisfactory results. To address this, our study introduces "Enlighten-Your-Voice", a multimodal enhancement framework that innovatively enriches user interaction through voice and textual commands. This approach does not merely signify a technical leap but also represents a paradigm shift in user engagement. Our model is equipped with a Dual Collaborative Attention Module (DCAM) that meticulously caters to distinct content and color discrepancies, thereby facilitating nuanced enhancements. Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play module that synergizes semantic context with low-light enhancement operations, sharpening the algorithm's efficacy. Crucially, "Enlighten-Your-Voice" showcases remarkable generalization in unsupervised zero-shot scenarios. The source code can be accessed from https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱光图像增强是一项至关重要的视觉任务，许多无监督方法往往忽略了弱光场景中可见信息的退化，这对互补信息的融合产生了不利影响，并阻碍了令人满意的结果的产生。为了解决这一问题，我们的研究引入了“启发你的声音”，这是一个多模式增强框架，通过语音和文本命令创新地丰富了用户交互。这种方法不仅意味着技术上的飞跃，而且代表着用户参与度的范式转变。我们的模型配备了双协作注意力模块（DCAM），该模块可精心处理不同的内容和颜色差异，从而促进细微的增强。作为补充，我们引入了一个语义特征融合（SFM）即插即用模块，该模块将语义上下文与弱光增强操作协同，提高了算法的有效性。至关重要的是，“启蒙青年之声”在无监督的零样本场景中展示了非凡的泛化能力。源代码可以从https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10109v2" target="_blank">2312.10109v2</a>
                              </td>
                              <td>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</td>
                              <td>Xiaofeng Zhang</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhangbaijin/enlighten-your-voice" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17592v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Feature Matching Using Deep Learning: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17592v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17592v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17592v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prevalent datasets and metrics to facilitate a quantitative comparison of state-of-the-art techniques. The paper also explores the practical application of local feature matching in diverse domains such as Structure from Motion, Remote Sensing Image Registration, and Medical Image Registration, underscoring its versatility and significance across various fields. Ultimately, we endeavor to outline the current challenges faced in this domain and furnish future research directions, thereby serving as a reference for researchers involved in local feature matching and its interconnected domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17592v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配在计算机视觉领域有着广泛的应用，包括图像检索、三维重建和物体识别等领域。然而，由于视点和照明变化等因素，在提高匹配的准确性和稳健性方面仍然存在挑战。近年来，深度学习模型的引入引发了对局部特征匹配技术的广泛探索。这项工作的目的是提供局部特征匹配方法的全面概述。根据探测器的存在，这些方法分为两个关键部分。基于检测器的类别包括检测然后描述、联合检测和描述、描述然后检测以及基于图的技术等模型。相比之下，无检测器类别包括基于CNN、基于转换器和基于补丁的方法。我们的研究超越了方法分析，结合了对流行数据集和指标的评估，以促进对最先进技术的定量比较。本文还探讨了局部特征匹配在运动结构、遥感图像配准和医学图像配准等不同领域的实际应用，强调了其在各个领域的通用性和意义。最终，我们努力概述该领域当前面临的挑战，并提供未来的研究方向，从而为参与局部特征匹配及其互连领域的研究人员提供参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17592v1" target="_blank">2401.17592v1</a>
                              </td>
                              <td>Local Feature Matching Using Deep Learning: A Survey</td>
                              <td>Shibiao Xu</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17592v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17592v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中，物体的定位是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v3" target="_blank">2304.07250v3</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion显著优于经典的SfM管道和学习的方法。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v4" target="_blank">2306.15667v4</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14289v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Speech foundation models on intelligibility prediction for hearing-impaired listeners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14289v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14289v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音基础模型（SFM）已经在许多语音处理任务上进行了基准测试，通常以最小的适应度实现最先进的性能。然而，对于语音感知社区感兴趣的应用，SFM范式的探索明显较少。在本文中，我们对10个SFM的一个应用进行了系统评估：语音可懂度预测。我们专注于清晰度预测挑战2（CPC2）的非侵入性设置，其中的任务是预测听障听众从噪声记录中的语音中正确感知的单词的百分比。我们提出了一种简单的方法，在冻结的SFM上学习轻量级的专业预测头来解决这个问题。我们的研究结果显示，SFM之间的性能存在统计学上的显著差异。我们的方法在CPC2中获胜，证明了它在语音感知应用中的前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14289v1" target="_blank">2401.14289v1</a>
                              </td>
                              <td>Speech foundation models on intelligibility prediction for hearing-impaired listeners</td>
                              <td>Santiago Cuervo</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14289v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14289v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11711v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11711v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11711v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11711v1" target="_blank">2401.11711v1</a>
                              </td>
                              <td>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</td>
                              <td>Zelin Gao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11711v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11711v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10886v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10886v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10886v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从场景的两个或多个视图中提取点对应关系是一个基本的计算机视觉问题，对于从运动中估计相对相机姿态和结构具有特别重要的意义。现有的局部特征匹配方法，在大规模数据集上进行对应监督训练，可以在测试集上获得高度准确的匹配。然而，与经典的特征提取器不同，它们不能很好地推广到与训练对象具有不同特征的新数据集。相反，它们需要微调，这假设地面实况对应关系或地面实况摄像机姿态和3D结构是可用的。我们通过取消3D结构（例如深度图或点云）的要求来放松这一假设，并且只需要相机姿态信息，这些信息可以从里程计中获得。我们通过用核极损失代替对应损失来做到这一点，这鼓励假定的匹配位于相关的核极线上。虽然弱于对应监督，但我们观察到，这一提示足以在新数据上微调现有模型。然后，我们通过在一种新颖的自举方法中使用姿势估计，进一步放宽了已知相机姿势的假设。我们在极具挑战性的数据集上进行评估，包括室内无人机数据集和室外智能手机相机数据集，并在没有强有力监督的情况下获得最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10886v1" target="_blank">2401.10886v1</a>
                              </td>
                              <td>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</td>
                              <td>Dominik A. Kloepfer</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10886v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10886v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09252v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09252v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and discuss commonly adopted datasets and figures of merit indicated for each purpose and list recent results for completeness. We conclude this paper by pointing out current and future trends.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09252v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对基于在全向光学系统下捕获的单个、两个或多个图像的先驱和最先进的3D场景几何估计方法进行了全面的综述。我们首先回顾了球面相机模型的基本概念，并回顾了适用于全向（也称为360$^\circ$，球面或全景）图像和视频的最常见的采集技术和表示格式。然后，我们调查了单目布局和深度推理方法，重点介绍了适用于球形数据的基于学习的解决方案的最新进展。然后在球面域上修改经典的立体匹配，其中检测和描述稀疏和密集特征的方法变得至关重要。然后，将立体匹配概念外推到多视图相机设置中，将其分类为光场、多视图立体和运动结构（或视觉同时定位和映射）。我们还汇编和讨论了常用的数据集和针对每种目的指出的优缺点，并列出了最新的结果以确保完整性。最后，我们指出了当前和未来的趋势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09252v1" target="_blank">2401.09252v1</a>
                              </td>
                              <td>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</td>
                              <td>Thiago Lopes Trugillo da Silveira</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09252v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09252v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08937v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08937v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08937v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08937v1" target="_blank">2401.08937v1</a>
                              </td>
                              <td>ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</td>
                              <td>Weiyao Wang</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08937v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08043v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08043v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08043v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于视觉的定位是一种具有成本效益的解决方案，因此对许多智能移动平台具有吸引力。然而，其准确性和特别是鲁棒性仍然受到低照明条件、照明变化和侵略性运动的影响。基于事件的相机是受生物启发的视觉传感器，在HDR条件下表现良好，具有高时间分辨率，因此在这种具有挑战性的场景中提供了一种有趣的替代方案。虽然纯基于事件的解决方案目前还没有产生令人满意的映射结果，但目前的工作证明了如果允许替代传感器进行映射，则纯基于事件跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关场景由深度相机支持的跟踪或基于地图的定位给出，其中半密集地图由基于常规图像的视觉SLAM或来自运动系统的结构预先创建。传统的基于边缘的3D-2D对准通过利用从事件流获得的带符号时间表面图（STSM）的新颖的极性感知配准来扩展。此外，我们还介绍了一种新的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08043v1" target="_blank">2401.08043v1</a>
                              </td>
                              <td>Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</td>
                              <td>Yi-Fan Zuo</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08043v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyfff/canny-evt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface. Our software is available on GitHub as an open-source ROS package.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用钻爆法进行隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测和测量任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，并且没有考虑动态障碍物。为了最大限度地提高自主性水平，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法使用分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级别的局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。我们的软件在GitHub上作为开源ROS包提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v3" target="_blank">2301.08422v3</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhefan-xu/cerlab-uav-autonomy" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索通过SfM从红外图像中进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v2" target="_blank">2304.03930v2</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05236v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05236v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our world is full of identical objects (\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05236v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的世界充满了相同的物体（例如，可乐罐、同一型号的汽车）。当这些重复出现在一起时，为我们有效地推理3D提供了额外而有力的线索。受这一观察结果的启发，我们引入了“重复结构”（SfD），这是一种新颖的逆图形框架，可以从包含多个相同对象的单个图像中重建几何体、材料和照明。SfD首先识别图像中对象的多个实例，然后联合估计所有实例的6DoF姿势。随后使用反向图形管道来联合推理对象的形状、材质和环境光，同时遵守实例之间的共享几何图形和材质约束。我们的主要贡献包括利用对象副本作为单图像逆图形的鲁棒先验，并提出用于联合6-DoF对象姿态估计的平面内旋转鲁棒运动结构（SfM）公式。通过利用来自单个图像的多视图线索，SfD生成了更真实、更详细的3D重建，显著优于具有相似或更多观测值的现有单个图像重建模型和多视图重建方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05236v1" target="_blank">2401.05236v1</a>
                              </td>
                              <td>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</td>
                              <td>Tianhang Cheng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05236v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05236v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tianhang-cheng/sfd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03450v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Classification of Critical Configurations for any Number of Projective Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03450v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03450v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是从一组图像中恢复有关相机和3D场景的信息的过程。通常，在无噪声设置中，如果提供了足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，也不可能进行独特的恢复；这些被称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明了它们形成了众所周知的代数变体，如二次曲面和次数最多为4的曲线。本文还改进了早期的结果，发现了以前未知的关键构型，并表明一些以前认为是关键的构型实际上不是。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03450v1" target="_blank">2401.03450v1</a>
                              </td>
                              <td>A Classification of Critical Configurations for any Number of Projective Views</td>
                              <td>Martin Bråtelund</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03450v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03450v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mabraate/critical-configurations" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_11153v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Research on Multilingual Natural Scene Text Detection Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_11153v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural scene text detection is a significant challenge in computer vision, with tremendous potential applications in multilingual, diverse, and complex text scenarios. We propose a multilingual text detection model to address the issues of low accuracy and high difficulty in detecting multilingual text in natural scenes. In response to the challenges posed by multilingual text images with multiple character sets and various font styles, we introduce the SFM Swin Transformer feature extraction network to enhance the model's robustness in detecting characters and fonts across different languages. Dealing with the considerable variation in text scales and complex arrangements in natural scene text images, we present the AS-HRFPN feature fusion network by incorporating an Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module. The feature fusion network improvements enhance the model's ability to detect text sizes and orientations. Addressing diverse backgrounds and font variations in multilingual scene text images is a challenge for existing methods. Limited local receptive fields hinder detection performance. To overcome this, we propose a Global Semantic Segmentation Branch, extracting and preserving global features for more effective text detection, aligning with the need for comprehensive information. In this study, we collected and built a real-world multilingual natural scene text image dataset and conducted comprehensive experiments and analyses. The experimental results demonstrate that the proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher than the baseline model. We also conducted extensive cross-dataset validation on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of our approach. The code and dataset can be found at https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_11153v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然场景文本检测是计算机视觉中的一个重大挑战，在多语言、多样化和复杂的文本场景中具有巨大的潜在应用。我们提出了一种多语言文本检测模型，以解决在自然场景中检测多语言文本的准确性低和难度高的问题。为了应对具有多个字符集和各种字体样式的多语言文本图像带来的挑战，我们引入了SFM Swin Transformer特征提取网络，以增强模型在检测不同语言的字符和字体时的鲁棒性。针对自然场景文本图像中文本尺度的巨大变化和复杂排列，我们结合自适应空间特征融合模块和空间金字塔池模块，提出了AS-HRFPN特征融合网络。特征融合网络的改进增强了模型检测文本大小和方向的能力。解决多语言场景文本图像中的不同背景和字体变化是对现有方法的挑战。有限的局部感受野阻碍了检测性能。为了克服这一点，我们提出了一个全局语义分割分支，提取并保留全局特征，以实现更有效的文本检测，从而满足对全面信息的需求。在本研究中，我们收集并构建了一个真实世界的多语言自然场景文本图像数据集，并进行了全面的实验和分析。实验结果表明，该算法的F测度为85.02%，比基线模型高4.71%。我们还对MSRA-TD500、ICDAR2017MLT和ICDAR2015数据集进行了广泛的跨数据集验证，以验证我们方法的通用性。代码和数据集位于https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.11153v2" target="_blank">2312.11153v2</a>
                              </td>
                              <td>Research on Multilingual Natural Scene Text Detection Algorithm</td>
                              <td>Tao Wang</td>
                              <td>2023-12-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_11153v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.11153v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code: \url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。代码：\url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v3" target="_blank">2306.09012v3</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v3" target="_blank">2310.03704v3</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15471v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Residual Learning for Image Point Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15471v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local image feature descriptors have had a tremendous impact on the development and application of computer vision methods. It is therefore unsurprising that significant efforts are being made for learning-based image point descriptors. However, the advantage of learned methods over handcrafted methods in real applications is subtle and more nuanced than expected. Moreover, handcrafted descriptors such as SIFT and SURF still perform better point localization in Structure-from-Motion (SfM) compared to many learned counterparts. In this paper, we propose a very simple and effective approach to learning local image descriptors by using a hand-crafted detector and descriptor. Specifically, we choose to learn only the descriptors, supported by handcrafted descriptors while discarding the point localization head. We optimize the final descriptor by leveraging the knowledge already present in the handcrafted descriptor. Such an approach of optimization allows us to discard learning knowledge already present in non-differentiable functions such as the hand-crafted descriptors and only learn the residual knowledge in the main network branch. This offers 50X convergence speed compared to the standard baseline architecture of SuperPoint while at inference the combined descriptor provides superior performance over the learned and hand-crafted descriptors. This is done with minor increase in the computations over the baseline learned descriptor. Our approach has potential applications in ensemble learning and learning with non-differentiable functions. We perform experiments in matching, camera localization and Structure-from-Motion in order to showcase the advantages of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15471v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部图像特征描述符对计算机视觉方法的发展和应用产生了巨大的影响。因此，对基于学习的图像点描述符做出重大努力并不令人惊讶。然而，在实际应用中，学习方法相对于手工制作方法的优势是微妙的，而且比预期的更微妙。此外，与许多学习的描述符相比，手工制作的描述符（如SIFT和SURF）在运动结构（SfM）中仍然执行更好的点定位。在本文中，我们提出了一种非常简单有效的方法，通过使用手工制作的检测器和描述符来学习局部图像描述符。具体来说，我们选择只学习描述符，由手工制作的描述符支持，同时丢弃点定位头。我们通过利用手工制作的描述符中已经存在的知识来优化最终描述符。这种优化方法允许我们丢弃已经存在于不可微函数中的学习知识，例如手工制作的描述符，并且只学习主网络分支中的剩余知识。与SuperPoint的标准基线架构相比，这提供了50倍的收敛速度，而在推理时，组合描述符提供了优于学习和手工制作的描述符的性能。这是在计算量比基线学习描述符略有增加的情况下完成的。我们的方法在集成学习和具有不可微函数的学习中具有潜在的应用。我们在匹配、相机定位和运动结构方面进行了实验，以展示我们方法的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15471v1" target="_blank">2312.15471v1</a>
                              </td>
                              <td>Residual Learning for Image Point Descriptors</td>
                              <td>Rashik Shrestha</td>
                              <td>2023-12-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15471v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15471v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13977v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13977v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13977v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经隐函数在多视图重建领域取得了显著的成果。然而，大多数现有的方法都是为密集视图量身定制的，并且在处理稀疏视图时表现出不令人满意的性能。已经提出了几种最新的方法来推广隐式重建以解决稀疏视图重建任务，但它们仍然存在较高的训练成本，并且仅在精心选择的视角下有效。在本文中，我们提出了一种新的稀疏视图重建框架，该框架利用表面先验来实现高度忠实的表面重建。具体而言，我们设计了全局几何对齐和局部几何细化的几个约束条件，以共同优化粗略形状和精细细节。为了实现这一点，我们训练神经网络从SfM获得的表面点学习全局隐式场，然后将其作为粗略的几何约束。为了利用局部几何一致性，我们将表面上的点投影到可见和不可见的视图上，将投影特征的一致丢失视为精细的几何约束。DTU和BlendedMVS数据集在两种流行的稀疏设置中的实验结果表明，与最先进的方法相比，有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13977v2" target="_blank">2312.13977v2</a>
                              </td>
                              <td>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</td>
                              <td>Han Huang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13977v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13977v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10529v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformers in Unsupervised Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10529v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers have revolutionized deep learning based computer vision with improved performance as well as robustness to natural corruptions and adversarial attacks. Transformers are used predominantly for 2D vision tasks, including image classification, semantic segmentation, and object detection. However, robots and advanced driver assistance systems also require 3D scene understanding for decision making by extracting structure-from-motion (SfM). We propose a robust transformer-based monocular SfM method that learns to predict monocular pixel-wise depth, ego vehicle's translation and rotation, as well as camera's focal length and principal point, simultaneously. With experiments on KITTI and DDAD datasets, we demonstrate how to adapt different vision transformers and compare them against contemporary CNN-based methods. Our study shows that transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust against natural corruptions, as well as untargeted and targeted attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10529v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers通过提高性能以及对自然腐蚀和对抗性攻击的鲁棒性，彻底改变了基于深度学习的计算机视觉。转换器主要用于2D视觉任务，包括图像分类、语义分割和对象检测。然而，机器人和先进的驾驶员辅助系统也需要3D场景理解，以便通过从运动中提取结构（SfM）来进行决策。我们提出了一种基于稳健变换器的单目SfM方法，该方法可以同时学习预测单目像素深度、自我车辆的平移和旋转以及相机的焦距和主点。通过在KITTI和DDAD数据集上的实验，我们展示了如何适应不同的视觉变换器，并将其与当代基于CNN的方法进行比较。我们的研究表明，基于转换器的体系结构虽然运行时效率较低，但可以实现相当的性能，同时对自然损坏以及无目标和有针对性的攻击更具鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10529v1" target="_blank">2312.10529v1</a>
                              </td>
                              <td>Transformers in Unsupervised Structure-from-Motion</td>
                              <td>Hemang Chawla</td>
                              <td>2023-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10529v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10529v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neurai-lab/mt-sfmlearner" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08863v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08863v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the reconstruction of high-fidelity 3D head models from static portrait image has made great progress. However, most methods require multi-view or multi-illumination information, which therefore put forward high requirements for data acquisition. In this paper, we study the reconstruction of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid structure from motion (NRSFM) methods have been widely used to solve such problems according to the two-dimensional correspondence between different frames. However, the inaccurate correspondence caused by high-complex hair structures and various facial expression changes would heavily influence the reconstruction accuracy. To tackle these problems, we propose a prior-guided dynamic implicit neural network. Specifically, we design a two-part dynamic deformation field to transform the current frame space to the canonical one. We further model the head geometry in the canonical space with a learnable signed distance field (SDF) and optimize it using the volumetric rendering with the guidance of two-main head priors to improve the reconstruction accuracy and robustness. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate the effectiveness and robustness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08863v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，从静态人像图像重建高保真三维头部模型取得了很大进展。然而，大多数方法都需要多视图或多照明信息，因此对数据采集提出了很高的要求。在本文中，我们研究了从任意单目视频中重建高保真3D头部模型。根据不同框架之间的二维对应关系，非刚性运动结构（NRSFM）方法已被广泛用于解决这些问题。然而，高度复杂的头发结构和各种面部表情变化导致的不准确对应将严重影响重建的准确性。为了解决这些问题，我们提出了一种先验引导的动态隐式神经网络。具体来说，我们设计了一个由两部分组成的动态变形场，将当前的帧空间转换为规范的帧空间。我们进一步用可学习的符号距离场（SDF）在正则空间中对头部几何结构进行建模，并在两个主要头部先验的指导下使用体积渲染对其进行优化，以提高重建精度和鲁棒性。广泛的消融研究和与最先进方法的比较证明了我们提出的方法的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08863v1" target="_blank">2312.08863v1</a>
                              </td>
                              <td>HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</td>
                              <td>Xueying Wang</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08863v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08760v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08760v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice. To overcome this limitation, we propose a novel \underline{c}amera parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion (SfM). Given a sequence of images, CF-NeRF estimates the camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset NeRFBuster which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to camera rotation and achieves state-of-the-art results without providing prior information and constraints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08760v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）在新的视图合成中表现出了令人印象深刻的性能。然而，NeRF及其大多数变体仍然依赖于传统的复杂管道来提供外在和内在的相机参数，如COLMAP。最近的工作，如NeRFmm、BARF和L2G NeRF，直接将相机参数视为可学习的，并通过差分体绘制进行估计。然而，这些方法适用于具有轻微运动的前瞻性场景，在实践中无法解决旋转场景。为了克服这一限制，我们提出了一种新颖的下划线{c}amera参数\下划线{f}ree神经辐射场（CF NeRF），其增量重建3D表示并从运动中恢复受增量结构启发的相机参数（SfM）。给定一系列图像，CF-NeRF逐个估计图像的相机参数，并通过初始化、隐式定位和隐式优化重建场景。为了评估我们的方法，我们使用了一个具有挑战性的真实世界数据集NeRFBuster，该数据集提供了复杂轨迹下的12个场景。结果表明，CF-NeRF对相机旋转具有鲁棒性，并且在不提供先验信息和约束的情况下实现了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08760v1" target="_blank">2312.08760v1</a>
                              </td>
                              <td>CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</td>
                              <td>Qingsong Yan</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08760v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08760v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COLMAP-Free 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然神经渲染在场景重建和新颖的视图合成方面取得了令人印象深刻的进展，但它在很大程度上依赖于精确预计算的相机姿态。为了放松这一限制，已经做出了多项努力来训练神经辐射场（NeRF），而不需要预处理相机姿势。然而，NeRF的隐式表示为同时优化3D结构和相机姿态提供了额外的挑战。另一方面，鉴于其明确的点云表示，最近提出的3D高斯飞溅提供了新的机会。本文利用输入视频流的显式几何表示和连续性来执行新的视图合成，而无需任何SfM预处理。我们以顺序的方式处理输入帧，并通过一次获取一个输入帧来逐步增长3D高斯集，而无需预先计算相机姿势。在大的运动变化下，我们的方法在视图合成和相机姿态估计方面比以前的方法有了显著的改进。我们的项目页面是https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07504v1" target="_blank">2312.07504v1</a>
                              </td>
                              <td>COLMAP-Free 3D Gaussian Splatting</td>
                              <td>Yang Fu</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06865v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06865v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes the incorporation of techniques from stereophotoclinometry (SPC) into a keypoint-based structure-from-motion (SfM) system to estimate the surface normal and albedo at detected landmarks to improve autonomous surface and shape characterization of small celestial bodies from in-situ imagery. In contrast to the current state-of-the-practice method for small body shape reconstruction, i.e., SPC, which relies on human-in-the-loop verification and high-fidelity a priori information to achieve accurate results, we forego the expensive maplet estimation step and instead leverage dense keypoint measurements and correspondences from an autonomous keypoint detection and matching method based on deep learning to provide the necessary photogrammetric constraints. Moreover, we develop a factor graph-based approach allowing for simultaneous optimization of the spacecraft's pose, landmark positions, Sun-relative direction, and surface normals and albedos via fusion of Sun sensor measurements and image keypoint measurements. The proposed framework is validated on real imagery of the Cornelia crater on Asteroid 4 Vesta, along with pose estimation and mapping comparison against an SPC reconstruction, where we demonstrate precise alignment to the SPC solution without relying on any a priori camera pose and topography information or humans-in-the-loop</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06865v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文建议将立体摄影测斜（SPC）技术纳入基于关键点的运动结构（SfM）系统，以估计探测到的地标的表面法线和反照率，从而从原位图像中改进小型天体的自主表面和形状特征。与小体型重建的实践方法（即SPC）的当前状态相反，SPC依赖于人在环验证和高保真度先验信息来实现准确的结果，我们放弃了昂贵的maplet估计步骤，而是利用基于深度学习的自主关键点检测和匹配方法的密集关键点测量和对应关系来提供必要的摄影测量约束。此外，我们开发了一种基于因子图的方法，通过融合太阳传感器测量和图像关键点测量，可以同时优化航天器的姿态、地标位置、太阳相对方向以及表面法线和反照率。所提出的框架在小行星4灶神星上科妮利亚陨石坑的真实图像上得到了验证，同时还进行了姿态估计和与SPC重建的映射比较，在SPC重建中，我们展示了与SPC解决方案的精确对准，而不依赖于任何先验的相机姿态和地形信息或环中的人类</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06865v1" target="_blank">2312.06865v1</a>
                              </td>
                              <td>Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</td>
                              <td>Travis Driver</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06865v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06865v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06741v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Splatting SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06741v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06741v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们首次将3D高斯散射应用于使用单个移动单目或RGB-D相机的增量3D重建。我们的同步定位和映射（SLAM）方法以3fps实时运行，使用高斯作为唯一的3D表示，统一了所需的表示，以实现准确、高效的跟踪、映射和高质量渲染。需要一些创新来从现场摄像机连续重建具有高保真度的3D场景。首先，为了超越最初的3DGS算法，该算法需要来自离线运动结构（SfM）系统的精确姿态，我们使用针对3D高斯的直接优化来制定3DGS的相机跟踪，并表明这能够实现快速而稳健的跟踪，并具有广泛的收敛范围。其次，通过利用高斯的显式性质，我们引入了几何验证和正则化来处理增量三维密集重建中出现的模糊性。最后，我们介绍了一个完整的SLAM系统，它不仅在新的视图合成和轨迹估计方面取得了最先进的结果，而且还重建了微小甚至透明的物体。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06741v1" target="_blank">2312.06741v1</a>
                              </td>
                              <td>Gaussian Splatting SLAM</td>
                              <td>Hidenobu Matsuki</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06741v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v3" target="_blank">2308.08479v3</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05889v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuperPrimitive: Scene Reconstruction at a Primitive Level</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05889v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05889v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其计算复杂性和固有的视觉模糊性，从一组图像或单目视频中进行联合相机姿态和密集几何估计仍然是一个具有挑战性的问题。大多数密集增量重建系统直接对图像像素进行操作，并使用多视图几何提示来求解其3D位置。这种像素级方法存在模糊性或违反多视图一致性的问题（例如，由无纹理或镜面引起）。我们用一种新的图像表示来解决这个问题，我们称之为超原始。超基元是通过将图像分割成语义相关的局部区域并用估计的表面法线方向对其进行增强来获得的，这两者都是由最先进的单图像神经网络预测的。这提供了每个SuperPrimitive的局部几何体估计，而它们的相对位置是基于多视图观测进行调整的。我们通过解决三个3D重建任务来展示我们新表示的多功能性：深度完成、运动中的少量视图结构和单目密集视觉里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05889v1" target="_blank">2312.05889v1</a>
                              </td>
                              <td>SuperPrimitive: Scene Reconstruction at a Primitive Level</td>
                              <td>Kirill Mazur</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05889v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05889v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v3" target="_blank">2311.17245v3</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04563v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Geometry Grounded Deep Structure From Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04563v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04563v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构（SfM）是计算机视觉界的一个长期问题，其目的是从一组不受约束的2D图像中重建场景的相机姿态和3D结构。经典框架通过检测和匹配关键点、配准图像、三角测量3D点和进行束调整，以增量的方式解决了这个问题。最近的研究工作主要围绕着利用深度学习技术的力量来增强特定元素（例如，关键点匹配），但仍基于原始的、不可微分的管道。相反，我们提出了一种新的深度流水线VGGSfM，其中每个组件都是完全可微的，因此可以以端到端的方式进行训练。为此，我们引入了新的机制和简化。首先，我们在深度2D点跟踪的最新进展的基础上提取可靠的像素精确轨迹，这消除了对成对匹配进行链接的需要。此外，我们根据图像和跟踪特征同时恢复所有相机，而不是逐渐注册相机。最后，我们优化相机，并通过可微分束调整层对3D点进行三角测量。我们在三个流行的数据集上获得了最先进的性能，即CO3D、IMC Phototourism和ETH3D。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04563v1" target="_blank">2312.04563v1</a>
                              </td>
                              <td>Visual Geometry Grounded Deep Structure From Motion</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04563v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04634v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04634v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04634v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04634v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一种可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04634v1" target="_blank">2311.04634v1</a>
                              </td>
                              <td>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</td>
                              <td>Linus Franke</td>
                              <td>2023-11-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04634v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04634v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lfranke/vet" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经辐射场（NeRF）的广泛采用确保了新视图合成领域的重大进展。这些模型捕捉场景的体积辐射场，通过使用简单的、可微分的渲染方程创建高度令人信服的、密集的照片真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本文中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。专注于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。代码位于：https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v2" target="_blank">2307.03404v2</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ysus33/rgb-d_plenoxel_mapping_tracking" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_14364v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_14364v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_14364v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating accurate 3D reconstructions from endoscopic video is a promising avenue for longitudinal radiation-free analysis of sinus anatomy and surgical outcomes. Several methods for monocular reconstruction have been proposed, yielding visually pleasant 3D anatomical structures by retrieving relative camera poses with structure-from-motion-type algorithms and fusion of monocular depth estimates. However, due to the complex properties of the underlying algorithms and endoscopic scenes, the reconstruction pipeline may perform poorly or fail unexpectedly. Further, acquiring medical data conveys additional challenges, presenting difficulties in quantitatively benchmarking these models, understanding failure cases, and identifying critical components that contribute to their precision. In this work, we perform a quantitative analysis of a self-supervised approach for sinus reconstruction using endoscopic sequences paired with optical tracking and high-resolution computed tomography acquired from nine ex-vivo specimens. Our results show that the generated reconstructions are in high agreement with the anatomy, yielding an average point-to-mesh error of 0.91 mm between reconstructions and CT segmentations. However, in a point-to-point matching scenario, relevant for endoscope tracking and navigation, we found average target registration errors of 6.58 mm. We identified that pose and depth estimation inaccuracies contribute equally to this error and that locally consistent sequences with shorter trajectories generate more accurate reconstructions. These results suggest that achieving global consistency between relative camera poses and estimated depths with the anatomy is essential. In doing so, we can ensure proper synergy between all components of the pipeline for improved reconstructions that will facilitate clinical application of this innovative technology.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_14364v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从内窥镜视频中生成准确的3D重建是对鼻窦解剖结构和手术结果进行纵向无辐射分析的一种很有前途的途径。已经提出了几种单目重建方法，通过从运动类型算法中检索具有结构的相对相机姿态并融合单目深度估计，产生视觉上令人愉快的3D解剖结构。然而，由于底层算法和内窥镜场景的复杂特性，重建管道可能表现不佳或意外失败。此外，获取医疗数据带来了额外的挑战，在定量基准测试这些模型、了解故障案例和确定有助于其准确性的关键组件方面存在困难。在这项工作中，我们对自监督鼻窦重建方法进行了定量分析，该方法使用内窥镜序列与从9个离体标本中采集的光学跟踪和高分辨率计算机断层扫描相结合。我们的结果表明，生成的重建与解剖结构高度一致，在重建和CT分割之间产生0.91mm的平均点到网格误差。然而，在与内窥镜跟踪和导航相关的点对点匹配场景中，我们发现平均目标配准误差为6.58 mm。我们发现，姿态和深度估计的不准确度对该误差的贡献相同，并且轨迹较短的局部一致序列会产生更准确的重建。这些结果表明，实现相对相机姿态和估计深度与解剖结构之间的全局一致性至关重要。通过这样做，我们可以确保管道的所有组成部分之间的适当协同作用，以改进重建，从而促进这项创新技术的临床应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.14364v1" target="_blank">2310.14364v1</a>
                              </td>
                              <td>A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy from Monocular Endoscopic Video</td>
                              <td>Jan Emily Mangulabnan</td>
                              <td>2023-10-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_14364v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.14364v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_13605v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_13605v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_13605v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local Feature Matching, an essential component of several computer vision tasks (e.g., structure from motion and visual localization), has been effectively settled by Transformer-based methods. However, these methods only integrate long-range context information among keypoints with a fixed receptive field, which constrains the network from reconciling the importance of features with different receptive fields to realize complete image perception, hence limiting the matching accuracy. In addition, these methods utilize a conventional handcrafted encoding approach to integrate the positional information of keypoints into the visual descriptors, which limits the capability of the network to extract reliable positional encoding message. In this study, we propose Feature Matching with Reconciliatory Transformer (FMRT), a novel Transformer-based detector-free method that reconciles different features with multiple receptive fields adaptively and utilizes parallel networks to realize reliable positional encoding. Specifically, FMRT proposes a dedicated Reconciliatory Transformer (RecFormer) that consists of a Global Perception Attention Layer (GPAL) to extract visual descriptors with different receptive fields and integrate global context information under various scales, Perception Weight Layer (PWL) to measure the importance of various receptive fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract deep aggregated multi-scale local feature representation. Extensive experiments demonstrate that FMRT yields extraordinary performance on multiple benchmarks, including pose estimation, visual localization, homography estimation, and image matching.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_13605v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配是计算机视觉任务（如运动结构和视觉定位）的重要组成部分，已通过基于Transformer的方法得到有效解决。然而，这些方法只将关键点之间的长程上下文信息与固定的感受野相结合，这限制了网络协调特征与不同感受野的重要性以实现完整的图像感知，从而限制了匹配精度。此外，这些方法利用传统的手工编码方法将关键点的位置信息集成到视觉描述符中，这限制了网络提取可靠位置编码消息的能力。在这项研究中，我们提出了具有协调变换器的特征匹配（FMRT），这是一种新的基于变换器的无检测器方法，它自适应地协调不同特征与多个感受野，并利用并行网络实现可靠的位置编码。具体而言，FMRT提出了一种专用的协调转换器（RecFormer），该转换器由全局感知注意力层（GPAL）组成，用于提取具有不同感受野的视觉描述符并整合各种尺度下的全局上下文信息，感知权重层（PWL）用于自适应地测量各种感受野的重要性，以及局部感知前馈网络（LPFFN）来提取深度聚合的多尺度局部特征表示。大量实验表明，FMRT在多个基准上产生了非凡的性能，包括姿态估计、视觉定位、单应性估计和图像匹配。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.13605v1" target="_blank">2310.13605v1</a>
                              </td>
                              <td>FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer</td>
                              <td>Xinyu Zhang</td>
                              <td>2023-10-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_13605v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.13605v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中进行高质量的3D对象重建。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了实现从随意图像捕获的3D重建的系统研究进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们认为NAVI有利于三维重建和对应关系估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v2" target="_blank">2306.09109v2</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_03303v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Nevermind: Instruction Override and Moderation in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03303v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03303v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03303v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines. Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03303v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>鉴于最近的大型语言模型（LLM）令人印象深刻的功能，我们研究并基准测试了最流行的专有和不同大小的开源模型，以在冲突的情况下执行显式指令，例如重写。其中包括模型在模型权重范围内覆盖知识的能力，在提示中覆盖（或缓和）提取的知识的能力以及最后执行全面越狱的能力。所进行的实验表明，有几个关键发现可以改善指令遵循——更大的模型在遵循指令方面表现最好，这些指令凌驾于内部和上下文指令之上，并且服从，甚至对错误也是如此。当通过绳索缩放扩展到更长的上下文时，需要从困惑悬崖的边缘保持一个重要的缓冲区，以保持指令跟随能力。最后，我们观察到，改进指令遵循，以及随后的指令覆盖/越狱，从根本上与语言模型遵循给定安全过滤器或准则的能力不一致。因此，我们假设，安全、可信的人工智能的最有效方法应该在LLM本身之外处理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03303v1" target="_blank">2402.03303v1</a>
                              </td>
                              <td>Nevermind: Instruction Override and Moderation in Large Language Models</td>
                              <td>Edward Kim</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03303v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03303v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/edk208/nevermind" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03299v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03299v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03299v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03299v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03299v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>发现了绕过大型语言模型（LLM）安全过滤器的“越狱”和有害反应，鼓励社区实施安全措施。一项主要的安全措施是在释放前主动对LLM进行越狱测试。因此，这种测试将需要一种能够大规模有效地生成越狱的方法。在本文中，我们遵循一种新颖而直观的策略，以人类这一代人的风格生成越狱。我们提出了一个角色扮演系统，为用户LLM分配四个不同的角色，以便在新的越狱中进行协作。此外，我们收集了现有的越狱者，并使用聚类频率和语义模式逐句将其划分为不同的独立特征。我们将这些特征组织到一个知识图中，使它们更容易访问和检索。我们的不同角色系统将利用这一知识图来生成新的越狱，事实证明，这在诱导LLM生成不道德或违反准则的反应方面是有效的。此外，我们还率先在我们的系统中设置了一个设置，该设置将自动遵循政府发布的指导方针来生成越狱，以测试LLM是否相应地遵循了指导方针。我们将我们的系统称为GUARD（通过自适应角色扮演诊断坚持指南）。我们已经根据经验验证了GUARD在三种尖端开源LLM（Vicuna-13B、LongChat-7B和Llama-2-7B）以及一种广泛使用的商业LLM（ChatGPT）上的有效性。此外，我们的工作扩展到了视觉语言模型领域（MiniGPT-v2和Gemini vision Pro），展示了GUARD的多功能性，并为开发更安全、更可靠的基于LLM的应用程序提供了宝贵的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03299v1" target="_blank">2402.03299v1</a>
                              </td>
                              <td>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</td>
                              <td>Haibo Jin</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03299v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03299v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14731v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distilled GPT for Source Code Summarization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14731v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14731v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14731v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as "changes all visible polygons to the color blue" can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation. Our model is small enough (350m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT-3.5 on this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14731v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>代码摘要是对源代码的简短自然语言描述。摘要通常只有一句话长，但却构成了开发人员文档的主干。简短的描述，如“将所有可见的多边形更改为蓝色”，可以让程序员在不阅读代码本身的情况下，对代码的功能有一个高级的了解。最近，基于大型语言模型的产品，如ChatGPT，已经证明了自动编写这些描述的强大能力。然而，要使用这些工具，程序员必须将代码发送给不受信任的第三方进行处理（例如，通过API调用）。许多组织不能接受这种失去监护权的情况。在本文中，我们提出了一种替代方案：在与知识蒸馏相关的过程中，我们使用GPT-3.5生成的样本输出来训练开源模型。我们的模型足够小（3.5亿个参数），可以在单个16gb GPU上运行，但我们在评估中表明，它足够大，可以在该任务中模拟GPT-3.5。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14731v2" target="_blank">2308.14731v2</a>
                              </td>
                              <td>Distilled GPT for Source Code Summarization</td>
                              <td>Chia-Yi Su</td>
                              <td>2023-08-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14731v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14731v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/apcl-research/jam-cgpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03289v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03289v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03289v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03289v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03289v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>用于寄存器传输级代码生成的现有大型语言模型（LLM）面临编译失败和次优功率、性能和面积（PPA）效率等挑战。这是由于在传统的变压器解码算法中缺乏PPA意识。作为回应，我们提出了一种自动转换器解码算法，该算法集成了蒙特卡洛树搜索的前瞻性，指导转换器生成可编译、功能正确和PPA优化的代码。在RTL代码集上使用微调的语言模型进行的经验评估表明，与仅提示的方法相比，我们提出的技术一致地生成功能正确的代码，并有效地解决了天真的大型语言模型的PPA不知情缺陷。对于最先进的LLM（16位加法器）产生的最大设计，我们的技术可以实现31.8%的面积延迟乘积改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03289v1" target="_blank">2402.03289v1</a>
                              </td>
                              <td>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS</td>
                              <td>Matthew DeLorenzo</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03289v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03289v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03284v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03284v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03284v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03284v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03284v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>有效的对话者解释了他人不确定的目标、信念和情绪。但是，即使是最优秀的人类健谈者也无法完全预见对话的轨迹。语言模型在多大程度上能够代表对话中固有的不确定性？我们提出了FortUne Dial，这是对长期以来的“对话预测”任务的扩展：评估不只是准确性，而是使用不确定性感知指标进行的，有效地实现了对个别情况的弃权。我们研究了语言模型潜在地表示结果不确定性的两种方式（内部使用分数和直接使用标记），并提出了微调策略来改进这两种表示的校准。在八个困难的谈判语料库上的实验表明，我们提出的微调策略（传统的监督策略和非政策强化学习策略）可以校准较小的开源模型，与10倍于其大小的预训练模型竞争。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03284v1" target="_blank">2402.03284v1</a>
                              </td>
                              <td>Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</td>
                              <td>Anthony Sicilia</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03284v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03284v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03282v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Framework for Partially Observed Reward-States in RLHF</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03282v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03282v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03282v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. We show that our models and guarantees in both settings generalize and extend existing ones. Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03282v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，人类反馈强化学习（RLHF）的研究因其在LLM发展中的作用而日益突出。神经科学研究表明，已知人类对刺激的反应取决于部分观察到的“内部状态”。不幸的是，目前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型不考虑中间反馈，这在经验工作中越来越重要，有助于提高样本复杂性和一致性。为了解决这些局限性，我们将RLHF建模为具有部分观察到的奖励状态的强化学习（PORRL）。我们展示了RLHF中人类反馈的两种主要形式的减少——基本反馈和决斗反馈到PORRL。对于基数反馈，我们开发了通用的统计高效算法，并实例化它们来呈现POR-UCRL和POR-UCBVI。对于决斗反馈，我们证明了对基数反馈的天真简化不能实现次线性决斗后悔。然后，我们提出了第一个明确的减少，将基本遗憾的保证转化为决斗遗憾。我们证明了我们在这两种情况下的模型和保证对现有模型和保证进行了推广和扩展。最后，我们在我们的模型上确定了一个递归结构，该结构可以提高PORRL的统计和计算可处理性，并给出了过去关于RLHF的工作的例子，以及学习PORRL所包含的完美奖励机。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03282v1" target="_blank">2402.03282v1</a>
                              </td>
                              <td>A Framework for Partially Observed Reward-States in RLHF</td>
                              <td>Chinmaya Kausik</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03282v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03282v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05707v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guiding Language Model Math Reasoning with Planning Tokens</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05707v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05707v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05707v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard fine-tuning baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05707v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近因其执行复杂推理任务（如思维链推理）的能力而引起了人们的极大兴趣。然而，大多数现有的增强这种能力的方法在很大程度上依赖于数据驱动的方法，而忽略了模型推理能力的结构方面。我们发现，虽然LLM可以很好地管理单个推理步骤，但它们很难在整个推理链中保持一致性。为了解决这个问题，我们在每个推理步骤开始时引入规划令牌，作为模型的指南，并将其嵌入到模型参数中。我们的方法需要可训练参数的可忽略不计的增加（仅0.001%），并且可以通过完全微调或更有效的参数方案来应用。我们通过将我们的方法应用于三种不同的LLM来证明我们的方法的有效性，在三个数学单词问题数据集上显示出与标准微调基线相比显著的准确性提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05707v3" target="_blank">2310.05707v3</a>
                              </td>
                              <td>Guiding Language Model Math Reasoning with Planning Tokens</td>
                              <td>Xinyi Wang</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05707v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05707v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14652v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14652v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14652v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14652v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \in \mathbb{R}^{n \times d}$, the polynomial method approximates the attention output $T \in \mathbb{R}^{n \times d}$. It accomplishes this by constructing $U_1, U_2 \in \mathbb{R}^{n \times t}$ to expedite attention ${\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\top \in \mathbb{R}^{n \times n}$ still necessitates $O(n^2)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in a streaming fashion. This method employs sublinear space $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14652v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>注意力计算同时具有$O（n^2）$的时间复杂度和$O（n ^2）美元的空间复杂度，这使得在涉及长上下文的流应用程序中部署大型语言模型（LLM）需要大量的计算资源。在最近的OpenAI DevDay（2023年11月6日）中，OpenAI发布了一个能够支持128K长文档的新模型，在我们的论文中，我们关注上下文长度$n$远大于128K（$n\gg 2^d$）时的内存效率问题。考虑到具有查询、键和值矩阵$Q，K，V\in\mathbb｛R｝^｛n \乘以d｝$的单层自注意，多项式方法近似于注意力输出$T\in\math bb｛R｝^{n \乘以d｝$。它通过构造$U_1，U_2\in\mathbb｛R｝^｛n \times t｝$来实现这一点，以在$n ^｛1+o（1）｝$时间执行内加速注意力$｛\sf Attn｝（Q，K，V）$计算。尽管如此，计算近似注意力矩阵$U_1U_2^\top\in\mathbb｛R｝^｛n \ times n｝$仍然需要$O（n^2）$空间，导致显著的内存使用。为了应对这些挑战，我们引入了一种新的算法，该算法只以流式方式读取一次数据。该方法使用次线性空间$o（n）$来存储三个草图矩阵，从而减少了对精确$K，V$存储的需求。值得注意的是，我们的算法在超长令牌的情况下表现出非凡的内存效率。随着令牌长度$n$的增加，我们的错误保证会减少，而内存使用率几乎保持不变。这一独特属性突显了我们的技术在流应用程序中高效处理LLM的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14652v2" target="_blank">2311.14652v2</a>
                              </td>
                              <td>One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space</td>
                              <td>Raghav Addanki</td>
                              <td>2023-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14652v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14652v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03271v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03271v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03271v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03271v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03271v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>面对不确定性，寻求信息的能力至关重要。在许多实际应用中，如医学诊断和故障排除，解决任务所需的信息最初并没有给出，必须通过询问后续问题来积极寻求（例如，医生向患者询问有关其症状的更多细节）。在这项工作中，我们介绍了思想的不确定性（UoT），这是一种通过提出有效问题来增强大型语言模型主动寻求信息的能力的算法。UoT结合了1）一种不确定性感知模拟方法，使模型能够模拟未来可能的情景及其发生的可能性，2）由激励模型寻求信息的信息增益激励的基于不确定性的奖励，以及3）一种奖励传播方案，以最大化预期奖励的方式选择要问的最佳问题。在医学诊断、故障排除和“20个问题”游戏的实验中，与直接提示相比，UoT在多个LLM中的任务成功完成率平均提高了57.8%，还提高了效率（即完成任务所需的问题数量）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03271v1" target="_blank">2402.03271v1</a>
                              </td>
                              <td>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models</td>
                              <td>Zhiyuan Hu</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03271v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03271v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiyuanhubj/UoT" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiyuanhubj/uot" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03264v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03264v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03264v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03264v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03264v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成模型在捕捉人类移动特征和生成合成轨迹方面显示出了有希望的结果。然而，确保生成的地理空间移动数据在语义上真实，包括一致的位置序列，并反映真实世界的特征，例如对地理空间限制的约束，仍然是一项挑战。为了解决这些问题，我们将人类流动建模重新格式化为自回归生成任务，利用生成预训练转换器（GPT）。为了确保其可控生成以缓解上述挑战，我们提出了一种地理空间感知的生成模型MobilityGPT。我们提出了一种基于重力的采样方法来训练语义序列相似性的转换器。然后，我们通过道路连通性矩阵来约束训练过程，该矩阵在轨迹生成中提供序列的连通性，从而将生成的轨迹保持在地理空间限制内。最后，我们构建了一个基于轨迹反馈的强化学习（RLTF），以最小化训练和综合生成的轨迹之间的行进距离。我们在真实世界数据集上的实验表明，MobilityGPT在生成高质量的移动轨迹方面优于最先进的方法，这些轨迹在起点-终点相似性、行程长度、行程半径、链路和重力分布方面最接近真实数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03264v1" target="_blank">2402.03264v1</a>
                              </td>
                              <td>MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</td>
                              <td>Ammar Haydari</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03264v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03264v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17256v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weak-to-Strong Jailbreaking on Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17256v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17256v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17256v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17256v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）容易受到越狱攻击，导致有害、不道德或有偏见的文本生成。然而，现有的越狱方法计算成本很高。在本文中，我们提出了弱到强越狱攻击，这是一种有效的方法来攻击对齐的LLM以产生有害文本。我们的关键直觉是基于这样一个观察，即越狱模型和对齐模型只在初始解码分布上有所不同。弱到强攻击的关键技术见解是使用两个较小的模型（一个安全模型和一个不安全模型）来对抗性地修改一个明显较大的安全模型的解码概率。我们评估了对来自3个组织的5个不同LLM的弱到强攻击。结果表明，我们的方法可以在两个数据集上将失准率提高到99%以上，每个例子只需一次前向通过。我们的研究揭示了在调整LLM时需要解决的一个紧迫的安全问题。作为初步尝试，我们提出了一种防御策略来抵御此类攻击，但创建更先进的防御仍然具有挑战性。用于复制该方法的代码位于https://github.com/XuandongZhao/weak-to-strong</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17256v2" target="_blank">2401.17256v2</a>
                              </td>
                              <td>Weak-to-Strong Jailbreaking on Large Language Models</td>
                              <td>Xuandong Zhao</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17256v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17256v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xuandongzhao/weak-to-strong" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03244v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03244v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03244v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03244v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03244v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）最近被用于交互式环境中的顺序决策。然而，利用环境奖励信号来持续改进LLM参与者并不简单。我们提出了技能集优化（SSO），通过构建和细化可转移技能集来提高LLM参与者的绩效。SSO通过提取具有高回报的常见子要素，并生成子目标和指令来表示每种技能，从而构建技能。这些技能在上下文中提供给LLM参与者，以强化具有高回报的行为。然后，SSO通过修剪不会继续获得高奖励的技能来进一步完善技能集。我们在经典视频游戏NetHack和文本环境ScienceWorld中评估了我们的方法，以展示SSO优化一系列技能和执行上下文策略改进的能力。SSO在我们的自定义NetHack任务中比基线高出40%，在ScienceWorld中比以前的最先进技术高出35%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03244v1" target="_blank">2402.03244v1</a>
                              </td>
                              <td>Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</td>
                              <td>Kolby Nottingham</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03244v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03244v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03242v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03242v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03242v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03242v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03242v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的技能匹配方法，使用合成训练数据进行分类或相似性模型训练，已经显示出有希望的结果，减少了对耗时和昂贵的注释的需求。然而，以前的合成数据集有局限性，例如每句话只有一个技能，并且通常包括短句。在本文中，我们介绍了JobSkape，这是一个生成合成数据的框架，可以解决这些限制，专门用于增强分类匹配的技能。在这个框架内，我们创建了SkillSkape，这是一个全面的开源合成招聘数据集，专门用于技能匹配任务。我们介绍了几个离线指标，这些指标表明我们的数据集与真实世界的数据相似。此外，我们还提出了一个使用大型语言模型（LLM）进行技能提取和匹配任务的多步骤管道，并与已知的监督方法进行了基准测试。我们概述了对真实世界数据的下游评估结果可以超过基线，强调了其有效性和适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03242v1" target="_blank">2402.03242v1</a>
                              </td>
                              <td>JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching</td>
                              <td>Antoine Magron</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03242v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03242v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/magantoine/jobskape" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11082v5_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fundamental Limitations of Alignment in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11082v5_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11082v5_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11082v5_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11082v5_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开发与人类交互的语言模型的一个重要方面是调整他们的行为，使其对人类用户有用且不伤人。这通常是通过以增强期望行为并抑制不期望行为的方式调整模型来实现的，这一过程被称为对齐。在本文中，我们提出了一种称为行为期望界限（BEB）的理论方法，该方法使我们能够正式研究大型语言模型中对齐的几个固有特征和限制。重要的是，我们证明，在这个框架的范围内，对于任何被模型表现出的概率有限的行为，都存在可以触发模型输出这种行为的提示，其概率随着提示的长度而增加。这意味着，任何削弱了不期望的行为但没有完全消除它的对齐过程，都不安全地抵御对抗性提示攻击。此外，我们的框架暗示了引导对齐方法（如从人类反馈中的强化学习）使LLM容易被提示进入不期望的行为的机制。这一理论结果正由所谓的当代“chatGPT越狱”大规模实验证明，在这种情况下，敌对用户通过触发LLM充当恶意角色来诱骗LLM打破其对齐护栏。我们的研究结果暴露了LLM一致性的根本局限性，并提出了设计确保人工智能安全的可靠机制的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11082v5" target="_blank">2304.11082v5</a>
                              </td>
                              <td>Fundamental Limitations of Alignment in Large Language Models</td>
                              <td>Yotam Wolf</td>
                              <td>2023-04-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11082v5_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11082v5" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12585v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLANG: New Concept Comprehension of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12585v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12585v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12585v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs' comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we propose a new benchmark $\textbf{SLANG}$, which can autonomously integrates novel data to stay dataset up-to-date, to assess LLMs' capability in comprehending emerging concepts and an approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves digesting real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the traditional models in terms of precision and relevance in the comprehension of Internet slang and memes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12585v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言的动态性，尤其是在互联网上的俚语和模因领域，对大型语言模型的适应性提出了严重挑战。传统上，这些模型以静态数据集为基础，往往难以跟上在线社区的快速语言进化特征。这项研究旨在通过提高LLM对互联网上不断发展的新概念的理解来弥补这一差距，而不需要高昂的持续再培训成本。为了实现这一目标，我们提出了一个新的基准$\textbf｛SLANG｝$，它可以自主集成新数据以保持数据集的最新状态，以评估LLM理解新兴概念的能力，以及一种方法$\textbf｛FOCUS｝，它使用因果推理来增强LLM理解新短语及其口语上下文的能力。我们的基准和方法包括消化现实世界中的语言变化实例，作为上下文信标，在新出现的表达及其含义之间形成更精确和上下文相关的联系。实证分析表明，在理解网络俚语和模因方面，我们基于因果推理的方法在准确性和相关性方面优于传统模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12585v3" target="_blank">2401.12585v3</a>
                              </td>
                              <td>SLANG: New Concept Comprehension of Large Language Models</td>
                              <td>Lingrui Mei</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12585v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12585v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/meirtz/focusonslang-toolbox" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03223v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03223v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03223v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03223v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data. Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03223v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本中的情绪分类是一项具有挑战性和主观性的任务，因为解释文本刺激需要涉及认知推理过程。此外，这组情感类别是高度特定于领域的。例如，文献分析可能需要使用审美情绪（例如，找到美丽的东西），而社交媒体分析可能受益于与基本情绪类别相比的细粒度集合（例如，将愤怒与烦恼分开）。这使得任务成为零样本分类的一个有趣领域，其中标签集在模型开发时是未知的。不幸的是，大多数用于情绪分析的资源都是英语，因此，大多数关于情绪分析的研究都是用英语进行的，包括那些涉及为文本标签提示语言模型的研究。这给我们留下了一个研究空白，我们在本文中解决了这一问题：我们应该用哪种语言在非英语文本上提示情感标签？当我们可以访问多语言大型语言模型时，这一点尤其令人感兴趣，因为即使对于非英语数据，我们也可以请求带有英语提示的标签。我们对基于自然语言推理的语言模型的实验表明，即使数据是用不同的语言，使用英语提示也始终更好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03223v1" target="_blank">2402.03223v1</a>
                              </td>
                              <td>English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts</td>
                              <td>Patrick Barreiß</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03223v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03223v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07964v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI-as-exploration: Navigating intelligence space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07964v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07964v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07964v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no less relevant to intelligence research, to those hypothesised for humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07964v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能是一个影响许多人生命的领域，这个词已经涵盖了各种科学和商业活动。在这篇论文中，我阐明了人工智能必须发挥的一个相当被忽视但核心的科学作用的轮廓，我称之为“人工智能即探索”。人工智能作为探索的基本主旨是创建和研究系统，这些系统可以揭示可能与我们熟悉的人类和动物智能形式不同的候选智能构建块。换句话说，我认为人工智能是我们探索智能空间的最佳工具之一，即可能的智能系统空间。我通过关注一个具体的案例研究来说明人工智能作为探索的价值，即最近关于将人类中新颖和发明的概念与大型语言模型相结合的能力的工作。我表明，尽管后者在这样的任务中表现出了人类水平的准确性，但很可能以与人类假设完全不同的方式解决问题，但与智力研究的相关性丝毫不逊色。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07964v2" target="_blank">2401.07964v2</a>
                              </td>
                              <td>AI-as-exploration: Navigating intelligence space</td>
                              <td>Dimitri Coelho Mollo</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07964v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07964v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03190v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unified Hallucination Detection for Multimodal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03190v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03190v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03190v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03190v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在多模式任务方面取得了重大进展，但多模式大型语言模型（MLLMs）仍受到幻觉这一关键问题的困扰。因此，在MLLMs中可靠地检测这种幻觉已成为模型评估和实际应用部署保障的一个重要方面。先前在这一领域的研究受到了对单一任务的狭隘关注、所涉及的幻觉类别范围不足以及缺乏详细粒度的限制。为了应对这些挑战，我们的工作拓展了幻觉检测的研究视野。我们提出了一种新的元评估基准，MHaluBench，它经过精心制作，有助于评估幻觉检测方法的进展。此外，我们还推出了一种新的统一多模式幻觉检测框架UNIHD，该框架利用一套辅助工具来有力地验证幻觉的发生。我们通过细致的评估和全面的分析，展示了UNIHD的有效性。我们还提供了关于解决各类幻觉的特定工具应用的战略见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03190v1" target="_blank">2402.03190v1</a>
                              </td>
                              <td>Unified Hallucination Detection for Multimodal Large Language Models</td>
                              <td>Xiang Chen</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03190v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03190v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/openkg-org/easydetect" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03182v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Empowering Time Series Analysis with Large Language Models: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03182v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03182v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03182v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03182v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，大型语言模型（LLM）取得了显著的进展，证明了它们在各种自然语言任务中前所未有的能力。然而，对于时间序列分析来说，从头开始完全训练一个大型通用模型是具有挑战性的，因为时间序列数据量大、种类多，而且非平稳性导致概念漂移阻碍了连续的模型自适应和重新训练。最近的进展表明，可以利用预先训练的LLM来捕获时间序列数据中的复杂依赖关系，并促进各种应用。在这项调查中，我们对利用LLM进行时间序列分析的现有方法进行了系统概述。具体来说，我们首先阐述了在时间序列背景下应用语言模型的挑战和动机，以及LLM的简要介绍。接下来，我们总结了基于LLM的时间序列分析的一般流程，将现有方法分为不同的组（即直接查询、标记化、提示设计、微调和模型集成），并强调了每组中的关键思想。我们还讨论了LLM在通用和时空时间序列数据中的应用，这些数据是针对特定领域定制的。最后，我们深入讨论了利用LLM进行时间序列分析的未来研究机会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03182v1" target="_blank">2402.03182v1</a>
                              </td>
                              <td>Empowering Time Series Analysis with Large Language Models: A Survey</td>
                              <td>Yushan Jiang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03182v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03182v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03181v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03181v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03181v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03181v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03181v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在各种应用程序中具有令人印象深刻的功能，但它们仍然存在可信度问题，如幻觉和错位。检索增强语言模型（RAG）已被提出，通过建立外部知识来提高世代的可信度，但对其世代风险的理论理解仍有待探索。在本文中，我们回答：1）RAG是否确实会导致低发电风险，2）如何对RAG和普通LLM的发电风险提供可证明的保证，以及3）什么样的充分条件使RAG模型能够降低发电风险。我们提出了C-RAG，这是第一个证明RAG模型发电风险的框架。具体来说，我们为RAG模型提供了保形风险分析，并证明了发电风险的置信上限，我们称之为保形发电风险。我们还为一般有界风险函数在测试分布转移下的保角生成风险提供了理论保证。我们证明，当检索模型和变换器的质量不平凡时，RAG实现了比单个LLM更低的保角生成风险。我们深入的经验结果表明，在四个最先进的检索模型上，我们在四个广泛使用的NLP数据集上的保形生成风险保证的稳健性和严密性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03181v1" target="_blank">2402.03181v1</a>
                              </td>
                              <td>C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</td>
                              <td>Mintong Kang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03181v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03181v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03177v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CIDAR: Culturally Relevant Instruction Dataset For Arabic</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03177v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03177v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03177v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at https://github.com/ARBML/CIDAR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03177v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指令调整已成为教授大型语言模型（LLM）遵循指令的一种突出方法。然而，目前的教学数据集主要迎合英语或源自英语主导的LLM，导致了对西方文化的固有偏见。这种偏见严重影响了阿拉伯语等非英语语言的语言结构，阿拉伯语有着独特的语法，反映了阿拉伯地区的不同文化。本文通过引入CIDAR来解决这一限制：https://hf.co/datasets/arbml/CIDAR，第一个由人类评审人员进行文化调整的开放式阿拉伯语教学调整数据集。CIDAR包含10000个代表阿拉伯地区的指令和输出对。我们通过与其他数据集上微调的其他模型的分析和比较，讨论了CIDAR的文化相关性。我们的实验表明，CIDAR可以帮助丰富LLM与阿拉伯文化相结合的研究工作。所有代码均可在https://github.com/ARBML/CIDAR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03177v1" target="_blank">2402.03177v1</a>
                              </td>
                              <td>CIDAR: Culturally Relevant Instruction Dataset For Arabic</td>
                              <td>Zaid Alyafeai</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03177v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03177v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/arbml/cidar" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03175v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Matrix: A Bayesian learning model for LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03175v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03175v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03175v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03175v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们引入了一个贝叶斯学习模型来理解大型语言模型（LLM）的行为。我们探索了基于预测下一个令牌的LLM的优化度量，并基于这一原理开发了一个新的模型。我们的方法包括构建一个理想的生成文本模型，该模型由具有先验的多项式转移概率矩阵表示，并研究LLM如何近似该矩阵。我们讨论了嵌入和多项式分布之间映射的连续性，并提出了Dirichlet逼近定理来逼近任何先验。此外，我们展示了LLM的文本生成如何与贝叶斯学习原理相一致，并深入研究了上下文中学习的含义，特别解释了为什么上下文中学习出现在更大的模型中，其中提示被视为要更新的样本。我们的研究结果表明，LLM的行为与贝叶斯学习一致，为其功能和潜在应用提供了新的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03175v1" target="_blank">2402.03175v1</a>
                              </td>
                              <td>The Matrix: A Bayesian learning model for LLMs</td>
                              <td>Siddhartha Dalal</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03175v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03175v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03173v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi: Multimodal Understanding Leaderboard with Text and Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03173v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03173v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03173v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03173v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式大型语言模型（MLLMs）的快速进展凸显了向学术界引入具有挑战性但现实的基准的必要性。现有的基准主要侧重于简单的自然图像理解，但Multi成为MLLMs的前沿基准，提供了一个全面的数据集，用于根据复杂的图表和科学问题来评估MLLMs。这一基准反映了当前现实的考试风格，提供了多模式的输入，需要精确或开放的回答，类似于现实生活中的学校考试。它向MLLMs挑战了各种任务，从公式推导到图像细节分析，以及跨模态推理。Multi包括18000多个问题，重点是基于科学的各种形式的QA。我们还介绍了Multi-Elite，一个用于测试MLLMs极限的500个问题子集，以及Multi-Extend，它用4500多个知识片段增强了上下文学习研究。我们的评估表明了MLLM进步的巨大潜力，GPT-4V在Multi上的准确率为63.7%，而其他MLLM的准确率在31.3%至53.7%之间。Multi不仅是一个强大的评估平台，而且为专家级人工智能的发展铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03173v1" target="_blank">2402.03173v1</a>
                              </td>
                              <td>Multi: Multimodal Understanding Leaderboard with Text and Images</td>
                              <td>Zichen Zhu</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03173v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03173v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03171v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Homograph Attacks on Maghreb Sentiment Analyzers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03171v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03171v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03171v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03171v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了同形词攻击对马格里布北非国家不同阿拉伯语方言的情感分析（SA）任务的影响。当数据以“阿拉伯文字”书写时，同源图攻击导致变压器分类从0.95的F1分数下降到0.33，下降了65.3%。本研究的目标是强调LLM的弱点，并优先考虑道德和负责任的机器学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03171v1" target="_blank">2402.03171v1</a>
                              </td>
                              <td>Homograph Attacks on Maghreb Sentiment Analyzers</td>
                              <td>Fatima Zahra Qachfar</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03171v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_15393v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DoGE: Domain Reweighting with Generalization Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_15393v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_15393v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_15393v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two-stage process consisting of (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learned domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets better perplexity and few-shot reasoning accuracies across $6$ tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, and consistently achieves better test perplexity on the target domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_15393v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预训练数据的覆盖范围和组成显著影响大型语言模型的泛化能力。尽管它很重要，但最近的LLM仍然依靠启发式和试错来增加或减少数据域的影响。我们提出了具有广义估计的DOmain重加权（DoGE），它以原则的方式优化了从每个域采样的概率（域权重）。我们的方法是一个两阶段的过程，包括（i）使用双层优化算法训练代理模型以获得域权重；（ii）通过根据所学习的域权重对训练域进行采样来训练较大的基础模型。在我们的实验中，我们广泛展示了DoGE如何提高基础模型对任何目标数据混合的泛化能力。在SlimPajama数据集上，与基线方法相比，我们的基本模型在$6$任务中获得了更好的困惑性和较少的镜头推理准确性。此外，DoGE旨在推广到域外目标任务，这在预训练语料库（OOD域）中是看不到的，它可以有效地识别域间依赖关系，并始终在目标域上实现更好的测试困惑。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.15393v2" target="_blank">2310.15393v2</a>
                              </td>
                              <td>DoGE: Domain Reweighting with Generalization Estimation</td>
                              <td>Simin Fan</td>
                              <td>2023-10-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_15393v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.15393v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03161v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03161v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03161v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03161v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03161v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>鉴于多模式大型语言模型（LLM）的最新进展，人们越来越关注将其从图像文本数据扩展到信息更丰富的真实世界视频。与静态图像相比，视频由于其时空动力学的建模，对有效的大规模预训练提出了独特的挑战。在本文中，我们通过有效的视频分解来解决视频语言预训练中的这些限制，该视频分解将每个视频表示为关键帧和时间运动。然后，使用精心设计的标记器将视觉和时间信息离散化为几个标记，将其应用于LLM，从而实现视频、图像和文本的统一生成预训练。在推断时，从LLM生成的令牌被小心地恢复到原始连续像素空间，以创建各种视频内容。我们提出的框架既能够理解和生成图像和视频内容，其在图像和视频理解和生成方面的13个多模式基准测试中的竞争性能证明了这一点。我们的代码和型号将在https://video-lavit.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03161v1" target="_blank">2402.03161v1</a>
                              </td>
                              <td>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</td>
                              <td>Yang Jin</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03161v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03161v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_12038v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_12038v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_12038v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_12038v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in non-English languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_12038v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，在图像到文本和文本到图像生成方面，多模式学习出现了显著的激增。然而，成功通常仅限于英语，其他语言在很大程度上落后了。由于非英语多模式数据的资源性质较低（即缺乏大规模、高质量的图像文本数据），用其他语言构建具有竞争力的对应数据极具挑战性。在这项工作中，我们提出了MPM，这是一种在非英语语言中训练大型多模式模型的有效训练范式。MPM证明了多语言语言模型可以在语言之间实现零样本多模式学习。具体来说，基于强大的多语言大型语言模型，在纯英文图像文本数据上预训练的多模式模型可以很好地以（准）-零样本的方式推广到其他语言，甚至超过了在母语图像文本数据上训练的模型。以中文为MPM的实践，我们在图像到文本和文本到图像的生成中建立了大型多模式模型VisCPM，实现了最先进的中文（开源）性能。为了促进未来的研究，我们在https://github.com/OpenBMB/VisCPM.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.12038v2" target="_blank">2308.12038v2</a>
                              </td>
                              <td>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</td>
                              <td>Jinyi Hu</td>
                              <td>2023-08-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_12038v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.12038v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/openbmb/viscpm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03147v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detecting Scams Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03147v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03147v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03147v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have gained prominence in various applications, including security. This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity. Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams. We present notable security applications of LLMs and discuss the unique challenges posed by scams. Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems. Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails. The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks. The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03147v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在包括安全性在内的各种应用中获得了突出地位。本文探讨了LLM在欺诈检测中的效用，这是网络安全的一个关键方面。与传统应用程序不同，我们为LLM提出了一个新颖的用例来识别骗局，如网络钓鱼、预付费欺诈和浪漫骗局。我们介绍了LLM的显著安全应用，并讨论了骗局带来的独特挑战。具体而言，我们概述了使用LLM构建有效骗局检测器所涉及的关键步骤，强调数据收集、预处理、模型选择、训练和集成到目标系统中。此外，我们使用GPT-3.5和GPT-4对重复的电子邮件进行了初步评估，强调了它们在识别网络钓鱼或欺诈电子邮件的常见迹象方面的熟练程度。结果证明了模型在识别可疑元素方面的有效性，但我们强调需要对各种语言任务进行全面评估。论文最后强调了持续改进和与网络安全专家合作以适应不断演变的威胁的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03147v1" target="_blank">2402.03147v1</a>
                              </td>
                              <td>Detecting Scams Using Large Language Models</td>
                              <td>Liming Jiang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03147v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03147v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03142v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03142v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03142v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03142v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03142v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于神经网络模型的复杂性及其在各个领域的广泛应用，神经网络修剪变得越来越重要。现有的修剪算法经常受到架构特异性、过度复杂和对复杂计算的依赖等限制，这使得它们在现实世界的应用中不切实际。在本文中，我们提出了KEN：一种基于核密度估计（KDE）的简单、通用和非结构化修剪算法。KEN旨在通过选择性地保留最重要的参数，同时将其他参数恢复到预训练状态，来构建优化的变压器模型。这种方法保持了模型性能，同时只允许存储优化的子网络，从而显著节省了内存。对七个变压器模型的广泛评估表明，KEN实现了与原始模型相同或更好的性能，最小参数减少了25%。与其他修剪和PEFT算法的深入比较证实了KEN的有效性。此外，我们还介绍了KEN_viz，这是一个可解释的工具，可以可视化优化的模型组成和KEN选择的子网络。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03142v1" target="_blank">2402.03142v1</a>
                              </td>
                              <td>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models</td>
                              <td>Michele Mastromattei</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03142v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03142v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03131v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Constrained Decoding for Cross-lingual Label Projection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03131v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03131v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03131v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03131v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用多语言LLM的零样本跨语言迁移已成为无标记训练数据的低资源语言的流行学习范式。然而，对于涉及对单词和短语进行细粒度预测的NLP任务，零样本跨语言迁移学习的性能远远落后于监督微调方法。因此，通常利用翻译和标签投影来进一步提高性能，方法是：（1）将高资源语言（如英语）中可用的训练数据与黄金标签一起翻译成低资源语言，和/或（2）将低资源语言中的测试数据翻译成高源语言以进行推理，然后将预测的跨度水平标签投影回原始测试数据上。然而，由于在翻译模型的输入中注入额外的标签标记，最先进的基于标记的标签投影方法遭受翻译质量下降的影响。在这项工作中，我们探索了一个新的方向，利用约束解码进行标签投影来克服上述问题。我们的新方法不仅可以保持翻译文本的质量，而且具有适用于翻译训练和翻译测试数据策略的多功能性。这种多功能性至关重要，因为我们的实验表明，与仅翻译训练数据相比，翻译测试数据可以显著提高性能。我们对跨越20种语言的两个跨语言迁移任务进行了评估，即命名实体识别和事件自变量提取。结果表明，我们的方法在很大程度上优于最先进的基于标记的方法，并且也显示出比其他依赖外部单词对齐的标签投影方法更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03131v1" target="_blank">2402.03131v1</a>
                              </td>
                              <td>Constrained Decoding for Cross-lingual Label Projection</td>
                              <td>Duong Minh Le</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03131v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03131v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03099v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03099v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03099v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03099v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\href{https://github.com/Eladlev/AutoPrompt}{here}$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03099v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于大型语言模型（LLM）对给定提示的高度敏感性和文本任务指令的固有歧义性，提示工程是一项具有挑战性的重要任务。自动提示工程对于实现LLM的优化性能至关重要。最近的研究证明了LLM通过使用元提示自动进行即时工程的能力，该元提示结合了上次试验的结果，并提出了改进的提示。然而，这需要一个高质量的基准来比较不同的提示，这在许多现实世界的用例中很难获得，而且成本高昂。在这项工作中，我们介绍了一种新的自动提示工程方法，使用一个校准过程，根据用户的意图反复细化提示。在优化过程中，系统联合生成边界用例的合成数据，并根据生成的数据集对提示进行优化。我们展示了我们的方法在现实世界任务（如审核和生成）中强大的专有模型方面的有效性。我们的方法在注释样本数量有限的情况下优于最先进的方法。此外，我们还验证了系统每个关键组件的优势。我们的系统以模块化的方式构建，便于轻松适应其他任务。代码可用$\href{https://github.com/Eladlev/AutoPrompt在这里</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03099v1" target="_blank">2402.03099v1</a>
                              </td>
                              <td>Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases</td>
                              <td>Elad Levi</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03099v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03099v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/eladlev/autoprompt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04076v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Do LLMs exhibit human-like response biases? A case study in survey design</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04076v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04076v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04076v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior. These inconsistencies tend to be more prominent in models that have been instruction fine-tuned. Furthermore, even if a model shows a significant change in the same direction as humans, we find that perturbations that are not meant to elicit significant changes in humans may also result in a similar change. These results highlight the potential pitfalls of using LLMs to substitute humans in parts of the annotation pipeline, and further underscore the importance of finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04076v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）的能力越来越强，在需要主观标签的现实世界任务中，如在调查和民意调查中，使用LLM作为人类代理的可能性越来越令人兴奋。LLM被广泛引用的一个障碍是它们对即时措辞的敏感性——但有趣的是，人类也以反应偏差的形式对指令变化表现出敏感性。因此，我们认为，如果LLM将被用来近似人类的意见，那么有必要调查LLM在多大程度上也反映了人类的反应偏差（如果有的话）。在这项工作中，我们使用调查设计作为案例研究，对“提示”单词排列引起的人类反应偏差进行了广泛研究。根据先前在社会心理学方面的工作，我们设计了一个数据集，并提出了一个框架来评估LLM在调查问卷中是否表现出类似人类的反应偏差。我们对九个模型的综合评估表明，流行的开放和商业LLM通常不能反映类人行为。这些不一致性往往在经过指令微调的模型中更加突出。此外，即使一个模型显示出与人类相同方向的显著变化，我们发现，不旨在引发人类显著变化的扰动也可能导致类似的变化。这些结果突出了在注释管道的某些部分使用LLM代替人类的潜在陷阱，并进一步强调了模型行为的细粒度特征的重要性。我们的代码、数据集和收集的样本可在https://github.com/lindiatjuatja/BiasMonkey</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04076v4" target="_blank">2311.04076v4</a>
                              </td>
                              <td>Do LLMs exhibit human-like response biases? A case study in survey design</td>
                              <td>Lindia Tjuatja</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04076v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04076v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lindiatjuatja/biasmonkey" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_04215v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_04215v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_04215v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_04215v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Retrieval augmentation enhances performance of traditional language models by incorporating additional context. However, the computational demands for retrieval augmented large language models (LLMs) pose a challenge when applying them to real-time tasks, such as composition assistance. To address this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework, a novel approach that efficiently combines a cloud-based LLM with a smaller, client-side, language model through retrieval augmented memory. This integration enables the client model to generate effective responses, benefiting from the LLM's capabilities and contextual information. Additionally, through an asynchronous memory update mechanism, the client model can deliver real-time completions swiftly to user inputs without the need to wait for responses from the cloud. Our experiments on five benchmark datasets demonstrate that HybridRAG significantly improves utility over client-only models while maintaining low latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_04215v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>检索增强通过结合额外的上下文来提高传统语言模型的性能。然而，当将检索增强的大型语言模型（LLM）应用于实时任务（如合成辅助）时，其计算需求带来了挑战。为了解决这一限制，我们提出了混合检索增强生成（HybridRAG）框架，这是一种通过检索增强内存将基于云的LLM与较小的客户端语言模型有效结合的新方法。这种集成使客户端模型能够生成有效的响应，受益于LLM的功能和上下文信息。此外，通过异步内存更新机制，客户端模型可以快速向用户输入提供实时完成，而无需等待来自云的响应。我们在五个基准数据集上的实验表明，与仅限客户端的模型相比，HybridRAG显著提高了实用性，同时保持了低延迟。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.04215v2" target="_blank">2308.04215v2</a>
                              </td>
                              <td>Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance</td>
                              <td>Menglin Xia</td>
                              <td>2023-08-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_04215v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.04215v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16332v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tradeoffs Between Alignment and Helpfulness in Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16332v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16332v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16332v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16332v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望的行为和抑制不期望的行为，实现人类与语言模型之间的安全交互。这通常是通过调整模型或插入预设对齐提示来完成的。最近，表示工程，一种通过在训练后改变其表示来改变模型行为的方法，被证明在对齐LLM方面是有效的（Zou et al.，2023a）。表示工程在面向对齐的任务中产生了收益，如抵抗对抗性攻击和减少社会偏见，但也被证明会导致模型执行基本任务的能力下降。在本文中，我们研究了模型的对齐增加和有用性减少之间的权衡。我们提出了一个理论框架，为这两个量提供了边界，并实证证明了它们的相关性。有趣的是，我们发现，虽然有用性通常会降低，但它与表示工程向量的范数成二次方关系，而对齐度则与之线性增加，这表明使用表示工程是有效的。我们从经验上验证了我们的发现，并绘制了表示工程对对齐有用性的界限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16332v2" target="_blank">2401.16332v2</a>
                              </td>
                              <td>Tradeoffs Between Alignment and Helpfulness in Language Models</td>
                              <td>Yotam Wolf</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16332v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16332v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13861v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13861v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13861v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13861v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the realm of Computational Social Science (CSS), practitioners often navigate complex, low-resource domains and face the costly and time-intensive challenges of acquiring and annotating data. We aim to establish a set of guidelines to address such challenges, comparing the use of human-labeled data with synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS classification tasks of varying complexity. Additionally, we examine the impact of training data sizes on performance. Our findings reveal that models trained on human-labeled data consistently exhibit superior or comparable performance compared to their synthetically augmented counterparts. Nevertheless, synthetic augmentation proves beneficial, particularly in improving performance on rare classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification and find that, while they generally display strong performance, they often fall short when compared to specialized classifiers trained on moderately sized training sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13861v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算社会科学（CSS）领域，从业者经常在复杂、低资源的领域中导航，并面临获取和注释数据的成本高昂和时间密集的挑战。我们的目标是建立一套应对这些挑战的指南，将人类标记数据与GPT-4和Llama-2合成生成的数据在十个不同复杂性的CSS分类任务中的使用进行比较。此外，我们还研究了训练数据大小对性能的影响。我们的研究结果表明，与合成增强的模型相比，基于人类标记数据训练的模型始终表现出优越或可比的性能。然而，合成增强被证明是有益的，特别是在多类任务中提高稀有类的性能方面。此外，我们利用GPT-4和Llama-2进行零样本分类，并发现尽管它们通常表现出强大的性能，但与在中等大小的训练集上训练的专用分类器相比，它们往往不足。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13861v2" target="_blank">2304.13861v2</a>
                              </td>
                              <td>The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks</td>
                              <td>Anders Giovanni Møller</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13861v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13861v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/andersgiovanni/worker_vs_gpt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03053v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03053v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03053v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03053v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All models released at https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03053v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们对马来西亚语言模型，特别是Llama2和Mistral，在涉及负对和正对的嵌入任务上的微调进行了全面的探索。我们发布了两个为语义相似性和检索增强生成（RAG）量身定制的不同模型。在语义相似性方面，我们的6亿参数Llama2模型在所有方面都优于OpenAI text-embedding-ada-002recall@kb.cari.com.my、c.cari.com.mi、马来新闻和马来西亚推特测试集的指标。在RAG模型领域，我们的方法在马来西亚的环境中与OpenAI text-embedding-ada-002相比具有竞争力。值得注意的是，我们的20亿参数Llama2模型实现了卓越的性能Recall@5, Recall@10用于“Melayu”关键词研究论文数据集，并擅长Recall@3, Recall@5和Recall@10lom.agrc.gov.my数据集。这些发现强调了我们微调策略的有效性，并强调了语义相似性和RAG任务的性能提升。所有型号发布于https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03053v1" target="_blank">2402.03053v1</a>
                              </td>
                              <td>Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations</td>
                              <td>Husein Zolkepli</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03053v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03053v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03049v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03049v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03049v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03049v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03049v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，指令调优越来越受到关注，并成为增强大型语言模型（LLM）能力的关键技术。为了构建高质量的指令数据集，人们提出了许多指令处理方法，旨在实现数据量和数据质量之间的微妙平衡。然而，由于各种教学处理方法之间的不一致性，社区没有标准的开源教学处理实施框架，这阻碍了从业者的进一步发展和进步。为了促进指令处理的研究和开发，我们提出了EasyInstruction，这是一个易于使用的LLM指令处理框架，它将指令生成、选择和提示模块化，同时考虑它们的组合和交互。EasyInstruction公开发布并积极维护https://github.com/zjunlp/EasyInstruct，以及正在运行的演示应用程序https://huggingface.co/spaces/zjunlp/EasyInstruct为了快速起步，呼吁以教学数据为中心进行更广泛的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03049v1" target="_blank">2402.03049v1</a>
                              </td>
                              <td>EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</td>
                              <td>Yixin Ou</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03049v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03049v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zjunlp/easyinstruct" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16822v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16822v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16822v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16822v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal large language models (MLLMs) have demonstrated remarkable success in vision and visual-language tasks within the natural image domain. Owing to the significant diversities between the natural and remote sensing (RS) images, the development of MLLMs in the RS domain is still in the infant stage. To fill the gap, a pioneer MLLM named EarthGPT integrating various multi-sensor RS interpretation tasks uniformly is proposed in this paper for universal RS image comprehension. In EarthGPT, three key techniques are developed including a visual-enhanced perception mechanism, a cross-modal mutual comprehension approach, and a unified instruction tuning method for multi-sensor multi-task in the RS domain. More importantly, a dataset named MMRS-1M featuring large-scale multi-sensor multi-modal RS instruction-following is constructed, comprising over 1M image-text pairs based on 34 existing diverse RS datasets and including multi-sensor images such as optical, synthetic aperture radar (SAR), and infrared. The MMRS-1M dataset addresses the drawback of MLLMs on RS expert knowledge and stimulates the development of MLLMs in the RS domain. Extensive experiments are conducted, demonstrating the EarthGPT's superior performance in various RS visual interpretation tasks compared with the other specialist models and MLLMs, proving the effectiveness of the proposed EarthGPT and offering a versatile paradigm for open-set reasoning tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16822v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模态大语言模型在自然图像领域的视觉和视觉语言任务中取得了显著的成功。由于自然图像和遥感图像之间的显著差异，MLLMs在遥感领域的发展仍处于初级阶段。为了填补这一空白，本文提出了一种名为EarthGPT的先驱MLLM，它将各种多传感器遥感解释任务统一集成在一起，用于通用遥感图像理解。在EarthGPT中，开发了三个关键技术，包括视觉增强感知机制、跨模态相互理解方法和RS域多传感器多任务的统一指令调整方法。更重要的是，构建了一个名为MMRS-1M的数据集，该数据集具有大规模多传感器多模态RS指令跟随功能，包括基于34个现有的不同RS数据集的超过1M的图像-文本对，包括光学、合成孔径雷达（SAR）和红外等多传感器图像。MMRS-1M数据集解决了MLLMs在RS专家知识方面的缺陷，并刺激了MLLMs在RS领域的发展。进行了广泛的实验，证明了与其他专业模型和MLLMs相比，EarthGPT在各种RS视觉解释任务中的优越性能，证明了所提出的EarthGPT的有效性，并为开集推理任务提供了一种通用的范式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16822v2" target="_blank">2401.16822v2</a>
                              </td>
                              <td>EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain</td>
                              <td>Wei Zhang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16822v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16822v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03009v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UniMem: Towards a Unified View of Long-Context Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03009v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03009v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03009v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03009v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>长上下文处理是制约大型语言模型适用性的关键能力。尽管有各种方法致力于增强大型语言模型（LLM）的长上下文处理能力，但它们都是以孤立的方式开发的，缺乏对其优势的系统分析和整合，阻碍了进一步的发展。在本文中，我们介绍了UniMem，这是一个统一的框架，从LLM的记忆增强的角度重新表述了现有的长上下文方法。UniMem有四个关键维度：记忆管理、记忆写作、记忆阅读和记忆注入，为理解各种长上下文方法提供了系统的理论基础。我们在UniMem的基础上重新制定了16种现有方法，并将四种具有代表性的方法：Transformer XL、Memorizing Transformer、RMT和Longformer分析为等效的UniMem形式，以揭示它们的设计原理和优势。基于这些分析，我们提出了UniMix，这是一种融合了这些算法优势的创新方法。实验结果表明，UniMix在处理长上下文方面取得了优异的性能，其困惑度明显低于基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03009v1" target="_blank">2402.03009v1</a>
                              </td>
                              <td>UniMem: Towards a Unified View of Long-Context Large Language Models</td>
                              <td>Junjie Fang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03009v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03009v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01391v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01391v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01391v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01391v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01391v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的发展极大地推动了代码生成领域的发展。先前的工作将强化学习（RL）与编译器反馈相结合，用于探索LLM的输出空间，以提高代码生成质量。然而，LLM为响应复杂的人类需求而生成的冗长代码使RL探索成为一个挑战。此外，由于单元测试可能无法涵盖复杂的代码，因此通过使用这些未执行的代码片段来优化LLM是无效的。为了应对这些挑战，我们引入了StepCoder，这是一种新的代码生成RL框架，由两个主要组件组成：CCCS通过将长序列代码生成任务分解为代码完成子任务的课程来解决探索挑战，而FGO仅通过屏蔽未执行的代码段来优化模型，以提供细粒度优化。此外，我们还构建了用于RL训练的APPS+数据集，该数据集经过手动验证，以确保单元测试的正确性。实验结果表明，我们的方法提高了探索输出空间的能力，并在相应的基准测试中优于最先进的方法。我们的数据集APPS+和StepCoder可在线获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01391v2" target="_blank">2402.01391v2</a>
                              </td>
                              <td>StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback</td>
                              <td>Shihan Dou</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01391v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01391v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ablustrund/apps_plus" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02987v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Conversation Reconstruction Attack Against GPT Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02987v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02987v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02987v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous conversations, specifically the UNR attack and the PBU attack. Our experimental findings indicate that the PBU attack yields substantial performance across all models, achieving semantic similarity scores exceeding 0.60, while the UNR attack is effective solely on GPT-3.5. Our results reveal the concern about privacy risks associated with conversations involving GPT models and aim to draw the community's attention to prevent the potential misuse of these models' remarkable capabilities. We will responsibly disclose our findings to the suppliers of related large language models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02987v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，以GPT系列模型为代表的大型语言模型（LLM）领域取得了重大进展。为了优化任务执行，用户经常与云环境中托管的GPT模型进行多轮对话。这些可能充满私人信息的多轮对话需要在云中进行传输和存储。然而，这种作战模式引入了额外的攻击面。在本文中，我们首先介绍了一种针对GPT模型的特定会话重建攻击。我们介绍的会话重建攻击由两个步骤组成：劫持会话和重建会话。随后，当GPT模型受到所提出的攻击时，我们对对话中固有的隐私风险进行了详尽的评估。然而，GPT-4对所提出的攻击表现出一定的鲁棒性。然后，我们介绍了两种旨在更好地重建先前对话的高级攻击，特别是UNR攻击和PBU攻击。我们的实验结果表明，PBU攻击在所有模型中都能产生显著的性能，语义相似性得分超过0.60，而UNR攻击仅对GPT-3.5有效。我们的研究结果揭示了人们对涉及GPT模型的对话所带来的隐私风险的担忧，旨在引起社区的注意，防止这些模型的显著功能被潜在滥用。我们将负责任地向相关大型语言模型的供应商披露我们的调查结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02987v1" target="_blank">2402.02987v1</a>
                              </td>
                              <td>Conversation Reconstruction Attack Against GPT Models</td>
                              <td>Junjie Chu</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02987v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01376v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LoTR: Low Tensor Rank Weight Adaptation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01376v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01376v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01376v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01376v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文推广和推广了基于Transformer体系结构的大型语言模型（LLM）的低秩自适应（LoRA）思想。广泛使用的微调LLM的类LoRA方法是基于梯度更新的矩阵分解。我们介绍了LoTR，这是一种用于LLM的参数有效微调的新方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器被构造为三个矩阵的乘积，并且张量结构产生于层之间共享该乘积的左和右乘法器。具有低秩张量表示的层序列的同时压缩允许LoTR归档比LoRA更好的参数效率，尤其是对于深度模型。此外，核心张量不依赖于原始的权重维度，并且可以任意变小，这允许极其廉价和快速的下游微调。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01376v2" target="_blank">2402.01376v2</a>
                              </td>
                              <td>LoTR: Low Tensor Rank Weight Adaptation</td>
                              <td>Daniel Bershatsky</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01376v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01376v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07445v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07445v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07445v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07445v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, under-represented language families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset will encourage a more inclusive evaluation of multilingual language models on a more diverse set of languages. https://github.com/dadelani/sib-200</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07445v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管我们在过去几年中在多语言自然语言处理方面取得了进展，但评估通常仅限于具有可用数据集的一小部分语言，其中排除了大量低资源语言。在本文中，我们创建了SIB-200——一个用于200种语言和方言的主题分类的大规模开源基准数据集，以解决自然语言理解（NLU）缺乏评估数据集的问题。对于SIB-200中涵盖的许多语言，这是NLU的第一个公开可用的评估数据集。该数据集基于Flores-200机器翻译语料库。我们对数据集的英语部分进行了注释，并将句子级注释扩展到语料库中涵盖的其余203种语言。尽管这项任务很简单，但我们在全监督环境、跨语言迁移环境和大型语言模型提示环境中的评估表明，当多语言评估扩展到许多世界语言时，高资源语言和低资源语言的性能之间仍有很大差距。我们发现，在多语言语言模型的预训练过程中看不到的语言，代表性不足的语系（如尼洛蒂奇语和Altantic刚果语），以及来自非洲、美洲、大洋洲和东南亚地区的语言，在我们的主题分类数据集中往往表现最低。我们希望我们的数据集将鼓励在更多样化的语言集上对多语言语言模型进行更具包容性的评估。https://github.com/dadelani/sib-200</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07445v2" target="_blank">2309.07445v2</a>
                              </td>
                              <td>SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects</td>
                              <td>David Ifeoluwa Adelani</td>
                              <td>2023-09-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07445v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07445v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dadelani/sib-200" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01256v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An energy-based comparative analysis of common approaches to text classification in the Legal domain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01256v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01256v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01256v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they follow different implementation procedures and also require different resources. The results indicate that very often, the simplest algorithms achieve performance very close to that of large LLMs but with very low power consumption and lower resource demands. The results obtained could suggest companies to include additional evaluations in the choice of Machine Learning (ML) solutions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01256v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数机器学习研究都根据性能来评估最佳解决方案。然而，在竞争表现最好的车型时，许多重要方面往往被忽视，相反，它们应该被仔细考虑。事实上，有时不同方法之间的性能差距是可以忽略的，而生产成本、能源消耗和碳足迹等因素必须考虑在内。大型语言模型（LLM）在学术界和工业界被广泛采用来解决NLP问题。在这项工作中，我们在LexGLUE基准上对LLM和传统方法（如SVM）进行了详细的定量比较，该基准考虑了性能（标准指数）和替代指标，如时间、功耗和成本，总之：碳足迹。在我们的分析中，我们分别考虑了原型设计阶段（通过训练验证测试迭代进行的模型选择）和生产阶段，因为它们遵循不同的实施程序，也需要不同的资源。结果表明，通常情况下，最简单的算法实现的性能非常接近大型LLM，但具有非常低的功耗和更低的资源需求。所获得的结果可能会建议公司在选择机器学习（ML）解决方案时加入额外的评估。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01256v2" target="_blank">2311.01256v2</a>
                              </td>
                              <td>An energy-based comparative analysis of common approaches to text classification in the Legal domain</td>
                              <td>Sinan Gultekin</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01256v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01256v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02896v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02896v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02896v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02896v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02896v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然在大型语言模型（LLM）的研究中，代理交互和个性化都是充满活力的主题，但对语言交互对受角色制约的LLM代理行为的影响的关注有限。这样的努力对于确保代理人与他们指定的特征保持一致，同时能够进行公开的、自然主义的对话非常重要。在我们的实验中，我们通过提示对GPT-3.5进行人格特征调节，并使用简单的变异性诱导采样算法创建两组LLM代理。然后，我们进行性格测试，并将代理提交给合作写作任务，发现不同的个人资料表现出不同程度的性格一致性和与对话伙伴的语言一致性。我们的研究旨在为更好地理解LLM之间基于对话的互动奠定基础，并强调需要新的方法来为互动环境打造强大、更人性化的LLM人物角色。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02896v1" target="_blank">2402.02896v1</a>
                              </td>
                              <td>LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models</td>
                              <td>Ivar Frisch</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02896v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02896v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ivarfresh/interaction_llms" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17072v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17072v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17072v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17072v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for the evaluation of instruction-tuned LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17072v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>调整指令的大型语言模型（LLM）最近在生成对自然语言指令的拟合响应的能力方面取得了显著进步。然而，目前的许多工作都依赖于手动评估来判断生成的响应的质量。由于这种手动评估是耗时的，它不容易扩展到多个模型和模型变体的评估。在这篇简短的论文中，我们提出了一种简单但非常有效的评估指标，称为SemScore，其中我们使用语义文本相似性（STS）直接将模型输出与黄金目标响应进行比较。我们使用8个广泛使用的文本生成评估指标，对12个突出的指令调优LLM的模型输出进行了比较评估。我们发现，就与人类评估的相关性而言，我们提出的SemScore指标优于所有其他（在许多情况下更复杂）评估指标。这些发现表明了我们提出的指标对教学调整LLM评估的效用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17072v2" target="_blank">2401.17072v2</a>
                              </td>
                              <td>SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity</td>
                              <td>Ansar Aynetdinov</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17072v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17072v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02872v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02872v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02872v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02872v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning. Our code will be released on github.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02872v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们探索了情境学习的机制，并使用定位和项目法提出了一个假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层次上，在上下文中，头部做出了巨大贡献。在每个上下文头中，值输出矩阵提取标签的特征。查询和关键矩阵计算输入文本和每个演示之间的注意力权重。注意力权重越大，转移到用于预测下一个单词的最后一个标记中的标签信息就越多。查询矩阵和关键矩阵可以被视为学习输入文本和每个演示之间的相似性度量的两个塔。基于这一假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们对GPT2大分子Llama 7B、13B和30B进行了实验。结果可以支持我们的分析。总之，我们的研究为理解语境学习的机制提供了一种新的方法和合理的假设。我们的代码将在github上发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02872v1" target="_blank">2402.02872v1</a>
                              </td>
                              <td>How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning</td>
                              <td>Zeping Yu</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02872v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02872v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02834v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Shortened LLaMA: A Simple Depth Pruning for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02834v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02834v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02834v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02834v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代大型语言模型（LLM）的结构化修剪已经成为减少其高计算需求的一种方式。宽度修剪减少了投影权重矩阵的大小（例如，通过移除注意力头），同时保持了层数。相反，深度修剪会删除整个层或块，同时保持剩余权重的大小不变。目前的大多数研究都集中在仅宽度修剪或宽度和深度修剪的混合上，很少对这两个单元（宽度与深度）对LLM推理效率的影响进行比较分析。在这项工作中，我们证明了简单的深度修剪方法可以在零样本任务性能方面与最近的宽度修剪方法相竞争。我们的修剪方法提高了推理速度，特别是在内存受限的条件下，运行LLM需要有限的批大小，而宽度修剪是无效的。我们希望这项工作能够帮助在本地和边缘设备上部署LLM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02834v1" target="_blank">2402.02834v1</a>
                              </td>
                              <td>Shortened LLaMA: A Simple Depth Pruning for Large Language Models</td>
                              <td>Bo-Kyeong Kim</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02834v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02834v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_03103v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scoped Effects as Parameterized Algebraic Theories</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03103v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03103v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03103v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Notions of computation can be modelled by monads. Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions. However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources. Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories. We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories. The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations. We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03103v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算的概念可以用monad来建模。代数效应从代数运算和等式公理的角度提供了单子的特征，其中运算是基本的编程特征，例如读取或更新状态，而公理指定了可观等价的表达式。然而，许多有用的编程功能依赖于其他机制，如定界的作用域或动态分配的资源。这种机制可以通过代数效应的扩展来支持，包括范围效应和参数化代数理论。我们通过翻译成参数化代数理论的变体，对范围效应提出了新的观点。这一翻译为范围效应的等式推理提供了一种新的方法，并产生了单元的另一种表征，即涉及范围运算和代数运算的生成器和方程。我们通过对几个已知的范围效应模型的等式刻画，展示了我们新视角的力量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03103v1" target="_blank">2402.03103v1</a>
                              </td>
                              <td>Scoped Effects as Parameterized Algebraic Theories</td>
                              <td>Sam Lindley</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03103v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03103v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02985v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02985v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02985v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02985v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02985v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在分析无人机图像中的道路场景时，提出了两个挑战。首先，无人机图像的高分辨率使处理变得困难。其次，有监督的深度学习方法需要大量的手动注释来训练鲁棒和准确的模型。本文介绍了一种无监督道路解析框架，该框架利用了视觉语言模型和基本计算机视觉模型的最新进展。最初，采用视觉语言模型来有效处理超大分辨率无人机图像，以快速检测图像中感兴趣的道路区域。随后，利用视觉基础模型SAM来生成没有类别信息的道路区域的掩模。然后，自监督表示学习网络从所有掩蔽区域提取特征表示。最后，应用无监督聚类算法对这些特征表示进行聚类，并为每个聚类分配ID。将掩蔽区域与相应的ID组合以生成初始伪标签，从而启动用于规则语义分割的迭代自训练过程。所提出的方法在不依赖任何手动注释的情况下，在开发数据集上实现了令人印象深刻的89.96%的mIoU。特别值得注意的是，所提出的方法具有非凡的灵活性，它甚至超越了人类定义的类别的限制，能够从数据集本身获取新类别的知识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02985v1" target="_blank">2402.02985v1</a>
                              </td>
                              <td>Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing</td>
                              <td>Zihan Ma</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02985v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02985v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17539v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Sharpness-aware Minimization under Overparameterization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17539v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17539v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17539v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. With evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. However, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   In this work, we analyze SAM under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on SAM. Specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that SAM continues to benefit from increasing overparameterization. We also discover compelling cases where the effect of overparameterization is more pronounced or even diminished along with a series of ablation studies. On the theoretical side, we use standard techniques in optimization and prove that SAM can achieve a linear rate of convergence under overparameterization in a stochastic setting. We also show that overparameterization can improve generalization of SAM based on an analysis of two-layer networks, and further, that the linearly stable minima found by SAM have more uniform Hessian moments compared to SGD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17539v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管训练损失水平相同，但训练过参数化的神经网络可以产生不同泛化能力的最小化器。有证据表明，极小值的锐度与其推广误差之间存在相关性，因此，人们越来越努力开发一种优化方法，将平坦极小值明确地作为更具推广性的解决方案。然而，对于这种清晰度感知最小化（SAM）策略是否以及如何受到过参数化的影响，还没有进行过多的研究。在这项工作中，我们分析了不同程度的过度参数化下的SAM，并给出了经验和理论结果，表明过度参数化对SAM的关键影响。具体而言，我们在各个领域进行了广泛的数值实验，并表明SAM继续受益于不断增加的过度参数。随着一系列消融研究，我们还发现了令人信服的案例，其中过度参数化的影响更加明显，甚至减弱。在理论方面，我们在优化中使用了标准技术，并证明了在随机环境中，SAM可以在过参数化的情况下实现线性收敛率。基于两层网络的分析，我们还证明了过参数化可以提高SAM的泛化能力，此外，与SGD相比，SAM发现的线性稳定极小值具有更均匀的Hessian矩。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17539v2" target="_blank">2311.17539v2</a>
                              </td>
                              <td>Analyzing Sharpness-aware Minimization under Overparameterization</td>
                              <td>Sungbin Shin</td>
                              <td>2023-11-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17539v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17539v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/log-postech/sam-overparam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02352v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Region-Based Representations Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02352v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong unsupervised representations like DINOv2 and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The compactness of the representation also makes it well-suited to video analysis and other problems requiring inference across many images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02352v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究基于区域的表示是否对识别有效。区域曾经是识别方法的支柱，但现在几乎完全使用基于像素和补丁的特征。我们表明，最近的类不可知分割器（如SAM）可以与强无监督表示（如DINOv2）有效结合，并用于各种任务，包括语义分割、基于对象的图像检索和多图像分析。一旦提取了掩码和特征，即使使用线性解码器，这些表示也能实现有竞争力的性能，使其非常适合需要自定义查询的应用程序。该表示的紧凑性也使其非常适合于视频分析和其他需要在许多图像上进行推理的问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02352v1" target="_blank">2402.02352v1</a>
                              </td>
                              <td>Region-Based Representations Revisited</td>
                              <td>Michal Shlapentokh-Rothman</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02352v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02352v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01274v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01274v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01274v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01274v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01274v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，自监督学习因其从未标记数据中学习鲁棒特征表示的能力而脱颖而出。通过自我监督预训练的网络可以作为下游任务的有效特征提取器，包括少镜头学习。虽然在图像中对无监督的少镜头学习方法的评估是公认的，但在声学中却明显缺乏。这项研究通过评估大规模自监督模型在少镜头音频分类中的性能来解决这一差距。此外，我们还探讨了模型的少镜头学习能力与其他下游任务基准之间的关系。我们的研究结果揭示了一些少镜头问题（如SpeechCommandsv2）的最先进性能，以及基于语音的少镜头问题与各种下游音频任务之间的强相关性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01274v1" target="_blank">2402.01274v1</a>
                              </td>
                              <td>On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification</td>
                              <td>Calum Heggan</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01274v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01274v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01188v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Any Change</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01188v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01188v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01188v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01188v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型在零样本图像分类和分割方面取得了显著的成果，但零样本变化检测仍然是一个悬而未决的问题。在本文中，我们提出了分段任意变化模型（AnyChange），这是一种新型的变化检测模型，支持对看不见的变化类型和数据分布的零样本预测和泛化。AnyChange是通过我们的无训练自适应方法，双时态潜在匹配，建立在分段任意模型（SAM）上的。通过揭示和利用SAM潜在空间中的图像内和图像间语义相似性，双时态潜在匹配以无训练的方式赋予SAM零样本变化检测能力。我们还提出了一种点查询机制，以实现AnyChange的零样本对象中心变化检测功能。我们进行了大量实验，以确认AnyChange对零样本变化检测的有效性。AnyChange在第二个无监督变化检测基准上创下了新纪录，超过了之前的SOTA高达4.4%F$_1$的分数，并在监督变化检测中实现了可忽略的手动注释（每张图像1个像素）的可比精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01188v1" target="_blank">2402.01188v1</a>
                              </td>
                              <td>Segment Any Change</td>
                              <td>Zhuo Zheng</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01188v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01188v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05112v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05112v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05112v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05112v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of practical guides on medical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05112v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM），如ChatGPT，由于其理解和生成人类语言的能力而受到广泛关注。医学LLM帮助医生进行患者护理，正成为人工智能和临床医学领域一个有前景的研究方向。这篇综述全面概述了LLM在医学中的原理、应用和面临的挑战。我们解决以下具体问题：1）医疗LLM应该如何构建？2） 医疗LLM下游绩效的衡量标准是什么？3） 医学LLM应如何在现实世界的临床实践中使用？4） 医疗LLM的使用带来了哪些挑战？以及5）我们应该如何更好地构建和利用医学LLM？这篇综述旨在深入了解LLM在医学中的机遇和挑战，并为构建有效的医学LLM提供实用资源。我们还维护并定期更新医疗LLM实用指南列表，网址为https://github.com/AI-in-Health/MedLLMsPracticalGuide.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05112v3" target="_blank">2311.05112v3</a>
                              </td>
                              <td>A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges</td>
                              <td>Hongjian Zhou</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05112v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05112v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ai-in-health/medllmspracticalguide" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04308v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Cross-Table Masked Pretraining for Web Data Mining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04308v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04308v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04308v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tabular data pervades the landscape of the World Wide Web, playing a foundational role in the digital architecture that underpins online information. Given the recent influence of large-scale pretrained models like ChatGPT and SAM across various domains, exploring the application of pretraining techniques for mining tabular data on the web has emerged as a highly promising research direction. Indeed, there have been some recent works around this topic where most (if not all) of them are limited in the scope of a fixed-schema/single table. Due to the scale of the dataset and the parameter size of the prior models, we believe that we have not reached the ''BERT moment'' for the ubiquitous tabular data. The development on this line significantly lags behind the counterpart research domains such as natural language processing. In this work, we first identify the crucial challenges behind tabular data pretraining, particularly overcoming the cross-table hurdle. As a pioneering endeavor, this work mainly (i)-contributes a high-quality real-world tabular dataset, (ii)-proposes an innovative, generic, and efficient cross-table pretraining framework, dubbed as CM2, where the core to it comprises a semantic-aware tabular neural network that uniformly encodes heterogeneous tables without much restriction and (iii)-introduces a novel pretraining objective -- prompt Masked Table Modeling (pMTM) -- inspired by NLP but intricately tailored to scalable pretraining on tables. Our extensive experiments demonstrate CM2's state-of-the-art performance and validate that cross-table pretraining can enhance various downstream tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04308v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>表格数据遍布万维网，在支撑在线信息的数字架构中发挥着基础性作用。鉴于最近大规模预训练模型（如ChatGPT和SAM）在各个领域的影响，探索预训练技术在网络上挖掘表格数据中的应用已成为一个非常有前途的研究方向。事实上，最近有一些关于这个主题的工作，其中大多数（如果不是全部的话）都被限制在固定模式/单个表的范围内。由于数据集的规模和先前模型的参数大小，我们认为我们还没有达到普遍存在的表格数据的“BERT时刻”。这方面的发展明显落后于自然语言处理等相应的研究领域。在这项工作中，我们首先确定了表格数据预训练背后的关键挑战，特别是克服跨表障碍。作为一项开创性的努力，这项工作主要（i）-提供了一个高质量的真实世界表格数据集，（ii）-提出了一个创新、通用和高效的跨表预训练框架，称为CM2，其中其核心包括一个语义感知的表格神经网络，该网络在没有太多限制的情况下对异构表进行统一编码，以及（iii）-引入了一种新的预训练目标——即时掩蔽表建模（pMTM）——其灵感来自NLP，但复杂地适应了表的可扩展预训练。我们的大量实验证明了CM2最先进的性能，并验证了跨表预训练可以增强各种下游任务。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04308v2" target="_blank">2307.04308v2</a>
                              </td>
                              <td>Towards Cross-Table Masked Pretraining for Web Data Mining</td>
                              <td>Chao Ye</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04308v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04308v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08083v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UV-SAM: Adapting Segment Anything Model for Urban Village Identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08083v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08083v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08083v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08083v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>城中村被定义为城市中心内或周围的非正规住宅区，其特点是基础设施不足和生活条件恶劣，与关于贫困、适足住房和可持续城市的可持续发展目标密切相关。传统上，政府在很大程度上依赖实地调查方法来监测城中村，但这是耗时、劳动密集型的，而且可能会延迟。得益于广泛可用和及时更新的卫星图像，最近的研究开发了计算机视觉技术来有效地检测城市村庄。然而，现有的研究要么侧重于简单的城中村图像分类，要么未能提供准确的边界信息。为了从卫星图像中准确识别城中村边界，我们利用视觉基础模型的力量，将分段任意模型（SAM）应用于城中村分割，称为UV-SAM。具体而言，UV-SAM首先利用小型语义分割模型为城中村生成混合提示，包括掩码、边界框和图像表示，然后将其输入SAM进行细粒度边界识别。在中国两个数据集上的广泛实验结果表明，UV-SAM优于现有基线，多年的识别结果表明，城中村的数量和面积都在随着时间的推移而减少，这为深入了解城中村的发展趋势提供了更深入的见解，并为可持续城市的愿景基础模型提供了启示。本研究的数据集和代码可在https://github.com/tsinghua-fib-lab/UV-SAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08083v2" target="_blank">2401.08083v2</a>
                              </td>
                              <td>UV-SAM: Adapting Segment Anything Model for Urban Village Identification</td>
                              <td>Xin Zhang</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08083v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08083v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tsinghua-fib-lab/uv-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17857v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Anything in 3D Gaussians</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17857v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17857v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17857v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed. Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain. Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc. In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters. We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians. Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods. We also propose a cross-view label-voting approach to assign labels from different views. In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks. Codes will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17857v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D高斯散射作为神经辐射场（NeRF）的一种替代3D表示方式，得益于其高质量的渲染结果和实时渲染速度。考虑到3D高斯表示仍然未解析，有必要首先在此域内执行对象分割。随后，可以执行场景编辑和碰撞检测，这对许多应用程序至关重要，如虚拟现实（VR）、增强现实（AR）、游戏/电影制作等。在本文中，我们提出了一种新的方法，通过无需任何训练过程和学习参数的交互式过程来实现3D高斯中的对象分割。我们将所提出的方法称为SA-GS，用于3D高斯中的任意分段。给定单个输入视图中的一组点击点，SA-GS可以通过所提出的多视图掩模生成和逐视图标签分配方法来推广SAM以实现3D一致分割。我们还提出了一种跨视图标签投票方法，以分配来自不同视图的标签。此外，为了解决由位于边界处的3D高斯的不可忽略的空间大小引起的分割对象的边界粗糙度问题，SA-GS结合了简单但有效的高斯分解方案。大量实验表明，SA-GS实现了高质量的三维分割结果，也可以很容易地应用于场景编辑和碰撞检测任务。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17857v2" target="_blank">2401.17857v2</a>
                              </td>
                              <td>Segment Anything in 3D Gaussians</td>
                              <td>Xu Hu</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17857v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17857v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00295v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00295v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00295v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00295v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. Yet, on-site characterisation of individual piles poses a formidable challenge. The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches. This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00295v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>矿山排土场的稳定性取决于弃土堆的精确布置，并考虑到其地质和岩土特性。然而，单个桩的现场特征化带来了巨大的挑战。利用基于图像的技术对弃土堆进行表征，通过无人机系统远程获取数据，是一种很有前途的互补解决方案。图像处理，如基于对象的分类和特征提取，依赖于有效的分割。本研究对各种分割方法进行了改进和并置，特别是基于颜色和基于形态学的技术。目标是在采矿环境的背景下，增强和评估基于对象的弃土特征分析方法。此外，还对传统的分割方法和植根于深度学习方法的分割方法进行了比较分析。在评估的各种分割方法中，与其他方法相比，基于形态学的深度学习分割方法Segment Anything Model（SAM）表现出优异的性能。这一结果强调了将先进的形态学和深度学习技术结合起来进行准确有效的弃土堆特征描述的有效性。这项研究的发现为分割策略的优化提供了有价值的见解，从而推动了基于图像的技术在采矿环境中弃土堆特征化中的应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00295v1" target="_blank">2402.00295v1</a>
                              </td>
                              <td>Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images</td>
                              <td>Sureka Thiruchittampalam</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00295v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00295v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17904v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17904v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17904v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17904v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM), a profound vision foundation model pre-trained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in text segmentation across four hierarchies, including stroke, word, text-line, and paragraph, while realizing layout analysis as well. Specifically, we first turn SAM into a high-quality text stroke segmentation (TSS) model through a parameter-efficient fine-tuning approach. We use this TSS model to iteratively generate the text stroke labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TSS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the promptable mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for text stroke segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring 20x fewer training epochs. The code is available at https://github.com/ymy-k/Hi-SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17904v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）是一种在大规模数据集上预先训练的深度视觉基础模型，打破了一般分割的界限，激发了各种下游应用。本文介绍了Hi-SAM，一种利用SAM进行分层文本分割的统一模型。Hi-SAM擅长四个层次的文本分割，包括笔划、单词、文本行和段落，同时还可以实现布局分析。具体来说，我们首先通过参数有效的微调方法将SAM转化为高质量的文本笔划分割（TSS）模型。我们使用这个TSS模型以半自动的方式迭代生成文本笔划标签，统一HierText数据集中四个文本层次结构的标签。随后，有了这些完整的标签，我们推出了基于TSS架构的端到端可训练Hi-SAM，该架构具有定制的分层掩码解码器。在推理过程中，Hi-SAM提供自动掩码生成（AMG）模式和提示分割模式。在AMG模式方面，Hi-SAM首先对文本笔划前景掩码进行分段，然后对前景点进行采样以生成分层文本掩码，并通过采样实现布局分析。至于提示模式，Hi-SAM通过单点点击提供单词、文本行和段落掩码。实验结果表明，我们的TSS模型具有最先进的性能：对于文本笔划分割，在Total Text上的fgIOU为84.86%，在TextSeg上的fgIOU为88.96%。此外，与之前在HierText上进行联合分层检测和布局分析的专家相比，Hi-SAM实现了显著的改进：文本行级别的PQ为4.73%，F1为5.39%，段落级别的布局分析的PQ和F1为5.49%，所需的训练时间减少了20倍。代码位于https://github.com/ymy-k/Hi-SAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17904v1" target="_blank">2401.17904v1</a>
                              </td>
                              <td>Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation</td>
                              <td>Maoyuan Ye</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17904v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17904v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ymy-k/hi-sam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17868v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17868v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17868v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17868v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17868v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）是图像分割的基础框架。虽然它在典型场景中表现出显著的零样本泛化，但当应用于医学图像和遥感等专业领域时，其优势就会减弱。为了解决这一局限性，本文引入了Conv-LoRA，这是一种简单而有效的参数高效微调方法。通过将超轻量级卷积参数集成到低秩自适应（LoRA）中，Conv-LoRA可以将图像相关的归纳偏差注入到普通ViT编码器中，进一步加强SAM的局部先验假设。值得注意的是，Conv-LoRA不仅保留了SAM广泛的分割知识，而且恢复了其学习高级图像语义的能力，这受到SAM前景-背景分割预训练的约束。跨多个领域的不同基准的全面实验强调了Conv-LoRA在将SAM适应现实世界语义分割任务方面的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17868v1" target="_blank">2401.17868v1</a>
                              </td>
                              <td>Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</td>
                              <td>Zihan Zhong</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17868v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17868v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/autogluon/autogluon" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17803v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17803v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17803v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17803v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data. Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks. In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks. We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes. Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework. SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models. We conduct extensive experiments on nine datasets of six downstream tasks. The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs. Code is available at: https://github.com/zongzi13545329/SimAda</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17803v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在通用视觉场景中表现出了出色的泛化能力，但缺乏对专业数据的理解。尽管许多工作都集中在优化下游任务的SAM上，但这些特定于任务的方法通常限制了对其他下游任务的可推广性。在本文中，我们旨在研究通用视觉模块对SAM微调的影响，并使其能够在所有下游任务中进行推广。我们提出了一个名为SimAda的简单统一框架，用于在表现不佳的场景中调整SAM。具体而言，我们的框架将不同方法的通用模块抽象为基本设计元素，并基于共享的理论框架设计了四种变体。SimAda简单而有效，它删除了所有特定于数据集的设计，只专注于一般优化，确保SimAda可以应用于所有基于SAM甚至基于Transformer的模型。我们在六个下游任务的九个数据集上进行了广泛的实验。结果表明，SimAda显著提高了SAM在多个下游任务上的性能，并在大多数任务上实现了最先进的性能，而不需要特定任务的设计。代码位于：https://github.com/zongzi13545329/SimAda</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17803v1" target="_blank">2401.17803v1</a>
                              </td>
                              <td>SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes</td>
                              <td>Yiran Song</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17803v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17803v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zongzi13545329/simada" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11319v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11319v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11319v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11319v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructure, and on average, respectively, representing a momentous leap in leveraging foundation models to segment mobility infrastructure including both road and pedestrian infrastructure in geographical images. The source code can be found on this GitHub repository: https://github.com/rafiibnsultan/GeoSAM/tree/main.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11319v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分段任意模型（SAM）在应用于自然图像分割时显示出令人印象深刻的性能。然而，它很难处理航空和卫星图像等地理图像，尤其是在分割包括道路、人行道和人行横道在内的移动基础设施时。这种较差的性能源于这些对象的狭窄特征，它们的纹理与周围环境融为一体，以及来自树木、建筑物、车辆和行人等对象的干扰，所有这些都会使模型迷失方向，产生不准确的分割图。为了应对这些挑战，我们提出了地理SAM（GeoSAM），这是一种新的基于SAM的框架，它使用来自零样本学习的密集视觉提示和来自预先训练的CNN分割模型的稀疏视觉提示来实现微调策略。所提出的GeoSAM在地理图像分割方面优于现有方法，特别是在道路基础设施、行人基础设施和平均值方面分别提高了26%、7%和17%，这代表着在利用基础模型分割移动基础设施（包括地理图像中的道路和行人基础设施）方面的重大飞跃。源代码可以在此GitHub存储库中找到：https://github.com/rafiibnsultan/GeoSAM/tree/main.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11319v2" target="_blank">2311.11319v2</a>
                              </td>
                              <td>GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure</td>
                              <td>Rafi Ibn Sultan</td>
                              <td>2023-11-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11319v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11319v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/rafiibnsultan/geosam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17221v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MouSi: Poly-Visual-Expert Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17221v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17221v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17221v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的大型视觉语言模型（VLM）经常遇到诸如单个视觉组件能力不足和视觉标记过长等挑战。这些问题可能会限制模型准确解释复杂视觉信息和过长上下文信息的有效性。应对这些挑战对于提高VLM的性能和适用性至关重要。本文提出使用集成专家技术来协同各个视觉编码器的能力，包括那些擅长图像文本匹配、OCR、图像分割等的编码器。该技术引入了一个融合网络来统一处理不同视觉专家的输出，同时弥合图像编码器和预训练LLM之间的差距。此外，我们探索了不同的位置编码方案，以减轻图像特征序列过长造成的位置编码浪费，有效地解决了位置溢出和长度限制的问题。例如，在我们的实现中，该技术显著降低了SAM等模型中的位置占用率，从相当大的4096降低到更高效和可管理的64，甚至降低到1。实验结果表明，与孤立的视觉编码器相比，具有多个专家的VLM始终表现出优异的性能，并且随着更多专家的集成，性能显著提高。我们已经开源了本报告中使用的培训代码。所有这些资源都可以在我们的项目网站上找到。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17221v1" target="_blank">2401.17221v1</a>
                              </td>
                              <td>MouSi: Poly-Visual-Expert Vision-Language Models</td>
                              <td>Xiaoran Fan</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17221v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17221v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fudannlplab/mousi" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17083v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online Robot Navigation and and Manipulation with Distilled Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17083v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17083v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17083v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications. Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment. We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds. First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model. Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label. In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation. Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17083v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态未知环境中的自主机器人导航对移动机器人应用具有至关重要的意义，包括最后一英里配送中的机器人导航以及工业和医院配送应用中的机器人自动化供应。目前的解决方案仍然存在局限性，例如机器人无法实时识别未知物体，也无法在动态、狭窄和复杂的环境中自由导航。我们提出了一个完整的软件框架，用于在非常密集的障碍物和密集的人群中进行自主机器人感知和导航。首先，我们提出了一个框架，以零样本的方式准确地检测和分割开放世界对象类别，这克服了当前SAM模型的过度分割限制。其次，我们提出了提取知识的提取策略，以在没有标签的情况下分割机器人导航通道的自由空间。与此同时，我们设计了与蒸馏协同工作的微调策略，以实现轻量级推理，从而在自主导航期间将神经网络部署在NVIDIA-TX2或Xavier NX等边缘设备上。集成到机器人导航系统中，大量实验表明，我们提出的框架在机器人场景感知和自主机器人导航方面的准确性和效率都取得了优异的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17083v1" target="_blank">2401.17083v1</a>
                              </td>
                              <td>Online Robot Navigation and and Manipulation with Distilled Vision-Language Models</td>
                              <td>Kangcheng Liu</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17083v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17083v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15266v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-based instance segmentation models for the automation of structural damage detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15266v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15266v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15266v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automating visual inspection for capturing defects based on civil structures appearance is crucial due to its currently labour-intensive and time-consuming nature. An important aspect of automated inspection is image acquisition, which is rapid and cost-effective considering the pervasive developments in both software and hardware computing in recent years. Previous studies largely focused on concrete and asphalt, with less attention to masonry cracks. The latter also lacks publicly available datasets. In this paper, we first present a corresponding data set for instance segmentation with 1,300 annotated images (640 pixels x 640 pixels), named as MCrack1300, covering bricks, broken bricks, and cracks. We then test several leading algorithms for benchmarking, including the latest large-scale model, the prompt-based Segment Anything Model (SAM). We fine-tune the encoder using Low-Rank Adaptation (LoRA) and proposed two novel methods for automation of SAM execution. The first method involves abandoning the prompt encoder and connecting the SAM encoder to other decoders, while the second method introduces a learnable self-generating prompter. In order to ensure the seamless integration of the two proposed methods with SAM encoder section, we redesign the feature extractor. Both proposed methods exceed state-of-the-art performance, surpassing the best benchmark by approximately 3% for all classes and around 6% for cracks specifically. Based on successful detection, we propose a method based on a monocular camera and the Hough Line Transform to automatically transform images into orthographic projection maps. By incorporating known real sizes of brick units, we accurately estimate crack dimensions, with the results differing by less than 10% from those obtained by laser scanning. Overall, we address important research gaps in automated masonry crack detection and size estimation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15266v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于土木结构外观的缺陷自动视觉检测至关重要，因为其目前劳动密集且耗时。自动检测的一个重要方面是图像采集，考虑到近年来软件和硬件计算的普遍发展，图像采集快速且具有成本效益。以前的研究主要集中在混凝土和沥青上，很少关注砌体裂缝。后者也缺乏公开的数据集。在本文中，我们首先提出了一个相应的数据集，例如具有1300个注释图像（640像素x 640像素）的分割，称为MCrack1300，覆盖砖块、碎砖和裂缝。然后，我们测试了几种领先的基准测试算法，包括最新的大规模模型，基于提示的分段任意模型（SAM）。我们使用低秩自适应（LoRA）对编码器进行微调，并提出了两种新的SAM执行自动化方法。第一种方法包括放弃提示编码器并将SAM编码器连接到其他解码器，而第二种方法引入了可学习的自生成提示器。为了确保所提出的两种方法与SAM编码器部分的无缝集成，我们重新设计了特征提取器。两种提出的方法都超过了最先进的性能，所有类别都超过了最佳基准约3%，特别是裂纹超过了约6%。在成功检测的基础上，我们提出了一种基于单眼相机和霍夫线变换的方法，将图像自动转换为正交投影图。通过结合已知的砖单元实际尺寸，我们准确地估计了裂缝尺寸，结果与激光扫描结果相差不到10%。总的来说，我们解决了自动化砌体裂缝检测和尺寸估计方面的重要研究空白。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15266v2" target="_blank">2401.15266v2</a>
                              </td>
                              <td>SAM-based instance segmentation models for the automation of structural damage detection</td>
                              <td>Zehao Ye</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15266v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15266v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16741v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MESA: Matching Everything by Segmenting Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16741v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16741v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16741v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature matching is a crucial task in the field of computer vision, which involves finding correspondences between images. Previous studies achieve remarkable performance using learning-based feature comparison. However, the pervasive presence of matching redundancy between images gives rise to unnecessary and error-prone computations in these methods, imposing limitations on their accuracy. To address this issue, we propose MESA, a novel approach to establish precise area (or region) matches for efficient matching redundancy reduction. MESA first leverages the advanced image understanding capability of SAM, a state-of-the-art foundation model for image segmentation, to obtain image areas with implicit semantic. Then, a multi-relational graph is proposed to model the spatial structure of these areas and construct their scale hierarchy. Based on graphical models derived from the graph, the area matching is reformulated as an energy minimization task and effectively resolved. Extensive experiments demonstrate that MESA yields substantial precision improvement for multiple point matchers in indoor and outdoor downstream tasks, e.g. +13.61% for DKM in indoor pose estimation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16741v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征匹配是计算机视觉领域的一项关键任务，它涉及到寻找图像之间的对应关系。先前的研究使用基于学习的特征比较实现了显著的性能。然而，图像之间普遍存在的匹配冗余导致了这些方法中不必要且容易出错的计算，从而限制了它们的准确性。为了解决这个问题，我们提出了MESA，这是一种新的方法来建立精确的区域（或区域）匹配，以有效地减少匹配冗余。MESA首先利用SAM的高级图像理解能力，这是一种最先进的图像分割基础模型，以获得具有隐含语义的图像区域。然后，提出了一种多关系图来对这些区域的空间结构进行建模，并构建它们的尺度层次。基于从图中导出的图形模型，将区域匹配重新表述为能量最小化任务，并有效地解决了该问题。大量实验表明，MESA在室内和室外下游任务中为多点匹配器带来了显著的精度提高，例如，在室内姿态估计中，DKM的精度提高了13.61%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16741v1" target="_blank">2401.16741v1</a>
                              </td>
                              <td>MESA: Matching Everything by Segmenting Anything</td>
                              <td>Yesheng Zhang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16741v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02245v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02245v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02245v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02245v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of large language models, many remarkable linguistic systems like ChatGPT have thrived and achieved astonishing success on many tasks, showing the incredible power of foundation models. In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks. However, whether SAM can be adapted to 3D vision tasks has yet to be explored, especially 3D object detection. With this inspiration, we explore adapting the zero-shot ability of SAM to 3D object detection in this paper. We propose a SAM-powered BEV processing pipeline to detect objects and get promising results on the large-scale Waymo open dataset. As an early attempt, our method takes a step toward 3D object detection with vision foundation models and presents the opportunity to unleash their power on 3D vision tasks. The code is released at https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02245v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型的发展，像ChatGPT这样的许多卓越的语言系统蓬勃发展，并在许多任务上取得了惊人的成功，显示了基础模型令人难以置信的力量。本着释放基础模型在视觉任务上的能力的精神，最近提出了用于图像分割的视觉基础模型Segment Anything Model（SAM），并在许多下游2D任务上表现出强大的零样本能力。然而，SAM是否能够适应3D视觉任务还有待探索，尤其是3D对象检测。在这种启发下，我们探索将SAM的零样本能力应用于三维物体检测。我们提出了一种SAM驱动的BEV处理管道来检测对象，并在大规模Waymo开放数据集上获得有希望的结果。作为早期的尝试，我们的方法向使用视觉基础模型进行3D对象检测迈出了一步，并提供了在3D视觉任务中释放其力量的机会。代码发布于https://github.com/DYZhang09/SAM3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02245v2" target="_blank">2306.02245v2</a>
                              </td>
                              <td>SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</td>
                              <td>Dingyuan Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02245v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02245v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dyzhang09/sam3d" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12665v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12665v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12665v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了良好的性能。然而，基于CLIP或基于SAM的ZSAS方法仍然存在不可忽略的关键缺点：1）CLIP主要关注不同输入的全局特征对齐，导致局部异常部分的分割不精确；2） SAM倾向于在没有适当提示约束的情况下生成大量冗余掩码，从而导致复杂的后处理要求。在这项工作中，我们创新性地为ZSAS提出了一个名为ClipSAM的CLIP和SAM合作框架。ClipSAM背后的见解是利用CLIP的语义理解能力进行异常定位和粗略分割，这进一步被用作SAM细化异常分割结果的提示约束。详细地说，我们介绍了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在CLIP的多个尺度上与视觉特征进行交互，以确定异常位置。然后，我们设计了一个新的多级掩码细化（MMR）模块，该模块利用位置信息作为SAM的多级提示来获取分层掩码并将其合并。大量实验验证了我们方法的有效性，在MVTec AD和VisA数据集上实现了最佳分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12665v2" target="_blank">2401.12665v2</a>
                              </td>
                              <td>ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</td>
                              <td>Shengze Li</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12665v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12665v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lszcoding/clipsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_11787v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_11787v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_11787v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_11787v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here, we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate improved seed samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_11787v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器人和自动化为解决材料发现等棘手的多变量科学问题提供了巨大的加速，但可用的搜索空间可能大得惊人。贝叶斯优化（BO）已成为一种流行的样本高效优化引擎，在目标函数/属性的分析形式未知的任务中蓬勃发展。在这里，我们以假设的形式利用人类专家知识，将贝叶斯搜索更快地引导到化学空间的有希望的区域。以前的方法使用了从现有实验测量中得出的潜在分布，这对于新的、未经探索的科学任务来说是不可行的。此外，这样的分布无法捕捉到复杂的假设。我们提出的方法，我们称之为HypBO，使用人类专家的假设来生成改进的种子样本。没有希望的种子会自动贴现，而有希望的种子则用于增加代理模型数据，从而实现更好的知情采样。这一过程以全局搜索与局部搜索的方式继续，在双层优化框架中组织。我们验证了我们的方法在一系列合成函数上的性能，并在实际的化学设计任务中证明了它的实用性，在该任务中，专家假设的使用显著加快了搜索性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.11787v3" target="_blank">2308.11787v3</a>
                              </td>
                              <td>HypBO: Accelerating Black-Box Scientific Experiments Using Experts' Hypotheses</td>
                              <td>Abdoulatif Cisse</td>
                              <td>2023-08-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_11787v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.11787v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luinardi/hypermapper" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15282v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15282v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15282v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15282v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: https://github.com/isbrycee/GEM-Glass-Segmentor.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15282v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于玻璃区域的透明度和反射特性的模糊性，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型基于大量数据进行训练，在图像感知和图像生成方面表现出惊人的性能。为了更高精度地分割玻璃表面，我们充分利用了两个视觉基础模型：分割任何东西（SAM）和稳定扩散。具体来说，我们设计了一个名为GEM的简单玻璃表面分割器，它只由SAM主干、简单特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将其分配为掩码解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为S-GSD，通过四种不同尺度的扩散模型，包含原始真实数据大小的1x、5x、10x和20x。该数据集是迁移学习的一个可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，这种改善将逐渐饱和。大量实验表明，GEM在GSD-S验证集上达到了最先进的水平（IoU+2.1%）。代码和数据集可在：https://github.com/isbrycee/GEM-Glass-Segmentor.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15282v1" target="_blank">2401.15282v1</a>
                              </td>
                              <td>GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis</td>
                              <td>Jing Hao</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15282v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15282v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/isbrycee/gem-glass-segmentor" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17191v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17191v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17191v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17191v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17191v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比自监督学习因其能够从大型未标记数据集中创建高质量表示而受到关注。这些强大的特征能够实现下游任务的数据高效学习的一个关键原因是，它们提供了增强不变性，这通常是一种有用的归纳偏差。然而，首选不变量的数量和类型在先验上是未知的，并且在不同的下游任务中有所不同。因此，我们提出了一种多任务自监督框架（MT-SLVR），该框架以参数有效的方式学习变异和不变特征。我们的多任务表示提供了一个强大而灵活的功能，有利于不同的下游任务。我们在从各种音频领域提取的少量镜头分类任务上评估了我们的方法，并在所有这些任务上展示了改进的分类性能</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17191v2" target="_blank">2305.17191v2</a>
                              </td>
                              <td>MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations</td>
                              <td>Calum Heggan</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17191v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17191v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cheggan/mt-slvr" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14686v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14686v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14686v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14686v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduced SSR, which utilizes SAM (segment-anything) as a strong regularizer during training, to greatly enhance the robustness of the image encoder for handling various domains. Specifically, given the fact that SAM is pre-trained with a large number of images over the internet, which cover a diverse variety of domains, the feature encoding extracted by the SAM is obviously less dependent on specific domains when compared to the traditional ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image encoder is still a mature choice of backbone for the semantic segmentation task, especially when the SAM is category-irrelevant. As a result, our SSR provides a simple yet highly effective design. It uses the ImageNet pre-trained image encoder as the backbone, and the intermediate feature of each stage (ie there are 4 stages in MiT-B5) is regularized by SAM during training. After extensive experimentation on GTA5$\rightarrow$Cityscapes, our SSR significantly improved performance over the baseline without introducing any extra inference overhead.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14686v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了SSR，它在训练过程中利用SAM（分割任何东西）作为强正则化子，以大大增强图像编码器处理各种域的鲁棒性。具体而言，考虑到SAM是通过互联网上的大量图像进行预训练的，这些图像覆盖了不同的领域，与传统的ImageNet预训练图像编码器相比，SAM提取的特征编码对特定领域的依赖性明显较小。同时，ImageNet预训练的图像编码器仍然是语义分割任务的成熟骨干选择，尤其是当SAM与类别无关时。因此，我们的SSR提供了一种简单而高效的设计。它使用ImageNet预训练的图像编码器作为骨干，每个阶段（即MiT-B5中有4个阶段）的中间特征在训练期间由SAM正则化。在GTA5$\rightarrow$Cityscapes上进行了广泛的实验后，我们的SSR在没有引入任何额外推理开销的情况下显著提高了性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14686v1" target="_blank">2401.14686v1</a>
                              </td>
                              <td>SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation</td>
                              <td>Yanqi Ge</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14686v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14686v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01429v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01429v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01429v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01429v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve the change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01429v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型（VFM）（如Segment Anything Model（SAM））允许对视觉内容进行零样本或交互式分割，因此它们可以快速应用于各种视觉场景。然而，由于遥感图像的特殊成像特性，它们在许多遥感应用中的直接使用往往不令人满意。在这项工作中，我们的目标是利用VFM强大的视觉识别能力来改进高分辨率遥感图像（RSI）的变化检测。我们使用FastSAM的视觉编码器，一种有效的SAM变体，来提取RS场景中的视觉表示。为了使FastSAM专注于RS场景中的一些特定地面对象，我们提出了一种卷积适配器来聚合面向任务的变化信息。此外，为了利用SAM特征固有的语义表示，我们引入了一个任务不可知的语义学习分支来对双时间RSI中潜在的语义进行建模。与SOTA方法相比，所得到的方法SAMCD获得了优越的精度，并表现出与半监督CD方法相当的样本有效学习能力。据我们所知，这是第一项将VFM应用于HR RSI CD的工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01429v4" target="_blank">2309.01429v4</a>
                              </td>
                              <td>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</td>
                              <td>Lei Ding</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01429v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01429v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ggsding/sam-cd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_04668v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deciding Equations in the Time Warp Algebra</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_04668v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_04668v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_04668v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Join-preserving maps on the discrete time scale $\omega^+$, referred to as time warps, have been proposed as graded modalities that can be used to quantify the growth of information in the course of program execution. The set of time warps forms a simple distributive involutive residuated lattice -- called the time warp algebra -- that is equipped with residual operations relevant to potential applications. In this paper, we show that although the time warp algebra generates a variety that lacks the finite model property, it nevertheless has a decidable equational theory. We also describe an implementation of a procedure for deciding equations in this algebra, written in the OCaml programming language, that makes use of the Z3 theorem prover.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_04668v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>离散时间尺度$\omega^+$上的连接保留映射，称为时间扭曲，已被提议作为分级模式，可用于量化程序执行过程中的信息增长。这组时间扭曲形成了一个简单的分配对合残差格，称为时间扭曲代数，它配备了与潜在应用相关的残差运算。在本文中，我们证明了尽管时间扭曲代数生成了一个缺乏有限模型性质的变种，但它仍然具有可判定的方程理论。我们还描述了用OCaml编程语言编写的、使用Z3定理证明器的、用于判定该代数中的方程的过程的实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.04668v4" target="_blank">2302.04668v4</a>
                              </td>
                              <td>Deciding Equations in the Time Warp Algebra</td>
                              <td>Sam van Gool</td>
                              <td>2023-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_04668v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.04668v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13961v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13961v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13961v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13961v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we address a significant gap in the field of neuroimaging by introducing the largest-to-date public benchmark, BvEM, designed specifically for cortical blood vessel segmentation in Volume Electron Microscopy (VEM) images. The intricate relationship between cerebral blood vessels and neural function underscores the vital role of vascular analysis in understanding brain health. While imaging techniques at macro and mesoscales have garnered substantial attention and resources, the microscale VEM imaging, capable of revealing intricate vascular details, has lacked the necessary benchmarking infrastructure. As researchers delve deeper into the microscale intricacies of cerebral vasculature, our BvEM benchmark represents a critical step toward unraveling the mysteries of neurovascular coupling and its impact on brain function and pathology. The BvEM dataset is based on VEM image volumes from three mammal species: adult mouse, macaque, and human. We standardized the resolution, addressed imaging variations, and meticulously annotated blood vessels through semi-automatic, manual, and quality control processes, ensuring high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical blood vessel segmentation method named TriSAM, which leverages the powerful segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to 3D volume segmentation, TriSAM employs a multi-seed tracking framework, leveraging the reliability of certain image planes for tracking while using others to identify potential turning points. This approach, consisting of Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively achieves long-term 3D blood vessel segmentation without model training or fine-tuning. Experimental results show that TriSAM achieved superior performances on the BvEM benchmark across three species.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13961v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过引入迄今为止最大的公共基准BvEM来解决神经成像领域的一个重大差距，该基准专门为体积电子显微镜（VEM）图像中的皮层血管分割而设计。大脑血管和神经功能之间错综复杂的关系突显了血管分析在理解大脑健康方面的重要作用。虽然宏观和中尺度的成像技术已经获得了大量的关注和资源，但能够揭示复杂血管细节的微尺度VEM成像缺乏必要的基准基础设施。随着研究人员深入研究脑血管系统的微观复杂性，我们的BvEM基准代表着解开神经-血管耦合及其对大脑功能和病理影响的奥秘的关键一步。BvEM数据集基于三种哺乳动物的VEM图像体积：成年小鼠、猕猴和人类。我们通过半自动、手动和质量控制流程标准化了分辨率，解决了成像变化问题，并仔细注释了血管，确保了高质量的3D分割。此外，我们开发了一种名为TriSAM的零样本皮层血管分割方法，该方法利用强大的分割模型SAM进行3D分割。为了将SAM从2D分割提升到3D体积分割，TriSAM采用了多种子跟踪框架，利用某些图像平面的可靠性进行跟踪，同时使用其他图像平面来识别潜在的转折点。这种方法由三平面选择、基于SAM的跟踪和递归重定向组成，有效地实现了长期的3D血管分割，而无需模型训练或微调。实验结果表明，TriSAM在三个物种的BvEM基准上获得了优异的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13961v1" target="_blank">2401.13961v1</a>
                              </td>
                              <td>TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images</td>
                              <td>Jia Wan</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13961v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13961v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10809v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Neglected Hessian component explains mysteries in Sharpness regularization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10809v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10809v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10809v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10809v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，像SAM这样显式或隐式惩罚二阶信息的方法可以提高深度学习中的泛化能力。看似相似的方法，如权重噪声和梯度惩罚，往往无法提供这样的好处。我们表明，这些差异可以用黑森损失的结构来解释。首先，我们证明了Hessian的常见分解可以定量地解释为将特征开发和特征探索分离。特征探索可以用非线性建模误差矩阵（NME）来描述，但在文献中通常被忽视，因为它在插值时消失了。我们的工作表明，NME实际上很重要，因为它可以解释为什么梯度惩罚对激活函数的选择很敏感。利用这一洞察力，我们设计干预措施来提高绩效。我们还提供了挑战权重噪声和梯度惩罚的长期等价性的证据。这种等价性依赖于NME可以被忽略的假设，我们发现这对于现代网络来说是不成立的，因为它们涉及显著的特征学习。我们发现，规范化特征开发而不是特征探索会产生类似于梯度惩罚的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10809v2" target="_blank">2401.10809v2</a>
                              </td>
                              <td>Neglected Hessian component explains mysteries in Sharpness regularization</td>
                              <td>Yann N. Dauphin</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10809v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10809v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_03251v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP Can Understand Depth</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03251v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03251v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03251v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03251v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近关于将CLIP推广用于单目深度估计的研究表明，在网络爬行数据上预先训练的CLIP在推导图像补丁和深度相关提示之间的适当相似性方面效率低下。在本文中，我们将CLIP应用于具有密集预测的单目深度估计的有意义的质量，而无需微调其原始视觉语言对齐。通过将一个紧凑的去卷积解码器与一个名为mirror的微小可学习嵌入矩阵联合训练，作为其文本编码器的静态提示，CLIP能够理解深度。使用这种方法，我们的模型在NYU Depth v2和KITTI数据集上表现出了令人印象深刻的性能，与之前的几个最先进的仅视觉模型相匹配，大大优于所有基于CLIP的深度估计模型。关于时间深度一致性和空间连续性的实验表明，我们提出的框架可以有效地细化CLIP的先验知识。此外，对镜子的消融研究证明，尽管没有给出任何以人为方式书写的提示，但最终模型不仅利用图像编码器的知识，而且利用文本编码器的知识来估计深度。这项研究表明，通过最小的调整，视觉语言基础模型（如CLIP）的先验知识可以推广到预训练中学习具有挑战性的领域。我们促进了未来的工作，重点是使用非人类语言提示调整视觉语言模型的次优先验知识的方法，实现与特定任务的最先进方法相同的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03251v1" target="_blank">2402.03251v1</a>
                              </td>
                              <td>CLIP Can Understand Depth</td>
                              <td>Dunam Kim</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03251v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03251v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/JoyceQuinn/Best-Advertising-And-Marketing-Management" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03241v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03241v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03241v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03241v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03241v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了一个有效的开放式词汇动作识别框架FROSTER。CLIP模型在一系列基于图像的任务中取得了显著的成功，得益于其强大的泛化能力，该能力源于对大量图像-文本对的预训练。然而，由于CLIP的预训练中缺乏时间信息，将CLIP直接应用于开放词汇动作识别任务是具有挑战性的。此外，在动作识别数据集上微调CLIP可能会导致过度拟合并阻碍其可推广性，导致在处理看不见的动作时结果不令人满意。为了解决这些问题，FROSTER采用了残差特征提取方法，以确保CLIP在有效适应动作识别任务的同时保持其泛化能力。具体而言，残差特征提取将冻结的CLIP模型视为教师，以保持原始CLIP所表现出的可推广性，并监督用于提取视频特定特征的特征学习，以弥合图像和视频之间的差距。同时，它使用残差子网络进行特征提取，以在学习可概括特征和视频特定特征这两个不同目标之间达到平衡。我们在基于新数据集和跨数据集的两种设置下，对FROSTER的开放词汇动作识别基准进行了广泛评估。FROSTER在所有数据集上始终实现最先进的性能。项目页面：https://visual-ai.github.io/froster.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03241v1" target="_blank">2402.03241v1</a>
                              </td>
                              <td>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition</td>
                              <td>Xiaohu Huang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03241v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03241v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/Visual-AI/FROSTER" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03614v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising-Diffusion Alignment for Continuous Sign Language Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03614v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03614v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03614v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As a key to social good, continuous sign language recognition (CSLR) aims to promote active and accessible communication for the hearing impaired. Current CSLR research adopts a cross-modality alignment scheme to learn the mapping relationship between "video clip-textual gloss". However, this local alignment method, especially with weak data annotation, ignores the contextual information of modalities and directly reduces the generalization of visual features. To this end, we propose a novel Denoising-Diffusion global Alignment scheme (DDA), which focuses on modeling the mapping of the "entire video-gloss sequence". DDA consists of a partial noising process strategy and a denoising-diffusion autoencoder. The former is used to achieve efficient guidance of the text modality to the visual modality; the latter learns the global alignment information of the two modalities in a denoising manner. Our DDA confirms the feasibility of diffusion models for visual representation learning in CSLR. Experiments on three public benchmarks demonstrate that our method achieves state-of-the-art performances. Furthermore, the proposed method can be a plug-and-play optimization to generalize other CSLR methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03614v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>作为社会公益的关键，持续手语识别（CSLR）旨在促进听力受损者积极和无障碍的交流。当前的CSLR研究采用跨模态对齐方案来学习“视频片段-文本光泽”之间的映射关系。然而，这种局部对齐方法，尤其是在弱数据注释的情况下，忽略了模态的上下文信息，直接降低了视觉特征的泛化能力。为此，我们提出了一种新的去噪扩散全局对准方案（DDA），该方案侧重于对“整个视频光泽序列”的映射进行建模。DDA由部分去噪处理策略和去噪扩散自动编码器组成。前者用于实现文本模态对视觉模态的有效引导；后者以去噪的方式学习两种模态的全局对准信息。我们的DDA证实了扩散模型在CSLR中用于视觉表示学习的可行性。在三个公共基准上的实验表明，我们的方法实现了最先进的性能。此外，所提出的方法可以是即插即用优化，以推广其他CSLR方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03614v3" target="_blank">2305.03614v3</a>
                              </td>
                              <td>Denoising-Diffusion Alignment for Continuous Sign Language Recognition</td>
                              <td>Leming Guo</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03614v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03614v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02851v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Compositional Generalization via Compositional Feature Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02851v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02851v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习模型的真实世界应用经常面临数据分布的变化，其中训练和测试数据分布之间存在差异。在常见的多域多类设置中，随着类和域的数量增加，为每个域-类组合收集训练数据变得不可行。这一挑战自然导致了对具有组合泛化（CG）能力的模型的探索，其中模型可以泛化到看不见的领域类组合。为了深入研究CG挑战，我们开发了CG Bench，这是一套从现有的真实世界图像数据集中导出的CG基准，并观察到在基础模型（如CLIP和DINOv2）上流行的预训练微调范式难以应对这一挑战。为了应对这一挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，i）在预训练的编码器上学习关于类和域标签的两个正交线性头，以及ii）在新学习的头冻结的情况下微调编码器。我们从理论和经验上证明了CFA鼓励预训练模型的组成特征学习。我们进一步在CLIP和DINOv2这两个强大的预训练视觉基础模型的CG平台上进行了广泛的实验。实验结果表明，CFA在合成泛化方面优于常用的微调技术，证实了CFA在组合特征学习方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02851v1" target="_blank">2402.02851v1</a>
                              </td>
                              <td>Enhancing Compositional Generalization via Compositional Feature Alignment</td>
                              <td>Haoxiang Wang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02851v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02851v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/haoxiang-wang/compositional-feature-alignment" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_05591v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Linear Alignment of Vision-language Models for Image Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_05591v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_05591v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_05591v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, vision-language models like CLIP have advanced the state of the art in a variety of multi-modal tasks including image captioning and caption evaluation. Many approaches adapt CLIP-style models to a downstream task by training a mapping network between CLIP and a language model. This is costly as it usually involves calculating gradients for large models. We propose a more efficient training protocol that fits a linear mapping between image and text embeddings of CLIP via a closed-form solution. This bypasses the need for gradient computation and results in a lightweight captioning method called ReCap, which can be trained up to 1000 times faster than existing lightweight methods. Moreover, we propose two new learning-based image-captioning metrics that build on CLIP score along with our linear mapping. Furthermore, we combine ReCap with our new metrics to design an iterative datastore-augmentation loop (DAL) based on synthetic captions. We evaluate ReCap on MS-COCO, Flickr30k, VizWiz, and MSRVTT. ReCap achieves performance comparable to state-of-the-art lightweight methods on established metrics while outperforming them on our new metrics, which are better aligned with human ratings on Flickr8k-Expert and Flickr8k-Crowdflower. Finally, we demonstrate that ReCap transfers well to other domains and that our DAL leads to a performance boost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_05591v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，像CLIP这样的视觉语言模型在包括图像字幕和字幕评估在内的各种多模式任务中提高了技术水平。许多方法通过训练CLIP和语言模型之间的映射网络，使CLIP风格的模型适应下游任务。这是昂贵的，因为它通常涉及计算大型模型的梯度。我们提出了一种更有效的训练协议，该协议通过闭式解决方案适合CLIP的图像和文本嵌入之间的线性映射。这绕过了梯度计算的需要，并产生了一种称为ReCap的轻量级字幕方法，该方法的训练速度比现有的轻量级方法快1000倍。此外，我们提出了两种新的基于学习的图像字幕指标，它们建立在CLIP分数和我们的线性映射的基础上。此外，我们将ReCap与我们的新指标相结合，设计了一个基于合成字幕的迭代数据存储扩充循环（DAL）。我们在MS-COCO、Flickr30k、VizWiz和MSRVTT上评估ReCap。ReCap在已建立的指标上实现了与最先进的轻量级方法相当的性能，同时在我们的新指标上优于它们，这些指标与Flickr8k Expert和Flickr8k Crowdflower上的人工评分更为一致。最后，我们证明了ReCap可以很好地转移到其他领域，并且我们的DAL可以提高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.05591v2" target="_blank">2307.05591v2</a>
                              </td>
                              <td>Linear Alignment of Vision-language Models for Image Captioning</td>
                              <td>Fabian Paischer</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_05591v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.05591v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ml-jku/semantic-image-text-alignment" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02586v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02586v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02586v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02586v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers have revolutionized various real-world applications from natural language processing to computer vision. However, traditional von-Neumann computing paradigm faces memory and bandwidth limitations in accelerating transformers owing to their massive model sizes. To this end, In-memory Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs) with high energy-efficiencies, have emerged as a promising solution for accelerating transformers. However, analog MVM operations in crossbars introduce non-idealities, such as stochastic read & write noise, which affect the inference accuracy of the deployed transformers. Specifically, we find pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the impact of write noise on the dynamically-generated Key (K) and Value (V) matrices in the attention layers, an effect not accounted for in prior studies. We, thus, propose ClipFormer, a transformation on the K and V matrices during inference, to boost the non-ideal accuracies of pre-trained ViT models. ClipFormer requires no additional hardware and training overhead and is amenable to transformers deployed on any memristive crossbar platform. Our experiments on Imagenet-1k dataset using pre-trained DeiT-S transformers, subjected to standard training and variation-aware-training, show >10-40% higher non-ideal accuracies at the high write noise regime by applying ClipFormer.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02586v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>变形金刚已经彻底改变了从自然语言处理到计算机视觉的各种现实世界应用程序。然而，传统的冯·诺依曼计算范式由于其庞大的模型尺寸，在加速变压器中面临内存和带宽限制。为此，基于非易失性存储器（NVM）的存储器内计算（IMC）交叉开关由于其能够以高能效执行高度并行化的矩阵矢量乘法（MVM），已成为加速变压器的一种有前途的解决方案。然而，交叉开关中的模拟MVM操作引入了非理想性，如随机读写噪声，这影响了部署的变换器的推理精度。具体而言，我们发现，由于写噪声对注意力层中动态生成的Key（K）和Value（V）矩阵的影响，预训练的视觉转换器（ViTs）在交叉开关上很容易受到攻击，这一影响在先前的研究中没有考虑到。因此，我们提出了ClipFormer，这是推理过程中对K和V矩阵的变换，以提高预训练的ViT模型的非理想精度。ClipFormer不需要额外的硬件和训练开销，并且适用于部署在任何忆阻横杆平台上的转换器。我们在Imagenet-1k数据集上使用预训练的DeiT-S变换器进行的实验，经过标准训练和变化感知训练，显示通过应用ClipFormer，在高写噪声状态下，非理想精度提高了>10-40%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02586v1" target="_blank">2402.02586v1</a>
                              </td>
                              <td>ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation</td>
                              <td>Abhiroop Bhattacharjee</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02586v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02586v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02555v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Entity Grounding via Assistance of Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02555v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02555v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02555v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we propose a novel approach to densely ground visual entities from a long caption. We leverage a large multimodal model (LMM) to extract semantic nouns, a class-agnostic segmentation model to generate entity-level segmentation, and the proposed multi-modal feature fusion module to associate each semantic noun with its corresponding segmentation mask. Additionally, we introduce a strategy of encoding entity segmentation masks into a colormap, enabling the preservation of fine-grained predictions from features of high-resolution masks. This approach allows us to extract visual features from low-resolution images using the CLIP vision encoder in the LMM, which is more computationally efficient than existing approaches that use an additional encoder for high-resolution images. Our comprehensive experiments demonstrate the superiority of our method, outperforming state-of-the-art techniques on three tasks, including panoptic narrative grounding, referring expression segmentation, and panoptic segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02555v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们提出了一种从长标题中提取密集地面视觉实体的新方法。我们利用大型多模态模型（LMM）提取语义名词，利用类不可知分割模型生成实体级分割，并利用所提出的多模态特征融合模块将每个语义名词与其对应的分割掩码相关联。此外，我们引入了一种将实体分割掩模编码到颜色图中的策略，从而能够从高分辨率掩模的特征中保留细粒度预测。这种方法使我们能够使用LMM中的CLIP视觉编码器从低分辨率图像中提取视觉特征，这在计算上比现有的对高分辨率图像使用额外编码器的方法更高效。我们的综合实验证明了我们的方法的优越性，在三项任务上优于最先进的技术，包括全景叙事基础、参考表达分割和全景分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02555v1" target="_blank">2402.02555v1</a>
                              </td>
                              <td>Generalizable Entity Grounding via Assistance of Large Language Model</td>
                              <td>Lu Qi</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02555v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02555v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02453v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI Art Neural Constellation: Revealing the Collective and Contrastive State of AI-Generated and Human Art</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02453v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02453v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02453v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Discovering the creative potentials of a random signal to various artistic expressions in aesthetic and conceptual richness is a ground for the recent success of generative machine learning as a way of art creation. To understand the new artistic medium better, we conduct a comprehensive analysis to position AI-generated art within the context of human art heritage. Our comparative analysis is based on an extensive dataset, dubbed ``ArtConstellation,'' consisting of annotations about art principles, likability, and emotions for 6,000 WikiArt and 3,200 AI-generated artworks. After training various state-of-the-art generative models, art samples are produced and compared with WikiArt data on the last hidden layer of a deep-CNN trained for style classification. We actively examined the various art principles to interpret the neural representations and used them to drive the comparative knowledge about human and AI-generated art. A key finding in the semantic analysis is that AI-generated artworks are visually related to the principle concepts for modern period art made in 1800-2000. In addition, through Out-Of-Distribution (OOD) and In-Distribution (ID) detection in CLIP space, we find that AI-generated artworks are ID to human art when they depict landscapes and geometric abstract figures, while detected as OOD when the machine art consists of deformed and twisted figures. We observe that machine-generated art is uniquely characterized by incomplete and reduced figuration. Lastly, we conducted a human survey about emotional experience. Color composition and familiar subjects are the key factors of likability and emotions in art appreciation. We propose our whole methodologies and collected dataset as our analytical framework to contrast human and AI-generated art, which we refer to as ``ArtNeuralConstellation''. Code is available at: https://github.com/faixan-khan/ArtNeuralConstellation</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02453v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>发现随机信号在美学和概念丰富性方面对各种艺术表达的创造性潜力，是生成性机器学习作为一种艺术创作方式最近取得成功的基础。为了更好地理解新的艺术媒介，我们进行了全面的分析，将人工智能生成的艺术定位在人类艺术遗产的背景下。我们的比较分析基于一个名为“ArtConstellation”的广泛数据集，该数据集由6000件WikiArt和3200件人工智能生成的艺术品的艺术原理、可爱程度和情感注释组成。在训练了各种最先进的生成模型后，生成艺术样本，并将其与经过风格分类训练的深度CNN的最后一个隐藏层上的WikiArt数据进行比较。我们积极研究了各种艺术原理来解释神经表征，并用它们来驱动关于人类和人工智能生成的艺术的比较知识。语义分析中的一个关键发现是，人工智能生成艺术品在视觉上与1800-2000年现代艺术的原理概念有关。此外，通过CLIP空间中的分布外（OOD）和分布内（ID）检测，我们发现，当人工智能生成的艺术品描绘风景和几何抽象图形时，它们是人类艺术的ID，而当机器艺术由变形和扭曲的图形组成时，它们被检测为OOD。我们观察到，机器生成的艺术以不完整和简化的外形为独特特征。最后，我们进行了一项关于情感体验的人类调查。在艺术欣赏中，色彩构成和熟悉的主题是决定喜爱度和情感的关键因素。我们提出了我们的整体方法和收集的数据集作为我们的分析框架，以对比人类和人工智能生成的艺术，我们称之为“ArtNeuralConstellation”。代码位于：https://github.com/faixan-khan/ArtNeuralConstellation</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02453v1" target="_blank">2402.02453v1</a>
                              </td>
                              <td>AI Art Neural Constellation: Revealing the Collective and Contrastive State of AI-Generated and Human Art</td>
                              <td>Faizan Farooq Khan</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02453v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02453v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/faixan-khan/artneuralconstellation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02447v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Breaking MLPerf Training: A Case Study on Optimizing BERT</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02447v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02447v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02447v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization as well as the fast training of gradient clipping before allreduce. We also re-evaluate existing optimizers via hyperparameter optimization and utilize ADAM, which also contributes to fast training via larger batches than existing methods. Our proposed methods, all combined, give the fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100 GPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one) submissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results are available at MLPerf v1.1~v2.1.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02447v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>加快大规模分布式训练具有挑战性，因为它需要改进训练的各个组成部分，包括负载平衡、通信、优化器等。我们提出了BERT模型的快速大规模训练的新方法，该方法单独改进了每个组成部分，从而使BERT训练性能达到新的水平。负载均衡在分布式BERT训练中是必不可少的，因为它的训练数据集由不同长度的样本表征。通信成本与分布式训练的规模成正比，需要通过有用的计算来隐藏。此外，需要在大规模分布式训练的背景下仔细重新评估优化器，例如ADAM、LAMB等。我们提出了两个新的想法，（1）用于负载平衡的基于数据集分层的局部预分类和（2）在allreduce之前的逐桶梯度裁剪，这使我们能够受益于梯度计算和同步的重叠以及在allreduct之前的梯度裁剪的快速训练。我们还通过超参数优化重新评估现有的优化器，并利用ADAM，这也有助于通过比现有方法更大的批量进行快速训练。我们提出的所有方法相结合，在1024个NVIDIA A100 GPU上提供了25.1（22.3）秒的最快MLPerf BERT训练，这比MLPerf v1.1（v2.0）的其他前两（一）个提交文件快1.33倍（1.13倍）和1.57倍。我们的实施和评估结果可在MLPerf v1.1~v2.1上获得。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02447v1" target="_blank">2402.02447v1</a>
                              </td>
                              <td>Breaking MLPerf Training: A Case Study on Optimizing BERT</td>
                              <td>Yongdeok Kim</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02447v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02447v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18961v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18961v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18961v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18961v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18961v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本异常检测（ZSAD）需要使用辅助数据训练的检测模型来检测异常，而无需在目标数据集中使用任何训练样本。当由于各种问题（例如数据隐私）而无法访问训练数据时，这是一项至关重要的任务，但这是一个挑战，因为模型需要推广到不同领域的异常，其中前景对象、异常区域和背景特征的外观，如不同产品/器官上的缺陷/肿瘤，可能会有很大差异。最近，大型预先训练的视觉语言模型（VLM），如CLIP，在包括异常检测在内的各种视觉任务中表现出强大的零样本识别能力。然而，它们的ZSAD性能较弱，因为VLM更侧重于对前景对象的类语义建模，而不是对图像中的异常/正常性建模。在本文中，我们介绍了一种新的方法，即AnomalyCLIP，以使CLIP适应不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习对象不可知的文本提示，无论图像的前景对象如何，都可以捕捉图像中的一般正常和异常。这使我们的模型能够专注于异常图像区域，而不是对象语义，从而实现对不同类型对象的广义正态和异常识别。在17个真实世界异常检测数据集上进行的大规模实验表明，AnomalyCLIP在来自各种缺陷检测和医学成像领域的具有高度不同类别语义的数据集中实现了检测和分割异常的优异零样本性能。代码将在提供https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18961v4" target="_blank">2310.18961v4</a>
                              </td>
                              <td>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</td>
                              <td>Qihang Zhou</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18961v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18961v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zqhang/anomalyclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15896v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15896v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15896v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15896v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new benchmarks in both languages for multimodal retrieval and classification tasks. Notably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of 88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification setting, surpassing previously reported SoTA methods by 2.2% and 21.1%, respectively. The $M^2$-Encoder series represents one of the most comprehensive bilingual image-text foundation models to date, so we are making it available to the research community for further exploration and development.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15896v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的视觉语言基础模型已经彻底改变了人工智能领域。然而，由于大规模预训练数据集的相对稀缺，支持多语言（如中文和英文）的VLM模型已经落后。为此，我们引入了一个综合的双语（汉英）数据集BM-6B，该数据集包含超过60亿个图像-文本对，旨在增强多模式基础模型，以更好地理解两种语言的图像。为了处理这样规模的数据集，我们提出了一种新的用于图像-文本对比损失计算的分组聚合方法，该方法显著降低了通信开销和GPU内存需求，有助于训练速度提高60%。我们在BM-6B上预训练了一系列具有增强的细粒度理解能力的双语图像-文本基础模型，这些模型被称为$M^2$-编码器（发音为“M-Square”），为多模式检索和分类任务在两种语言中设置了新的基准。值得注意的是，在零样本分类设置下，我们最大的$M^2$-Encoder-10B模型在ImageNet和ImageNet-CN上分别获得了88.5%和80.7%的前1准确率，分别超过了之前报道的SoTA方法2.2%和21.1%。$M^2$-Encoder系列代表了迄今为止最全面的双语图像文本基础模型之一，因此我们正在将其提供给研究界进行进一步的探索和开发。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15896v2" target="_blank">2401.15896v2</a>
                              </td>
                              <td>M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining</td>
                              <td>Qingpei Guo</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15896v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15896v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_Encoder" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02335v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Editing for Video Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02335v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02335v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02335v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Though pre-training vision-language models have demonstrated significant benefits in boosting video-text retrieval performance from large-scale web videos, fine-tuning still plays a critical role with manually annotated clips with start and end times, which requires considerable human effort. To address this issue, we explore an alternative cheaper source of annotations, single timestamps, for video-text retrieval. We initialise clips from timestamps in a heuristic way to warm up a retrieval model. Then a video clip editing method is proposed to refine the initial rough boundaries to improve retrieval performance. A student-teacher network is introduced for video clip editing. The teacher model is employed to edit the clips in the training set whereas the student model trains on the edited clips. The teacher weights are updated from the student's after the student's performance increases. Our method is model agnostic and applicable to any retrieval models. We conduct experiments based on three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip. Experiments conducted on three video retrieval datasets, YouCook2, DiDeMo and ActivityNet-Captions show that our edited clips consistently improve retrieval performance over initial clips across all the three retrieval models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02335v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管预训练的视觉语言模型在提高大规模网络视频的视频文本检索性能方面表现出了显著的优势，但微调仍然在具有开始和结束时间的手动注释剪辑中发挥着关键作用，这需要相当大的人工努力。为了解决这个问题，我们探索了一种更便宜的注释来源，即用于视频文本检索的单个时间戳。我们以启发式的方式从时间戳初始化剪辑，以预热检索模型。然后提出了一种视频剪辑编辑方法来细化初始粗糙边界，以提高检索性能。介绍了一个用于视频剪辑编辑的师生网络。教师模型被用于编辑训练集中的片段，而学生模型在编辑的片段上进行训练。在学生的成绩提高后，教师的权重会根据学生的权重进行更新。我们的方法与模型无关，适用于任何检索模型。我们基于三种最先进的检索模型进行实验，即COOT、VideoCLIP和CLIP4Clip。在YouCook2、DiDeMo和ActivityNet Captions三个视频检索数据集上进行的实验表明，在所有三个检索模型中，我们编辑的剪辑始终比初始剪辑提高了检索性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02335v1" target="_blank">2402.02335v1</a>
                              </td>
                              <td>Video Editing for Video Retrieval</td>
                              <td>Bin Zhu</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02335v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02335v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12678v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Uni-Fusion: Universal Continuous Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12678v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12678v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12678v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Uni-Fusion, a universal continuous mapping framework for surfaces, surface properties (color, infrared, etc.) and more (latent features in CLIP embedding space, etc.). We propose the first universal implicit encoding model that supports encoding of both geometry and different types of properties (RGB, infrared, features, etc.) without requiring any training. Based on this, our framework divides the point cloud into regular grid voxels and generates a latent feature in each voxel to form a Latent Implicit Map (LIM) for geometries and arbitrary properties. Then, by fusing a local LIM frame-wisely into a global LIM, an incremental reconstruction is achieved. Encoded with corresponding types of data, our Latent Implicit Map is capable of generating continuous surfaces, surface property fields, surface feature fields, and all other possible options. To demonstrate the capabilities of our model, we implement three applications: (1) incremental reconstruction for surfaces and color (2) 2D-to-3D transfer of fabricated properties (3) open-vocabulary scene understanding by creating a text CLIP feature field on surfaces. We evaluate Uni-Fusion by comparing it in corresponding applications, from which Uni-Fusion shows high-flexibility in various applications while performing best or being competitive. The project page of Uni-Fusion is available at https://jarrome.github.io/Uni-Fusion/ .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12678v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Uni-Fusion，这是一个用于表面、表面属性（颜色、红外等）等（CLIP嵌入空间中的潜在特征等）的通用连续映射框架。我们提出了第一个通用隐式编码模型，该模型支持对几何体和不同类型的属性（RGB、红外、特征等）进行编码，而无需任何训练。基于此，我们的框架将点云划分为规则网格体素，并在每个体素中生成一个潜在特征，以形成几何图形和任意属性的潜在隐式映射（LIM）。然后，通过将局部LIM帧明智地融合为全局LIM，实现了增量重建。通过相应类型的数据编码，我们的潜在隐式映射能够生成连续曲面、曲面属性字段、曲面特征字段和所有其他可能的选项。为了证明我们模型的能力，我们实现了三个应用程序：（1）表面和颜色的增量重建（2）制造属性的二维到三维转换（3）通过在表面上创建文本CLIP特征场来理解开放词汇场景。我们通过在相应应用中进行比较来评估Uni-Fusion，从中可以看出，Uni-FFusion在各种应用中表现出很高的灵活性，同时表现最好或具有竞争力。Uni Fusion的项目页面可在https://jarrome.github.io/Uni-Fusion/ .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12678v3" target="_blank">2303.12678v3</a>
                              </td>
                              <td>Uni-Fusion: Universal Continuous Mapping</td>
                              <td>Yijun Yuan</td>
                              <td>2023-03-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12678v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12678v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02055v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02055v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02055v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02055v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical advantage of VAS over random or other existing data selection. Experimentally, applying VAS and CLIP scores together can outperform baselines by a margin of $1.3\%$ average on 38 evaluation sets for noisy dataset DataComp and $2.5\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation study also shows visual features are better than text for calculating VAS, and the related classical experimental design methods may fail under this context.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02055v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，数据选择已成为大规模视觉语言模型预训练的核心问题，尤其是在嘈杂的网络策划数据集上。一种广泛采用的策略为每个样本分配质量分数，例如CLIP相似性，并保留具有最高分数的数据对。然而，这些方法对数据分布是不可知的，并且总是无法选择信息量最大的样本。为了解决这个问题，我们提出了一个简单但理论上有原则的度量，称为方差校准分数（VAS），其形式为$\langle\Sigma\{\text{test}｝，\Sigma_i\langle$。这里，$\Sigma_｛\text｛test｝｝$表示我们的目标（交叉）协方差矩阵，可能基于先验知识，而$\Sigma _i$表示第$i$个样本的单模态或多模态表示的张量积。我们进一步设计了一种新的数据选择方法，使总VAS最大化。我们在简化的设置中提供理论分析，以证明VAS相对于随机或其他现有数据选择的理论优势。在实验上，对于噪声数据集DataComp，将VAS和CLIP得分一起应用在38个评估集上的平均值可以优于基线1.3\%$，对于高质量数据集CC12M，在VTAB上的平均得分可以优于基线2.5\%$。此外，我们的消融研究还表明，在计算VAS时，视觉特征比文本更好，在这种情况下，相关的经典实验设计方法可能会失败。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02055v1" target="_blank">2402.02055v1</a>
                              </td>
                              <td>Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning</td>
                              <td>Yiping Wang</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02055v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02055v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01974v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hypergraph-Transformer (HGT) for Interactive Event Prediction in Laparoscopic and Robotic Surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01974v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01974v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01974v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Understanding and anticipating intraoperative events and actions is critical for intraoperative assistance and decision-making during minimally invasive surgery. Automated prediction of events, actions, and the following consequences is addressed through various computational approaches with the objective of augmenting surgeons' perception and decision-making capabilities. We propose a predictive neural network that is capable of understanding and predicting critical interactive aspects of surgical workflow from intra-abdominal video, while flexibly leveraging surgical knowledge graphs. The approach incorporates a hypergraph-transformer (HGT) structure that encodes expert knowledge into the network design and predicts the hidden embedding of the graph. We verify our approach on established surgical datasets and applications, including the detection and prediction of action triplets, and the achievement of the Critical View of Safety (CVS). Moreover, we address specific, safety-related tasks, such as predicting the clipping of cystic duct or artery without prior achievement of the CVS. Our results demonstrate the superiority of our approach compared to unstructured alternatives.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01974v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>了解和预测术中事件和行动对于微创手术中的术中辅助和决策至关重要。通过各种计算方法实现对事件、动作和以下后果的自动预测，目的是增强外科医生的感知和决策能力。我们提出了一种预测神经网络，能够从腹内视频中理解和预测手术工作流程的关键交互方面，同时灵活利用手术知识图。该方法结合了超图变换器（HGT）结构，将专家知识编码到网络设计中，并预测图的隐藏嵌入。我们在已建立的外科数据集和应用程序上验证了我们的方法，包括动作三元组的检测和预测，以及安全关键视图（CVS）的实现。此外，我们还处理了特定的、与安全相关的任务，例如在没有事先实现CVS的情况下预测囊性导管或动脉的夹闭。我们的结果表明，与非结构化替代方案相比，我们的方法具有优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01974v1" target="_blank">2402.01974v1</a>
                              </td>
                              <td>Hypergraph-Transformer (HGT) for Interactive Event Prediction in Laparoscopic and Robotic Surgery</td>
                              <td>Lianhao Yin</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01974v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01974v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01950v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01950v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01950v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01950v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most of the existing works on arbitrary 3D NeRF style transfer required retraining on each single style condition. This work aims to achieve zero-shot controlled stylization in 3D scenes utilizing text or visual input as conditioning factors. We introduce ConRF, a novel method of zero-shot stylization. Specifically, due to the ambiguity of CLIP features, we employ a conversion process that maps the CLIP feature space to the style space of a pre-trained VGG network and then refine the CLIP multi-modal knowledge into a style transfer neural radiation field. Additionally, we use a 3D volumetric representation to perform local style transfer. By combining these operations, ConRF offers the capability to utilize either text or images as references, resulting in the generation of sequences with novel views enhanced by global or local stylization. Our experiment demonstrates that ConRF outperforms other existing methods for 3D scene and single-text stylization in terms of visual quality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01950v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数现有的关于任意3D NeRF风格转移的工作都需要对每个单一的风格条件进行重新训练。这项工作旨在利用文本或视觉输入作为条件因素，在3D场景中实现零样本控制的风格化。我们介绍了一种新的零样本风格化方法ConRF。具体来说，由于CLIP特征的模糊性，我们采用了一种转换过程，将CLIP特征空间映射到预先训练的VGG网络的风格空间，然后将CLIP多模态知识细化为风格转移神经辐射场。此外，我们使用三维体积表示来执行局部样式转换。通过结合这些操作，ConRF提供了利用文本或图像作为参考的能力，从而生成具有通过全局或局部风格化增强的新视图的序列。我们的实验表明，在视觉质量方面，ConRF在3D场景和单个文本风格化方面优于其他现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01950v1" target="_blank">2402.01950v1</a>
                              </td>
                              <td>ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation Fields</td>
                              <td>Xingyu Miao</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01950v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01950v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xingy038/conrf" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01832v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01832v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01832v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01832v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01832v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了SynthCLIP，这是一种用完全合成的文本图像对训练CLIP模型的新框架，与以前依赖真实数据的方法明显不同。利用最近的文本到图像（TTI）生成网络和大型语言模型（LLM），我们能够在任何规模上生成图像和相应字幕的合成数据集，而无需人工干预。通过大规模训练，SynthCLIP实现了与在真实数据集上训练的CLIP模型相当的性能。我们还介绍了SynthCI-30M，这是一个纯合成的数据集，包含3000万张字幕图像。我们的代码、经过训练的模型和生成的数据发布在https://github.com/hammoudhasan/SynthCLIP</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01832v1" target="_blank">2402.01832v1</a>
                              </td>
                              <td>SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?</td>
                              <td>Hasan Abed Al Kader Hammoud</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01832v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01832v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hammoudhasan/synthclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03610v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Data Curation in Image Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03610v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03610v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03610v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets without increasing the total number of samples. We explore the effect of using three data curation methods within the training process: complete removal of an sample, caption replacement, or image replacement via a text-to-image generation model. Experiments on the Flickr30K and COCO datasets with the BLIP and BEiT-3 models demonstrate that these curation methods do indeed yield improved image captioning models, underscoring their efficacy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03610v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像字幕模型通常是通过平等对待所有样本来训练的，忽略了不匹配或其他困难的数据点。相比之下，最近的工作通过使用课程学习策略对数据进行调度，显示了训练模型的有效性。本文通过在不增加样本总数的情况下积极管理数据集中的困难样本，为这一方向做出了贡献。我们探讨了在训练过程中使用三种数据管理方法的效果：完全去除样本、字幕替换或通过文本到图像生成模型进行图像替换。使用BLIP和BEiT-3模型在Flickr30K和COCO数据集上进行的实验表明，这些策展方法确实产生了改进的图像字幕模型，突出了它们的功效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03610v2" target="_blank">2305.03610v2</a>
                              </td>
                              <td>The Role of Data Curation in Image Captioning</td>
                              <td>Wenyan Li</td>
                              <td>2023-05-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03610v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03610v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01399v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Probabilistic Model to explain Self-Supervised Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01399v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01399v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01399v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows the gap to discriminative methods on _content_ classification and, as our analysis predicts, outperforms them where _style_ information is required, taking a step toward task-agnostic representations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01399v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）通过利用辅助的无监督任务来学习表示，例如对语义相关的样本进行分类，例如不同的数据增强或模态。在SSL的许多方法中，对比方法，如SimCLR、CLIP和VicREG，因其获得接近监督学习的下游性能的学习表示而受到关注。然而，对这些方法背后的机制缺乏理论上的理解。我们为数据提出了一个生成潜变量模型，并表明包括对比方法在内的几个判别自监督算法家族近似地在表示上诱导了其潜在结构，提供了一个统一的理论框架。我们还证明了相互信息的联系和投影头的使用是合理的。将我们的模型作为SimVE进行生成拟合，在通用基准（例如FashionMNIST、CIFAR10、CelebA）上比以前的VAE方法提高了性能，缩小了与内容分类判别方法的差距，并且正如我们的分析预测的那样，在需要_样式_信息的情况下优于它们，朝着任务不可知的表示迈出了一步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01399v1" target="_blank">2402.01399v1</a>
                              </td>
                              <td>A Probabilistic Model to explain Self-Supervised Representation Learning</td>
                              <td>Alice Bizeul</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01399v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01399v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01241v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01241v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01241v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01241v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation. The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results. Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images? Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality? This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus. Our comprehensive analysis assesses CISP's guidance performance against CLIP guided models, focusing on generation quality, diversity, and coherence of the produced shapes with the conditioning image. We find that, while matching CLIP in generation quality and diversity, CISP substantially improves coherence with input images, underscoring the value of incorporating 3D knowledge into generative models. These findings suggest a promising direction for advancing the synthesis of 3D visual content by integrating multimodal systems with 3D representations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01241v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度生成模型的最新进展，特别是对比语言图像预训练（CLIP）在去噪扩散概率模型（DDPM）中的应用，已经证明了文本到图像生成的显著有效性。CLIP结构良好的嵌入空间也被扩展到DDPM的图像到形状生成，产生了显著的结果。尽管取得了这些成功，但还是出现了一些基本问题：CLIP是否能确保从图像中生成形状的最佳结果？我们能利用条件反射将明确的3D知识引入生成过程并获得更好的质量吗？本研究介绍了CISP（对比图像形状预训练），旨在增强二维图像引导下的三维形状合成。CISP旨在通过在共享的嵌入空间中将2D图像与3D形状对齐来丰富CLIP框架，特别是捕捉CLIP的文本图像焦点可能忽略的3D特征。我们的综合分析评估了CISP相对于CLIP引导模型的引导性能，重点关注生成的形状与条件图像的生成质量、多样性和一致性。我们发现，在生成质量和多样性方面与CLIP相匹配的同时，CISP显著提高了与输入图像的一致性，强调了将3D知识融入生成模型的价值。这些发现为通过将多模式系统与3D表示相集成来推进3D视觉内容的合成提供了一个有希望的方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01241v1" target="_blank">2402.01241v1</a>
                              </td>
                              <td>Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?</td>
                              <td>Cristian Sbrolli</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01241v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01241v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12425v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Neglected Tails of Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12425v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12425v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12425v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language models (VLMs) excel in zero-shot recognition but their performance varies greatly across different visual concepts. For example, although CLIP achieves impressive accuracy on ImageNet (60-80%), its performance drops below 10% for more than ten concepts like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in VLMs' large-scale datasets is challenging. We address this by using large language models (LLMs) to count the number of pretraining texts that contain synonyms of these concepts. Our analysis confirms that popular datasets, such as LAION, exhibit a long-tailed concept distribution, yielding biased performance in VLMs. We also find that downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and text-to-image models (e.g., Stable Diffusion), often fail to recognize or generate images of rare concepts identified by our method. To mitigate the imbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented Learning (REAL). First, instead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly human-engineered and LLM-enriched prompts over nine benchmark datasets. Second, REAL trains a linear classifier on a small yet balanced set of pretraining data retrieved using concept synonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage and 10,000x less training time!</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12425v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言模型（VLM）在零样本识别方面表现出色，但它们的性能在不同的视觉概念之间差异很大。例如，尽管CLIP在ImageNet上实现了令人印象深刻的准确性（60-80%），但对于像夜蛇这样的十多个概念，其性能下降到10%以下，这可能是由于它们在预训练数据中的存在有限。然而，测量VLM大规模数据集中概念的频率具有挑战性。我们通过使用大型语言模型（LLM）来计算包含这些概念的同义词的预训练文本的数量来解决这一问题。我们的分析证实，流行的数据集，如LAION，表现出长尾概念分布，在VLM中产生有偏差的性能。我们还发现，VLM的下游应用程序，包括视觉聊天机器人（如GPT-4V）和文本到图像模型（如稳定扩散），往往无法识别或生成我们的方法识别的罕见概念的图像。为了缓解零样本VLM的不平衡性能，我们提出了改进学习（REAL）。首先，REAL不使用原始类名提示VLM，而是使用预训练文本中最常见的同义词。在九个基准数据集中，这一简单的更改已经优于成本高昂的人工设计和LLM丰富的提示。其次，REAL在使用概念同义词检索的一组小而平衡的预训练数据上训练线性分类器。REAL超越了以前的零样本SOTA，使用的存储空间减少了400倍，训练时间减少了10000倍！</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12425v2" target="_blank">2401.12425v2</a>
                              </td>
                              <td>The Neglected Tails of Vision-Language Models</td>
                              <td>Shubham Parashar</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12425v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12425v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00827v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00827v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00827v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00827v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00827v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>艺术视频人像生成是计算机图形学和视觉领域中一项重要且备受追捧的任务。虽然已经开发了各种方法，将NeRF或StyleGANs与教学编辑模型相结合，用于创建和编辑可驾驶的肖像，但这些方法面临着一些挑战。它们通常严重依赖大型数据集，需要大量的定制过程，并经常导致图像质量下降。为了解决上述问题，我们提出了通过延迟神经渲染的高效单音视频风格化身（Emo Avatar），该化身增强了StyleGAN制作动态、可驱动的人像视频的能力。我们提出了一种两阶段延迟神经渲染流水线。在第一阶段，我们利用少镜头PTI初始化，通过从视频中采样的几个极端姿势来初始化StyleGAN生成器，以从目标肖像中捕捉对齐人脸的一致表示。在第二阶段，我们提出了一种拉普拉斯金字塔，用于从通过动态表达流变形的UV图中进行高频纹理采样，用于运动感知纹理的先验集成，以提供躯干特征，从而增强StyleGAN生成完整和上身的能力，用于人像视频渲染。与现有方法相比，Emo Avatar将风格定制时间从数小时缩短到仅5分钟。此外，Emo Avatar只需要一张参考图像进行编辑，并采用了具有语义不变CLIP指导的区域感知对比学习，确保了一致的高分辨率输出和身份保存。通过定量和定性评估，Emo Avatar在训练效率、渲染质量和可编辑性方面表现出了优于现有方法的自我和交叉再现性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00827v1" target="_blank">2402.00827v1</a>
                              </td>
                              <td>Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering</td>
                              <td>Pinxin Liu</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00827v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00827v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18001v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DP-SGD with weight clipping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18001v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18001v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18001v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging a public upper bound of the Lipschitz value of the current model and its current location within the search domain, we can achieve refined noise level adjustments. We present a new algorithm with improved differential privacy guarantees and a systematic empirical evaluation, showing that our new approach outperforms existing approaches also in practice.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18001v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，由于深度神经网络和其他方法的流行，其训练通常依赖于目标函数的优化，并且由于对数据隐私的担忧，人们对差异私有梯度下降方法产生了很大的兴趣。为了在最小的噪声量下实现差分隐私保证，能够精确地约束参与者将观察到的信息的灵敏度是很重要的。在这项研究中，我们提出了一种新的方法，可以减轻传统梯度剪裁带来的偏差。通过利用当前模型的Lipschitz值的公共上界及其在搜索域内的当前位置，我们可以实现精细的噪声水平调整。我们提出了一种新的算法，该算法具有改进的差分隐私保证和系统的经验评估，表明我们的新方法在实践中也优于现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18001v2" target="_blank">2310.18001v2</a>
                              </td>
                              <td>DP-SGD with weight clipping</td>
                              <td>Antoine Barczewski</td>
                              <td>2023-10-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18001v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18001v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00703v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Vehicle Perception from Satellite</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00703v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00703v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00703v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00703v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>卫星能够捕捉高分辨率视频。它使卫星对车辆的感知成为可能。与街道监控、行车记录仪或其他设备相比，卫星视频提供了更广阔的城市视野，从而捕捉和显示了全球交通动态场景。卫星交通监测是一项具有巨大应用潜力的新任务，包括交通堵塞预测、路径规划、车辆调度、emph等。实际上，受分辨率和视野的限制，捕获的车辆非常小（只有几个像素），移动缓慢。更糟糕的是，这些卫星位于近地轨道（LEO），用于拍摄此类高分辨率视频，因此背景也在移动。在这种情况下，从卫星视角进行交通监测是一项极具挑战性的任务。为了吸引更多的研究人员进入这一领域，我们建立了一个大规模的卫星交通监测基准。它支持多项任务，包括微小物体检测、计数和密度估计。该数据集是基于GTA-V记录的12个卫星视频和14个合成视频构建的。它们被分为408个视频片段，其中包含7336个真实卫星图像和1960个合成图像。总共注释了128801辆车，每张图像中的车辆数量从0到101不等。在数据集上评估了传统计算机视觉中的几种经典和最先进的方法，以比较不同方法的性能，分析这项任务中的挑战，并讨论未来的前景。数据集位于：https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00703v1" target="_blank">2402.00703v1</a>
                              </td>
                              <td>Vehicle Perception from Satellite</td>
                              <td>Bin Zhao</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00703v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00703v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/chenxi1510/vehicle-perception-from-satellite-videos" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14232v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parrot Captions Teach CLIP to Spot Text</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14232v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14232v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14232v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite CLIP being the foundation model in numerous vision-language applications, the CLIP suffers from a severe text spotting bias. Such bias causes CLIP models to `Parrot' the visual text embedded within images while disregarding the authentic visual semantics. We uncover that in the most popular image-text dataset LAION-2B, the captions also densely parrot (spell) the text embedded in images. Our analysis shows that around 50% of images are embedded with visual text content, and around 30% of captions words are in these embedded visual content. Based on such observation, we thoroughly inspect the different released versions of CLIP models and verify that the visual text is the dominant factor in measuring the LAION-style image-text similarity for these models. To examine whether these parrot captions shape the text spotting bias, we train a series of CLIP models with LAION subsets curated by different parrot-caption-oriented criteria. We show that training with parrot captions easily shapes such bias but harms the expected visual-language representation learning in CLIP models. This suggests that it is urgent to revisit either the design of CLIP-like models or the existing image-text dataset curation pipeline built on CLIP score filtering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14232v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管CLIP是许多视觉语言应用程序的基础模型，但CLIP存在严重的文本识别偏见。这种偏见导致CLIP模型“模仿”嵌入图像中的视觉文本，而忽略真实的视觉语义。我们发现，在最流行的图像文本数据集LAION-2B中，字幕也密集地鹦鹉学舌（拼写）嵌入图像中的文本。我们的分析表明，大约50%的图像嵌入了视觉文本内容，大约30%的字幕词嵌入了这些嵌入的视觉内容。基于这样的观察，我们彻底检查了CLIP模型的不同发布版本，并验证了视觉文本是衡量这些模型LAION风格的图像文本相似性的主导因素。为了检验这些鹦鹉字幕是否影响了文本识别偏差，我们用不同的面向鹦鹉字幕的标准策划的LAION子集训练了一系列CLIP模型。我们发现，使用鹦鹉字幕的训练很容易形成这种偏见，但会损害CLIP模型中预期的视觉语言表示学习。这表明，迫切需要重新审视类似CLIP模型的设计，或者基于CLIP分数过滤的现有图像-文本数据集管理管道。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14232v3" target="_blank">2312.14232v3</a>
                              </td>
                              <td>Parrot Captions Teach CLIP to Spot Text</td>
                              <td>Yiqi Lin</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14232v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14232v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/opendatalab/clip-parrot-bias" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_18035v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimizing contrastive learning for cortical folding pattern detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_18035v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_18035v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_18035v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The human cerebral cortex has many bumps and grooves called gyri and sulci. Even though there is a high inter-individual consistency for the main cortical folds, this is not the case when we examine the exact shapes and details of the folding patterns. Because of this complexity, characterizing the cortical folding variability and relating them to subjects' behavioral characteristics or pathologies is still an open scientific problem. Classical approaches include labeling a few specific patterns, either manually or semi-automatically, based on geometric distances, but the recent availability of MRI image datasets of tens of thousands of subjects makes modern deep-learning techniques particularly attractive. Here, we build a self-supervised deep-learning model to detect folding patterns in the cingulate region. We train a contrastive self-supervised model (SimCLR) on both Human Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with topological-based augmentations on the cortical skeletons, which are topological objects that capture the shape of the folds. We explore several backbone architectures (convolutional network, DenseNet, and PointNet) for the SimCLR. For evaluation and testing, we perform a linear classification task on a database manually labeled for the presence of the "double-parallel" folding pattern in the cingulate region, which is related to schizophrenia characteristics. The best model, giving a test AUC of 0.76, is a convolutional network with 6 layers, a 10-dimensional latent space, a linear projection head, and using the branch-clipping augmentation. This is the first time that a self-supervised deep learning model has been applied to cortical skeletons on such a large dataset and quantitatively evaluated. We can now envisage the next step: applying it to other brain regions to detect other biomarkers.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_18035v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类大脑皮层有许多突起和凹槽，称为脑回和脑沟。尽管主要皮层褶皱具有很高的个体间一致性，但当我们检查折叠模式的确切形状和细节时，情况并非如此。由于这种复杂性，表征皮层折叠的可变性并将其与受试者的行为特征或病理联系起来仍然是一个悬而未决的科学问题。经典的方法包括根据几何距离手动或半自动标记一些特定的模式，但最近数万名受试者的MRI图像数据集的可用性使现代深度学习技术特别有吸引力。在这里，我们建立了一个自监督的深度学习模型来检测扣带区域的折叠模式。我们在人类连接体项目（1101名受试者）和英国生物银行（21070名受试人）数据集上训练了一个对比自监督模型（SimCLR），该模型在皮层骨架上进行了基于拓扑的增强，皮层骨架是捕捉褶皱形状的拓扑对象。我们探索了SimCLR的几种骨干架构（卷积网络、DenseNet和PointNet）。为了评估和测试，我们在一个数据库上执行线性分类任务，该数据库手动标记扣带区域是否存在“双平行”折叠模式，这与精神分裂症特征有关。给出0.76的测试AUC的最佳模型是具有6层、10维潜在空间、线性投影头并使用分支剪切增强的卷积网络。这是首次将自监督深度学习模型应用于如此大的数据集上的皮层骨骼并进行定量评估。我们现在可以设想下一步：将其应用于其他大脑区域，以检测其他生物标志物。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.18035v1" target="_blank">2401.18035v1</a>
                              </td>
                              <td>Optimizing contrastive learning for cortical folding pattern detection</td>
                              <td>Aymeric Gaudin</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_18035v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.18035v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neurospin-projects/2022_jchavas_cingulate_inhibitory_control" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17797v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17797v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17797v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17797v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17797v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一个多模式配方，用于推进基于自适应的预训练，以实现有效和高效的零样本视频文本检索，称为M2-RAAP。在CLIP等流行的图像文本模型上，当前大多数基于自适应的视频文本预训练方法都面临三个主要问题，即数据语料库噪声大、预训练耗时和性能增益有限。为此，我们进行了一项全面的研究，包括视频文本预训练的四个关键步骤。具体而言，我们研究了1）数据过滤和细化，2）视频输入类型选择，3）时间建模，以及4）视频特征增强。然后，我们将这项实证研究总结到M2-RAAP配方中，其中我们的技术贡献在于1）数据过滤和文本重写管道，产生了1M个高质量的双语视频-文本对，2）用关键帧替换视频输入，以加速预训练，3）辅助字幕引导（ACG）策略，以增强视频特征。我们在来自不同语言的两个精炼视频文本数据集上对三个图像文本基础模型进行了广泛的实验，验证了M2-RAAP对基于自适应的预训练的鲁棒性和再现性。结果表明，M2-RAAP在显著减少数据（-90%）和时间消耗（-95%）的情况下产生了优异的性能，在四个英文零样本检索数据集和两个中文检索数据集上建立了新的SOTA。我们正在准备完善的双语数据注释和代码库，可在https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17797v1" target="_blank">2401.17797v1</a>
                              </td>
                              <td>M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval</td>
                              <td>Xingning Dong</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17797v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17797v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alipay/Ant-Multi-Modal-Framework" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12445v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12445v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12445v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12445v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12445v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文献中的大量例子证明，深度学习模型能够很好地处理多模式数据。最近，CLIP使深度学习系统能够学习图像和文本描述之间的共享潜在空间，在下游任务中具有出色的零或少量拍摄结果。在本文中，我们探索了CLIP提出的相同想法，但将其应用于语音领域，语音空间和声学空间通常共存。我们训练了一个基于CLIP的模型，目的是学习语音和声学空间的共享表示。结果表明，所提出的模型对语音变化是敏感的，当随机替换20%的音素时，分数下降了91%，同时对不同类型的噪声提供了相当大的鲁棒性，当将音频与75%的高斯噪声混合时，性能下降了10%。我们还提供了经验证据，表明所产生的嵌入对于各种下游应用是有用的，例如可懂度评估和在语音生成任务中利用丰富的预训练语音嵌入的能力。最后，我们讨论了对语音生成和识别领域具有有趣意义的潜在应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12445v2" target="_blank">2307.12445v2</a>
                              </td>
                              <td>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</td>
                              <td>Ivan Vallés-Pérez</td>
                              <td>2023-07-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12445v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12445v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17186v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17186v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17186v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17186v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at \url{https://github.com/yangbang18/CLFM}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17186v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然视觉语言预训练模型（VL-PTM）近年来在多模式研究方面取得了进展，但它们对英语等少数语言的掌握限制了它们在更广泛社区中的适用性。为此，人们对通过联合学习设置开发多语言VL模型越来越感兴趣，然而，由于成本高昂和数据可用性，这可能是不现实的。在这项工作中，我们建议通过持续语言学习（CLL）来扩展VL-PTM的语言能力，其中模型需要在不遭受灾难性遗忘（CF）的情况下逐步更新其语言知识。我们首先介绍了一个名为CLL-CLIP的模型，该模型建立在CLIP的基础上，CLIP是一种流行的VL-PTM，它已经获得了图像-英语-文本对齐。具体来说，CLL-CLIP包含一个可扩展的令牌嵌入层来处理语言差异。它只训练令牌嵌入以提高记忆稳定性，并在跨模态和跨语言目标下进行优化，以学习图像和多语言文本之间的对齐。为了减轻协变移位和词汇重叠引起的CF，我们进一步提出了一种新的方法，该方法确保初始化期间所有令牌嵌入的分布相同，并在训练期间正则化令牌嵌入学习。我们基于MSCOCO和XM3600数据集构建了一个覆盖36种语言的CLL基准，然后评估了多语言图像文本检索性能。大量实验验证了CLL-CLIP的有效性，并表明我们的方法可以提高CLL-CLP，例如，在文本到图像的平均值中提高6.7%Recall@1在XM3600上，并不断改进各种最先进的方法。我们的代码和数据位于\url{https://github.com/yangbang18/CLFM}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17186v1" target="_blank">2401.17186v1</a>
                              </td>
                              <td>Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning</td>
                              <td>Bang Yang</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17186v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17186v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16702v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-granularity Correspondence Learning from Long-term Noisy Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16702v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16702v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16702v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing video-language studies mainly focus on learning short video clips, leaving long-term temporal dependencies rarely explored due to over-high computational cost of modeling long videos. To address this issue, one feasible solution is learning the correspondence between video clips and captions, which however inevitably encounters the multi-granularity noisy correspondence (MNC) problem. To be specific, MNC refers to the clip-caption misalignment (coarse-grained) and frame-word misalignment (fine-grained), hindering temporal learning and video understanding. In this paper, we propose NOise Robust Temporal Optimal traNsport (Norton) that addresses MNC in a unified optimal transport (OT) framework. In brief, Norton employs video-paragraph and clip-caption contrastive losses to capture long-term dependencies based on OT. To address coarse-grained misalignment in video-paragraph contrast, Norton filters out the irrelevant clips and captions through an alignable prompt bucket and realigns asynchronous clip-caption pairs based on transport distance. To address the fine-grained misalignment, Norton incorporates a soft-maximum operator to identify crucial words and key frames. Additionally, Norton exploits the potential faulty negative samples in clip-caption contrast by rectifying the alignment target with OT assignment to ensure precise temporal modeling. Extensive experiments on video retrieval, videoQA, and action segmentation verify the effectiveness of our method. Code is available at https://lin-yijie.github.io/projects/Norton.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16702v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的视频语言研究主要集中在学习短视频片段上，由于长视频建模的计算成本过高，长期的时间依赖性很少被探索。为了解决这个问题，一个可行的解决方案是学习视频片段和字幕之间的对应关系，然而这不可避免地会遇到多粒度噪声对应关系（MNC）问题。具体来说，MNC指的是剪辑字幕错位（粗粒度）和帧词错位（细粒度），阻碍了时间学习和视频理解。在本文中，我们提出了在统一最优传输（OT）框架中解决MNC问题的NOise鲁棒时间最优traNsport（Norton）。简而言之，Norton采用视频段落和剪辑字幕对比损失来捕捉基于OT的长期依赖关系。为了解决视频段落对比中的粗粒度错位问题，Norton通过可对齐的提示桶过滤掉不相关的剪辑和字幕，并根据传输距离重新对齐异步剪辑和字幕对。为了解决细粒度的错位问题，Norton引入了一个软最大算子来识别关键单词和关键帧。此外，Norton通过OT分配校正对准目标，利用剪辑字幕对比度中潜在的错误负样本，以确保精确的时间建模。在视频检索、视频质量保证和动作分割方面的大量实验验证了我们方法的有效性。代码位于https://lin-yijie.github.io/projects/Norton.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16702v1" target="_blank">2401.16702v1</a>
                              </td>
                              <td>Multi-granularity Correspondence Learning from Long-term Noisy Videos</td>
                              <td>Yijie Lin</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16702v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16702v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16347v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Coordination Across a Diverse Set of Input Modalities</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16347v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16347v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16347v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16347v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>跨模态检索是通过使用不同模态的查询来检索给定模态的样本的任务。由于实际应用范围广泛，问题主要集中在视觉和语言方面，例如文本到图像检索，其中像CLIP这样的模型已被证明在解决此类任务方面是有效的。学习这种协调表示的主要方法是将它们投影到一个公共空间上，在该空间中，匹配的视图保持接近，而来自不匹配对的视图则被推离。尽管这种跨模态协调也适用于其他成对组合，但将其扩展到任意数量的不同模态是一个文献中尚未充分探讨的问题。在本文中，我们提出了两种不同的方法来解决这个问题。第一种是基于将CLIP对比目标扩展到任意数量的输入模态，而第二种则偏离对比公式，通过将跨模态相似性回归到反映跨模态检索任务的两个简单直观约束的目标来解决协调问题。我们在两个不同的数据集上，在不同的输入模式组合上进行了实验，结果表明该方法不仅简单有效，而且可以以新颖的方式解决检索问题。除了捕获一组更多样的成对交互之外，我们还表明，我们可以使用学习到的表示，通过组合来自两个或多个这样的模态的嵌入来提高检索性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16347v1" target="_blank">2401.16347v1</a>
                              </td>
                              <td>Cross-Modal Coordination Across a Diverse Set of Input Modalities</td>
                              <td>Jorge Sánchez</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16347v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16347v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16280v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16280v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16280v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16280v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: "Fall", "Lying" and "Other/Activities of daily living (ADL)". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16280v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作探索了大型视频理解基础模型在未修剪视频上人类跌倒检测下游任务中的性能，并利用预训练的视觉转换器进行多类动作检测，包括“跌倒”、“说谎”和“日常生活的其他/活动（ADL）”。演示了一种基于未修剪视频的简单剪切的时间动作定位方法。该方法包括一个预处理管道，用于将带有时间戳动作注释的数据集转换为短动作片段的标记数据集。介绍了简单有效的剪辑采样策略。已在公开的高质量秋季模拟数据集（HQFSD）上对所提出方法的有效性进行了实证评估。实验结果验证了所提出的管道的性能。该结果有希望用于实时应用，并且在给定的实验设置下，在HQFSD数据集上以最先进的0.96F1分数在视频级别上检测到跌倒。源代码将在GitHub上提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16280v1" target="_blank">2401.16280v1</a>
                              </td>
                              <td>Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model</td>
                              <td>Till Grutschus</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16280v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16280v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16265v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CO2: Efficient Distributed Training with Full Communication-Computation Overlap</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16265v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16265v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16265v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16265v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的根本成功取决于大规模分布式训练技术的有效实现。尽管如此，构建一个具有高速通信互连性的庞大、高性能集群的成本高得令人望而却步，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并在带宽有限的集群中实现大规模训练的民主化。我们提出了一种称为CO2的新方法，该方法将本地更新和异步通信引入分布式数据并行训练，从而促进了CO2通信与计算的完全重叠。即使在受非常有限的通信带宽限制的广泛的多节点集群上，CO2也能够获得高可扩展性。我们进一步提出了过时间隙惩罚和外部动量修剪技术以及CO2，以增强其收敛性和训练稳定性。此外，CO2与成熟的ZeRO系列优化器无缝集成，通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上界。此外，我们通过一系列广泛的实践实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验用于证明当在包括多达128个A100 GPU的配置中部署时，CO2在收敛性、通用性和可扩展性方面的能力。无论是在具有800Gbps RDMA或80Gbps TCP/IP节点间连接的集群上，结果都强调了CO2的卓越容量，可以极大地提高可扩展性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16265v1" target="_blank">2401.16265v1</a>
                              </td>
                              <td>CO2: Efficient Distributed Training with Full Communication-Computation Overlap</td>
                              <td>Weigao Sun</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16265v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16265v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12665v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12665v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12665v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12665v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了良好的性能。然而，基于CLIP或基于SAM的ZSAS方法仍然存在不可忽略的关键缺点：1）CLIP主要关注不同输入的全局特征对齐，导致局部异常部分的分割不精确；2） SAM倾向于在没有适当提示约束的情况下生成大量冗余掩码，从而导致复杂的后处理要求。在这项工作中，我们创新性地为ZSAS提出了一个名为ClipSAM的CLIP和SAM合作框架。ClipSAM背后的见解是利用CLIP的语义理解能力进行异常定位和粗略分割，这进一步被用作SAM细化异常分割结果的提示约束。详细地说，我们介绍了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在CLIP的多个尺度上与视觉特征进行交互，以确定异常位置。然后，我们设计了一个新的多级掩码细化（MMR）模块，该模块利用位置信息作为SAM的多级提示来获取分层掩码并将其合并。大量实验验证了我们方法的有效性，在MVTec AD和VisA数据集上实现了最佳分割性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12665v2" target="_blank">2401.12665v2</a>
                              </td>
                              <td>ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</td>
                              <td>Shengze Li</td>
                              <td>2024-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12665v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12665v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lszcoding/clipsam" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16048v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16048v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16048v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16048v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces innovative benchmarks to evaluate Vision-Language Models (VLMs) in real-world zero-shot recognition tasks, focusing on the granularity and specificity of prompting text. We propose a unique evaluation protocol using adapted ImageNet and MS-COCO datasets to assess models' consistency in recognizing concepts at varying granularity levels and their sensitivity to the specificity of language inputs. Our extensive evaluation reveals that state-of-the-art VLMs, including contrastive models like CLIP, struggle with granularity and are sensitive to text specificity, impacting their effectiveness in open-world settings. This comprehensive study, a first in evaluating VLMs from these perspectives, provides valuable insights and tools for the community, highlighting the limitations and paving the way for enhanced models with better generalization in zero-shot recognition.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16048v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了在现实零样本识别任务中评估视觉语言模型（VLM）的创新基准，重点关注提示文本的粒度和特异性。我们提出了一种独特的评估协议，使用经过调整的ImageNet和MS-COCO数据集来评估模型在不同粒度水平上识别概念的一致性及其对语言输入的特异性的敏感性。我们的广泛评估表明，最先进的VLM，包括CLIP等对比模型，在粒度上很吃力，并且对文本特异性很敏感，影响了它们在开放世界环境中的有效性。这项综合研究首次从这些角度评估VLM，为社区提供了有价值的见解和工具，突出了其局限性，并为增强模型在零样本识别中更好地泛化铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16048v2" target="_blank">2306.16048v2</a>
                              </td>
                              <td>Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity</td>
                              <td>Zhenlin Xu</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16048v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16048v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16025v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Simple Policy Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16025v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16025v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16025v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose SPO (Simple Policy Optimization) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. SPO can effectively enforce the trust region constraints in almost all environments, while still maintaining the simplicity of a first-order algorithm. Comparative experiments in Atari 2600 environments show that SPO sometimes provides stronger performance than PPO. Code is available at https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16025v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>PPO（Proximal Policy Optimization）算法在许多领域都表现出了优异的性能，被认为是TRPO（Trust Region Policy Optimization）算法的一个简单版本。然而，PPO中的比率裁剪操作可能并不总是有效地执行信任区域约束，这可能是影响算法稳定性的潜在因素。在本文中，我们提出了SPO（简单策略优化）算法，该算法引入了一种新的裁剪方法来解决旧策略和当前策略之间的KL分歧。SPO可以在几乎所有环境中有效地执行信任区域约束，同时仍然保持一阶算法的简单性。在雅达利2600环境中的对比实验表明，SPO有时比PPO提供更强的性能。代码位于https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16025v1" target="_blank">2401.16025v1</a>
                              </td>
                              <td>Simple Policy Optimization</td>
                              <td>Zhengpeng Xie</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16025v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16025v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/myrepositories-hub/simple-policy-optimization" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15657v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data-Free Generalized Zero-Shot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15657v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15657v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15657v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models have the ability to extract rich knowledge from large-scale datasets. However, the sharing of data has become increasingly challenging due to concerns regarding data copyright and privacy. Consequently, this hampers the effective transfer of knowledge from existing data to novel downstream tasks and concepts. Zero-shot learning (ZSL) approaches aim to recognize new classes by transferring semantic knowledge learned from base classes. However, traditional generative ZSL methods often require access to real images from base classes and rely on manually annotated attributes, which presents challenges in terms of data restrictions and model scalability. To this end, this paper tackles a challenging and practical problem dubbed as data-free zero-shot learning (DFZSL), where only the CLIP-based base classes data pre-trained classifier is available for zero-shot classification. Specifically, we propose a generic framework for DFZSL, which consists of three main components. Firstly, to recover the virtual features of the base data, we model the CLIP features of base class images as samples from a von Mises-Fisher (vMF) distribution based on the pre-trained classifier. Secondly, we leverage the text features of CLIP as low-cost semantic information and propose a feature-language prompt tuning (FLPT) method to further align the virtual image features and textual features. Thirdly, we train a conditional generative model using the well-aligned virtual image features and corresponding semantic text features, enabling the generation of new classes features and achieve better zero-shot generalization. Our framework has been evaluated on five commonly used benchmarks for generalized ZSL, as well as 11 benchmarks for the base-to-new ZSL. The results demonstrate the superiority and effectiveness of our approach. Our code is available in https://github.com/ylong4/DFZSL</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15657v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型能够从大规模数据集中提取丰富的知识。然而，由于对数据版权和隐私的担忧，数据共享变得越来越具有挑战性。因此，这阻碍了知识从现有数据到新的下游任务和概念的有效转移。零样本学习（ZSL）方法旨在通过转移从基类学习的语义知识来识别新的类。然而，传统的生成ZSL方法通常需要从基类访问真实图像，并依赖于手动注释的属性，这在数据限制和模型可扩展性方面带来了挑战。为此，本文解决了一个具有挑战性和实用性的问题，称为无数据零样本学习（DFZSL），其中只有基于CLIP的基类数据预训练分类器可用于零样本分类。具体来说，我们为DFZSL提出了一个通用框架，它由三个主要组件组成。首先，为了恢复基本数据的虚拟特征，我们基于预训练的分类器，将基类图像的CLIP特征建模为来自von Mises Fisher（vMF）分布的样本。其次，我们利用CLIP的文本特征作为低成本的语义信息，提出了一种特征语言提示调整（FLPT）方法来进一步调整虚拟图像特征和文本特征。第三，我们使用对齐的虚拟图像特征和相应的语义文本特征来训练条件生成模型，从而能够生成新的类特征，并实现更好的零样本泛化。我们的框架已经在通用ZSL的五个常用基准以及新ZSL的基础的11个基准上进行了评估。结果证明了我们的方法的优越性和有效性。我们的代码位于https://github.com/ylong4/DFZSL</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15657v1" target="_blank">2401.15657v1</a>
                              </td>
                              <td>Data-Free Generalized Zero-Shot Learning</td>
                              <td>Bowen Tang</td>
                              <td>2024-01-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15657v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15657v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17891v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17891v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17891v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17891v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces the novel concept of few-shot weakly supervised learning for pathology Whole Slide Image (WSI) classification, denoted as FSWC. A solution is proposed based on prompt learning and the utilization of a large language model, GPT-4. Since a WSI is too large and needs to be divided into patches for processing, WSI classification is commonly approached as a Multiple Instance Learning (MIL) problem. In this context, each WSI is considered a bag, and the obtained patches are treated as instances. The objective of FSWC is to classify both bags and instances with only a limited number of labeled bags. Unlike conventional few-shot learning problems, FSWC poses additional challenges due to its weak bag labels within the MIL framework. Drawing inspiration from the recent achievements of vision-language models (V-L models) in downstream few-shot classification tasks, we propose a two-level prompt learning MIL framework tailored for pathology, incorporating language prior knowledge. Specifically, we leverage CLIP to extract instance features for each patch, and introduce a prompt-guided pooling strategy to aggregate these instance features into a bag feature. Subsequently, we employ a small number of labeled bags to facilitate few-shot prompt learning based on the bag features. Our approach incorporates the utilization of GPT-4 in a question-and-answer mode to obtain language prior knowledge at both the instance and bag levels, which are then integrated into the instance and bag level language prompts. Additionally, a learnable component of the language prompts is trained using the available few-shot labeled data. We conduct extensive experiments on three real WSI datasets encompassing breast cancer, lung cancer, and cervical cancer, demonstrating the notable performance of the proposed method in bag and instance classification. All codes will be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17891v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了用于病理学全玻片图像（WSI）分类的少镜头弱监督学习的新概念，称为FSWC。提出了一种基于即时学习和利用大型语言模型GPT-4的解决方案。由于WSI太大，需要分为多个补丁进行处理，因此WSI分类通常被视为多实例学习（MIL）问题。在这种情况下，每个WSI都被视为一个包，并且所获得的补丁被视为实例。FSWC的目标是用有限数量的贴有标签的袋子对袋子和实例进行分类。与传统的少镜头学习问题不同，FSWC由于其在MIL框架内的薄弱袋标签而带来了额外的挑战。从视觉语言模型（V-L模型）在下游少镜头分类任务中的最新成就中汲取灵感，我们提出了一个针对病理学量身定制的两级即时学习MIL框架，结合了语言先验知识。具体来说，我们利用CLIP为每个补丁提取实例特征，并引入一种提示引导的池策略，将这些实例特征聚合到一个包特征中。随后，我们使用少量标记的袋子来促进基于袋子特征的少镜头提示学习。我们的方法在问答模式中结合了GPT-4的使用，以获得实例和包级别的语言先验知识，然后将其集成到实例和包级的语言提示中。此外，使用可用的少量镜头标记数据来训练语言提示的可学习成分。我们在三个真实的WSI数据集上进行了广泛的实验，包括乳腺癌症、癌症和癌症，证明了所提出的方法在袋和实例分类中的显著性能。所有代码都可用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17891v2" target="_blank">2305.17891v2</a>
                              </td>
                              <td>The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification</td>
                              <td>Linhao Qu</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17891v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17891v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15362v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15362v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15362v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15362v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However, the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this paper, we propose a TransClippedCLR model by encoding the global context of an image using Transformer having local context through patch based processing, by generating the hash codes through product quantization and by avoiding the potential false negative pairs through clipped contrastive learning. The proposed model is tested with superior performance for unsupervised image retrieval on benchmark datasets, including CIFAR10, NUS-Wide and Flickr25K, as compared to the recent state-of-the-art deep models. The results using the proposed clipped contrastive learning are greatly improved on all datasets as compared to same backbone network with vanilla contrastive learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15362v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督图像检索旨在学习重要的视觉特征，而不需要任何给定的级别来检索给定查询图像的相似图像。基于卷积神经网络（CNN）的方法已被广泛用于图像哈希的自监督对比学习。然而，由于细胞神经网络缺乏对全局特征的有效利用，以及在对比学习中假阴性对造成的偏见，现有的方法受到了影响。在本文中，我们提出了一种TransClippedCLR模型，通过基于补丁的处理，使用具有局部上下文的Transformer对图像的全局上下文进行编码，通过乘积量化生成哈希码，并通过剪裁对比学习避免潜在的假负对。与最近最先进的深度模型相比，所提出的模型在包括CIFAR10、NUS-Wide和Flickr25K在内的基准数据集上以优异的无监督图像检索性能进行了测试。与使用普通对比学习的相同骨干网络相比，使用所提出的剪裁对比学习的结果在所有数据集上都得到了极大的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15362v1" target="_blank">2401.15362v1</a>
                              </td>
                              <td>Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval</td>
                              <td>Ayush Dubey</td>
                              <td>2024-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15362v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15362v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14111v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14111v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14111v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14111v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14111v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成模型的进步引发了人们对在遵守特定结构准则的同时生成图像的极大兴趣。场景图到图像生成是生成与给定场景图一致的图像的一个这样的任务。然而，视觉场景的复杂性对基于场景图中的指定关系准确对齐对象提出了挑战。现有的方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来完成这项任务。在这项工作中，我们介绍了一种从场景图生成图像的新方法，该方法无需预测中间布局。我们利用预先训练的文本到图像扩散模型和CLIP指导将图形知识转化为图像。为此，我们首先预训练我们的图编码器，使用基于GAN的训练将图特征与相应图像的CLIP特征对齐。此外，我们将图特征与给定场景图中存在的对象标签的CLIP嵌入相融合，以创建图一致的CLIP引导的条件信号。在条件输入中，对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。最后，我们用具有重建和CLIP对准损失的图一致性条件信号来微调预训练的扩散模型。详细的实验表明，我们的方法在COCO材料和Visual Genome数据集的标准基准上优于现有方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14111v2" target="_blank">2401.14111v2</a>
                              </td>
                              <td>Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs</td>
                              <td>Rameshwar Mishra</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14111v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14111v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14733v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Personality Perception in Human Videos Altered by Motion Transfer Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14733v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14733v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14733v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The successful portrayal of personality in digital characters improves communication and immersion. Current research focuses on expressing personality through modifying animations using heuristic rules or data-driven models. While studies suggest motion style highly influences the apparent personality, the role of appearance can be similarly essential. This work analyzes the influence of movement and appearance on the perceived personality of short videos altered by motion transfer networks. We label the personalities in conference video clips with a user study to determine the samples that best represent the Five-Factor model's high, neutral, and low traits. We alter these videos using the Thin-Plate Spline Motion Model, utilizing the selected samples as the source and driving inputs. We follow five different cases to study the influence of motion and appearance on personality perception. Our comparative study reveals that motion and appearance influence different factors: motion strongly affects perceived extraversion, and appearance helps convey agreeableness and neuroticism.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14733v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在数字人物中成功地刻画了个性，提高了沟通和沉浸感。目前的研究重点是通过使用启发式规则或数据驱动模型修改动画来表达个性。虽然研究表明，动作风格对外表个性有很大影响，但外表的作用也同样重要。本文分析了运动和外表对运动传递网络改变的短视频感知个性的影响。我们通过用户研究对会议视频剪辑中的人物进行标记，以确定最能代表五因素模型的高、中性和低特征的样本。我们使用薄板样条线运动模型来更改这些视频，使用选定的样本作为源和驱动输入。我们遵循五个不同的案例来研究动作和外表对人格感知的影响。我们的比较研究表明，运动和外表影响不同的因素：运动强烈影响感知的外向性，外表有助于传达宜人性和神经质。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14733v1" target="_blank">2401.14733v1</a>
                              </td>
                              <td>Personality Perception in Human Videos Altered by Motion Transfer Networks</td>
                              <td>Ayda Yurtoğlu</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14733v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14733v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sinansonlu/motion-transfer-personality" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_03138v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03138v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03138v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03138v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03138v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对强化学习中的探索采取了以表征为中心的观点，从根本上将探索视为一个密度估计问题。我们研究了聚类表示在三维环境中用于探索的有效性，基于这样的观察，即与二维环境相比，在三维环境下转换之间的像素变化的重要性不那么明显，在二维环境中转换之间的象素变化通常是明显的。我们提出了一种方法，该方法对随机表示和预训练的DINO表示执行情节和全局聚类，以计数状态，即估计伪计数。令人惊讶的是，即使是随机特征也可以有效地聚类，以计算三维环境中的状态，然而，当这些特征在视觉上变得更加复杂时，由于表示中预先训练的归纳偏差，预先训练的DINO表示更有效。总的来说，这为将预先训练的偏见融入探索提供了一条途径。我们在VizDoom和Habitat环境中评估了我们的方法，证明我们的方法在这些环境中超越了其他众所周知的探索方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03138v1" target="_blank">2402.03138v1</a>
                              </td>
                              <td>Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations</td>
                              <td>Stefan Sylvius Wagner</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03138v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03138v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02851v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Compositional Generalization via Compositional Feature Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02851v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02851v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习模型的真实世界应用经常面临数据分布的变化，其中训练和测试数据分布之间存在差异。在常见的多域多类设置中，随着类和域的数量增加，为每个域-类组合收集训练数据变得不可行。这一挑战自然导致了对具有组合泛化（CG）能力的模型的探索，其中模型可以泛化到看不见的领域类组合。为了深入研究CG挑战，我们开发了CG Bench，这是一套从现有的真实世界图像数据集中导出的CG基准，并观察到在基础模型（如CLIP和DINOv2）上流行的预训练微调范式难以应对这一挑战。为了应对这一挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，i）在预训练的编码器上学习关于类和域标签的两个正交线性头，以及ii）在新学习的头冻结的情况下微调编码器。我们从理论和经验上证明了CFA鼓励预训练模型的组成特征学习。我们进一步在CLIP和DINOv2这两个强大的预训练视觉基础模型的CG平台上进行了广泛的实验。实验结果表明，CFA在合成泛化方面优于常用的微调技术，证实了CFA在组合特征学习方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02851v1" target="_blank">2402.02851v1</a>
                              </td>
                              <td>Enhancing Compositional Generalization via Compositional Feature Alignment</td>
                              <td>Haoxiang Wang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02851v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02851v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/haoxiang-wang/compositional-feature-alignment" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02352v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Region-Based Representations Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02352v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong unsupervised representations like DINOv2 and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The compactness of the representation also makes it well-suited to video analysis and other problems requiring inference across many images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02352v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究基于区域的表示是否对识别有效。区域曾经是识别方法的支柱，但现在几乎完全使用基于像素和补丁的特征。我们表明，最近的类不可知分割器（如SAM）可以与强无监督表示（如DINOv2）有效结合，并用于各种任务，包括语义分割、基于对象的图像检索和多图像分析。一旦提取了掩码和特征，即使使用线性解码器，这些表示也能实现有竞争力的性能，使其非常适合需要自定义查询的应用程序。该表示的紧凑性也使其非常适合于视频分析和其他需要在许多图像上进行推理的问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02352v1" target="_blank">2402.02352v1</a>
                              </td>
                              <td>Region-Based Representations Revisited</td>
                              <td>Michal Shlapentokh-Rothman</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02352v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02352v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07193v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINOv2: Learning Robust Visual Features without Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07193v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07193v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07193v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07193v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在用于对大量数据进行模型预训练的自然语言处理方面取得的突破为计算机视觉中的类似基础模型开辟了道路。这些模型可以通过产生通用的视觉特征，即在不进行微调的情况下跨图像分布和任务工作的特征，极大地简化图像在任何系统中的使用。这项工作表明，如果在来自不同来源的足够精心策划的数据上进行训练，现有的预训练方法，特别是自监督方法，可以产生这样的特征。我们重新审视现有的方法，并结合不同的技术，在数据和模型大小方面扩展我们的预训练。大多数技术贡献旨在加速和稳定大规模培训。在数据方面，我们提出了一种自动管道，以建立一个专门的、多样化的、精心策划的图像数据集，而不是像在自我监督的文献中通常做的那样，建立未经处理的数据集。在模型方面，我们训练了一个具有1B参数的ViT模型（Dosovitskiy et al.，2020），并将其提取成一系列较小的模型，这些模型在图像和像素级别的大多数基准上超过了可用的最佳通用功能OpenCLIP（Ilharco et al.，2021）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07193v2" target="_blank">2304.07193v2</a>
                              </td>
                              <td>DINOv2: Learning Robust Visual Features without Supervision</td>
                              <td>Maxime Oquab</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07193v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07193v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/dinov2" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17981v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17981v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17981v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17981v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17981v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管多模式大型语言模型在整合文本和图像模式方面具有令人印象深刻的能力，但在准确解释详细的视觉元素方面仍然存在挑战。本文对使用最先进的（SOTA）对象检测和光学字符识别模型增强MLLMs进行了实证研究，以提高对细粒度图像的理解并减少响应中的幻觉。我们的研究调查了基于嵌入的检测信息注入，这种注入对MLLMs原始能力的影响，以及检测模型的互换性。我们对LLaVA-1.5、DINO和PaddleOCRv2等模型进行了系统实验，结果表明，我们的方法不仅提高了MLLM在特定视觉任务中的性能，而且保持了它们的原始优势。由此增强的MLLMs在10个基准中有9个优于SOTA模型，在归一化平均分数上提高了12.99%，标志着多模态理解的显著进步。我们发布代码是为了进一步探索MLLM的细粒度多模式对话功能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17981v1" target="_blank">2401.17981v1</a>
                              </td>
                              <td>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</td>
                              <td>Qirui Jiao</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17981v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17981v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17632v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17632v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17632v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17632v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech is represented. Furthermore, we conduct direct comparisons to measure the similarities between layers within and across models. Our analysis unveils that 1) the capacity to represent content information is somewhat unrelated to enhanced speaker representation, 2) specific layers of speech SSL models would be partly specialized in capturing linguistic information, and 3) speaker SSL models tend to disregard linguistic information but exhibit more sophisticated speaker representation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17632v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在学习有意义的语音表征方面引起了越来越多的关注。语音SSL模型，如WavLM，采用掩蔽预测训练来编码通用表示。相反，说话者SSL模型，以基于DINO的模型为例，主要针对说话者表示采用话语水平训练目标。了解这些模型如何表示信息对于提高模型的效率和有效性至关重要。与语音SSL的各种分析不同，对说话者SSL捕获的信息以及其表示与语音SSL或其他完全监督的说话者模型的不同之处的研究有限。本文解决了这些基本问题。我们通过将SUPERB评估探测任务应用于语音和说话者SSL模型来探索捕获各种语音属性的能力。我们还检查了每个任务主要使用哪些层来识别语音表示方式的差异。此外，我们进行直接比较，以衡量模型内和模型间各层之间的相似性。我们的分析表明，1）表示内容信息的能力与增强的说话者表示有些无关，2）语音SSL模型的特定层将部分专门用于捕获语言信息，3）说话者SSL模型倾向于忽略语言信息，但表现出更复杂的说话者表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17632v1" target="_blank">2401.17632v1</a>
                              </td>
                              <td>What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</td>
                              <td>Takanori Ashihara</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17632v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17632v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08873v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08873v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08873v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08873v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework's effectiveness and adaptability to diverse environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08873v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文利用大型语言和视觉语言模型提出了一种交互式导航框架，使机器人能够在有可穿越障碍物的环境中导航。我们利用大型语言模型（GPT-3.5）和开放集视觉语言模型（Grounding DINO）创建一个动作感知成本图，在不进行微调的情况下执行有效的路径规划。使用大型模型，我们可以实现一个端到端的系统，从“你能穿过窗帘给我送药吗？”这样的文本说明，到具有动作感知属性的边界框（例如窗帘）。它们可以用于将激光雷达点云划分为两部分：可遍历部分和不可遍历部分，然后构建行动感知成本图以生成可行路径。经过预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，允许在交互式导航任务中快速部署。我们选择使用多个可遍历对象，如窗帘和草，通过指示机器人遍历它们来进行验证。此外，还测试了在医疗场景中穿过窗帘的情况。所有实验结果都证明了所提出的框架的有效性和对不同环境的适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08873v2" target="_blank">2310.08873v2</a>
                              </td>
                              <td>Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</td>
                              <td>Zhen Zhang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08873v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08873v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05925v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05925v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians' segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05925v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了紧凑和快速分割3D高斯（CoSSegGaussians），这是一种在仅输入RGB图像的情况下以快速渲染速度进行紧凑的3D一致场景分割的方法。以前基于NeRF的分割方法依赖于耗时的神经场景优化。虽然最近的3D高斯Splatting显著提高了速度，但现有的基于高斯的分割方法很难产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于他们将可学习参数直接分配给每个高斯，导致对交叉视图不一致的2D机器生成标签缺乏鲁棒性。我们的方法旨在通过使用双特征融合网络作为高斯分割域来解决这个问题。具体来说，我们首先在RGB监督下优化3D高斯。高斯定位后，通过显式反投影应用从图像中提取的DINO特征，并将其与高效点云处理网络的空间特征进一步融合。利用特征聚合将它们融合在全局到局部的策略中，以实现紧凑的分割特征。实验结果表明，与基于NeRF的方法相比，我们的模型在语义和全景零样本分割任务上都优于基线，同时消耗不到10%的推理时间。代码和更多结果将在https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05925v3" target="_blank">2401.05925v3</a>
                              </td>
                              <td>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</td>
                              <td>Bin Dou</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05925v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05925v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14555v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Active Learning in the Era of Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14555v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14555v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础视觉或视觉语言模型在大的未标记或有噪声的数据上进行训练，并学习稳健的表示，这些表示可以在不同的任务上实现令人印象深刻的零或少镜头性能。考虑到这些特性，它们自然适合主动学习（AL），其目的是最大限度地提高标签效率，但尚未在主动学习的背景下，特别是在低预算制度下，探索基础模型的全部潜力。在这项工作中，我们评估了基础模型如何影响有效AL的三个关键组成部分，即1）初始标记池选择，2）确保多样化采样，以及3）代表性采样和不确定性采样之间的权衡。我们系统地研究了基础模型（DINOv2，OpenCLIP）的鲁棒表示如何挑战主动学习中的现有发现。我们的观察结果为一种新的简单而优雅的AL策略的原则性构建提供了信息，该策略平衡了通过丢弃估计的不确定性和样本多样性。我们在许多具有挑战性的图像分类基准上广泛测试了我们的策略，包括自然图像以及AL文献中研究相对不足的领域外生物医学图像。将提供源代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14555v1" target="_blank">2401.14555v1</a>
                              </td>
                              <td>Revisiting Active Learning in the Era of Vision Foundation Models</td>
                              <td>Sanket Rajan Gupte</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14555v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14555v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tempconfx/al-foundation-models" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13987v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13987v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13987v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数少数镜头学习工作依赖于基础任务和目标任务之间的相同领域假设，阻碍了它们的实际应用。本文提出了一种自适应变换网络（ADAPTER），这是一种简单但有效的跨域少镜头学习解决方案，其中基本任务和目标任务之间存在较大的域偏移。ADAPTER建立在双向交叉注意力的思想之上，以学习两个领域之间的可转移特征。所提出的体系结构是用DINO训练的，以产生多样化的、较少偏见的特征，从而避免监督崩溃的问题。此外，还提出了标签平滑方法，通过考虑嵌入空间中紧密样本的预测标签来提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准中得到了严格评估，在这些基准中，它以显著的优势优于现有技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13987v1" target="_blank">2401.13987v1</a>
                              </td>
                              <td>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</td>
                              <td>Naeem Paeedeh</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13987v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/naeem-paeedeh/adapter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11673v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11673v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11673v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于学习的多视图立体（MVS）方法的最新进展突出了具有注意力机制的基于变换器的模型。然而，现有的方法没有彻底研究变压器对不同MVS模块的深刻影响，导致深度估计能力有限。在本文中，我们介绍了MVSFormer++，这是一种谨慎地最大化注意力的固有特性以增强MVS管道的各个组件的方法。从形式上讲，我们的方法包括将跨视图信息注入预先训练的DINOv2模型中，以促进MVS学习。此外，我们对特征编码器和代价体积正则化采用了不同的注意力机制，分别关注特征和空间聚合。此外，我们发现一些设计细节会显著影响MVS中转换器模块的性能，包括归一化的3D位置编码、自适应注意力缩放和层归一化的位置。在DTU、Tanks and Temples、BlendedMVS和ETH3D上进行的综合实验验证了该方法的有效性。值得注意的是，MVSFormer++在具有挑战性的DTU和坦克与神庙基准上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11673v1" target="_blank">2401.11673v1</a>
                              </td>
                              <td>MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</td>
                              <td>Chenjie Cao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11673v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11673v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11311v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11311v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11311v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着计算机视觉的快速发展，出现了各种视觉基础模型，每种模型都适合特定的数据类型和任务。虽然大型语言模型通常有一个共同的借口任务，但视觉基础模型的多样性源于它们不同的训练目标。在这项研究中，我们深入探讨了为少镜头语义分割（计算机视觉中的一项关键任务）确定最有效的视觉基础模型的探索。具体而言，我们对四个突出的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders和在COCO数据集上预训练的直接ResNet50。我们的研究重点是它们对新的语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，在各种数据集和适应方法中，DINO V2始终优于其他考虑的基础模型。这一结果突显了与同类产品相比，DINO V2在适应语义分割任务方面的卓越能力。此外，我们的观察结果表明，各种适配器方法表现出相似的性能，强调了选择鲁棒特征提取器的至关重要性，而不是自适应技术本身的复杂性。这一见解揭示了特征提取在少镜头语义分割中的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了有价值的见解，而且突出了鲁棒特征提取器在该领域的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11311v1" target="_blank">2401.11311v1</a>
                              </td>
                              <td>A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</td>
                              <td>Reda Bensaid</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11311v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11311v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10815v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10815v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10815v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言监督预训练已被证明是从图像中提取语义上有意义的特征的一种有价值的方法，是计算机视觉和医学成像领域中多模式系统的基础元素。但是，生成的特征受到文本中包含的信息的限制。这在医学成像中尤其有问题，因为放射科医生的书面发现侧重于特定的观察结果；由于担心个人健康信息的泄露，配对成像文本数据的稀缺性加剧了这一挑战。在这项工作中，我们从根本上挑战了学习通用生物医学成像编码器时普遍依赖语言监督的现状。我们介绍了RAD-DINO，这是一种仅在单峰生物医学成像数据上预训练的生物医学图像编码器，在各种基准上获得与最先进的生物医学语言监督模型相似或更高的性能。具体而言，学习表示的质量是在标准成像任务（分类和语义分割）和视觉语言对齐任务（从图像生成文本报告）上评估的。为了进一步证明语言监督的缺点，我们发现RAD-DINO的特征与其他医疗记录（如性别或年龄）的相关性比语言监督模型更好，而语言监督模型通常在放射学报告中没有提及。最后，我们进行了一系列烧蚀，确定了影响RAD-DINO性能的因素；值得注意的是，我们观察到RAD-DINO的下游性能随着训练数据的数量和多样性而良好地扩展，这表明仅图像监督是训练基础生物医学图像编码器的一种可扩展方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10815v1" target="_blank">2401.10815v1</a>
                              </td>
                              <td>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</td>
                              <td>Fernando Pérez-García</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10815v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10815v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07951v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image Similarity using An Ensemble of Context-Sensitive Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07951v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07951v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像相似性在计算机视觉中得到了广泛的研究。近年来，机器学习模型已经显示出其比传统的多元度量编码更多语义的能力。然而，在标记相似性时，将数字分数分配给一对图像不如确定一个图像a是否比另一个图像B更接近参考图像R直观。在这项工作中，我们提出了一种基于a:R与B:R形式的标记数据构建图像相似性模型的新方法。我们通过使用集成模型来解决图像空间（R，A，B）中的稀疏采样和使用基于上下文的数据训练的模型中的偏差的挑战。特别地，我们使用了两种ML技术来构建这样的集成模型，即降维和MLP回归。我们的测试结果表明，所构建的集成模型的性能比最好的单个上下文敏感模型好约5%。它们的性能也优于用混合图像数据训练的模型以及现有的相似性模型，例如CLIP和DINO。这项工作表明，当使用适当的集成方法来缓解稀疏采样带来的限制时，基于上下文的标记和模型训练是有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07951v1" target="_blank">2401.07951v1</a>
                              </td>
                              <td>Image Similarity using An Ensemble of Context-Sensitive Models</td>
                              <td>Zukang Liao</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07951v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07951v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06013v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06013v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06013v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目的：机器人手术中的深度估计在三维重建、手术导航和增强现实可视化中至关重要。尽管基础模型在许多视觉任务中表现出出色的性能，包括深度估计（例如，DINOv2），但最近的工作观察到其在医学和外科领域特定应用中的局限性。这项工作提出了一种用于手术深度估计的基础模型的低阶自适应（LoRA）。方法：我们设计了一种基于基础模型的深度估计方法，称为Surgical DINO，这是对DINOv2的低阶自适应，用于内窥镜手术中的深度估计。我们构建了LoRA层，并将其集成到DINO中，以适应手术特定领域的知识，而不是传统的微调。在训练过程中，我们冻结了显示出出色视觉表示能力的DINO图像编码器，并仅优化了LoRA层和深度解码器，以集成来自手术场景的特征。结果：我们的模型在SCARED的MICCAI挑战数据集上得到了广泛验证，该数据集是从达芬奇Xi内窥镜手术中收集的。我们的经验表明，外科DINO在内窥镜深度估计任务中显著优于所有最先进的模型。消融研究的分析表明，我们的LoRA层和适应具有显著效果。结论：外科DINO为基础模型成功适应外科领域进行深度估计提供了一些启示。结果中有明确证据表明，对计算机视觉数据集中预先训练的权重进行零样本预测或简单微调不足以直接在外科领域使用基础模型。代码位于https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06013v2" target="_blank">2401.06013v2</a>
                              </td>
                              <td>Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</td>
                              <td>Beilei Cui</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06013v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06013v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/beileicui/surgicaldino" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练健壮和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识有帮助，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v4" target="_blank">2305.14093v4</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kunhao-liu/3d-ovs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02957v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02957v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We delve into a nuanced but significant challenge inherent to Vision Transformers (ViTs): feature maps of these models exhibit grid-like artifacts, which detrimentally hurt the performance of ViTs in downstream tasks. Our investigations trace this fundamental issue down to the positional embeddings at the input stage. To address this, we propose a novel noise model, which is universally applicable to all ViTs. Specifically, the noise model dissects ViT outputs into three components: a semantics term free from noise artifacts and two artifact-related terms that are conditioned on pixel locations. Such a decomposition is achieved by enforcing cross-view feature consistency with neural fields in a per-image basis. This per-image optimization process extracts artifact-free features from raw ViT outputs, providing clean features for offline applications. Expanding the scope of our solution to support online functionality, we introduce a learnable denoiser to predict artifact-free features directly from unprocessed ViT outputs, which shows remarkable generalization capabilities to novel data without the need for per-image optimization. Our two-stage approach, termed Denoising Vision Transformers (DVT), does not require re-training existing pre-trained ViTs and is immediately applicable to any Transformer-based architecture. We evaluate our method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP, DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT consistently and significantly improves existing state-of-the-art general-purpose models in semantic and geometric tasks across multiple datasets (e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT design, especially regarding the naive use of positional embeddings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02957v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02957v1" target="_blank">2401.02957v1</a>
                              </td>
                              <td>Denoising Vision Transformers</td>
                              <td>Jiawei Yang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02957v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02361v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02361v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release all our models to the research community. Codes and trained models are released at https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02361v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding DINO是一种最先进的开放集检测模型，可处理多种视觉任务，包括开放词汇检测（OVD）、短语基础（PG）和参考表达理解（REC）。它的有效性导致它被广泛采用为各种下游应用程序的主流架构。然而，尽管其意义重大，但由于其培训代码的不可用，最初的Grounding DINO模型缺乏全面的公共技术细节。为了弥补这一差距，我们推出了MM Grounding DINO，这是一个开源、全面、用户友好的基线，它是用MMDetection工具箱构建的。它采用丰富的视觉数据集进行预训练，并采用各种检测和基础数据集进行微调。我们对每一个报告的结果进行了全面的分析，并对复制进行了详细的设置。对上述基准的广泛实验表明，我们的MM Grounding DINO Tiny优于Grounding DINO Tiny基线。我们向研究界发布所有模型。代码和经过训练的模型发布于https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02361v2" target="_blank">2401.02361v2</a>
                              </td>
                              <td>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</td>
                              <td>Xiangyu Zhao</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02361v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02361v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/open-mmlab/mmdetection" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12735v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12735v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with negligible performance loss, demonstrating the potential to be a powerful backbone for downstream vision tasks. The code is available at: github.com/sunsmarterjie/iTPN.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12735v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了整体预训练的变换金字塔网络（iTPN），以联合优化网络主干和瓶颈，从而使表示模型和下游任务之间的传输间隙最小。iTPN诞生于两个精心设计：1）第一个预先训练的视觉转换器上的特征金字塔（ViT）。2） 使用掩蔽特征建模（MFM）对特征金字塔进行多阶段监督。iTPN更新为Fast iTPN，通过两种灵活的设计减少了计算内存开销并加速了推理。1） 令牌迁移：丢弃主干的冗余令牌，同时在功能金字塔中补充它们，而无需注意操作。2） 代币采集：通过引入少量采集代币，降低全球关注带来的计算成本。基本/大级别Fast iTPN在ImageNet-1K上实现了88.75%/89.5%的前1级精度。在使用DINO的1x训练计划的情况下，基本/大级别Fast iTPN在COCO对象检测上实现了58.4%/58.8%的box AP，在使用MaskDINO的ADE20K语义分割上实现了57.5%/58.7%的mIoU。快速iTPN可以将推理过程加速70%，而性能损失可以忽略不计，这表明它有潜力成为下游视觉任务的强大支柱。该代码位于：github.com/sunsmarterjie/iTPN。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12735v2" target="_blank">2211.12735v2</a>
                              </td>
                              <td>Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</td>
                              <td>Yunjie Tian</td>
                              <td>2022-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12735v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunsmarterjie/itpn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01013v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01013v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired by InfoNCE, we introduce a novel contrastive loss function that facilitates smoother training and better convergence, thereby enhancing performance in artifact classification. In summary, this study establishes the efficacy of SSL in leveraging unlabeled data, particularly in enhancing the capabilities of the Transformer model. This approach holds promise for broader applications in PICU environments, where annotated data is often limited.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01013v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CHU Sainte Justine儿科重症监护室（PICU）最近的研究表明，传统的机器学习方法，如半监督标签传播和K近邻，在PPG信号的伪影检测方面，主要是在数据有限的情况下，优于基于Transformer的模型。这项研究通过使用自监督学习（SSL）从这些数据中提取潜在特征，然后对标记数据进行微调，解决了大量未标记数据利用不足的问题。我们的实验表明，SSL显著增强了Transformer模型学习表示的能力，提高了其在工件分类任务中的稳健性。在各种SSL技术中，包括掩蔽、对比学习和DINO（无标签的自蒸馏），对比学习在小型PPG数据集中表现出最稳定和优越的性能。此外，我们深入研究了优化对比损失函数，这对对比SSL至关重要。受InfoNCE的启发，我们引入了一种新的对比损失函数，该函数有助于更平滑的训练和更好的收敛，从而提高伪像分类的性能。总之，本研究确定了SSL在利用未标记数据方面的有效性，特别是在增强Transformer模型的能力方面。这种方法有望在PICU环境中获得更广泛的应用，在PICU中，注释数据通常是有限的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01013v1" target="_blank">2401.01013v1</a>
                              </td>
                              <td>Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</td>
                              <td>Thanh-Dung Le</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01013v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00463v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Local Representations of Self-supervised Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00463v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design an evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval, and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Furthermore, we demonstrate that removing these high-variance features enhances k-NN by providing an analysis of the benchmarks for this work and for Scale-MAE, a recent extension of masked autoencoders. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute-intensive counterpart DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00463v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对各种自监督视觉变压器（ViT）进行了比较分析，重点分析了它们的局部代表性。受大型语言模型的启发，我们研究了ViT在几乎没有微调的情况下执行各种计算机视觉任务的能力。我们设计了一个评估框架来分析局部（即补丁级别）表示在少镜头语义分割、实例识别、对象检索和跟踪背景下的质量。我们发现，与掩蔽图像建模相比，基于对比学习的方法（如DINO）产生了更通用的补丁表示，可以立即应用于下游任务，而无需参数调整。使用后一种方法学习的嵌入，例如在掩码自动编码器中，具有高方差特征，这会损害基于距离的算法，例如k-NN，并且不包含用于大多数下游任务的有用信息。此外，我们证明，通过为这项工作和Scale MAE（掩蔽自动编码器的最近扩展）提供基准分析，去除这些高方差特征可以增强k-NN。最后，我们找到了一个对象实例检索设置，其中DINOv2，一个在两个数量级以上的数据上预训练的模型，其性能比计算密集度较低的对应DINO差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00463v1" target="_blank">2401.00463v1</a>
                              </td>
                              <td>Analyzing Local Representations of Self-supervised Vision Transformers</td>
                              <td>Ani Vanyan</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00463v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v4" target="_blank">2310.03940v4</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18628v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18628v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised semantic segmentation aims to categorize each pixel in an image into a corresponding class without the use of annotated data. It is a widely researched area as obtaining labeled datasets is expensive. While previous works in the field have demonstrated a gradual improvement in model accuracy, most required neural network training. This made segmentation equally expensive, especially when dealing with large-scale datasets. We thus propose a lightweight clustering framework for unsupervised semantic segmentation. We discovered that attention features of the self-supervised Vision Transformer exhibit strong foreground-background differentiability. Therefore, clustering can be employed to effectively separate foreground and background image patches. In our framework, we first perform multilevel clustering across the Dataset-level, Category-level, and Image-level, and maintain consistency throughout. Then, the binary patch-level pseudo-masks extracted are upsampled, refined and finally labeled. Furthermore, we provide a comprehensive analysis of the self-supervised Vision Transformer features and a detailed comparison between DINO and DINOv2 to justify our claims. Our framework demonstrates great promise in unsupervised semantic segmentation and achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18628v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督语义分割旨在将图像中的每个像素分类到相应的类别中，而不使用注释数据。这是一个广泛研究的领域，因为获得标记的数据集是昂贵的。虽然该领域先前的工作已经证明模型精度逐渐提高，但大多数都需要神经网络训练。这使得分割同样昂贵，尤其是在处理大规模数据集时。因此，我们提出了一种用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉转换器的注意力特征表现出很强的前景-背景可微性。因此，可以采用聚类来有效地分离前景和背景图像块。在我们的框架中，我们首先在数据集级别、类别级别和图像级别执行多级聚类，并始终保持一致性。然后，对提取的二进制补丁级伪掩码进行上采样、细化和最终标记。此外，我们对自监督视觉转换器的功能进行了全面分析，并对DINO和DINOv2进行了详细比较，以证明我们的说法是正确的。我们的框架在无监督语义分割方面表现出了巨大的前景，并在PASCAL VOC和MS COCO数据集上取得了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18628v2" target="_blank">2311.18628v2</a>
                              </td>
                              <td>A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</td>
                              <td>Yau Shing Jonathan Cheung</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18628v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18628v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17742v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Vision from Models Rivals Learning Vision from Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17742v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data. We synthesize a large dataset of image captions using LLMs, then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning, treating images sharing the same caption as positive pairs. The resulting representations transfer well to many downstream tasks, competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR outperforms previous self-supervised methods by a significant margin, e.g., improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17742v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SynCLR，这是一种专门从合成图像和合成字幕中学习视觉表示的新方法，无需任何真实数据。我们使用LLM合成了一个大型的图像字幕数据集，然后使用现成的文本到图像模型来生成与每个合成字幕相对应的多个图像。我们通过对比学习对这些合成图像进行视觉表征学习，将共享同一字幕的图像视为正对。由此产生的表示很好地转移到许多下游任务，在图像分类任务中与其他通用视觉表示学习器（如CLIP和DINO v2）竞争。此外，在语义分割等密集预测任务中，SynCLR显著优于以前的自监督方法，例如，在ViT-B/16的ADE20k上比MAE和iBOT提高了6.2和4.3mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17742v1" target="_blank">2312.17742v1</a>
                              </td>
                              <td>Learning Vision from Models Rivals Learning Vision from Data</td>
                              <td>Yonglong Tian</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17742v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17742v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/syn-rep-learn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02366v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02366v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02366v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据注释的资源密集型过程以及这些系统无法推广到不同的数据分布，阻碍了深度学习系统与医疗保健的集成。基础模型是在大型数据集上预先训练的模型，已成为减少对注释数据的依赖并增强模型可推广性和稳健性的解决方案。DINOv2是一个开源的基础模型，通过对1.42亿张策划的自然图像进行自我监督学习进行预训练，在各种视觉任务中表现出有希望的能力。然而，关于DINOv2对放射学成像的适应性，以及其特征是否足够通用以有利于放射学图像分析，一个关键问题仍未得到解答。因此，本研究全面评估了DINOv2的放射学，在不同的模式（X射线、CT和MRI）下进行了100多项实验。为了衡量DINOv2特征表示的有效性和可推广性，我们分析了医学图像分析任务中的模型，包括2D和3D图像上的疾病分类和器官分割，以及在不同的设置下，如kNN、少镜头学习、线性探测、端到端微调和参数有效微调。与已建立的监督、自监督和弱监督模型的比较分析揭示了DINOv2的优越性能和跨任务可推广性。这些发现有助于深入了解优化医学成像预训练策略的潜在途径，并增进对DINOv2在弥合自然图像分析和放射学图像分析之间差距方面的作用的更广泛理解。我们的代码可在https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02366v3" target="_blank">2312.02366v3</a>
                              </td>
                              <td>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</td>
                              <td>Mohammed Baharoon</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02366v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02366v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mohammedsb/dinov2forradiology" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Visual Reinforcement Learning with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够推广到看不见的环境中的学习策略是视觉强化学习（RL）中的一个基本挑战。虽然目前的大多数方法都侧重于通过辅助监督、预训练或数据增强来获取稳健的视觉表示，但现代视觉基础模型的潜力仍然不足。在这项工作中，我们介绍了可泛化视觉RL的分段任意模型（SAM-G），这是一个新的框架，利用分段任意模型的可提示分割能力来增强视觉RL代理的泛化能力。我们利用DINOv2和SAM的图像特征来寻找与SAM的点提示对应关系，然后SAM直接为代理生成高质量的掩码图像。在8个DMControl任务和3个Adroit任务中进行评估后，SAM-G显著提高了视觉泛化能力，而不会改变RL代理的架构，而只是改变他们的观察结果。值得注意的是，与最先进的方法相比，SAM-G在DMControl和Adroit上具有挑战性的视频硬设置上分别实现了44%和29%的相对改进。视频和代码：https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17116v1" target="_blank">2312.17116v1</a>
                              </td>
                              <td>Generalizable Visual Reinforcement Learning with Segment Anything Model</td>
                              <td>Ziyu Wang</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wadiuvatzy/sam-g" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16084v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LangSplat: 3D Language Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16084v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16084v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类生活在3D世界中，通常使用自然语言与3D场景进行交互。最近，对3D语言字段进行建模以支持3D中的开放式语言查询越来越受到关注。本文介绍了LangSplat，它构建了一个三维语言字段，可以在三维空间中进行精确高效的开放式词汇查询。与现有的将CLIP语言嵌入NeRF模型的方法不同，LangSplat通过利用3D高斯集合来表示语言领域，从而推进了这一领域的发展，每个高斯集合都对从CLIP中提取的语言特征进行编码。通过使用基于瓦片的飞溅技术来渲染语言特征，我们避免了NeRF中固有的昂贵的渲染过程。LangSplat不是直接学习CLIP嵌入，而是首先训练场景式语言自动编码器，然后在特定场景的潜在空间上学习语言特征，从而减轻显式建模带来的大量内存需求。现有的方法难以处理不精确和模糊的3D语言字段，这些字段无法辨别对象之间的清晰边界。我们深入研究了这个问题，并建议使用SAM学习分层语义，从而消除了在各种规模上广泛查询语言字段和DINO特征正则化的需要。在开放词汇三维对象定位和语义分割方面的大量实验表明，LangSplat显著优于先前最先进的方法LERF。值得注意的是，LangSplat非常高效，与分辨率为1440$\times$1080的LERF相比，它实现了｛\speed｝$\times$的加速。我们强烈建议读者在上查看我们的视频结果https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16084v1" target="_blank">2312.16084v1</a>
                              </td>
                              <td>LangSplat: 3D Language Gaussian Splatting</td>
                              <td>Minghan Qin</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16084v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16084v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/minghanqin/LangSplat" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06709v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06709v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework.   Code: https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06709v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近出现了一些视觉基础模型（VFM）作为许多下游任务的骨干。像CLIP、DINOv2、SAM这样的VFM都是以不同的目标进行训练的，在各种下游任务中表现出独特的特征。我们发现，尽管这些模型在概念上存在差异，但通过多教师提炼，它们可以有效地合并为一个统一的模型。我们将这种方法命名为AM-RADIO（聚集模型——将所有域归一）。这种综合方法不仅超越了个别教师模型的性能，而且融合了它们的独特特征，如零样本视觉语言理解、详细的像素级理解和开放的词汇分割能力。为了追求硬件效率最高的主干，我们使用相同的培训方法评估了多教师蒸馏管道中的许多架构。这导致了一种新型架构（E-RADIO）的开发，其性能超过了其前身，并且至少比教师模型快7倍。我们的全面基准测试过程涵盖下游任务，包括ImageNet分类、ADE20k语义分割、COCO对象检测和LLaVa-1.5框架。密码https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06709v3" target="_blank">2312.06709v3</a>
                              </td>
                              <td>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</td>
                              <td>Mike Ranzinger</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06709v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06709v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nvlabs/radio" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16211v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16211v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT with a causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16211v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果网络被广泛应用于许多领域，包括流行病学、社会科学、医学和工程，以对变量之间的复杂关系进行建模。虽然直接从观测数据中用算法推断这些模型很方便，但由此产生的网络往往存在错误边缘。审核和纠正这些网络可能需要分析员经常无法获得的领域专业知识。我们建议使用大型语言模型（如ChatGPT）作为因果网络的审计员。我们的方法为ChatGPT提供了一个因果网络，一次一个边缘，以产生关于边缘方向性、可能的混杂因素和中介变量的见解。我们要求ChatGPT反思每个因果关系的各个方面，然后我们生成可视化结果，总结这些观点，供人类分析师指导边缘、收集更多数据或测试进一步的假设。我们设想一个系统，其中大型语言模型、自动因果推理以及人类分析师和领域专家作为一个团队携手合作，为任何给定的案例场景推导出整体和全面的因果模型。本文介绍了一个新兴原型获得的第一个结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16211v1" target="_blank">2312.16211v1</a>
                              </td>
                              <td>An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</td>
                              <td>Yanming Zhang</td>
                              <td>2023-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16211v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16211v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14810v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14810v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider optimal experimental design (OED) for nonlinear Bayesian inverse problems governed by large-scale partial differential equations (PDEs). For the optimality criteria of Bayesian OED, we consider both expected information gain and summary statistics including the trace and determinant of the information matrix that involves the evaluation of the parameter-to-observable (PtO) map and its derivatives. However, it is prohibitive to compute and optimize these criteria when the PDEs are very expensive to solve, the parameters to estimate are high-dimensional, and the optimization problem is combinatorial, high-dimensional, and non-convex. To address these challenges, we develop an accurate, scalable, and efficient computational framework to accelerate the solution of Bayesian OED. In particular, the framework is developed based on derivative-informed neural operator (DINO) surrogates with proper dimension reduction techniques and a modified swapping greedy algorithm. We demonstrate the high accuracy of the DINO surrogates in the computation of the PtO map and the optimality criteria compared to high-fidelity finite element approximations. We also show that the proposed method is scalable with increasing parameter dimensions. Moreover, we demonstrate that it achieves high efficiency with over 1000X speedup compared to a high-fidelity Bayesian OED solution for a three-dimensional PDE example with tens of thousands of parameters, including both online evaluation and offline construction costs of the surrogates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14810v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑由大规模偏微分方程（PDE）控制的非线性贝叶斯反问题的最优实验设计（OED）。对于贝叶斯OED的最优性标准，我们考虑了预期信息增益和汇总统计，包括信息矩阵的迹和行列式，该信息矩阵涉及对参数-可观测（PtO）图及其导数的评估。然而，当偏微分方程的求解非常昂贵，要估计的参数是高维的，并且优化问题是组合的、高维的和非凸的时，计算和优化这些准则是禁止的。为了应对这些挑战，我们开发了一个准确、可扩展和高效的计算框架来加速贝叶斯OED的解决方案。特别是，该框架是基于导数知情神经算子（DINO）代理，采用适当的降维技术和改进的交换贪婪算法开发的。与高保真有限元近似相比，我们证明了DINO替代物在PtO映射计算中的高精度和最优性标准。我们还证明了所提出的方法随着参数维数的增加是可扩展的。此外，我们证明，对于具有数万个参数的三维PDE示例，与高保真度贝叶斯OED解决方案相比，它实现了超过1000倍的加速，包括代理的在线评估和离线构建成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14810v1" target="_blank">2312.14810v1</a>
                              </td>
                              <td>Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</td>
                              <td>Jinwoo Go</td>
                              <td>2023-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14810v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14810v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12359v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-DINOiser: Teaching CLIP a few DINO tricks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12359v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12359v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, self-supervised representation methods have demonstrated good localization properties without human-made annotations nor explicit supervision. In this work, we take the best of both worlds and propose a zero-shot open-vocabulary semantic segmentation method, which does not require any annotations. We propose to locally improve dense MaskCLIP features, computed with a simple modification of CLIP's last pooling layer, by integrating localization priors extracted from self-supervised features. By doing so, we greatly improve the performance of MaskCLIP and produce smooth outputs. Moreover, we show that the used self-supervised feature properties can directly be learnt from CLIP features therefore allowing us to obtain the best results with a single pass through CLIP model. Our method CLIP-DINOiser needs only a single forward pass of CLIP and two light convolutional layers at inference, no extra supervision nor extra memory and reaches state-of-the-art results on challenging and fine-grained benchmarks such as COCO, Pascal Context, Cityscapes and ADE20k. The code to reproduce our results is available at https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12359v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其与任意文本提示的无缝交互，流行的CLIP模型显示了令人印象深刻的零样本功能。然而，它缺乏空间意识，不适合于密集的计算机视觉任务，例如语义分割，而不需要额外的微调步骤，该步骤通常使用注释，并可能抑制其原始的开放词汇特性。同时，自监督表示方法在没有人为注释和明确监督的情况下表现出良好的定位特性。在这项工作中，我们两全其美，提出了一种零样本开放词汇语义分割方法，该方法不需要任何注释。我们建议通过集成从自监督特征中提取的定位先验，局部改进密集的MaskCLIP特征，该特征通过对CLIP的最后一个池化层进行简单修改来计算。通过这样做，我们大大提高了MaskCLIP的性能，并产生了平滑的输出。此外，我们证明了所使用的自监督特征属性可以直接从CLIP特征中学习，因此允许我们使用单次通过的CLIP模型获得最佳结果。我们的方法CLIP DINOiser在推理时只需要一次CLIP的前向传递和两个轻卷积层，无需额外的监督和额外的内存，并且在具有挑战性的细粒度基准（如COCO、Pascal Context、Cityscapes和ADE20k）上达到了最先进的结果。重现我们结果的代码可在https://github.com/wysoczanska/clip_dinoiser.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12359v1" target="_blank">2312.12359v1</a>
                              </td>
                              <td>CLIP-DINOiser: Teaching CLIP a few DINO tricks</td>
                              <td>Monika Wysoczańska</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12359v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12359v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wysoczanska/clip_dinoiser" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_10912v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_10912v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_10912v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompt techniques. Specifically, IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_10912v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，计算机视觉领域发生了范式转变，反映了大型语言模型在自然语言处理领域的变革性影响。本文深入探讨了开放世界分割的探索，提出了一种称为图像提示分割（IPSeg）的新方法，该方法利用了视觉基础模型的力量。IPSeg是无训练范式的原则，它利用了图像提示技术。具体来说，IPSeg利用包含主观视觉概念的单个图像作为灵活的提示来查询视觉基础模型，如DINOv2和Stable Diffusion。我们的方法提取提示图像和输入图像的鲁棒特征，然后通过一个新颖的特征交互模块将输入表示与提示表示进行匹配，以生成突出显示输入图像中目标对象的点提示。生成的点提示进一步用于引导Segment Anything Model对输入图像中的目标对象进行分割。所提出的方法通过消除对详尽培训课程的需求而脱颖而出，从而提供了更高效和可扩展的解决方案。在COCO、PASCAL VOC和其他数据集上的实验证明了IPSeg使用直观的图像提示进行灵活的开放世界分割的有效性。这项工作开创了通过图像中传达的视觉概念来挖掘开放世界理解的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.10912v2" target="_blank">2310.10912v2</a>
                              </td>
                              <td>Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</td>
                              <td>Lv Tang</td>
                              <td>2023-10-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_10912v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.10912v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11125v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11125v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11125v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11125v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>类别级物体姿态估计旨在从已知类别中预测物体的6D姿态和3D尺寸，通常难以应对大的类内形状变化。利用平均形状的现有作品往往无法捕捉到这种变化。为了解决这个问题，我们提出了SecondPose，这是一种将DINOv2中特定于对象的几何特征与语义类别先验相结合的新方法。利用DINOv2在提供SE（3）一致语义特征方面的优势，我们分层提取了两种类型的SE（3（3）不变几何特征，以进一步封装局部到全局的特定对象信息。然后，这些几何特征与DINOv2特征点对齐，以在SE（3）变换下建立一致的对象表示，促进从相机空间到预定义规范空间的映射，从而进一步增强姿态估计。在NOCS-REAL275上进行的大量实验表明，SecondPose比最先进的技术进步了12.4%。此外，在更复杂的数据集HouseCat6D上，SecondPose仍然以很大的优势超过了其他竞争对手，该数据集提供了具有光度挑战性的物体。代码将很快发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11125v2" target="_blank">2311.11125v2</a>
                              </td>
                              <td>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</td>
                              <td>Yamei Chen</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11125v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11125v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/NOrangeeroli/SecondPose" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08825v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Guided Diffusion from Self-Supervised Diffusion Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08825v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08825v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Guidance serves as a key concept in diffusion models, yet its effectiveness is often limited by the need for extra data annotation or classifier pretraining. That is why guidance was harnessed from self-supervised learning backbones, like DINO. However, recent studies have revealed that the feature representation derived from diffusion model itself is discriminative for numerous downstream tasks as well, which prompts us to propose a framework to extract guidance from, and specifically for, diffusion models. Our research has yielded several significant contributions. Firstly, the guidance signals from diffusion models are on par with those from class-conditioned diffusion models. Secondly, feature regularization, when based on the Sinkhorn-Knopp algorithm, can further enhance feature discriminability in comparison to unconditional diffusion models. Thirdly, we have constructed an online training approach that can concurrently derive guidance from diffusion models for diffusion models. Lastly, we have extended the application of diffusion models along the constant velocity path of ODE to achieve a more favorable balance between sampling steps and fidelity. The performance of our methods has been outstanding, outperforming related baseline comparisons in large-resolution datasets, such as ImageNet256, ImageNet256-100 and LSUN-Churches. Our code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08825v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>指导是扩散模型中的一个关键概念，但其有效性往往受到额外数据注释或分类器预训练需求的限制。这就是为什么指导是由自我监督的学习骨干，如DINO来利用的。然而，最近的研究表明，从扩散模型本身导出的特征表示对许多下游任务也是有区别的，这促使我们提出一个框架来从扩散模型中提取指导，特别是针对扩散模型。我们的研究取得了一些重大贡献。首先，来自扩散模型的引导信号与来自类条件扩散模型的指导信号是一致的。其次，与无条件扩散模型相比，基于Sinkhorn-Knopp算法的特征正则化可以进一步提高特征的可分辨性。第三，我们构建了一种在线培训方法，可以同时从扩散模型中获得对扩散模型的指导。最后，我们扩展了扩散模型在ODE等速路径上的应用，以在采样步骤和保真度之间实现更有利的平衡。我们的方法的性能非常出色，在大分辨率数据集（如ImageNet256、ImageNet256-100和LSUN Churches）中优于相关的基线比较。我们的代码将会发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08825v1" target="_blank">2312.08825v1</a>
                              </td>
                              <td>Guided Diffusion from Self-Supervised Diffusion Features</td>
                              <td>Vincent Tao Hu</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08825v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08825v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09118v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WildlifeDatasets: An open-source toolkit for animal re-identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09118v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09118v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09118v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了WildlifeDataset(https://github.com/WildlifeDatasets/wildlife-datasets)-主要面向生态学家和计算机视觉/机器学习研究人员的开源工具包。WildlifeDataset是用Python编写的，允许直接访问公开可用的野生动物数据集，并为数据集预处理、性能分析和模型微调提供了多种方法。我们在各种场景和基线实验中展示了该工具包，据我们所知，包括对野生动物重新识别的数据集和方法进行最全面的实验比较，包括局部描述符和深度学习方法。此外，我们提供了第一个用于广泛物种内个体重新识别的基础模型MegaDescriptor，该模型在动物重新识别数据集上提供了最先进的性能，并显著优于其他预先训练的模型，如CLIP和DINOv2。为了向公众提供该模型，并允许与任何现有的野生动物监测应用程序轻松集成，我们通过HuggingFace中心提供多种MegaDescriptor风格（即小型、中型和大型）(https://huggingface.co/BVRA).</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09118v2" target="_blank">2311.09118v2</a>
                              </td>
                              <td>WildlifeDatasets: An open-source toolkit for animal re-identification</td>
                              <td>Vojtěch Čermák</td>
                              <td>2023-11-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09118v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09118v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wildlifedatasets/wildlife-tools" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03999v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adapting Self-Supervised Representations to Multi-Domain Setups</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03999v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03999v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained with DDM show significantly improved generalization (7.4%) to unseen domains compared to baselines. Therefore, DDM can efficiently adapt self-supervised encoders to provide high-quality, generalizable representations for diverse multi-domain data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03999v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前最先进的自监督方法在单个领域上训练时是有效的，但在看不见的领域上表现出有限的泛化能力。我们观察到，即使在混合域上训练，这些模型的泛化能力也很差，这使得它们不适合在不同的现实世界设置下部署。因此，我们提出了一种通用的、轻量级的域解纠缠模块（DDM），该模块可以插入任何自监督编码器，以在具有或不具有共享类的多个不同域上有效地执行表示学习。在根据自监督损失进行预训练期间，DDM通过将表示空间拆分为域变体和域不变部分，在表示空间中强制解纠缠。当域标签不可用时，DDM使用稳健的集群方法来发现伪域。我们发现，在包括PACS、DomainNet和WILDS在内的多领域基准测试上，在包括SimCLR、MoCo、BYOL、DINO、SimSiam和Barlow Twins在内的最先进的自监督模型上，使用DDM的预训练可以显示高达3.5%的线性探测精度提高。与基线相比，用DDM训练的模型对看不见的领域的泛化能力显著提高（7.4%）。因此，DDM可以有效地调整自监督编码器，为不同的多域数据提供高质量、可推广的表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03999v2" target="_blank">2309.03999v2</a>
                              </td>
                              <td>Adapting Self-Supervised Representations to Multi-Domain Setups</td>
                              <td>Neha Kalibhat</td>
                              <td>2023-09-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03999v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03999v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v6_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v6_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v6_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to 40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v6_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们的习得表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作预训练编码器上的正则化术语，以补救低质量表示。与基线相比，使用Q-Score正则化进行微调可以在ImageNet-100上将SSL模型的线性探测精度提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性密切相关，通过Q分数正则化增强这些特征使SSL表示更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v6" target="_blank">2203.01881v6</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v6_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v6" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07006v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixed Pseudo Labels for Semi-Supervised Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07006v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07006v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While the pseudo-label method has demonstrated considerable success in semi-supervised object detection tasks, this paper uncovers notable limitations within this approach. Specifically, the pseudo-label method tends to amplify the inherent strengths of the detector while accentuating its weaknesses, which is manifested in the missed detection of pseudo-labels, particularly for small and tail category objects. To overcome these challenges, this paper proposes Mixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled data, to mitigate the negative impact of missed detections and balance the model's learning across different object scales. Additionally, the model's detection performance on tail categories is improved by resampling labeled data with relevant instances. Notably, MixPL consistently improves the performance of various detectors and obtains new state-of-the-art results with Faster R-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore, MixPL also exhibits good scalability on large models, improving DINO Swin-L by 2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017 benchmark without extra annotations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07006v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然伪标签方法在半监督对象检测任务中取得了相当大的成功，但本文揭示了该方法的显著局限性。具体而言，伪标签方法倾向于放大检测器的固有优势，同时强调其弱点，这表现在伪标签的遗漏检测，特别是对于小型和尾部类别的对象。为了克服这些挑战，本文提出了混合伪标签（MixPL），由伪标签数据的Mixup和Mosaic组成，以减轻遗漏检测的负面影响，并平衡模型在不同对象尺度上的学习。此外，通过对带有相关实例的标记数据进行重新采样，提高了模型在尾部类别上的检测性能。值得注意的是，MixPL持续改进了各种探测器的性能，并在COCO标准和COCO完整基准上使用Faster R-CNN、FCOS和DINO获得了最先进的新结果。此外，MixPL在大型模型上也表现出良好的可扩展性，在没有额外注释的情况下，将DINO Swin-L提高了2.5%的mAP，并在COCO val2017基准上实现了非平凡的新记录（60.2%的mAP）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07006v1" target="_blank">2312.07006v1</a>
                              </td>
                              <td>Mixed Pseudo Labels for Semi-Supervised Object Detection</td>
                              <td>Zeming Chen</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07006v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07006v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/czm369/mixpl" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15404v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoMa: Robust Dense Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15404v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15404v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Feature matching is an important computer vision task that involves estimating correspondences between two images of a 3D scene, and dense methods estimate all such correspondences. The aim is to learn a robust model, i.e., a model able to match under challenging real-world changes. In this work, we propose such a model, leveraging frozen pretrained features from the foundation model DINOv2. Although these features are significantly more robust than local features trained from scratch, they are inherently coarse. We therefore combine them with specialized ConvNet fine features, creating a precisely localizable feature pyramid. To further improve robustness, we propose a tailored transformer match decoder that predicts anchor probabilities, which enables it to express multimodality. Finally, we propose an improved loss formulation through regression-by-classification with subsequent robust regression. We conduct a comprehensive set of experiments that show that our method, RoMa, achieves significant gains, setting a new state-of-the-art. In particular, we achieve a 36% improvement on the extremely challenging WxBS benchmark. Code is provided at https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15404v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>特征匹配是一项重要的计算机视觉任务，涉及估计3D场景的两个图像之间的对应关系，密集方法估计所有这些对应关系。其目的是学习一个稳健的模型，即能够在具有挑战性的现实世界变化下进行匹配的模型。在这项工作中，我们提出了这样一个模型，利用来自基础模型DINOv2的冻结预训练特征。尽管这些特征明显比从头开始训练的局部特征更健壮，但它们本质上是粗糙的。因此，我们将它们与专门的ConvNet精细特征相结合，创建了一个可精确定位的特征金字塔。为了进一步提高鲁棒性，我们提出了一种定制的变换器匹配解码器，该解码器预测锚概率，使其能够表达多模态。最后，我们通过分类回归和随后的稳健回归，提出了一个改进的损失公式。我们进行了一系列全面的实验，结果表明我们的RoMa方法取得了显著的成果，创造了新的最先进水平。特别是，我们在极具挑战性的WxBS基准上实现了36%的改进。代码提供于https://github.com/Parskatt/RoMa</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15404v2" target="_blank">2305.15404v2</a>
                              </td>
                              <td>RoMa: Robust Dense Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15404v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15404v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/roma" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析主要的MVSSL族。通过这个ER界，我们证明了基于聚类的方法，如DeepCluster和SwAV，最大化了MI。我们还重新解释了基于蒸馏的方法（如BYOL和DINO）的机制，表明它们明确地最大化了重建项，隐含地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v2" target="_blank">2307.10907v2</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/apple/ml-entropy-reconstruction" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05464v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05464v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05464v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models can encounter unexpected failures, especially when dealing with challenging sub-populations. One common reason for these failures is the occurrence of objects in backgrounds that are rarely seen during training. To gain a better understanding of these failure modes, human-interpretable descriptions are crucial for further analysis and improvement which is expensive. In this study, we propose an end-to-end framework that utilizes the capabilities of large language models (ChatGPT) and vision-language deep models (CLIP) to generate text descriptions of failure modes associated with spurious correlations (e.g. rarely seen backgrounds) without human-in-the-loop intervention. These descriptions can be used to generate synthetic data using generative models, such as diffusion models. The model can now use this generated data to learn from its weaknesses and enhance its performance on backgrounds that are uncommon for each class of data. Our approach serves as a broad solution, promising progress in comprehending model failure modes and strengthening deep learning models across a wide range of failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot manner. Our experiments have shown remarkable \textbf{improvements in accuracy ($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong background association) across $40$ different models, such as ResNets, EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05464v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型可能会遇到意想不到的失败，尤其是在处理具有挑战性的子群体时。这些失败的一个常见原因是在训练过程中很少看到背景中的物体。为了更好地理解这些故障模式，人类可解释的描述对于进一步的分析和改进至关重要，这是昂贵的。在这项研究中，我们提出了一个端到端的框架，该框架利用大型语言模型（ChatGPT）和视觉语言深度模型（CLIP）的能力，在没有人工干预的情况下，生成与虚假相关性（如罕见背景）相关的故障模式的文本描述。这些描述可以用于使用生成模型（例如扩散模型）生成合成数据。该模型现在可以使用这些生成的数据来学习其弱点，并提高其在每类数据中都不常见的背景下的性能。我们的方法是一个广泛的解决方案，有望在理解模型故障模式和以少量方式自动加强各种故障场景（如百家乐、颜色）的深度学习模型方面取得进展。我们的实验表明，在ImageNet-1000、CIFAR-10和CIFAR-100等各种数据集上，在40美元的不同模型（如ResNets、EfficientNets、DenseNets、Vision Transformer（ViT）、SwAVs、MoCos、DINO和CLIP）中，硬子种群（特别是错误的背景关联）的准确度显著提高（$\sim\textbf｛21%｝$）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05464v1" target="_blank">2312.05464v1</a>
                              </td>
                              <td>Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation</td>
                              <td>Atoosa Chegini</td>
                              <td>2023-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05464v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05464v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05189v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Autonomous Organizations as Public Services Supplying Platform</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05189v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05189v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA is a public company owned by Municipality of L Aquila, it supplies the institution with network services and software applications for distributing services to citizens. The future policy of the company is to enlarge the offer of its services to nearby communities that are unable to set up and maintain their own network and software structures. This paper presents thus a possible architecture model to support small municipalities in supplying public services to citizens, with the aid of SED Spa. Through second level platforms based on Blockchain networks and Multi-agents Systems running on smart contracts, the system will focus on Waste Tax (Ta.Ri) management system in the Fascicolo del Cittadino environment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05189v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Servizi Elaborazioni Dati SpA是拉奎拉市拥有的一家上市公司，为该机构提供网络服务和软件应用程序，用于向公民分发服务。该公司未来的政策是扩大向附近社区提供的服务，这些社区无法建立和维护自己的网络和软件结构。因此，本文提出了一种可能的建筑模型，以支持小城市在SED Spa的帮助下向公民提供公共服务。通过基于区块链网络的二级平台和运行在智能合约上的多代理系统，该系统将专注于Cittadino Fascolo环境中的废物税（Ta.Ri）管理系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05189v1" target="_blank">2312.05189v1</a>
                              </td>
                              <td>Distributed Autonomous Organizations as Public Services Supplying Platform</td>
                              <td>Giovanni De Gasperis</td>
                              <td>2023-12-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05189v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05189v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/jettbrains/-L-" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>