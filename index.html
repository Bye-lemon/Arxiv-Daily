<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2023-07-21</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_10984v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10984v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10984v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained with over 8 million images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10984v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从图像中重建精确的3D场景是一项长期的视觉任务。由于单图像重建问题的不适定性，大多数公认的方法都建立在多视图几何的基础上。现有技术（SOTA）单目度量深度估计方法只能处理单个相机模型，并且由于度量模糊性而无法执行混合数据训练。同时，在大型混合数据集上训练的SOTA单目方法通过学习仿射不变深度实现了零样本泛化，无法恢复真实世界的度量。在这项工作中，我们表明，零样本单视图度量深度模型的关键在于结合大规模数据训练和解决来自各种相机模型的度量模糊性。我们提出了一个规范的相机空间转换模块，该模块明确地解决了模糊性问题，并且可以毫不费力地插入到现有的单目模型中。配备我们的模块后，单目模型可以用800多万张图像和数千个相机模型进行稳定训练，从而将零样本泛化为具有不可见相机设置的野生图像。实验证明了该方法在7个零样本基准上的SOTA性能。值得注意的是，我们的方法在第二届单目深度估计挑战赛中获得了冠军。我们的方法能够在随机收集的互联网图像上准确恢复度量三维结构，为合理的单图像度量铺平了道路。潜在的好处延伸到下游任务，只需插入我们的模型即可显著改善这些任务。例如，我们的模型缓解了单目SLAM的尺度漂移问题（图1），导致了高质量的度量尺度密集映射。代码位于https://github.com/YvanYin/Metric3D.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10984v1" target="_blank">2307.10984v1</a>
                              </td>
                              <td>Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</td>
                              <td>Wei Yin</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10984v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07894v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">iSLAM: Imperative SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07894v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07894v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) stands as one of the critical challenges in robot navigation. Recent advancements suggest that methods based on supervised learning deliver impressive performance in front-end odometry, while traditional optimization-based methods still play a vital role in the back-end for minimizing estimation drift. In this paper, we found that such decoupled paradigm can lead to only sub-optimal performance, consequently curtailing system capabilities and generalization potential. To solve this problem, we proposed a novel self-supervised learning framework, imperative SLAM (iSLAM), which fosters reciprocal correction between the front-end and back-end, thus enhancing performance without necessitating any external supervision. Specifically, we formulate a SLAM system as a bi-level optimization problem so that the two components are bidirectionally connected. As a result, the front-end model is able to learn global geometric knowledge obtained through pose graph optimization by back-propagating the residuals from the back-end. This significantly improves the generalization ability of the entire system and thus achieves the accuracy improvement up to 45%. To the best of our knowledge, iSLAM is the first SLAM system showing that the front-end and back-end can learn jointly and mutually contribute to each other in a self-supervised manner.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07894v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人导航中的关键挑战之一。最近的进展表明，基于监督学习的方法在前端里程计中提供了令人印象深刻的性能，而传统的基于优化的方法在后端仍然发挥着至关重要的作用，以最大限度地减少估计漂移。在本文中，我们发现这种解耦范式只能导致次优性能，从而削弱系统能力和泛化潜力。为了解决这个问题，我们提出了一种新的自我监督学习框架，即命令式SLAM（iSLAM），它促进了前端和后端之间的相互校正，从而在不需要任何外部监督的情况下提高了性能。具体来说，我们将SLAM系统公式化为双层优化问题，使两个组件双向连接。因此，前端模型能够通过从后端反向传播残差来学习通过位姿图优化获得的全局几何知识。这显著提高了整个系统的泛化能力，从而实现了高达45%的精度提高。据我们所知，iSLAM是第一个SLAM系统，表明前端和后端可以以自我监督的方式共同学习并相互贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07894v3" target="_blank">2306.07894v3</a>
                              </td>
                              <td>iSLAM: Imperative SLAM</td>
                              <td>Taimeng Fu</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07894v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07894v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10015v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimizing the extended Fourier Mellin Transformation Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10015v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10015v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the increasing application of robots, stable and efficient Visual Odometry (VO) algorithms are becoming more and more important. Based on the Fourier Mellin Transformation (FMT) algorithm, the extended Fourier Mellin Transformation (eFMT) is an image registration approach that can be applied to downward-looking cameras, for example on aerial and underwater vehicles. eFMT extends FMT to multi-depth scenes and thus more application scenarios. It is a visual odometry method which estimates the pose transformation between three overlapping images. On this basis, we develop an optimized eFMT algorithm that improves certain aspects of the method and combines it with back-end optimization for the small loop of three consecutive frames. For this we investigate the extraction of uncertainty information from the eFMT registration, the related objective function and the graph-based optimization. Finally, we design a series of experiments to investigate the properties of this approach and compare it with other VO and SLAM (Simultaneous Localization and Mapping) algorithms. The results show the superior accuracy and speed of our o-eFMT approach, which is published as open source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10015v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着机器人应用的日益广泛，稳定高效的视觉测距算法变得越来越重要。基于傅立叶-梅林变换（FMT）算法，扩展傅立叶-梅林转换（eFMT）是一种图像配准方法，可应用于向下看的相机，例如航空和水下航行器上的相机。eFMT将FMT扩展到多深度场景，从而扩展到更多的应用场景。这是一种视觉里程计方法，用于估计三个重叠图像之间的姿态变换。在此基础上，我们开发了一种优化的eFMT算法，该算法改进了该方法的某些方面，并将其与三个连续帧的小循环的后端优化相结合。为此，我们研究了从eFMT配准中提取不确定性信息、相关目标函数和基于图的优化。最后，我们设计了一系列实验来研究该方法的性能，并将其与其他VO和SLAM（同时定位和映射）算法进行了比较。结果表明，我们的o-eFMT方法具有卓越的准确性和速度，该方法以开源形式发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10015v1" target="_blank">2307.10015v1</a>
                              </td>
                              <td>Optimizing the extended Fourier Mellin Transformation Algorithm</td>
                              <td>Wenqing Jiang</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10015v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10015v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09531v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09531v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09531v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09531v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local geometric information, i.e. normal and point distribution, is crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it provides constrains for data association, which further determines the direction of optimization and ultimately affects the accuracy of poses. However, estimating normal and point distribution are time-consuming tasks even with the assistance of the KDtree or volumetic maps. To achieve fast normal estimation, we look into the structural information of LiDAR scan and propose a novel fast approximate least squares (FALS) method. With the pre-computed bearing information, estimating the normal requires only the range information of the points when a new scan arrives. To efficiently estimate the distribution of points, we extend the ikd-tree to manage the map in voxels and update its point cloud distribution incrementally while maintaining its consistency with the normals. For scan points that satisfy visibility and consistency checks based on normal, we devise a robust and accurate hierarchical data association schema considering the distribution where point-to-surfel is prioritized over point-to-plane. We further fix voxels after the distribution convergences to balance the time consumption and the correctness of representation. Extensive experiments on diverse public datasets demonstrate the advantages of our system compared to other state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09531v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部几何信息，即法线和点分布，对于基于激光雷达的同时定位和映射（SLAM）至关重要，因为它为数据关联提供了约束，从而进一步决定了优化的方向，并最终影响姿态的准确性。然而，即使在KDtree或体积图的帮助下，估计正态分布和点分布也是耗时的任务。为了实现快速正态估计，我们研究了激光雷达扫描的结构信息，并提出了一种新的快速近似最小二乘法。利用预先计算的方位信息，当新的扫描到达时，估计法线只需要点的距离信息。为了有效地估计点的分布，我们扩展了ikd树来管理体素中的贴图，并在保持其与法线的一致性的同时逐步更新其点云分布。对于满足基于正态的可见性和一致性检查的扫描点，我们设计了一种稳健而准确的分层数据关联模式，考虑到点到表面优先于点到平面的分布。我们在分布收敛后进一步固定体素，以平衡时间消耗和表示的正确性。在不同的公共数据集上进行的大量实验表明，与其他最先进的方法相比，我们的系统具有优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09531v1" target="_blank">2307.09531v1</a>
                              </td>
                              <td>LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric Information Estimation</td>
                              <td>Kai Huang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09531v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09531v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09044v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09044v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09044v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For the SLAM system in robotics and autonomous driving, the accuracy of front-end odometry and back-end loop-closure detection determine the whole intelligent system performance. But the LiDAR-SLAM could be disturbed by current scene moving objects, resulting in drift errors and even loop-closure failure. Thus, the ability to detect and segment moving objects is essential for high-precision positioning and building a consistent map. In this paper, we address the problem of moving object segmentation from 3D LiDAR scans to improve the odometry and loop-closure accuracy of SLAM. We propose a novel 3D Sequential Moving-Object-Segmentation (3D-SeqMOS) method that can accurately segment the scene into moving and static objects, such as moving and static cars. Different from the existing projected-image method, we process the raw 3D point cloud and build a 3D convolution neural network for MOS task. In addition, to make full use of the spatio-temporal information of point cloud, we propose a point cloud residual mechanism using the spatial features of current scan and the temporal features of previous residual scans. Besides, we build a complete SLAM framework to verify the effectiveness and accuracy of 3D-SeqMOS. Experiments on SemanticKITTI dataset show that our proposed 3D-SeqMOS method can effectively detect moving objects and improve the accuracy of LiDAR odometry and loop-closure detection. The test results show our 3D-SeqMOS outperforms the state-of-the-art method by 12.4%. We extend the proposed method to the SemanticKITTI: Moving Object Segmentation competition and achieve the 2nd in the leaderboard, showing its effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09044v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于机器人和自动驾驶领域的SLAM系统来说，前端里程计和后端闭环检测的准确性决定了整个智能系统的性能。但激光雷达SLAM可能会受到当前场景移动物体的干扰，导致漂移误差，甚至环路闭合失败。因此，检测和分割移动物体的能力对于高精度定位和构建一致的地图至关重要。在本文中，我们解决了从3D激光雷达扫描中分割运动对象的问题，以提高SLAM的里程计和闭环精度。我们提出了一种新的3D顺序运动对象分割（3D-SeqMOS）方法，该方法可以将场景准确地分割为运动和静态对象，如运动和静态汽车。与现有的投影图像方法不同，我们对原始的三维点云进行了处理，并构建了用于MOS任务的三维卷积神经网络。此外，为了充分利用点云的时空信息，我们利用当前扫描的空间特征和先前残差扫描的时间特征，提出了一种点云残差机制。此外，我们建立了一个完整的SLAM框架来验证3D SeqMOS的有效性和准确性。在SemanticKITTI数据集上的实验表明，我们提出的3D SeqMOS方法可以有效地检测运动物体，并提高激光雷达里程计和闭环检测的准确性。测试结果表明，我们的3D SeqMOS比最先进的方法高12.4%。我们将所提出的方法扩展到SemanticKITTI:运动对象分割竞赛中，并在排行榜上排名第二，显示了其有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09044v1" target="_blank">2307.09044v1</a>
                              </td>
                              <td>3D-SeqMOS: A Novel Sequential 3D Moving Object Segmentation in Autonomous Driving</td>
                              <td>Qipeng Li</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09044v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09044v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08221v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08221v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08221v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop-closure detection, also known as place recognition, aiming to identify previously visited locations, is an essential component of a SLAM system. Existing research on lidar-based loop closure heavily relies on dense point cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal Distribution Transform) based global descriptor, NDT-Map-Code, designed for both on-road driving and underground valet parking scenarios. NDT-Map-Code can be directly extracted from the NDT map without the need for a dense point cloud, resulting in excellent scalability and low maintenance cost. The NDT representation is leveraged to identify representative patterns, which are further encoded according to their spatial location (bearing, range, and height). Experimental results on the NIO underground parking lot dataset and the KITTI dataset demonstrate that our method achieves significantly better performance compared to the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08221v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测，也称为位置识别，旨在识别以前访问过的位置，是SLAM系统的重要组成部分。现有的基于激光雷达的闭环研究在很大程度上依赖于密集点云和360视场激光雷达。本文提出了一种开箱即用的基于无损检测（正态分布变换）的全局描述符无损检测地图代码，该描述符专为道路驾驶和地下代客泊车场景设计。无损检测图代码可以直接从无损检测图中提取，而不需要密集的点云，具有良好的可扩展性和较低的维护成本。无损检测表示用于识别代表性图案，并根据其空间位置（方位、范围和高度）对其进行进一步编码。在NIO地下停车场数据集和KITTI数据集上的实验结果表明，与最先进的方法相比，我们的方法实现了显著更好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08221v1" target="_blank">2307.08221v1</a>
                              </td>
                              <td>NDT-Map-Code: A 3D global descriptor for real-time loop closure detection in lidar SLAM</td>
                              <td>Lizhou Liao</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08221v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08221v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07936v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Joint Beam Management and SLAM for mmWave Communication Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07936v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07936v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The millimeter-wave (mmWave) communication technology, which employs large-scale antenna arrays, enables inherent sensing capabilities. Simultaneous localization and mapping (SLAM) can utilize channel multipath angle estimates to realize integrated sensing and communication design in 6G communication systems. However, existing works have ignored the significant overhead required by the mmWave beam management when implementing SLAM with angle estimates. This study proposes a joint beam management and SLAM design that utilizes the strong coupling between the radio map and channel multipath for simultaneous beam management, localization, and mapping. In this approach, we first propose a hierarchical sweeping and sensing service design. The path angles are estimated in the hierarchical sweeping, enabling angle-based SLAM with the aid of an inertial measurement unit (IMU) to realize sensing service. Then, feature-aided tracking is proposed that utilizes prior angle information generated from the radio map and IMU. Finally, a switching module is introduced to enable flexible switching between hierarchical sweeping and feature-aided tracking. Simulations show that the proposed joint design can achieve sub-meter level localization and mapping accuracy (with an error < 0.5 m). Moreover, the beam management overhead can be reduced by approximately 40% in different wireless environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07936v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>毫米波（mmWave）通信技术采用了大规模天线阵列，实现了固有的传感能力。同时定位和映射（SLAM）可以利用信道多径角度估计来实现6G通信系统中的集成传感和通信设计。然而，现有工作忽略了毫米波波束管理在使用角度估计实现SLAM时所需的大量开销。本研究提出了一种联合波束管理和SLAM设计，该设计利用无线电映射和信道多径之间的强耦合来同时进行波束管理、定位和映射。在这种方法中，我们首先提出了一种分层的扫描和感知服务设计。在分层扫描中估计路径角度，使基于角度的SLAM能够在惯性测量单元（IMU）的帮助下实现传感服务。然后，提出了利用无线电地图和IMU生成的先验角度信息的特征辅助跟踪。最后，引入了一个切换模块，以实现分层扫描和特征辅助跟踪之间的灵活切换。仿真表明，所提出的联合设计可以实现亚米级的定位和映射精度（误差＜0.5m）。此外，在不同的无线环境中，波束管理开销可以减少大约40%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07936v1" target="_blank">2307.07936v1</a>
                              </td>
                              <td>Joint Beam Management and SLAM for mmWave Communication Systems</td>
                              <td>Hang Que</td>
                              <td>2023-07-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07936v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07936v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07763v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07763v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07763v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07763v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人依靠SLAM（同步定位和映射）在复杂和未知的环境中提供自主导航和任务执行。然而，由于动态和具有挑战性的情况，例如较差的照明条件和运动模糊，很难为移动机器人开发专用算法。为了解决这个问题，我们提出了一种基于几何特征的紧密耦合激光雷达视觉SLAM，它包括两个子系统（激光雷达和单目视觉SLAM）和一个融合框架。融合框架将多模态几何特征的深度和语义相关联，以补充视觉线地标，并在束调整（BA）中添加方向优化。这进一步限制了视觉里程计。另一方面，视觉子系统检测到的整个线段克服了激光雷达子系统只能对几何特征进行局部计算的限制。它调整线性特征点的方向并过滤掉异常值，从而实现更高精度的里程计系统。最后，我们使用一个模块来检测子系统的操作，在视觉子系统跟踪失败时，将激光雷达子系统的输出作为我们系统的补充轨迹。从各种室内和室外场景中的地面机器人收集的公共数据集M2DGR上的评估结果表明，与当前最先进的多模态方法相比，我们的系统实现了更准确、更稳健的姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07763v1" target="_blank">2307.07763v1</a>
                              </td>
                              <td>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</td>
                              <td>Ke Cao</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07763v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07763v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07607v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07607v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07607v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07607v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, significant progress has been made in the field of simultaneous localization and mapping (SLAM) research. However, current state-of-the-art solutions still struggle with limited accuracy and robustness in real-world applications. One major reason is the lack of datasets that fully capture the conditions faced by robots in the wild. To address this problem, we present SubT-MRS, an extremely challenging real-world dataset designed to push the limits of SLAM and perception algorithms.   SubT-MRS is a multi-modal, multi-robot dataset collected mainly from subterranean environments having multi-degraded conditions including structureless corridors, varying lighting conditions, and perceptual obscurants such as smoke and dust. Furthermore, the dataset packages information from a diverse range of time-synchronized sensors, including LiDAR, visual cameras, thermal cameras, and IMUs captured using varied vehicular motions like aerial, legged, and wheeled, to support research in sensor fusion, which is essential for achieving accurate and robust robotic perception in complex environments. To evaluate the accuracy of SLAM systems, we also provide a dense 3D model with sub-centimeter-level accuracy, as well as accurate 6DoF ground truth. Our benchmarking approach includes several state-of-the-art methods to demonstrate the challenges our datasets introduce, particularly in the case of multi-degraded environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07607v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，同步定位与映射（SLAM）研究取得了重大进展。然而，当前最先进的解决方案在现实应用中仍然难以达到有限的准确性和稳健性。一个主要原因是缺乏能够完全捕捉机器人在野外所面临状况的数据集。为了解决这个问题，我们提出了SubT-MRS，这是一个极具挑战性的真实世界数据集，旨在突破SLAM和感知算法的极限。SubT MRS是一个多模态、多机器人数据集，主要从具有多种退化条件的地下环境中收集，包括无结构走廊、不同的照明条件和烟雾和灰尘等感知遮蔽物。此外，该数据集封装了来自各种时间同步传感器的信息，包括激光雷达、视觉相机、热像仪和使用各种车辆运动（如空中、腿部和轮式）捕获的IMU，以支持传感器融合研究，这对于在复杂环境中实现准确和稳健的机器人感知至关重要。为了评估SLAM系统的精度，我们还提供了一个亚厘米级精度的密集3D模型，以及精确的6DoF地面实况。我们的基准测试方法包括几种最先进的方法，以证明我们的数据集带来的挑战，特别是在多重退化环境的情况下。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07607v1" target="_blank">2307.07607v1</a>
                              </td>
                              <td>SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</td>
                              <td>Shibo Zhao</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07607v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07607v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07296v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07296v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07296v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored areas, while reinforcement learning optimizes the robot's movement by assigning rewards for optimal frontier points. Graph SLAM is then used to integrate the robot's sensory data and build an accurate map of the environment. The proposed algorithm aims to improve the efficiency and accuracy of ExploreORB by optimizing the exploration process of frontiers to build a more accurate map. To evaluate the effectiveness of the proposed approach, experiments will be conducted in various virtual environments using Gazebo, a robot simulation software. Results of these experiments will be compared with existing methods to demonstrate the potential of the proposed approach as an optimal solution for SLAM in autonomous robotics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07296v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动同时定位和映射（SLAM）是自主机器人中的一个关键问题，它使机器人能够导航到新的区域，同时建立其周围环境的准确模型。视觉SLAM是一种流行的技术，它使用虚拟元素来增强体验。然而，在存在距离相似的多个边界的情况下，现有的基于边界的勘探策略可能会导致非最优路径。这个问题可能会影响视觉SLAM的效率和准确性，而视觉SLAM对于搜索和救援、勘探和测绘等广泛的机器人应用至关重要。为了解决这个问题，本研究将现有的可视化图形SLAM ExploreORB与强化学习相结合。所提出的算法允许机器人通过基于奖励的系统学习和优化探索路线，以创建具有适当边界选择的准确环境地图。基于边界的探索用于检测未探索的区域，而强化学习通过为最佳边界点分配奖励来优化机器人的运动。然后使用图形SLAM来整合机器人的感官数据，并构建准确的环境地图。所提出的算法旨在通过优化边界探索过程来构建更准确的地图，从而提高ExploreORB的效率和准确性。为了评估所提出方法的有效性，将使用机器人仿真软件Gazebo在各种虚拟环境中进行实验。将这些实验的结果与现有方法进行比较，以证明所提出的方法作为自主机器人中SLAM的最佳解决方案的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07296v1" target="_blank">2307.07296v1</a>
                              </td>
                              <td>Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment</td>
                              <td>Kenji Leong</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07296v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07296v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_05086v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stereo Event-based Visual-Inertial Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_05086v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_05086v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_05086v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Event-based cameras are new type vision sensors whose pixels work independently and respond asynchronously to brightness change with microsecond resolution, instead of providing standard intensity frames. Compared with traditional cameras, event-based cameras have low latency, no motion blur, and high dynamic range (HDR), which provide possibilities for robots to deal with some challenging scenes. We propose a visual-inertial odometry for stereo event-based cameras based on Error-State Kalman Filter (ESKF). The visual module updates the pose relies on the edge alignment of a semi-dense 3D map to a 2D image, and the IMU module updates pose by median integral. We evaluate our method on public datasets with general 6-DoF motion and compare the results against ground truth. We show that our proposed pipeline provides improved accuracy over the result of the state-of-the-art visual odometry for stereo event-based cameras, while running in real-time on a standard CPU (low-resolution cameras). To the best of our knowledge, this is the first published visual-inertial odometry for stereo event-based cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_05086v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于事件的相机是一种新型的视觉传感器，其像素独立工作，并以微秒的分辨率异步响应亮度变化，而不是提供标准的强度帧。与传统相机相比，基于事件的相机具有低延迟、无运动模糊和高动态范围（HDR），这为机器人处理一些具有挑战性的场景提供了可能性。我们提出了一种基于误差状态卡尔曼滤波器（ESKF）的立体事件相机视觉惯性里程计。视觉模块根据半密集3D地图到2D图像的边缘对齐来更新姿态，IMU模块通过中值积分来更新姿态。我们在具有一般6-DoF运动的公共数据集上评估了我们的方法，并将结果与地面实况进行了比较。我们表明，我们提出的管道在标准CPU（低分辨率相机）上实时运行的同时，为基于立体事件的相机提供了比最先进的视觉里程计结果更高的精度。据我们所知，这是第一个发表的基于立体事件的相机的视觉惯性里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.05086v3" target="_blank">2303.05086v3</a>
                              </td>
                              <td>Stereo Event-based Visual-Inertial Odometry</td>
                              <td>Kunfeng Wang</td>
                              <td>2023-03-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_05086v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.05086v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_07308v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_07308v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_07308v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_07308v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuSE，一种新的用于对象的Neural SE（3）-等变量嵌入，并说明了它如何支持对象SLAM在长期场景变化的情况下实现一致的空间理解。NeuSE是根据部分对象观测创建的一组潜在对象嵌入。它充当完整对象模型的紧凑点云代理，对完整形状信息进行编码，同时与物理世界中的对象一起等变地变换SE（3）。使用NeuSE，可以直接从推断的潜在代码中导出相对帧变换。我们提出的SLAM范式，使用NeuSE进行物体形状和姿态表征，可以独立运行或与典型的SLAM系统结合运行。它直接推断SE（3）相机姿势约束，这些约束与通用SLAM姿势图优化兼容，同时还保持了一个适应现实世界变化的轻量级以对象为中心的贴图。我们的方法是在以变化的对象为特征的合成序列和真实世界序列上进行评估的，当单独或与通用SLAM管道联合工作时，显示出改进的定位精度和变化感知映射能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.07308v2" target="_blank">2303.07308v2</a>
                              </td>
                              <td>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</td>
                              <td>Jiahui Fu</td>
                              <td>2023-03-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_07308v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.07308v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08978v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08978v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08978v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel visual-LiDAR odometry and mapping method with low-drift characteristics. The proposed method is based on two popular approaches, ORB-SLAM and A-LOAM, with monocular scale correction and visual-bootstrapped LiDAR poses initialization modifications. The scale corrector calculates the proportion between the depth of image keypoints recovered by triangulation and that provided by LiDAR, using an outlier rejection process for accuracy improvement. Concerning LiDAR poses initialization, the visual odometry approach gives the initial guesses of LiDAR motions for better performance. This methodology is not only applicable to high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate the proposed SLAM system's robustness and accuracy, we conducted experiments on the KITTI Odometry and S3E datasets. Experimental results illustrate that our method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore, regarding the accuracy of visual odometry with scale correction, our method performs similarly to the stereo-mode ORB-SLAM2.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08978v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的具有低漂移特性的视觉激光雷达里程计和测绘方法。该方法基于两种流行的方法，ORB-SLAM和A-LOAM，并对单目尺度校正和视觉自举激光雷达姿态初始化进行了修改。比例校正器使用用于提高精度的异常值排除过程来计算通过三角测量恢复的图像关键点的深度与由激光雷达提供的深度之间的比例。关于激光雷达姿态初始化，视觉里程计方法给出了激光雷达运动的初始猜测，以获得更好的性能。该方法不仅适用于高分辨率激光雷达，也适用于低分辨率激光雷达。为了评估所提出的SLAM系统的鲁棒性和准确性，我们在KITTI Odometry和S3E数据集上进行了实验。实验结果表明，我们的方法显著优于独立的ORB-SLAM2和A-LOAM。此外，关于带刻度校正的视觉里程计的准确性，我们的方法与立体模式ORB-SLAM2类似。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08978v2" target="_blank">2304.08978v2</a>
                              </td>
                              <td>Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping</td>
                              <td>Hanyu Cai</td>
                              <td>2023-04-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08978v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08978v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03890v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03890v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03890v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>High-quality datasets can speed up breakthroughs and reveal potential developing directions in SLAM research. To support the research on corner cases of visual SLAM systems, this paper presents Ground-Challenge: a challenging dataset comprising 36 trajectories with diverse corner cases such as aggressive motion, severe occlusion, changing illumination, few textures, pure rotation, motion blur, wheel suspension, etc. The dataset was collected by a ground robot with multiple sensors including an RGB-D camera, an inertial measurement unit (IMU), a wheel odometer and a 3D LiDAR. All of these sensors were well-calibrated and synchronized, and their data were recorded simultaneously. To evaluate the performance of cutting-edge SLAM systems, we tested them on our dataset and demonstrated that these systems are prone to drift and fail on specific sequences. We will release the full dataset and relevant materials upon paper publication to benefit the research community. For more information, visit our project website at https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03890v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>高质量的数据集可以加快SLAM研究的突破并揭示潜在的发展方向。为了支持对视觉SLAM系统拐角情况的研究，本文提出了Ground Challenge：一个具有挑战性的数据集，包括36条具有不同拐角情况的轨迹，如剧烈运动、严重遮挡、不断变化的照明、少量纹理、纯旋转、运动模糊、车轮悬架等。数据集由地面机器人收集，该机器人具有多个传感器，包括RGB-D相机、惯性测量单元（IMU）、车轮里程表和3D激光雷达。所有这些传感器都经过了良好的校准和同步，并且同时记录了它们的数据。为了评估尖端SLAM系统的性能，我们在数据集上对其进行了测试，并证明这些系统在特定序列上容易漂移和失败。我们将在论文发表后发布完整的数据集和相关材料，以造福研究界。欲了解更多信息，请访问我们的项目网站https://github.com/sjtuyinjie/Ground-Challenge.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03890v1" target="_blank">2307.03890v1</a>
                              </td>
                              <td>Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</td>
                              <td>Jie Yin</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03890v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03890v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17673v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OSPC: Online Sequential Photometric Calibration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17673v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17673v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Photometric calibration is essential to many computer vision applications. One of its key benefits is enhancing the performance of Visual SLAM, especially when it depends on a direct method for tracking, such as the standard KLT algorithm. Another advantage could be in retrieving the sensor irradiance values from measured intensities, as a pre-processing step for some vision algorithms, such as shape-from-shading. Current photometric calibration systems rely on a joint optimization problem and encounter an ambiguity in the estimates, which can only be resolved using ground truth information. We propose a novel method that solves for photometric parameters using a sequential estimation approach. Our proposed method achieves high accuracy in estimating all parameters; furthermore, the formulations are linear and convex, which makes the solution fast and suitable for online applications. Experiments on a Visual Odometry system validate the proposed method and demonstrate its advantages.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17673v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光度校准对许多计算机视觉应用至关重要。它的一个关键好处是提高了Visual SLAM的性能，尤其是当它依赖于直接的跟踪方法时，例如标准的KLT算法。另一个优点可以是从测量的强度中检索传感器辐照度值，作为一些视觉算法的预处理步骤，例如来自阴影的形状。当前的光度校准系统依赖于联合优化问题，并且在估计中遇到模糊性，这只能使用地面实况信息来解决。我们提出了一种新的方法，使用顺序估计方法解决光度参数。我们提出的方法在估计所有参数方面实现了高精度；此外，该公式具有线性和凸性，使得求解速度快，适合在线应用。在视觉里程计系统上的实验验证了所提出的方法，并证明了其优点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17673v2" target="_blank">2305.17673v2</a>
                              </td>
                              <td>OSPC: Online Sequential Photometric Calibration</td>
                              <td>Jawad Haidar</td>
                              <td>2023-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17673v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17673v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01121v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01121v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01121v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Geometric navigation is nowadays a well-established field of robotics and the research focus is shifting towards higher-level scene understanding, such as Semantic Mapping. When a robot needs to interact with its environment, it must be able to comprehend the contextual information of its surroundings. This work focuses on classifying and localising objects within a map, which is under construction (SLAM) or already built. To further explore this direction, we propose a framework that can autonomously detect and localize predefined objects in a known environment using a multi-modal sensor fusion approach (combining RGB and depth data from an RGB-D camera and a lidar). The framework consists of three key elements: understanding the environment through RGB data, estimating depth through multi-modal sensor fusion, and managing artifacts (i.e., filtering and stabilizing measurements). The experiments show that the proposed framework can accurately detect 98% of the objects in the real sample environment, without post-processing, while 85% and 80% of the objects were mapped using the single RGBD camera or RGB + lidar setup respectively. The comparison with single-sensor (camera or lidar) experiments is performed to show that sensor fusion allows the robot to accurately detect near and far obstacles, which would have been noisy or imprecise in a purely visual or laser-based approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01121v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几何导航是当今机器人学的一个成熟领域，研究重点正转向更高层次的场景理解，如语义映射。当机器人需要与环境互动时，它必须能够理解周围环境的上下文信息。这项工作的重点是对正在建造或已经建造的地图中的对象进行分类和定位。为了进一步探索这一方向，我们提出了一个框架，该框架可以使用多模态传感器融合方法（结合来自RGB-D相机和激光雷达的RGB和深度数据）在已知环境中自主检测和定位预定义对象。该框架由三个关键元素组成：通过RGB数据了解环境，通过多模态传感器融合估计深度，以及管理伪影（即过滤和稳定测量）。实验表明，所提出的框架可以在真实样本环境中准确检测98%的物体，而无需进行后处理，而85%和80%的物体分别使用单个RGBD相机或RGB+激光雷达装置进行了映射。与单传感器（相机或激光雷达）实验的比较表明，传感器融合使机器人能够准确检测远近障碍物，而在纯视觉或基于激光的方法中，这些障碍物可能会有噪声或不精确。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01121v1" target="_blank">2307.01121v1</a>
                              </td>
                              <td>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization</td>
                              <td>Federico Rollo</td>
                              <td>2023-07-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01121v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01121v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_10029v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextSLAM: Visual SLAM with Semantic Planar Text Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_10029v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_10029v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel visual SLAM method that integrates text objects tightly by treating them as semantic features via fully exploring their geometric and semantic prior. The text object is modeled as a texture-rich planar patch whose semantic meaning is extracted and updated on the fly for better data association. With the full exploration of locally planar characteristics and semantic meaning of text objects, the SLAM system becomes more accurate and robust even under challenging conditions such as image blurring, large viewpoint changes, and significant illumination variations (day and night). We tested our method in various scenes with the ground truth data. The results show that integrating texture features leads to a more superior SLAM system that can match images across day and night. The reconstructed semantic 3D text map could be useful for navigation and scene understanding in robotic and mixed reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_10029v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的视觉SLAM方法，该方法通过充分探索文本对象的几何和语义先验，将文本对象视为语义特征，从而紧密地集成文本对象。文本对象被建模为纹理丰富的平面补丁，其语义被实时提取和更新以获得更好的数据关联。随着对文本对象局部平面特征和语义的充分探索，即使在图像模糊、大的视点变化和显著的光照变化（白天和晚上）等具有挑战性的条件下，SLAM系统也变得更加准确和稳健。我们用地面实况数据在各种场景中测试了我们的方法。结果表明，集成纹理特征可以获得更优越的SLAM系统，该系统可以匹配昼夜图像。重建的语义3D文本图可用于机器人和混合现实应用中的导航和场景理解。我们的项目页面：https://github.com/SJTU-ViSYS/TextSLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.10029v2" target="_blank">2305.10029v2</a>
                              </td>
                              <td>TextSLAM: Visual SLAM with Semantic Planar Text Features</td>
                              <td>Boying Li</td>
                              <td>2023-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_10029v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.10029v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00488v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00488v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00488v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00488v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在缓慢变化的场景中同时定位和映射（SLAM）对于长期完成机器人任务非常重要。未能检测到场景变化可能会导致地图不准确，最终导致机器人丢失。经典的SLAM算法假设静态场景，最近的工作考虑了动态，但要求在连续帧中观察场景变化。半静态场景中，物体随着时间的推移出现、消失或缓慢移动，通常被忽视，但对长期操作至关重要。我们提出了一个对象感知的因子图SLAM框架，用于跟踪和重建半静态对象级别的变化。我们新的变分期望最大化策略用于优化因子图，该因子图涉及潜在变化对象的高斯均匀双峰测量似然。我们在模拟中评估了我们的方法以及最先进的SLAM解决方案，并在四个月内在仓库中捕获了我们新颖的真实世界SLAM数据集。我们的方法在半静态变化的情况下提高了定位的鲁棒性，提供了关于场景的对象级推理。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00488v1" target="_blank">2307.00488v1</a>
                              </td>
                              <td>POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments</td>
                              <td>Jingxing Qian</td>
                              <td>2023-07-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00488v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00488v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_17529v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_17529v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_17529v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered based on whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose filter, resulting in improved recall across a range of localization tolerances from $0.25m$ to $5m$, compared to a state-of-the-art baseline single image PnP method and its vanilla pose filtering. Our constraint detection system is active for approximately $35\%$ of the time on the Ford AV dataset and localization is particularly improved when the constraint detection is active.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_17529v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数6-DoF定位和SLAM系统使用静态地标，但忽略动态对象，因为它们不能有效地合并到典型的管道中。在包含动态对象的情况下，典型的方法试图对这些对象进行相对复杂的识别和定位，限制了它们的稳健性或通用性。在这项研究中，我们提出了一种中间立场，在自动驾驶汽车的背景下进行了演示，使用动态车辆在6-DoF逐帧PnP-RANSAC定位管道中提供有限的姿态约束信息。我们用运动模型改进了初始姿态估计，并提出了一种计算未来姿态估计预测质量的方法，该方法基于自动驾驶车辆的运动是否受到环境中动态车辆的相对帧间位置的约束而触发。我们的方法检测并识别合适的动态车辆，以定义这些姿势约束，从而修改姿势过滤器，与最先进的基线单图像PnP方法及其普通姿势过滤相比，在25万美元至500万美元的定位公差范围内，提高了召回率。在Ford AV数据集上，我们的约束检测系统在大约$35\%$的时间内是活动的，并且当约束检测是活动的时，定位特别改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.17529v1" target="_blank">2306.17529v1</a>
                              </td>
                              <td>Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</td>
                              <td>Stephen Hausler</td>
                              <td>2023-06-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_17529v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.17529v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构从运动技术中假设，除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在可变形环境中的视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'住宅/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16585v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16585v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16585v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The availability of real-time semantics greatly improves the core geometric functionality of SLAM systems, enabling numerous robotic and AR/VR applications. We present a new methodology for real-time semantic mapping from RGB-D sequences that combines a 2D neural network and a 3D network based on a SLAM system with 3D occupancy mapping. When segmenting a new frame we perform latent feature re-projection from previous frames based on differentiable rendering. Fusing re-projected feature maps from previous frames with current-frame features greatly improves image segmentation quality, compared to a baseline that processes images independently. For 3D map processing, we propose a novel geometric quasi-planar over-segmentation method that groups 3D map elements likely to belong to the same semantic classes, relying on surface normals. We also describe a novel neural network design for lightweight semantic map post-processing. Our system achieves state-of-the-art semantic mapping quality within 2D-3D networks-based systems and matches the performance of 3D convolutional networks on three real indoor datasets, while working in real-time. Moreover, it shows better cross-sensor generalization abilities compared to 3D CNNs, enabling training and inference with different depth sensors. Code and data will be released on project page: http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16585v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实时语义的可用性大大提高了SLAM系统的核心几何功能，实现了众多机器人和AR/VR应用。我们提出了一种从RGB-D序列进行实时语义映射的新方法，该方法将2D神经网络和基于SLAM系统的3D网络与3D占用映射相结合。在分割新帧时，我们基于可微分渲染从先前帧执行潜在特征重新投影。与独立处理图像的基线相比，将来自先前帧的重新投影的特征图与当前帧特征融合在一起大大提高了图像分割质量。对于3D地图处理，我们提出了一种新的几何准平面过分割方法，该方法根据曲面法线对可能属于相同语义类的3D地图元素进行分组。我们还描述了一种用于轻量级语义图后处理的新型神经网络设计。我们的系统在基于2D-3D网络的系统中实现了最先进的语义映射质量，并在三个真实的室内数据集上匹配3D卷积网络的性能，同时实时工作。此外，与3D细胞神经网络相比，它显示出更好的跨传感器泛化能力，能够使用不同的深度传感器进行训练和推理。代码和数据将在项目页面上发布：http://jingwenwang95.github.io/SeMLaPS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16585v1" target="_blank">2306.16585v1</a>
                              </td>
                              <td>SeMLaPS: Real-time Semantic Mapping with Latent Prior Networks and Quasi-Planar Segmentation</td>
                              <td>Jingwen Wang</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16585v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16585v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_05916v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_05916v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_05916v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based visual odometry, TartanVO. TartanVO, like other direct methods rather than feature-based, estimates camera pose through dense optical flow, which only applies to static scenes and disregards dynamic objects. Due to the color constancy assumption, optical flow is not able to differentiate between dynamic and static pixels. Therefore, to reconstruct a static map through such direct methods, our pipeline resolves dynamic/static segmentation by leveraging the optical flow output, and only fuse static points into the map. Moreover, we rerender the input frames such that the dynamic pixels are removed and iteratively pass them back into the visual odometry to refine the pose estimate.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_05916v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于学习的视觉里程计TartanVO的密集动态RGB-D SLAM管道。TartanVO和其他直接方法一样，而不是基于特征的方法，通过密集的光流来估计相机姿态，这只适用于静态场景，而忽略了动态对象。由于颜色恒定性假设，光流无法区分动态像素和静态像素。因此，为了通过这种直接的方法重建静态地图，我们的管道通过利用光流输出来解决动态/静态分割，并且只将静态点融合到地图中。此外，我们重新绘制输入帧，以便去除动态像素，并迭代地将它们传递回视觉里程计，以细化姿态估计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.05916v2" target="_blank">2205.05916v2</a>
                              </td>
                              <td>Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry</td>
                              <td>Shihao Shen</td>
                              <td>2022-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_05916v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.05916v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16530v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16530v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16530v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Using Unmanned Aerial Vehicles (UAVs) to perform high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16530v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用无人机执行高空操纵任务，而不仅仅是被动视觉应用，可以减少人类工人的时间、成本和风险。先前对空中操纵的研究依赖于地面实况状态估计或GPS/全站仪以及一些同时定位和测绘（SLAM）算法，这对于许多接近GPS信号退化或无特征环境的基础设施的应用来说可能不实用。视觉伺服可以避免估计机器人姿态的需要。用于空中操纵的视觉伺服的现有工作要么仅涉及末端执行器位置控制，要么依赖于精确的速度测量和具有已知模式的预定义视觉标记。此外，以前的大多数工作都使用欠驱动无人机，导致末端执行器的机械设计和控制设计复杂。本文利用全驱动无人机开发了一种基于图像的桥梁维护视觉伺服控制策略。主要部件是（1）视觉线检测和跟踪系统，（2）混合阻抗力和运动控制系统。我们的方法既不依赖于来自外部定位系统的机器人姿态/速度估计，也不依赖于预定义的视觉标记。由于完全致动的性质，机械系统和控制器结构的复杂性也被最小化。实验表明，该系统只需对桥梁涂装进行视觉引导，就能有效地进行运动跟踪和力保持。据我们所知，这是第一批使用视觉伺服进行空中操纵的研究之一，该伺服能够在不需要外部姿态/速度信息或预定义视觉引导的情况下实现运动和力控制。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16530v1" target="_blank">2306.16530v1</a>
                              </td>
                              <td>Image-based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</td>
                              <td>Guanqi He</td>
                              <td>2023-06-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16530v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16530v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_09553v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_09553v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_09553v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_09553v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了Mu$^｛2｝$SLAM，这是一种多语言序列到序列模型，在100多种语言的自动语音识别（ASR）、自动语音翻译（AST）和机器翻译（MT）中，对未标记语音、未标记文本和监督数据进行联合预训练。通过利用语音的量化表示作为目标，Mu$^{2}$SLAM用解码器上类似于T5的序列到序列掩蔽去噪目标和编码器上的掩蔽语言建模（MLM）目标来训练语音文本模型，用于未标记的语音和文本，同时利用监督任务来改进模型内的跨语言和跨模态表示对齐。在CoVoST AST上，Mu$^｛2｝$SLAM为在公共数据集上训练的模型建立了一个新的最先进的技术，在xx-en翻译上比以前的最佳翻译提高了1.9 BLEU点，在en-xx翻译上提高了1.1 BLEU点。在Voxpopuli ASR上，尽管使用了相对较弱的序列到序列架构，但我们的模型与使用RNN-T解码器微调的mSLAM模型的性能相匹配。在文本理解任务上，我们的模型在XNLI上比mSLAM改进了6\%以上，更接近于XNLI和TydiQA上容量相当的mT5模型的性能，为所有语音和文本理解任务的单一模型铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.09553v2" target="_blank">2212.09553v2</a>
                              </td>
                              <td>Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models</td>
                              <td>Yong Cheng</td>
                              <td>2022-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_09553v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.09553v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14812v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14812v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14812v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurate detection of movable and moving objects in LiDAR is of vital importance for navigation. Most existing works focus on extracting and removing moving objects during navigation. Movable objects like pedestrians, parked vehicles, etc. although static may move in the future. This leads to erroneous navigation and accidents. In such cases, it becomes necessary to detect potentially movable objects. To this end, we present a learning-based approach that segments movable and moving objects by generating static parts of scenes that are otherwise occluded. Our model performs superior to existing baselines on static LiDAR reconstructions using 3 datasets including a challenging sparse industrial dataset. We achieve this without the assistance of any segmentation labels because such labels might not always be available for less popular yet important settings like industrial environments. The non-movable static parts of the scene generated by our model are of vital importance for downstream navigation for SLAM. The movable objects detected by our model can be fed to a downstream 3D detector for aiding navigation. Though we do not use segmentation, we evaluate our method against navigation baselines that use it to remove dynamic objects for SLAM. Through extensive experiments on several datasets, we showcase that our model surpasses these baselines on navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14812v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>激光雷达中可移动和移动物体的精确检测对导航至关重要。现有的大多数工作都集中在导航过程中提取和移除移动对象。可移动的物体，如行人、停放的车辆等，尽管静止的物体将来可能会移动。这会导致错误导航和事故。在这种情况下，有必要检测潜在的可移动物体。为此，我们提出了一种基于学习的方法，通过生成场景中被遮挡的静态部分来分割可移动和移动对象。在使用3个数据集（包括一个具有挑战性的稀疏工业数据集）进行静态激光雷达重建时，我们的模型表现优于现有基线。我们在没有任何细分标签的帮助下实现了这一点，因为这样的标签可能并不总是适用于不太流行但重要的环境，如工业环境。我们的模型生成的场景中不可移动的静态部分对于SLAM的下游导航至关重要。我们的模型检测到的可移动物体可以被馈送到下游的3D检测器，用于辅助导航。虽然我们不使用分割，但我们根据导航基线来评估我们的方法，导航基线使用它来移除SLAM的动态对象。通过在几个数据集上进行广泛的实验，我们展示了我们的模型在导航方面超过了这些基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14812v1" target="_blank">2306.14812v1</a>
                              </td>
                              <td>MOVESe: MOVablE and Moving LiDAR Scene Segmentation with Improved Navigation in Seg-label free settings</td>
                              <td>Prashant Kumar</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14812v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14812v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14137v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14137v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14137v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rapid developments of mobile robotics and autonomous navigation over the years are largely empowered by public datasets for testing and upgrading, such as SLAM and localization tasks. Impressive demos and benchmark results have arisen, indicating the establishment of a mature technical framework. However, from the view point of real-world deployments, there are still critical defects of robustness in challenging environments, especially in large-scale, GNSS-denied, textural-monotonous, and unstructured scenarios. To meet the pressing validation demands in such scope, we build a novel challenging robot navigation dataset in a large botanic garden of more than 48000m2. Comprehensive sensors are employed, including high-res/rate stereo Gray&RGB cameras, rotational and forward 3D LiDARs, and low-cost and industrial-grade IMUs, all of which are well calibrated and accurately hardware-synchronized. An all-terrain wheeled robot is configured to mount the sensor suite and provide odometry data. A total of 32 long and short sequences of 2.3 million images are collected, covering scenes of thick woods, riversides, narrow paths, bridges, and grasslands that rarely appeared in previous resources. Excitedly, both highly-accurate ego-motions and 3D map ground truth are provided, along with fine-annotated vision semantics. Our goal is to contribute a high-quality dataset to advance robot navigation and sensor fusion research to a higher level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14137v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多年来，移动机器人和自主导航的快速发展在很大程度上得益于用于测试和升级的公共数据集，如SLAM和定位任务。令人印象深刻的演示和基准测试结果已经出现，表明建立了一个成熟的技术框架。然而，从现实世界部署的角度来看，在具有挑战性的环境中，尤其是在大规模、拒绝全球导航卫星系统、纹理单调和非结构化的场景中，仍然存在稳健性的关键缺陷。为了满足这一范围内紧迫的验证需求，我们在一个超过48000平方米的大型植物园中构建了一个具有挑战性的机器人导航数据集。采用了全面的传感器，包括高分辨率/速率立体声Gray和RGB相机、旋转和正向3D激光雷达，以及低成本和工业级IMU，所有这些都经过了良好的校准和精确的硬件同步。全地形轮式机器人被配置为安装传感器套件并提供里程测量数据。共收集了32个长短序列，共230万幅图像，涵盖了以前资源中很少出现的茂密的树林、河岸、狭窄的小路、桥梁和草地的场景。令人兴奋的是，提供了高度准确的自我运动和3D地图地面实况，以及精细的注释视觉语义。我们的目标是提供高质量的数据集，将机器人导航和传感器融合研究推向更高的水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14137v1" target="_blank">2306.14137v1</a>
                              </td>
                              <td>BotanicGarden: A high-quality and large-scale robot navigation dataset in challenging natural environments</td>
                              <td>Yuanzhi Liu</td>
                              <td>2023-06-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14137v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14137v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03872v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03872v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03872v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Loop Closure Detection (LCD) is an essential component of visual simultaneous localization and mapping (SLAM) systems. It enables the recognition of previously visited scenes to eliminate pose and map estimate drifts arising from long-term exploration. However, current appearance-based LCD methods face significant challenges, including high computational costs, viewpoint variance, and dynamic objects in scenes. This paper introduced an online appearance based LCD using local superpixel grids descriptor and dynamic node, i.e, LSGDDN-LCD, to find similarities between scenes via hand-crafted features extracted from LSGD. Unlike traditional Bag-of-Words (BoW) based LCD, which requires pre-training, we proposed an adaptive mechanism to group similar images called $\textbf{\textit{dynamic}}$ $\textbf{\textit{node}}$, which incrementally adjusted the database in an online manner, allowing for efficient and online retrieval of previously viewed images without need of the pre-training. Experimental results confirmed that the LSGDDN-LCD significantly improved LCD precision-recall and efficiency, and outperformed several state-of-the-art (SOTA) approaches on multiple typical datasets, indicating its great potential as a generic LCD framework.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03872v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>闭环检测（LCD）是视觉同步定位和映射（SLAM）系统的重要组成部分。它能够识别以前访问过的场景，以消除长期勘探产生的姿态和地图估计漂移。然而，当前基于外观的LCD方法面临着重大挑战，包括高计算成本、视点变化和场景中的动态对象。本文介绍了一种基于在线外观的LCD，该LCD使用局部超像素网格描述符和动态节点，即LSGDDN-LCD，通过从LSGD中提取的手工特征来查找场景之间的相似性。与传统的基于单词袋（BoW）的LCD需要预训练不同，我们提出了一种自适应机制来对类似图像进行分组，称为$\textbf｛\textit｛dynamic｝｝$\textbf｛\text it｛node｝}$，该机制以在线方式增量调整数据库，允许在不需要预训练的情况下高效在线检索先前查看的图像。实验结果证实，LSGDDN-LCD显著提高了LCD的查全率和效率，并在多个典型数据集上优于几种最先进的（SOTA）方法，表明其作为通用LCD框架的巨大潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03872v2" target="_blank">2304.03872v2</a>
                              </td>
                              <td>LSGDDN-LCD: An Appearance-based Loop Closure Detection using Local Superpixel Grid Descriptors and Incremental Dynamic Nodes</td>
                              <td>Baosheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03872v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03872v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_08207v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_08207v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_08207v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most feature-based stereo visual odometry (SVO) approaches estimate the motion of mobile robots by matching and tracking point features along a sequence of stereo images. However, in dynamic scenes mainly comprising moving pedestrians, vehicles, etc., there are insufficient robust static point features to enable accurate motion estimation, causing failures when reconstructing robotic motion. In this paper, we proposed DynPL-SVO, a complete dynamic SVO method that integrated united cost functions containing information between matched point features and re-projection errors perpendicular and parallel to the direction of the line features. Additionally, we introduced a \textit{dynamic} \textit{grid} algorithm to enhance its performance in dynamic scenes. The stereo camera motion was estimated through Levenberg-Marquard minimization of the re-projection errors of both point and line features. Comprehensive experimental results on KITTI and EuRoC MAV datasets showed that accuracy of the DynPL-SVO was improved by over 20\% on average compared to other state-of-the-art SVO systems, especially in dynamic scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_08207v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于特征的立体视觉里程计（SVO）方法通过沿着立体图像序列匹配和跟踪点特征来估计移动机器人的运动。然而，在主要包括移动的行人、车辆等的动态场景中，没有足够的鲁棒静态点特征来实现精确的运动估计，导致在重建机器人运动时失败。在本文中，我们提出了DynPL SVO，这是一种完整的动态SVO方法，它集成了包含匹配点特征之间信息的联合代价函数和垂直和平行于线特征方向的重投影误差。此外，我们引入了\textit｛dynamic｝\textit{grid｝算法，以增强其在动态场景中的性能。立体相机的运动是通过Levenberg-Marquard最小化点和线特征的重投影误差来估计的。在KITTI和EuRoC MAV数据集上的综合实验结果表明，与其他最先进的SVO系统相比，DynPL SVO的精度平均提高了20%以上，尤其是在动态场景中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.08207v3" target="_blank">2205.08207v3</a>
                              </td>
                              <td>DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes</td>
                              <td>Baosheng Zhang</td>
                              <td>2022-05-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_08207v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.08207v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2109_12910v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2109_12910v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2109_12910v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is one of the essential techniques and functionalities used by robots to perform autonomous navigation tasks. Inspired by the rodent hippocampus, this paper presents a biologically inspired SLAM system based on a LiDAR sensor using a hippocampal model to build a cognitive map and estimate the robot pose in indoor environments. Based on the biologically inspired models mimicking boundary cells, place cells, and head direction cells, the SLAM system using LiDAR point cloud data is capable of leveraging the self-motion cues from the LiDAR odometry and the boundary cues from the LiDAR boundary cells to build a cognitive map and estimate the robot pose. Experiment results show that with the LiDAR boundary cells the proposed SLAM system greatly outperforms the camera-based brain-inspired method in both simulation and indoor environments, and is competitive with the conventional LiDAR-based SLAM methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2109_12910v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是机器人执行自主导航任务的基本技术和功能之一。受啮齿动物海马体的启发，本文提出了一种基于激光雷达传感器的生物启发SLAM系统，该系统使用海马模型构建认知地图并估计机器人在室内环境中的姿势。基于模仿边界细胞、位置细胞和头部方向细胞的生物启发模型，使用激光雷达点云数据的SLAM系统能够利用来自激光雷达里程计的自运动线索和来自激光雷达边界细胞的边界线索来构建认知图并估计机器人姿态。实验结果表明，使用激光雷达边界单元，所提出的SLAM系统在模拟和室内环境中都大大优于基于相机的大脑启发方法，并且与传统的基于激光雷达的SLAM方法具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2109.12910v2" target="_blank">2109.12910v2</a>
                              </td>
                              <td>A Biologically-Inspired Simultaneous Localization and Mapping System Based on LiDAR Sensor</td>
                              <td>Genghang Zhuang</td>
                              <td>2021-09-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2109_12910v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2109.12910v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_11823v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_11823v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_11823v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes a novel visual simultaneous localization and mapping (SLAM) system called Hybrid Depth-augmented Panoramic Visual SLAM (HDPV-SLAM), that employs a panoramic camera and a tilted multi-beam LiDAR scanner to generate accurate and metrically-scaled trajectories. RGB-D SLAM was the design basis for HDPV-SLAM, which added depth information to visual features. It aims to solve the two major issues hindering the performance of similar SLAM systems. The first obstacle is the sparseness of LiDAR depth, which makes it difficult to correlate it with the extracted visual features of the RGB image. A deep learning-based depth estimation module for iteratively densifying sparse LiDAR depth was suggested to address this issue. The second issue pertains to the difficulties in depth association caused by a lack of horizontal overlap between the panoramic camera and the tilted LiDAR sensor. To surmount this difficulty, we present a hybrid depth association module that optimally combines depth information estimated by two independent procedures, feature-based triangulation and depth estimation. During a phase of feature tracking, this hybrid depth association module aims to maximize the use of more accurate depth information between the triangulated depth with visual features tracked and the deep learning-based corrected depth. We evaluated the efficacy of HDPV-SLAM using the 18.95 km-long York University and Teledyne Optech (YUTO) MMS dataset. The experimental results demonstrate that the two proposed modules contribute substantially to the performance of HDPV-SLAM, which surpasses that of the state-of-the-art (SOTA) SLAM systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_11823v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的视觉同时定位和映射（SLAM）系统，称为混合深度增强全景视觉SLAM（HDPV-SLAM），该系统使用全景相机和倾斜的多光束激光雷达扫描仪来生成精确且按度量比例缩放的轨迹。RGB-D SLAM是HDPV-SLAM的设计基础，它为视觉特征添加了深度信息。它旨在解决阻碍类似SLAM系统性能的两个主要问题。第一个障碍是激光雷达深度的稀疏性，这使得它很难与RGB图像的提取视觉特征相关联。为了解决这个问题，提出了一种基于深度学习的深度估计模块，用于迭代加密稀疏激光雷达深度。第二个问题涉及由于全景相机和倾斜的激光雷达传感器之间缺乏水平重叠而导致的深度关联的困难。为了克服这一困难，我们提出了一种混合深度关联模块，该模块将通过两个独立过程（基于特征的三角测量和深度估计）估计的深度信息进行最佳组合。在特征跟踪阶段，该混合深度关联模块旨在最大限度地利用具有跟踪的视觉特征的三角测量深度和基于深度学习的校正深度之间更准确的深度信息。我们使用长18.95公里的约克大学和Teledyne Optech（YUTO）MMS数据集评估了HDPV-SLAM的疗效。实验结果表明，所提出的两个模块对HDPV-SLAM的性能有很大贡献，超过了最先进的（SOTA）SLAM系统。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.11823v3" target="_blank">2301.11823v3</a>
                              </td>
                              <td>HDPV-SLAM: Hybrid Depth-augmented Panoramic Visual SLAM for Mobile Mapping System with Tilted LiDAR and Panoramic Visual Camera</td>
                              <td>Mostafa Ahmadi</td>
                              <td>2023-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_11823v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.11823v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12901v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Map Point Selection for Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12901v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12901v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localisation and mapping (SLAM) play a vital role in autonomous robotics. Robotic platforms are often resource-constrained, and this limitation motivates resource-efficient SLAM implementations. While sparse visual SLAM algorithms offer good accuracy for modest hardware requirements, even these more scalable sparse approaches face limitations when applied to large-scale and long-term scenarios. A contributing factor is that the point clouds resulting from SLAM are inefficient to use and contain significant redundancy.   This paper proposes the use of subset selection algorithms to reduce the map produced by sparse visual SLAM algorithms. Information-theoretic techniques have been applied to simpler related problems before, but they do not scale if applied to the full visual SLAM problem. This paper proposes a number of novel information\hyp{}theoretic utility functions for map point selection and optimises these functions using greedy algorithms. The reduced maps are evaluated using practical data alongside an existing visual SLAM implementation (ORB-SLAM 2). Approximate selection techniques proposed in this paper achieve trajectory accuracy comparable to an offline baseline while being suitable for online use. These techniques enable the practical reduction of maps for visual SLAM with competitive trajectory accuracy.   Results also demonstrate that SLAM front-end performance can significantly impact the performance of map point selection. This shows the importance of testing map point selection with a front-end implementation. To exploit this, this paper proposes an approach that includes a model of the front-end in the utility function when additional information is available. This approach outperforms alternatives on applicable datasets and highlights future research directions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12901v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）在自主机器人中发挥着至关重要的作用。机器人平台通常受到资源限制，这种限制促使实现资源高效的SLAM。虽然稀疏视觉SLAM算法为适度的硬件需求提供了良好的准确性，但即使是这些更具可扩展性的稀疏方法在应用于大规模和长期场景时也面临限制。一个促成因素是SLAM产生的点云使用效率低，并且包含显著的冗余。本文提出使用子集选择算法来减少稀疏视觉SLAM算法产生的映射。信息论技术以前曾被应用于更简单的相关问题，但如果应用于全视觉SLAM问题，它们就无法扩展。本文提出了许多新的用于地图点选择的信息论效用函数，并使用贪婪算法对这些函数进行了优化。使用实际数据以及现有的可视化SLAM实现（ORB-SLAM 2）来评估缩减后的映射。本文提出的近似选择技术实现了与离线基线相当的轨迹精度，同时适合在线使用。这些技术能够以具有竞争力的轨迹精度对视觉SLAM的地图进行实际缩减。结果还表明，SLAM前端性能会显著影响地图点选择的性能。这表明了使用前端实现测试地图点选择的重要性。为了利用这一点，本文提出了一种方法，当有额外信息可用时，在效用函数中包括前端的模型。这种方法在适用的数据集上优于其他方法，并突出了未来的研究方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12901v1" target="_blank">2306.12901v1</a>
                              </td>
                              <td>Map Point Selection for Visual SLAM</td>
                              <td>Christiaan J. Müller</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12901v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12901v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11048v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11048v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11048v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11048v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an uncertainty learning framework for dense neural simultaneous localization and mapping (SLAM). Estimating pixel-wise uncertainties for the depth input of dense SLAM methods allows to re-weigh the tracking and mapping losses towards image regions that contain more suitable information that is more reliable for SLAM. To this end, we propose an online framework for sensor uncertainty estimation that can be trained in a self-supervised manner from only 2D input data. We further discuss the advantages of the uncertainty learning for the case of multi-sensor input. Extensive analysis, experimentation, and ablations show that our proposed modeling paradigm improves both mapping and tracking accuracy and often performs better than alternatives that require ground truth depth or 3D. Our experiments show that we achieve a 38% and 27% lower absolute trajectory tracking error (ATE) on the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset on two types of depth sensors we report an 11% F1-score improvement on RGBD SLAM compared to the recent state-of-the-art neural implicit approaches. Our source code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11048v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一个用于密集神经同时定位和映射（SLAM）的不确定性学习框架。估计密集SLAM方法的深度输入的逐像素不确定性允许对包含更适合SLAM的信息的图像区域的跟踪和映射损失进行重新加权。为此，我们提出了一种用于传感器不确定性估计的在线框架，该框架可以仅从2D输入数据以自监督的方式进行训练。我们进一步讨论了不确定性学习在多传感器输入情况下的优势。广泛的分析、实验和消融表明，我们提出的建模范式提高了测绘和跟踪精度，并且通常比需要地面实况深度或3D的替代方案表现更好。我们的实验表明，在7场景和TUM-RGBD数据集上，我们分别实现了38%和27%的绝对轨迹跟踪误差（ATE）降低。在两种类型的深度传感器的流行副本数据集上，我们报告称，与最近最先进的神经隐式方法相比，RGBD SLAM的F1分数提高了11%。我们的源代码将提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11048v1" target="_blank">2306.11048v1</a>
                              </td>
                              <td>UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM</td>
                              <td>Erik Sandström</td>
                              <td>2023-06-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11048v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11048v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10561v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LiDAR-Based Place Recognition For Autonomous Driving: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10561v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10561v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10561v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based place recognition (LPR) plays a pivotal role in autonomous driving, which assists Simultaneous Localization and Mapping (SLAM) systems in reducing accumulated errors and achieving reliable localization. However, existing reviews predominantly concentrate on visual place recognition (VPR) methods. Despite notable advancements in LPR in recent years, there is yet a systematic review dedicated to this field to the best of our knowledge. This paper bridges the gap by providing a comprehensive review of place recognition methods employing LiDAR sensors, thus facilitating and encouraging further research. We commence by delving into the problem formulation of place recognition and exploring existing challenges, describing relations to previous surveys. Subsequently, we conduct an in-depth review of related research, which offers detailed classifications, strengths and weaknesses, and architectures. Finally, we summarize existing datasets, commonly used evaluation metrics, and comprehensive evaluation results from various methods on public datasets. This paper can serve as a valuable tutorial for newcomers entering the realm of place recognition and researchers interested in long-term robot localization. We pledge to maintain an up-to-date project on our website https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10561v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的位置识别（LPR）在自动驾驶中发挥着关键作用，它有助于同时定位和映射（SLAM）系统减少累积误差，实现可靠的定位。然而，现有的综述主要集中在视觉位置识别（VPR）方法上。尽管近年来LPR取得了显著进步，但据我们所知，仍有专门针对该领域的系统综述。本文通过对使用激光雷达传感器的位置识别方法进行全面综述来弥补这一差距，从而促进和鼓励进一步的研究。我们首先深入研究地点识别的问题公式，探索现有的挑战，描述与以往调查的关系。随后，我们对相关研究进行了深入回顾，提供了详细的分类、优势和劣势以及架构。最后，我们总结了现有的数据集、常用的评估指标，以及在公共数据集上各种方法的综合评估结果。本文可以为进入位置识别领域的新手和对长期机器人定位感兴趣的研究人员提供宝贵的指导。我们保证在我们的网站上保持最新的项目https://github.com/ShiPC-AI/LPR-Survey.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10561v1" target="_blank">2306.10561v1</a>
                              </td>
                              <td>LiDAR-Based Place Recognition For Autonomous Driving: A Survey</td>
                              <td>Pengcheng Shi</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10561v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10561v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10463v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10463v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10463v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10463v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous exploration to build a map of an unknown environment is a fundamental robotics problem. However, the quality of the map directly influences the quality of subsequent robot operation. Instability in a simultaneous localization and mapping (SLAM) system can lead to poorquality maps and subsequent navigation failures during or after exploration. This becomes particularly noticeable in consumer robotics, where compute budget and limited field-of-view are very common. In this work, we propose (i) the concept of lighthouses: panoramic views with high visual information content that can be used to maintain the stability of the map locally in their neighborhoods and (ii) the final stabilization strategy for global pose graph stabilization. We call our novel exploration strategy SLAM-aware exploration (SAE) and evaluate its performance on real-world home environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10463v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自主探索构建未知环境的地图是机器人学的一个基本问题。然而，地图的质量直接影响后续机器人操作的质量。同时定位和测绘（SLAM）系统的不稳定性可能导致勘探期间或之后的低质量地图和随后的导航故障。这在消费机器人中变得尤为明显，因为计算预算和有限的视野非常常见。在这项工作中，我们提出了（i）灯塔的概念：具有高视觉信息含量的全景视图，可用于在其邻域内局部保持地图的稳定性；以及（ii）全局姿态图稳定的最终稳定策略。我们将我们的新探索策略称为SLAM感知探索（SAE），并评估其在真实家庭环境中的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10463v1" target="_blank">2306.10463v1</a>
                              </td>
                              <td>Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute, Narrow-FoV Robots</td>
                              <td>Mohit Deshpande</td>
                              <td>2023-06-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10463v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10463v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01173v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01173v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01173v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01173v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford and Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01173v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了BAMF-SLAM，这是一种新颖的多鱼眼视觉惯性SLAM系统，它利用束平差（BA）和递归场变换（RFT）在具有挑战性的场景中实现精确和鲁棒的状态估计。首先，我们的系统直接对原始鱼眼图像进行操作，使我们能够充分利用鱼眼相机的宽视场（FoV）。其次，为了克服低纹理的挑战，我们通过统一的因子图探索了多相机输入和互补惯性测量的紧密耦合集成，并联合优化姿态和密集深度图。第三，为了全局一致性，鱼眼相机的宽FoV允许系统找到更多潜在的环路闭合，并且在RFT的宽收敛池的支持下，我们的系统可以执行非常宽的基线环路闭合，几乎没有重叠。此外，我们引入了一种半姿态图BA方法来避免昂贵的全全局BA。通过将相对姿态因子与闭环因子相结合，可以在保持高精度的同时，以适度的内存占用有效地调整全局状态。在TUM-VI、Hilti Oxford和Newer College数据集上的评估表明，与先前的工作相比，所提出的系统具有优越的性能。在2022年希尔蒂SLAM挑战赛中，我们的VIO版本获得了第二名。在随后的提交中，我们的完整系统，包括全球BA后端，优于获胜方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01173v2" target="_blank">2306.01173v2</a>
                              </td>
                              <td>BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms</td>
                              <td>Wei Zhang</td>
                              <td>2023-06-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01173v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01173v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08738v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigation of the Challenges of Underwater-Visual-Monocular-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08738v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08738v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08738v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comprehensive investigation of the challenges of Monocular Visual Simultaneous Localization and Mapping (vSLAM) methods for underwater robots. While significant progress has been made in state estimation methods that utilize visual data in the past decade, most evaluations have been limited to controlled indoor and urban environments, where impressive performance was demonstrated. However, these techniques have not been extensively tested in extremely challenging conditions, such as underwater scenarios where factors such as water and light conditions, robot path, and depth can greatly impact algorithm performance. Hence, our evaluation is conducted in real-world AUV scenarios as well as laboratory settings which provide precise external reference. A focus is laid on understanding the impact of environmental conditions, such as optical properties of the water and illumination scenarios, on the performance of monocular vSLAM methods. To this end, we first show that all methods perform very well in in-air settings and subsequently show the degradation of their performance in challenging underwater environments. The final goal of this study is to identify techniques that can improve accuracy and robustness of SLAM methods in such conditions. To achieve this goal, we investigate the potential of image enhancement techniques to improve the quality of input images used by the SLAM methods, specifically in low visibility and extreme lighting scenarios in scattering media. We present a first evaluation on calibration maneuvers and simple image restoration techniques to determine their ability to enable or enhance the performance of monocular SLAM methods in underwater environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08738v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对水下机器人的单目视觉同时定位和映射（vSLAM）方法的挑战进行了全面的研究。尽管在过去十年中，利用视觉数据的状态估计方法取得了重大进展，但大多数评估仅限于受控的室内和城市环境，在这些环境中表现出了令人印象深刻的性能。然而，这些技术尚未在极具挑战性的条件下进行广泛测试，例如水下场景，其中水和光线条件、机器人路径和深度等因素会极大地影响算法性能。因此，我们的评估是在真实世界的AUV场景以及提供精确外部参考的实验室环境中进行的。重点是了解环境条件对单目vSLAM方法性能的影响，如水和照明场景的光学特性。为此，我们首先证明了所有方法在空中环境中都表现得很好，随后证明了它们在具有挑战性的水下环境中的性能下降。本研究的最终目标是确定在这种情况下可以提高SLAM方法的准确性和稳健性的技术。为了实现这一目标，我们研究了图像增强技术提高SLAM方法使用的输入图像质量的潜力，特别是在散射介质中的低能见度和极端照明场景中。我们首次评估了校准操作和简单的图像恢复技术，以确定它们在水下环境中实现或增强单眼SLAM方法性能的能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08738v1" target="_blank">2306.08738v1</a>
                              </td>
                              <td>Investigation of the Challenges of Underwater-Visual-Monocular-SLAM</td>
                              <td>Michele Grimaldi</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08738v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08738v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08531v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FROG: A new people detection dataset for knee-high 2D range finders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08531v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08531v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08531v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Mobile robots require knowledge of the environment, especially of humans located in its vicinity. While the most common approaches for detecting humans involve computer vision, an often overlooked hardware feature of robots for people detection are their 2D range finders. These were originally intended for obstacle avoidance and mapping/SLAM tasks. In most robots, they are conveniently located at a height approximately between the ankle and the knee, so they can be used for detecting people too, and with a larger field of view and depth resolution compared to cameras.   In this paper, we present a new dataset for people detection using knee-high 2D range finders called FROG. This dataset has greater laser resolution, scanning frequency, and more complete annotation data compared to existing datasets such as DROW. Particularly, the FROG dataset contains annotations for 100% of its laser scans (unlike DROW which only annotates 5%), 17x more annotated scans, 100x more people annotations, and over twice the distance traveled by the robot. We propose a benchmark based on the FROG dataset, and analyze a collection of state-of-the-art people detectors based on 2D range finder data.   We also propose and evaluate a new end-to-end deep learning approach for people detection. Our solution works with the raw sensor data directly (not needing hand-crafted input data features), thus avoiding CPU preprocessing and releasing the developer of understanding specific domain heuristics. Experimental results show how the proposed people detector attains results comparable to the state of the art, while an optimized implementation for ROS can operate at more than 500 Hz.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08531v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>移动机器人需要了解环境，尤其是附近的人类。虽然最常见的检测人类的方法涉及计算机视觉，但用于检测人类的机器人的一个经常被忽视的硬件功能是它们的2D测距仪。这些最初用于避障和测绘/SLAM任务。在大多数机器人中，它们方便地位于脚踝和膝盖之间的高度，因此它们也可以用于检测人，并且与相机相比具有更大的视场和深度分辨率。在本文中，我们提出了一个新的数据集，用于使用膝盖高的二维测距仪进行人员检测，称为FROG。与DROW等现有数据集相比，该数据集具有更高的激光分辨率、扫描频率和更完整的注释数据。特别是，FROG数据集包含100%激光扫描的注释（与仅注释5%的DROW不同）、17倍多的注释扫描、100倍多的人注释，以及超过机器人行进距离两倍的距离。我们提出了一个基于FROG数据集的基准，并基于2D测距仪数据分析了一组最先进的人体探测器。我们还提出并评估了一种新的用于人员检测的端到端深度学习方法。我们的解决方案直接处理原始传感器数据（不需要手工制作的输入数据功能），从而避免了CPU预处理，并释放了理解特定领域启发式的开发人员。实验结果表明，所提出的人员检测器如何获得与现有技术相当的结果，而ROS的优化实现可以在超过500Hz的频率下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08531v1" target="_blank">2306.08531v1</a>
                              </td>
                              <td>FROG: A new people detection dataset for knee-high 2D range finders</td>
                              <td>Fernando Amodeo</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08531v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08531v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_08522v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Challenges of Indoor SLAM: A multi-modal multi-floor dataset for SLAM evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_08522v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_08522v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_08522v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robustness in Simultaneous Localization and Mapping (SLAM) remains one of the key challenges for the real-world deployment of autonomous systems. SLAM research has seen significant progress in the last two and a half decades, yet many state-of-the-art (SOTA) algorithms still struggle to perform reliably in real-world environments. There is a general consensus in the research community that we need challenging real-world scenarios which bring out different failure modes in sensing modalities. In this paper, we present a novel multi-modal indoor SLAM dataset covering challenging common scenarios that a robot will encounter and should be robust to. Our data was collected with a mobile robotics platform across multiple floors at Northeastern University's ISEC building. Such a multi-floor sequence is typical of commercial office spaces characterized by symmetry across floors and, thus, is prone to perceptual aliasing due to similar floor layouts. The sensor suite comprises seven global shutter cameras, a high-grade MEMS inertial measurement unit (IMU), a ZED stereo camera, and a 128-channel high-resolution lidar. Along with the dataset, we benchmark several SLAM algorithms and highlight the problems faced during the runs, such as perceptual aliasing, visual degradation, and trajectory drift. The benchmarking results indicate that parts of the dataset work well with some algorithms, while other data sections are challenging for even the best SOTA algorithms. The dataset is available at https://github.com/neufieldrobotics/NUFR-M3F.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_08522v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）的鲁棒性仍然是自主系统在现实世界中部署的关键挑战之一。在过去的二十五年里，SLAM研究取得了重大进展，但许多最先进的（SOTA）算法仍难以在现实世界环境中可靠运行。研究界普遍认为，我们需要具有挑战性的现实世界场景，在传感模式中产生不同的故障模式。在本文中，我们提出了一个新的多模态室内SLAM数据集，涵盖了机器人将遇到的具有挑战性的常见场景，并且应该具有鲁棒性。我们的数据是通过东北大学ISEC大楼多个楼层的移动机器人平台收集的。这种多层序列是典型的商业办公空间，其特征是楼层之间的对称性，因此，由于类似的楼层布局，容易出现感知混叠。传感器套件包括七个全球快门相机、一个高级MEMS惯性测量单元（IMU）、一个ZED立体相机和一个128通道高分辨率激光雷达。与数据集一起，我们对几种SLAM算法进行了基准测试，并强调了运行过程中面临的问题，如感知混叠、视觉退化和轨迹漂移。基准测试结果表明，部分数据集与某些算法配合良好，而其他数据部分即使是最好的SOTA算法也具有挑战性。数据集位于https://github.com/neufieldrobotics/NUFR-M3F.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.08522v1" target="_blank">2306.08522v1</a>
                              </td>
                              <td>Challenges of Indoor SLAM: A multi-modal multi-floor dataset for SLAM evaluation</td>
                              <td>Pushyami Kaveti</td>
                              <td>2023-06-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_08522v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.08522v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07363v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">H-SLAM: Hybrid Direct-Indirect Visual SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07363v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07363v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07363v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent success of hybrid methods in monocular odometry has led to many attempts to generalize the performance gains to hybrid monocular SLAM. However, most attempts fall short in several respects, with the most prominent issue being the need for two different map representations (local and global maps), with each requiring different, computationally expensive, and often redundant processes to maintain. Moreover, these maps tend to drift with respect to each other, resulting in contradicting pose and scene estimates, and leading to catastrophic failure. In this paper, we propose a novel approach that makes use of descriptor sharing to generate a single inverse depth scene representation. This representation can be used locally, queried globally to perform loop closure, and has the ability to re-activate previously observed map points after redundant points are marginalized from the local map, eliminating the need for separate and redundant map maintenance processes. The maps generated by our method exhibit no drift between each other, and can be computed at a fraction of the computational cost and memory footprint required by other monocular SLAM systems. Despite the reduced resource requirements, the proposed approach maintains its robustness and accuracy, delivering performance comparable to state-of-the-art SLAM methods (e.g., LDSO, ORB-SLAM3) on the majority of sequences from well-known datasets like EuRoC, KITTI, and TUM VI. The source code is available at: https://github.com/AUBVRL/fslam_ros_docker.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07363v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近混合方法在单目里程计中的成功导致了许多将性能增益推广到混合单目SLAM的尝试。然而，大多数尝试在几个方面都达不到要求，最突出的问题是需要两种不同的地图表示（局部地图和全局地图），每种地图表示都需要不同的、计算成本高昂且经常冗余的过程来维护。此外，这些地图往往会相互漂移，导致姿势和场景估计相互矛盾，并导致灾难性的失败。在本文中，我们提出了一种新的方法，该方法利用描述符共享来生成单个反向深度场景表示。这种表示可以在本地使用，全局查询以执行循环闭合，并且在冗余点从本地地图边缘化后，能够重新激活先前观察到的地图点，从而消除了对单独和冗余地图维护过程的需要。由我们的方法生成的映射彼此之间没有漂移，并且可以以其他单目SLAM系统所需的计算成本和内存占用的一小部分来计算。尽管减少了资源需求，但所提出的方法保持了其稳健性和准确性，在EuRoC、KITTI和TUM VI等知名数据集的大多数序列上提供了与最先进的SLAM方法（例如LDSO、ORB-SLAM3）相当的性能。源代码位于：https://github.com/AUBVRL/fslam_ros_docker.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07363v1" target="_blank">2306.07363v1</a>
                              </td>
                              <td>H-SLAM: Hybrid Direct-Indirect Visual SLAM</td>
                              <td>Georges Younes</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07363v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07363v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06850v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Volume-DROID: A Real-Time Implementation of Volumetric Mapping with DROID-SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06850v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06850v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06850v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Volume-DROID, a novel approach for Simultaneous Localization and Mapping (SLAM) that integrates Volumetric Mapping and Differentiable Recurrent Optimization-Inspired Design (DROID). Volume-DROID takes camera images (monocular or stereo) or frames from a video as input and combines DROID-SLAM, point cloud registration, an off-the-shelf semantic segmentation network, and Convolutional Bayesian Kernel Inference (ConvBKI) to generate a 3D semantic map of the environment and provide accurate localization for the robot. The key innovation of our method is the real-time fusion of DROID-SLAM and Convolutional Bayesian Kernel Inference (ConvBKI), achieved through the introduction of point cloud generation from RGB-Depth frames and optimized camera poses. This integration, engineered to enable efficient and timely processing, minimizes lag and ensures effective performance of the system. Our approach facilitates functional real-time online semantic mapping with just camera images or stereo video input. Our paper offers an open-source Python implementation of the algorithm, available at https://github.com/peterstratton/Volume-DROID.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06850v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了体积DROID，这是一种同时定位和映射（SLAM）的新方法，它集成了体积映射和可微递归优化启发设计（DROID）。Volume DROID从视频中获取相机图像（单眼或立体）或帧作为输入，并结合DROID-SLAM、点云配准、现成的语义分割网络和卷积贝叶斯核推断（ConvBKI）来生成环境的3D语义图，并为机器人提供准确的定位。我们方法的关键创新是DROID-SLAM和卷积贝叶斯核推断（ConvBKI）的实时融合，通过引入RGB深度帧的点云生成和优化的相机姿态来实现。这种集成旨在实现高效和及时的处理，最大限度地减少了延迟，并确保了系统的有效性能。我们的方法只需相机图像或立体视频输入，即可实现功能实时在线语义映射。我们的论文提供了该算法的开源Python实现，可在https://github.com/peterstratton/Volume-DROID.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06850v1" target="_blank">2306.06850v1</a>
                              </td>
                              <td>Volume-DROID: A Real-Time Implementation of Volumetric Mapping with DROID-SLAM</td>
                              <td>Peter Stratton</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06850v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06850v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02395v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NICE-SLAM with Adaptive Feature Grids</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02395v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02395v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02395v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM is a dense visual SLAM system that combines the advantages of neural implicit representations and hierarchical grid-based scene representation. However, the hierarchical grid features are densely stored, leading to memory explosion problems when adapting the framework to large scenes. In our project, we present sparse NICE-SLAM, a sparse SLAM system incorporating the idea of Voxel Hashing into NICE-SLAM framework. Instead of initializing feature grids in the whole space, voxel features near the surface are adaptively added and optimized. Experiments demonstrated that compared to NICE-SLAM algorithm, our approach takes much less memory and achieves comparable reconstruction quality on the same datasets. Our implementation is available at https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02395v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>NICE-SLAM是一个密集的视觉SLAM系统，它结合了神经隐式表示和基于层次网格的场景表示的优点。然而，分层网格特征存储密集，导致在将框架适应大型场景时出现内存爆炸问题。在我们的项目中，我们提出了稀疏NICE-SLAM，这是一个稀疏SLAM系统，将体素哈希的思想结合到NICE-SLM框架中。自适应地添加和优化曲面附近的体素特征，而不是初始化整个空间中的特征网格。实验表明，与NICE-SLAM算法相比，我们的方法占用的内存要少得多，并且在相同的数据集上实现了相当的重建质量。我们的实施可在https://github.com/zhangganlin/NICE-SLAM-with-Adaptive-Feature-Grids.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02395v2" target="_blank">2306.02395v2</a>
                              </td>
                              <td>NICE-SLAM with Adaptive Feature Grids</td>
                              <td>Ganlin Zhang</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02395v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02395v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_03806v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLaM: Student-Label Mixing for Distillation with Unlabeled Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_03806v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_03806v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_03806v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge distillation with unlabeled examples is a powerful training paradigm for generating compact and lightweight student models in applications where the amount of labeled data is limited but one has access to a large pool of unlabeled data. In this setting, a large teacher model generates ``soft'' pseudo-labels for the unlabeled dataset which are then used for training the student model. Despite its success in a wide variety of applications, a shortcoming of this approach is that the teacher's pseudo-labels are often noisy, leading to impaired student performance. In this paper, we present a principled method for knowledge distillation with unlabeled examples that we call Student-Label Mixing (SLaM) and we show that it consistently improves over prior approaches by evaluating it on several standard benchmarks. Finally, we show that SLaM comes with theoretical guarantees; along the way we give an algorithm improving the best-known sample complexity for learning halfspaces with margin under random classification noise, and provide the first convergence analysis for so-called ``forward loss-adjustment" methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_03806v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在标记数据量有限但可以访问大量未标记数据的应用中，使用未标记示例进行知识提取是生成紧凑和轻量级学生模型的强大训练范式。在这种设置中，大型教师模型为未标记的数据集生成“软”伪标签，然后用于训练学生模型。尽管这种方法在各种应用中都取得了成功，但它的一个缺点是，教师的伪标签往往很嘈杂，导致学生的表现受损。在本文中，我们提出了一种带有未标记示例的知识提取的原则性方法，我们称之为学生标签混合（SLaM），并通过在几个标准基准上对其进行评估，表明它比以前的方法持续改进。最后，我们证明了SLaM是有理论保证的；在此基础上，我们给出了一种改进随机分类噪声下带裕度半空间学习的已知样本复杂度的算法，并首次对所谓的前向损失调整方法进行了收敛性分析。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.03806v2" target="_blank">2302.03806v2</a>
                              </td>
                              <td>SLaM: Student-Label Mixing for Distillation with Unlabeled Examples</td>
                              <td>Vasilis Kontonis</td>
                              <td>2023-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_03806v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.03806v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04570v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04570v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04570v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In many robotics problems, there is a significant gain in collaborative information sharing between multiple robots, for exploration, search and rescue, tracking multiple targets, or mapping large environments. One of the key implicit assumptions when solving cooperative multi-robot problems is that all robots use the same (homogeneous) underlying algorithm. However, in practice, we want to allow collaboration between robots possessing different capabilities and that therefore must rely on heterogeneous algorithms. We present a system architecture and the supporting theory, to enable collaboration in a decentralized network of robots, where each robot relies on different estimation algorithms. To develop our approach, we focus on multi-robot simultaneous localization and mapping (SLAM) with multi-target tracking. Our theoretical framework builds on our idea of exploiting the conditional independence structure inherent to many robotics applications to separate between each robot's local inference (estimation) tasks and fuse only relevant parts of their non-equal, but overlapping probability density function (pdfs). We present a new decentralized graph-based approach to the multi-robot SLAM and tracking problem. We leverage factor graphs to split between different parts of the problem for efficient data sharing between robots in the network while enabling robots to use different local sparse landmark/dense/metric-semantic SLAM algorithms.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04570v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在许多机器人问题中，多个机器人之间的协作信息共享有很大的好处，用于探索、搜索和救援、跟踪多个目标或绘制大型环境地图。解决多机器人协作问题时的一个关键隐含假设是，所有机器人都使用相同（同质）的底层算法。然而，在实践中，我们希望允许具有不同能力的机器人之间进行协作，因此必须依赖于异构算法。我们提出了一种系统架构和支持理论，以实现分散机器人网络中的协作，其中每个机器人依赖于不同的估计算法。为了开发我们的方法，我们专注于具有多目标跟踪的多机器人同时定位和映射（SLAM）。我们的理论框架建立在我们的想法之上，即利用许多机器人应用程序固有的条件独立性结构，在每个机器人的局部推理（估计）任务之间进行分离，并仅融合其不相等但重叠的概率密度函数（pdfs）的相关部分。我们提出了一种新的基于分散图的方法来解决多机器人SLAM和跟踪问题。我们利用因子图在问题的不同部分之间进行划分，以实现网络中机器人之间的有效数据共享，同时使机器人能够使用不同的局部稀疏地标/密集/度量语义SLAM算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04570v1" target="_blank">2306.04570v1</a>
                              </td>
                              <td>Towards Decentralized Heterogeneous Multi-Robot SLAM and Target Tracking</td>
                              <td>Ofer Dagan</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04570v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04570v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03953v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03953v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03953v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localization and mapping (SLAM) is the task of building a map representation of an unknown environment while it at the same time is used for positioning. A probabilistic interpretation of the SLAM task allows for incorporating prior knowledge and for operation under uncertainty. Contrary to the common practice of computing point estimates of the system states, we capture the full posterior density through approximate Bayesian inference. This dynamic learning task falls under state estimation, where the state-of-the-art is in sequential Monte Carlo methods that tackle the forward filtering problem. In this paper, we introduce a framework for probabilistic SLAM using particle smoothing that does not only incorporate observed data in current state estimates, but it also back-tracks the updated knowledge to correct for past drift and ambiguities in both the map and in the states. Our solution can efficiently handle both dense and sparse map representations by Rao-Blackwellization of conditionally linear and conditionally linearized models. We show through simulations and real-world experiments how the principles apply to radio (BLE/Wi-Fi), magnetic field, and visual SLAM. The proposed solution is general, efficient, and works well under confounding noise.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03953v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）是在用于定位的同时构建未知环境的地图表示的任务。SLAM任务的概率解释允许结合先验知识并在不确定性下进行操作。与计算系统状态的点估计的常见做法相反，我们通过近似贝叶斯推理来获取完整的后验密度。这种动态学习任务属于状态估计，其中最先进的是解决前向滤波问题的顺序蒙特卡罗方法。在本文中，我们介绍了一种使用粒子平滑的概率SLAM框架，该框架不仅将观测数据纳入当前状态估计，而且还对更新后的知识进行回溯，以纠正地图和状态中过去的漂移和模糊性。我们的解决方案可以通过条件线性化和条件线性化模型的Rao-Blackwell化有效地处理密集和稀疏映射表示。我们通过模拟和真实世界的实验展示了这些原理如何应用于无线电（BLE/Wi-Fi）、磁场和视觉SLAM。所提出的解决方案是通用的、有效的，并且在混杂噪声下工作良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03953v1" target="_blank">2306.03953v1</a>
                              </td>
                              <td>Rao-Blackwellized Particle Smoothing for Simultaneous Localization and Mapping</td>
                              <td>Manon Kok</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03953v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03953v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2102_07448v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2102_07448v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2102_07448v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Surround View fisheye cameras are commonly deployed in automated driving for 360\deg{} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2102_07448v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>环绕视图鱼眼摄像头通常部署在自动驾驶中，用于车辆周围的360度近场传感。这项工作在未修正的鱼眼图像上提出了一个多任务视觉感知网络，使车辆能够感知周围环境。它由自动驾驶系统所需的六项主要任务组成：深度估计、视觉里程计、语义分割、运动分割、物体检测和镜头污染检测。我们证明，联合训练的模型比各自的单任务版本表现得更好。我们的多任务模型具有一个共享编码器，提供了显著的计算优势，并具有任务相互支持的协同解码器。我们提出了一种新的基于相机几何的自适应机制来对鱼眼失真模型进行训练和推理编码。这对于在WoodScape数据集上进行训练至关重要，该数据集由安装在三辆不同汽车上的12台不同相机收集的来自世界不同地区的数据组成，具有不同的本质和视角。考虑到边界框不是失真鱼眼图像的良好表示，我们还将对象检测扩展到使用具有非均匀采样顶点的多边形。此外，我们还在标准汽车数据集上评估了我们的模型，即KITTI和Cityscapes。我们在深度估计和姿态估计任务的KITTI上获得了最先进的结果，并在其他任务上获得了有竞争力的性能。我们对各种架构选择和任务加权方法进行了广泛的消融研究。上的短视频https://youtu.be/xbSjZ5OfPes提供了定性结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2102.07448v3" target="_blank">2102.07448v3</a>
                              </td>
                              <td>OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2021-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2102_07448v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2102.07448v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2007_06676v4_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2007_06676v4_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2007_06676v4_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190$^\circ$ horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet. Qualitative results on a distorted test scene video sequence indicate excellent performance https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2007_06676v4_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在经典的计算机视觉中，校正是多视图深度估计的一个组成部分。它通常包括对极校正和透镜畸变校正。该过程大大简化了深度估计，因此已被CNN方法所采用。然而，校正有几个副作用，包括视场（FOV）减小、重采样失真和对校准误差的敏感性。在严重失真的情况下，效果尤其明显（例如，广角鱼眼相机）。在本文中，我们提出了一种通用的尺度感知自监督管道，用于从未修正的单目视频中估计深度、欧氏距离和视觉里程。我们在未校正的KITTI数据集上证明了类似的精度水平，桶形失真与校正的KITT数据集相当。直觉是，校正步骤可以隐含地吸收在CNN模型中，CNN模型在不增加复杂性的情况下学习失真模型。我们的方法不会减少视野，并避免了推理时校正的计算成本。为了进一步说明所提出的框架的普遍适用性，我们将其应用于水平视场为190$^\circ$的广角鱼眼相机。训练框架UnRectDepthNet以相机失真模型为自变量，并相应地调整投影和非投影函数。在KITTI校正的数据集上进一步评估了所提出的算法，我们获得了最先进的结果，改进了我们之前的工作FisheyeDistanceNet。失真的测试场景视频序列的定性结果表明性能优异https://youtu.be/K6pbx3bU4Ss.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2007.06676v4" target="_blank">2007.06676v4</a>
                              </td>
                              <td>UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models</td>
                              <td>Varun Ravi Kumar</td>
                              <td>2020-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2007_06676v4_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2007.06676v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03660v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PQM: A Point Quality Evaluation Metric for Dense Maps</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03660v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03660v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LiDAR-based mapping/reconstruction are important for various applications, but evaluating the quality of the dense maps they produce is challenging. The current methods have limitations, including the inability to capture completeness, structural information, and local variations in error. In this paper, we propose a novel point quality evaluation metric (PQM) that consists of four sub-metrics to provide a more comprehensive evaluation of point cloud quality. The completeness sub-metric evaluates the proportion of missing data, the artifact score sub-metric recognizes and characterizes artifacts, the accuracy sub-metric measures registration accuracy, and the resolution sub-metric quantifies point cloud density. Through an ablation study using a prototype dataset, we demonstrate the effectiveness of each of the sub-metrics and compare them to popular point cloud distance measures. Using three LiDAR SLAM systems to generate maps, we evaluate their output map quality and demonstrate the metrics robustness to noise and artifacts. Our implementation of PQM, datasets and detailed documentation on how to integrate with your custom dense mapping pipeline can be found at github.com/droneslab/pqm</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03660v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于激光雷达的地图绘制/重建对于各种应用都很重要，但评估它们生成的密集地图的质量是一项挑战。目前的方法有局限性，包括无法捕获完整性、结构信息和错误的局部变化。在本文中，我们提出了一种新的点质量评估度量（PQM），该度量由四个子度量组成，以提供对点云质量的更全面的评估。完整性子度量评估缺失数据的比例，伪影得分子度量识别和表征伪影，准确性子度量测量配准准确性，分辨率子度量量化点云密度。通过使用原型数据集进行消融研究，我们证明了每个子指标的有效性，并将其与流行的点云距离测量进行了比较。使用三个激光雷达SLAM系统生成地图，我们评估了它们的输出地图质量，并证明了它们对噪声和伪影的鲁棒性。我们的PQM实施、数据集以及关于如何与您的自定义密集映射管道集成的详细文档，请访问github.com/droneslab/PQM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03660v1" target="_blank">2306.03660v1</a>
                              </td>
                              <td>PQM: A Point Quality Evaluation Metric for Dense Maps</td>
                              <td>Yash Turkar</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03660v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03660v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01891v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01891v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01891v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a robust approach for a visual parallel tracking and mapping (PTAM) system that excels in challenging environments. Our proposed method combines the strengths of heterogeneous multi-modal visual sensors, including stereo event-based and frame-based sensors, in a unified reference frame through a novel spatio-temporal synchronization of stereo visual frames and stereo event streams. We employ deep learning-based feature extraction and description for estimation to enhance robustness further. We also introduce an end-to-end parallel tracking and mapping optimization layer complemented by a simple loop-closure algorithm for efficient SLAM behavior. Through comprehensive experiments on both small-scale and large-scale real-world sequences of VECtor and TUM-VIE benchmarks, our proposed method (DH-PTAM) demonstrates superior performance compared to state-of-the-art methods in terms of robustness and accuracy in adverse conditions. Our implementation's research-based Python API is publicly available on GitHub for further research and development: https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01891v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种用于视觉并行跟踪和映射（PTAM）系统的鲁棒方法，该系统在具有挑战性的环境中表现出色。我们提出的方法通过立体视觉帧和立体事件流的新颖时空同步，在统一的参考系中结合了异构多模态视觉传感器的优势，包括基于立体事件和基于帧的传感器。我们采用基于深度学习的特征提取和描述进行估计，以进一步增强鲁棒性。我们还引入了一个端到端并行跟踪和映射优化层，并辅以一个简单的闭环算法，以实现高效的SLAM行为。通过对VECtor和TUM-VIE基准的小规模和大规模真实世界序列的综合实验，我们提出的方法（DH-PTAM）在不利条件下的稳健性和准确性方面优于最先进的方法。我们的实现基于研究的Python API在GitHub上公开提供，用于进一步的研究和开发：https://github.com/AbanobSoliman/DH-PTAM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01891v1" target="_blank">2306.01891v1</a>
                              </td>
                              <td>DH-PTAM: A Deep Hybrid Stereo Events-Frames Parallel Tracking And Mapping System</td>
                              <td>Abanob Soliman</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01891v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01891v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_09981v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lazy Visual Localization via Motion Averaging</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09981v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09981v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual (re)localization is critical for various applications in computer vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images. Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models. On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits. It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc. In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database. The key to achieving this owes to a tailored motion averaging over database-query pairs. Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods. Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09981v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉（再）定位对于计算机视觉和机器人的各种应用至关重要。其目标是基于一组摆姿势的数据库图像，估计每个查询图像的6个自由度（DoF）相机姿势。目前，所有领先的解决方案都是基于结构的，要么从数据库中显式地构建具有运动结构的3D度量图，要么用场景坐标回归模型隐式地编码3D信息。相反，在不重建3D场景的情况下进行视觉定位提供了明显的好处。它通过减少数据库预处理时间、释放存储需求、不受不完美重建的影响等方式使部署更加方便。在本技术报告中，我们证明了在不从数据库重建场景的情况下实现高定位精度是可能的。实现这一点的关键在于对数据库查询对进行定制的运动平均。实验表明，我们的视觉定位方案LazyLoc与最先进的基于结构的方法相比具有相当的性能。此外，我们还展示了LazyLoc的多功能性，它可以很容易地扩展到处理复杂的配置，如多查询协同定位和相机装备。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09981v1" target="_blank">2307.09981v1</a>
                              </td>
                              <td>Lazy Visual Localization via Motion Averaging</td>
                              <td>Siyan Dong</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09981v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09981v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07524v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reducing Causality to Functions with Structural Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07524v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07524v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The precise definition of causality is currently an open problem in philosophy and statistics. We believe causality should be defined as functions (in mathematics) that map causes to effects. We propose a reductive definition of causality based on Structural Functional Model (SFM). Using delta compression and contrastive forward inference, SFM can produce causal utterances like "X causes Y" and "X is the cause of Y" that match our intuitions. We compile a dataset of causal scenarios and use SFM in all of them. SFM is compatible with but not reducible to probability theory. We also compare SFM with other theories of causation and apply SFM to downstream problems like free will, causal explanation, and mental causation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07524v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果关系的精确定义目前是哲学和统计学中一个悬而未决的问题。我们认为因果关系应该被定义为（在数学中）将原因映射到结果的函数。基于结构函数模型，我们提出了因果关系的简化定义。使用delta压缩和对比前向推理，SFM可以产生与我们的直觉相匹配的因果话语，如“X导致Y”和“X是Y的原因”。我们汇编了一个因果场景的数据集，并在所有这些场景中使用了SFM。SFM与概率论是相容的，但不能简化为概率论。我们还将SFM与其他因果关系理论进行了比较，并将SFM应用于自由意志、因果解释和精神因果关系等下游问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07524v1" target="_blank">2307.07524v1</a>
                              </td>
                              <td>Reducing Causality to Functions with Structural Models</td>
                              <td>Tianyi Miao</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07524v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07524v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04520v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04520v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04520v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04520v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SfM（Structure from Motion）已被广泛用于无人机（UAV）的图像定向。其效率直接受到特征匹配的影响。尽管图像检索已被广泛用于匹配对选择，但由于大量的局部特征和所使用的码本的大尺寸，消耗了高计算成本。因此，本文提出了一种高效的匹配对检索方法，并实现了一个用于并行SfM重建的集成工作流。首先，考虑到无人机图像和局部特征的冗余性，在线训练单个码本，避免了训练码本与其他数据集的模糊性。其次，通过使用训练后的码本进行VLAD（Vector of Locally aggregated Descriptors）聚合，将每个图像的局部特征聚合为单个高维全局描述符，显著减少了图像索引中特征的数量和最近邻搜索的负担。第三，通过基于HNSW（分层导航小世界）的图结构对全局描述符进行索引，用于最近邻居搜索。然后通过使用自适应阈值选择策略来检索匹配对，并用于创建用于基于分治的并行SfM重建的视图图。最后，使用三个大型无人机数据集验证了所提出的解决方案的性能。测试结果表明，所提出的解决方案以36到108的加速比加速了匹配对检索，并在相对和绝对方向上以具有竞争力的精度提高了SfM重建的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04520v1" target="_blank">2307.04520v1</a>
                              </td>
                              <td>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</td>
                              <td>San Jiang</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04520v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04520v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03404v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RGB-D Mapping and Tracking in a Plenoxel Radiance Field</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03404v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03404v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Building on the success of Neural Radiance Fields (NeRFs), recent years have seen significant advances in the domain of novel view synthesis. These models capture the scene's volumetric radiance field, creating highly convincing dense photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this technical report, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both the mapping and tracking tasks while also being faster than competing neural network-based approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03404v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在神经辐射场（NeRFs）成功的基础上，近年来在新视图合成领域取得了重大进展。这些模型捕捉了场景的体积辐射场，通过使用简单、可微分的渲染方程创建了令人信服的密集真实感模型。尽管这些算法很受欢迎，但RGB传感器固有的视觉数据存在严重的模糊性，这意味着尽管通过视图合成生成的图像在视觉上看起来非常可信，但底层的3D模型往往是错误的。这大大限制了这些模型在机器人和扩展现实（XR）等实际应用中的有用性，否则，精确的密集3D重建将具有重要价值。在本技术报告中，我们介绍了视图合成模型和三维重建模型之间的重要区别。我们还评论了为什么深度传感器对于使用当前新型视图合成方法的范式在一般面向外的场景中建模精确的几何体至关重要。聚焦于运动任务中的结构，我们通过扩展Plenoxel辐射场模型来实际证明这一需求：在没有神经网络的情况下，基于RGB-D数据，提出了一种用于辐射场密集映射和跟踪的解析微分方法。我们的方法在映射和跟踪任务中都取得了最先进的结果，同时也比竞争对手的基于神经网络的方法更快。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03404v1" target="_blank">2307.03404v1</a>
                              </td>
                              <td>RGB-D Mapping and Tracking in a Plenoxel Radiance Field</td>
                              <td>Andreas L. Teigen</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03404v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03404v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01817v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Trajectory Forecasting with Explainable Behavioral Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01817v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01817v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01817v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类轨迹预测有助于理解和预测人类行为，实现从社交机器人到自动驾驶汽车的应用，因此受到了大量研究。大多数现有的方法可以分为无模型方法和基于模型的方法。无模型方法提供了优越的预测精度但缺乏可解释性，而基于模型的方法提供了可解释性但不能很好地预测。结合这两种方法，我们提出了一种新的贝叶斯神经随机微分方程模型BNSP-SFM，其中行为SDE模型与贝叶斯神经网络（BNNs）相结合。虽然神经网络提供了卓越的预测能力，但SDE提供了强大的可解释性，在行为和观察方面具有可量化的不确定性。我们表明，与11种最先进的方法相比，BNSP-SFM在预测精度上提高了50%。BNSP-SFM还可以更好地推广到具有不同环境和人群密度的截然不同的场景（比测试数据高约20倍）。最后，BNSP-SFM可以提供有信心的预测，以更好地解释行为的潜在原因。该代码将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01817v1" target="_blank">2307.01817v1</a>
                              </td>
                              <td>Human Trajectory Forecasting with Explainable Behavioral Uncertainty</td>
                              <td>Jiangbei Yue</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01817v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01817v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_16917v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_16917v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_16917v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard's Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_16917v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在可变形场景中估计相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构从运动技术中假设，除了变形场景部分之外，还观察静态场景部分，以建立锚定参考。然而，这一假设在某些相关应用案例中并不成立，例如内镜。可变形里程计和SLAM管道解决了最具挑战性的探索轨迹场景，但缺乏稳健性和适当的定量评估方法。为了用一个通用的基准来解决这个问题，我们引入了Drunkard的数据集，这是一个具有挑战性的合成数据集，旨在可变形环境中的视觉导航和重建。该数据集是第一个在3D场景中具有地面实况的大型探索相机轨迹集，其中每个表面随着时间的推移都表现出非刚性变形。在逼真的3D建筑中进行模拟可以让我们获得大量的数据和地面实况标签，包括相机姿态、RGB图像和深度、光流和高分辨率和高质量的法线图。我们进一步提出了一种新的可变形里程计方法，称为Drunkard里程计，该方法将光流估计分解为刚体相机运动和非刚体场景变形。为了验证我们的数据，我们的工作包括对几个基线的评估，以及一种新的跟踪误差度量，该度量不需要地面实况数据。数据集和代码：https://davidrecasens.github.io/TheDrunkard'住宅/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.16917v1" target="_blank">2306.16917v1</a>
                              </td>
                              <td>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</td>
                              <td>David Recasens</td>
                              <td>2023-06-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_16917v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.16917v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion比经典的SfM管道和学习的方法有了显著的改进。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v2" target="_blank">2306.15667v2</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15669v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Detector-Free Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15669v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15669v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new structure-from-motion framework to recover accurate camera poses and point clouds from unordered images. Traditional SfM systems typically rely on the successful detection of repeatable keypoints across multiple views as the first step, which is difficult for texture-poor scenes, and poor keypoint detection may break down the whole SfM system. We propose a new detector-free SfM framework to draw benefits from the recent success of detector-free matchers to avoid the early determination of keypoints, while solving the multi-view inconsistency issue of detector-free matchers. Specifically, our framework first reconstructs a coarse SfM model from quantized detector-free matches. Then, it refines the model by a novel iterative refinement pipeline, which iterates between an attention-based multi-view matching module to refine feature tracks and a geometry refinement module to improve the reconstruction accuracy. Experiments demonstrate that the proposed framework outperforms existing detector-based SfM systems on common benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate the capability of our framework to reconstruct texture-poor scenes. Based on this framework, we take $\textit{first place}$ in Image Matching Challenge 2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15669v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们从运动框架中提出了一种新的结构，以从无序图像中恢复准确的相机姿态和点云。传统的SfM系统通常依赖于跨多个视图的可重复关键点的成功检测作为第一步，这对于纹理较差的场景来说是困难的，并且较差的关键点检测可能会破坏整个SfM体系。我们提出了一种新的无检测器SfM框架，以从无检测器匹配器最近的成功中获益，避免早期确定关键点，同时解决无检测器匹配的多视图不一致问题。具体而言，我们的框架首先从量化的无检测器匹配中重建粗略的SfM模型。然后，它通过一种新的迭代精化流水线对模型进行精化，该流水线在基于注意力的多视图匹配模块和几何精化模块之间迭代以精化特征轨迹，从而提高重建精度。实验表明，所提出的框架在通用基准数据集上优于现有的基于检测器的SfM系统。我们还收集了一个纹理较差的SfM数据集，以证明我们的框架重建纹理较差场景的能力。基于这个框架，我们在2023年的图像匹配挑战中获得$\textit｛first place｝$。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15669v1" target="_blank">2306.15669v1</a>
                              </td>
                              <td>Detector-Free Structure from Motion</td>
                              <td>Xingyi He</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15669v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15669v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12770v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Reconstruction of Spherical Images based on Incremental Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12770v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12770v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12770v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维重建在现代摄影测量系统中发挥着越来越重要的作用。传统的卫星或航空遥感平台可以为大规模地形和城市的三维重建提供必要的数据源。即使使用低空无人机，由于相机帧之间频繁的跟踪故障和高昂的数据收集成本，在城市峡谷和室内场景等复杂情况下的3D重建也具有挑战性。最近，球形图像由于能够通过一台相机曝光记录周围环境而被广泛利用。然而，经典的3D重建管道不能用于球面图像。此外，用于球面图像的三维重建的软件包很少。基于球面相机的成像几何，本研究研究了使用球面对应关系的相对方位、使用场景和球面点之间的3D对应关系的绝对方位以及BA（束调整）优化的成本函数的算法。此外，已经使用上述算法针对球面图像提出了增量SfM（运动结构）工作流程。通过使用消费者级和专业球形相机拍摄的三个球形数据集，最终验证了所提出的解决方案。结果表明，所提出的SfM工作流可以成功地实现复杂场景的三维重建，并为开源软件包的实现提供了有用的线索。设计的SfM工作流程的源代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12770v2" target="_blank">2306.12770v2</a>
                              </td>
                              <td>3D Reconstruction of Spherical Images based on Incremental Structure from Motion</td>
                              <td>San Jiang</td>
                              <td>2023-06-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12770v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12770v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09109v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09109v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09109v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose NAVI: a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation. Project page: https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09109v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经重建的最新进展使得能够从随意捕获的图像集合中重建高质量的3D对象。当前的技术主要分析它们在相对简单的图像采集上的进展，其中运动结构（SfM）技术可以提供地面实况（GT）相机姿态。我们注意到，SfM技术在野生图像集合中往往失败，例如具有不同背景和照明的图像搜索结果。为了使从偶然图像捕获中进行3D重建的系统研究取得进展，我们提出了NAVI：一个具有高质量3D扫描的对象的类别不可知图像集合的新数据集，以及提供近乎完美的GT相机参数的每张图像2D-3D对齐。这些2D-3D比对允许我们提取精确的导数注释，例如密集像素对应、深度和分割图。我们展示了NAVI图像集合在不同问题设置中的使用，并表明NAVI能够实现现有数据集无法实现的更彻底的评估。我们相信NAVI有利于三维重建和对应估计的系统研究进展。项目页面：https://navidataset.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09109v1" target="_blank">2306.09109v1</a>
                              </td>
                              <td>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</td>
                              <td>Varun Jampani</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09109v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09109v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code will be released.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻居搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎普遍认为，全局嵌入对于视觉定位中的图像检索至关重要，尽管必须为每个查询图像计算两种特征类型有很大的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k-最近邻的联合解决方案。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。将发布代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v1" target="_blank">2306.09012v1</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06360v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D reconstruction using Structure for Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06360v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06360v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06360v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们正在使用安装在室内移动地板机器人上的一对立体视觉配置的HDR相机对室内空间进行3D重建，该机器人将各种纹理和空间特征捕捉为2D图像，这些数据同时被用作我们算法的反馈，这将使我们能够可视化深度图。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06360v1" target="_blank">2306.06360v1</a>
                              </td>
                              <td>3D reconstruction using Structure for Motion</td>
                              <td>Kshitij Karnawat</td>
                              <td>2023-06-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06360v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06360v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05410v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05410v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05410v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05410v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>阻碍NeRF模型在野外广泛部署的一个关键障碍是它们对精确相机姿势的依赖。因此，人们对扩展NeRF模型以联合优化相机姿态和场景表示越来越感兴趣，这为具有众所周知的故障模式的现成SfM管道提供了一种替代方案。现有的无基线NeRF方法在有限的假设下运行，例如先前的姿态分布或粗略的姿态初始化，这使得它们在一般情况下效果较差。在这项工作中，我们提出了一种新的方法，即LU-NeRF，该方法通过对姿势配置的宽松假设来联合估计相机姿势和神经辐射场。我们的方法以局部到全局的方式运行，首先对数据的局部子集进行优化，称为迷你场景。LU NeRF估计了这项具有挑战性的少镜头任务的局部姿态和几何结构。通过稳健的姿态同步步骤，将迷你场景姿态带入全局参考系，其中可以执行姿态和场景的最终全局优化。我们展示了我们的LU-NeRF流水线在没有对姿势先验进行限制性假设的情况下，在未建模的NeRF上优于先前的尝试。这使我们能够在一般的SE（3）姿势设置中进行操作，与基线不同。我们的结果还表明，我们的模型可以与基于特征的SfM管道互补，因为它在低纹理和低分辨率图像上优于COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05410v1" target="_blank">2306.05410v1</a>
                              </td>
                              <td>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</td>
                              <td>Zezhou Cheng</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05410v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05410v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用钻爆法的隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，而没有考虑动态障碍物。为了最大限度地提高自主性，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法采用了分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v2" target="_blank">2301.08422v2</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01938v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01938v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01938v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection and matching is a fundamental task in many computer vision problems, from shape reconstruction, to structure from motion, to AR/VR applications and robotics. It is a well-studied problem with remarkable successes such as SIFT, and more recent deep learning approaches. While great robustness is exhibited by these techniques with respect to noise, illumination variation, and rigid motion transformations, less attention has been placed on image distortion sensitivity. In this work, we focus on the case when this is caused by the geometry of the cameras used for image acquisition, and consider the keypoint detection and matching problem between the hybrid scenario of a fisheye and a projective image. We build on a state-of-the-art approach and derive a self-supervised procedure that enables training an interest point detector and descriptor network. We also collected two new datasets for additional training and testing in this unexplored scenario, and we demonstrate that current approaches are suboptimal because they are designed to work in traditional projective conditions, while the proposed approach turns out to be the most effective.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01938v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测和匹配是许多计算机视觉问题中的一项基本任务，从形状重建到结构从运动到AR/VR应用和机器人。这是一个研究得很好的问题，取得了显著的成功，如SIFT和最近的深度学习方法。虽然这些技术在噪声、照明变化和刚性运动变换方面表现出了很大的鲁棒性，但对图像失真灵敏度的关注较少。在这项工作中，我们重点关注由用于图像采集的相机的几何形状引起的情况，并考虑鱼眼和投影图像的混合场景之间的关键点检测和匹配问题。我们以最先进的方法为基础，推导出了一种自监督程序，该程序能够训练兴趣点检测器和描述符网络。我们还收集了两个新的数据集，用于在这个未探索的场景中进行额外的训练和测试，我们证明了当前的方法是次优的，因为它们被设计为在传统的投影条件下工作，而所提出的方法被证明是最有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01938v1" target="_blank">2306.01938v1</a>
                              </td>
                              <td>Self-supervised Interest Point Detection and Description for Fisheye and Perspective Images</td>
                              <td>Marcela Mera-Trujillo</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01938v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01938v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_00180v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_00180v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_00180v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_00180v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从姿态图像重建三维神经场已成为自监督表示学习的一种很有前途的方法。阻止将这些3D场景学习器部署在大规模视频数据上的关键挑战是，它们依赖于从结构到运动的精确相机姿态，这在规模上运行成本高得令人望而却步。我们提出了一种在线和单次前向联合重建相机姿态和3D神经场景表示的方法。我们通过可微分渲染将帧到帧的光流提升到3D场景流来估计姿态，保持图像处理主干的局部性和平移等变性。然后通过对场景流场的加权最小二乘拟合来执行SE（3）相机姿态估计。该公式使我们能够通过重新渲染输入视频来联合监督姿态估计和可推广的神经场景表示，从而在真实世界的视频数据集上进行端到端和完全自监督的训练。我们证明了我们的方法在不同的真实世界视频上表现稳健，尤其是在传统上对基于优化的姿态估计技术具有挑战性的序列上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.00180v1" target="_blank">2306.00180v1</a>
                              </td>
                              <td>FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow</td>
                              <td>Cameron Smith</td>
                              <td>2023-05-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_00180v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.00180v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_16342v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_16342v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_16342v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_16342v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征和全局特征对于自动语音识别（ASR）都是必不可少的。最近的许多方法已经证明，简单地结合局部和全局特征可以进一步提高ASR性能。然而，这些方法很少关注局部和全局特征的相互作用，并且它们的串联架构是刚性的，无法反映局部和全局关系。为了解决这些问题，本文提出了用于交互式局部和全局特征融合的InterFormer，以学习ASR的更好表示。具体地说，我们在并行设计中将卷积块与变换器块相结合。此外，我们提出了一个双向特征交互模块（BFIM）和一个选择性融合模块（SFM），分别实现局部和全局特征的交互和融合。在公共ASR数据集上进行的大量实验证明了我们提出的InterFormer的有效性及其优于其他Transformer和Conformer模型的优越性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.16342v2" target="_blank">2305.16342v2</a>
                              </td>
                              <td>InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition</td>
                              <td>Zhi-Hao Lai</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_16342v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.16342v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12036v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SIDAR: Synthetic Image Dataset for Alignment & Restoration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12036v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12036v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image alignment and image restoration are classical computer vision tasks. However, there is still a lack of datasets that provide enough data to train and evaluate end-to-end deep learning models. Obtaining ground-truth data for image alignment requires sophisticated structure-from-motion methods or optical flow systems that often do not provide enough data variance, i.e., typically providing a high number of image correspondences, while only introducing few changes of scenery within the underlying image sequences. Alternative approaches utilize random perspective distortions on existing image data. However, this only provides trivial distortions, lacking the complexity and variance of real-world scenarios. Instead, our proposed data augmentation helps to overcome the issue of data scarcity by using 3D rendering: images are added as textures onto a plane, then varying lighting conditions, shadows, and occlusions are added to the scene. The scene is rendered from multiple viewpoints, generating perspective distortions more consistent with real-world scenarios, with homographies closely resembling those of camera projections rather than randomized homographies. For each scene, we provide a sequence of distorted images with corresponding occlusion masks, homographies, and ground-truth labels. The resulting dataset can serve as a training and evaluation set for a multitude of tasks involving image alignment and artifact removal, such as deep homography estimation, dense image matching, 2D bundle adjustment, inpainting, shadow removal, denoising, content retrieval, and background subtraction. Our data generation pipeline is customizable and can be applied to any existing dataset, serving as a data augmentation to further improve the feature learning of any existing method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12036v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像对齐和图像恢复是经典的计算机视觉任务。然而，仍然缺乏提供足够数据来训练和评估端到端深度学习模型的数据集。获得用于图像对准的地面实况数据需要来自运动方法或光流系统的复杂结构，这些运动方法或光学流系统通常不能提供足够的数据方差，即，通常提供大量的图像对应，而在底层图像序列内仅引入很少的风景变化。替代方法利用现有图像数据上的随机透视失真。然而，这只提供了微不足道的扭曲，缺乏现实世界场景的复杂性和多样性。相反，我们提出的数据增强通过使用3D渲染有助于克服数据稀缺的问题：将图像作为纹理添加到平面上，然后将不同的照明条件、阴影和遮挡添加到场景中。场景是从多个视点渲染的，生成的透视扭曲更符合真实世界场景，单应性与相机投影的单应性非常相似，而不是随机单应性。对于每个场景，我们提供一系列扭曲的图像，其中包含相应的遮挡遮罩、单形图和基本事实标签。所得数据集可以作为涉及图像对齐和伪影去除的大量任务的训练和评估集，例如深度单应性估计、密集图像匹配、2D束调整、修复、阴影去除、去噪、内容检索和背景减法。我们的数据生成管道是可定制的，可以应用于任何现有的数据集，作为数据扩充，以进一步改进任何现有方法的特征学习。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12036v1" target="_blank">2305.12036v1</a>
                              </td>
                              <td>SIDAR: Synthetic Image Dataset for Alignment & Restoration</td>
                              <td>Monika Kwiatkowski</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12036v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12036v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_08810v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AutoRecon: Automated 3D Object Discovery and Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_08810v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_08810v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_08810v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>完全自动化的对象重建管道对于数字内容创建至关重要。虽然3D重建领域已经取得了深刻的发展，但去除背景以获得干净的对象模型仍然依赖于不同形式的手工劳动，如边界框标记、遮罩注释和网格操作。在本文中，我们提出了一个名为AutoRecon的新框架，用于从多视图图像中自动发现和重建对象。我们证明，通过利用自监督2D视觉变换器特征，可以从SfM点云中稳健地定位和分割前景对象。然后，我们在分解的点云提供的密集监督下重建分解的神经场景表示，从而实现精确的对象重建和分割。在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.08810v1" target="_blank">2305.08810v1</a>
                              </td>
                              <td>AutoRecon: Automated 3D Object Discovery and Reconstruction</td>
                              <td>Yuang Wang</td>
                              <td>2023-05-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_08810v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.08810v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06794v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-modal Multi-level Fusion for 3D Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06794v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06794v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06794v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维单目标跟踪在计算机视觉中起着至关重要的作用。主流的方法主要依靠点云来实现目标模板和搜索区域之间的几何匹配。然而，无纹理和不完整的点云使单模态跟踪器难以区分具有相似结构的对象。为了克服几何匹配的局限性，我们提出了一种多模态多级融合跟踪器（MMF Track），该跟踪器利用点云的图像纹理和几何特征来跟踪三维目标。具体而言，我们首先提出了一种空间对齐模块（SAM），用于将RGB图像与3D空间中的点云对齐，这是构建模态间关联的先决条件。然后，在特征交互层面，我们设计了一个基于双流结构的特征交互模块，该模块并行增强模态内特征，构建模态间语义关联。同时，为了细化每个模态特征，我们引入了一个从粗到细的交互模块（CFIM）来实现不同尺度的层次特征交互。最后，在相似性融合层面，我们提出了一个相似性融合模块（SFM）来聚合来自目标的几何和纹理线索。实验表明，我们的方法在KITTI上实现了最先进的性能（与以前的多模态方法相比，成功率为39%，精度提高了42%），在NuScenes上也具有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06794v1" target="_blank">2305.06794v1</a>
                              </td>
                              <td>Multi-modal Multi-level Fusion for 3D Single Object Tracking</td>
                              <td>Zhiheng Li</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06794v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06794v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05301v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05301v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05301v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization plays an important role in the positioning and navigation of robotics systems within previously visited environments. When visits occur over long periods of time, changes in the environment related to seasons or day-night cycles present a major challenge. Under water, the sources of variability are due to other factors such as water conditions or growth of marine organisms. Yet it remains a major obstacle and a much less studied one, partly due to the lack of data. This paper presents a new deep-sea dataset to benchmark underwater long-term visual localization. The dataset is composed of images from four visits to the same hydrothermal vent edifice over the course of five years. Camera poses and a common geometry of the scene were estimated using navigation data and Structure-from-Motion. This serves as a reference when evaluating visual localization techniques. An analysis of the data provides insights about the major changes observed throughout the years. Furthermore, several well-established visual localization methods are evaluated on the dataset, showing there is still room for improvement in underwater long-term visual localization. The data is made publicly available at https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05301v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位在机器人系统在先前访问的环境中的定位和导航中起着重要作用。当访问发生在长时间内时，与季节或昼夜周期相关的环境变化是一个重大挑战。在水下，变异的来源是由于其他因素，如水条件或海洋生物的生长。然而，它仍然是一个主要障碍，也是一个研究较少的障碍，部分原因是缺乏数据。本文提出了一个新的深海数据集，用于对水下长期视觉定位进行基准测试。该数据集由五年内四次访问同一热液喷口建筑物的图像组成。使用导航数据和“运动结构”来估计摄影机姿态和场景的常见几何体。这可作为评估视觉定位技术时的参考。对数据的分析提供了多年来观察到的主要变化的见解。此外，在数据集上评估了几种公认的视觉定位方法，表明水下长期视觉定位仍有改进空间。数据可在https://www.seanoe.org/data/00810/92226/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05301v1" target="_blank">2305.05301v1</a>
                              </td>
                              <td>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</td>
                              <td>Clémentin Boittiaux</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05301v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05301v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_05268v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rotation Synchronization via Deep Matrix Factorization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_05268v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_05268v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_05268v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们解决了旋转同步问题，其中目标是从成对旋转开始恢复绝对旋转，其中未知数和测度分别表示为图的节点和边。这个问题是结构从运动到同时定位和映射的一个重要任务。我们专注于通过神经网络进行同步的公式化，这是最近才开始在文献中进行探索的。受深度矩阵完备的启发，我们用深度神经网络的矩阵分解来表达旋转同步。我们的公式具有隐式正则化性质，更重要的是，它是无监督的，而以前的深度方法是有监督的。我们的实验表明，在大多数场景中，我们实现了与最接近的竞争对手相当的准确性，同时在较弱的假设下工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.05268v1" target="_blank">2305.05268v1</a>
                              </td>
                              <td>Rotation Synchronization via Deep Matrix Factorization</td>
                              <td>Gk Tejus</td>
                              <td>2023-05-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_05268v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.05268v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_05020v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_05020v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_05020v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose fast and communication-efficient optimization algorithms for multi-robot rotation averaging and translation estimation problems that arise from collaborative simultaneous localization and mapping (SLAM), structure-from-motion (SfM), and camera network localization applications. Our methods are based on theoretical relations between the Hessians of the underlying Riemannian optimization problems and the Laplacians of suitably weighted graphs. We leverage these results to design a collaborative solver in which robots coordinate with a central server to perform approximate second-order optimization, by solving a Laplacian system at each iteration. Crucially, our algorithms permit robots to employ spectral sparsification to sparsify intermediate dense matrices before communication, and hence provide a mechanism to trade off accuracy with communication efficiency with provable guarantees. We perform rigorous theoretical analysis of our methods and prove that they enjoy (local) linear rate of convergence. Furthermore, we show that our methods can be combined with graduated non-convexity to achieve outlier-robust estimation. Extensive experiments on real-world SLAM and SfM scenarios demonstrate the superior convergence rate and communication efficiency of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_05020v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们为多机器人旋转平均和平移估计问题提出了快速且高效的优化算法，这些问题源于协同同步定位和映射（SLAM）、运动结构（SfM）和相机网络定位应用。我们的方法基于基本黎曼优化问题的Hessians和适当加权图的Laplacian之间的理论关系。我们利用这些结果设计了一个协作求解器，其中机器人与中央服务器协调，通过在每次迭代中求解拉普拉斯系统来执行近似的二阶优化。至关重要的是，我们的算法允许机器人在通信前使用频谱稀疏化来稀疏中间密集矩阵，因此提供了一种在通信效率与准确性之间进行权衡的机制，并提供了可证明的保证。我们对我们的方法进行了严格的理论分析，并证明它们具有（局部）线性收敛率。此外，我们证明了我们的方法可以与分级非凸性相结合，以实现异常值的鲁棒估计。在真实世界的SLAM和SfM场景上进行的大量实验证明了我们的方法优越的收敛速度和通信效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.05020v3" target="_blank">2210.05020v3</a>
                              </td>
                              <td>Spectral Sparsification for Communication-Efficient Collaborative Rotation and Translation Estimation</td>
                              <td>Yulun Tian</td>
                              <td>2022-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_05020v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.05020v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10664v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10664v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10664v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRFs) are trained using a set of camera poses and associated images as input to estimate density and color values for each position. The position-dependent density learning is of particular interest for photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF coordinate system based on the object density. While traditional methods like Structure from Motion are commonly used for camera pose calculation in pre-processing for NeRFs, the HoloLens offers an interesting interface for extracting the required input data directly. We present a workflow for high-resolution 3D reconstructions almost directly from HoloLens data using NeRFs. Thereby, different investigations are considered: Internal camera poses from the HoloLens trajectory via a server application, and external camera poses from Structure from Motion, both with an enhanced variant applied through pose refinement. Results show that the internal camera poses lead to NeRF convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and enable a 3D reconstruction. Pose refinement enables comparable quality compared to external camera poses, resulting in improved training process with a PSNR of 27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform the conventional photogrammetric dense reconstruction using Multi-View Stereo in terms of completeness and level of detail.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10664v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用一组相机姿势和相关图像作为输入来训练神经辐射场（NeRF），以估计每个位置的密度和颜色值。位置相关密度学习对摄影测量特别感兴趣，它通过基于物体密度查询和过滤NeRF坐标系来实现3D重建。虽然在NeRF的预处理中，像“运动结构”这样的传统方法通常用于相机姿态计算，但HoloLens为直接提取所需的输入数据提供了一个有趣的界面。我们提出了一种使用NeRFs几乎直接从HoloLens数据进行高分辨率3D重建的工作流程。因此，考虑了不同的研究：通过服务器应用程序从HoloLens轨迹中获得的内部相机姿势，以及从运动中获得的结构中获得的外部相机姿势，两者都通过姿势细化应用了增强的变体。结果表明，内部相机姿态导致NeRF收敛，PSNR为25dB，绕x轴简单旋转，并实现3D重建。与外部相机姿势相比，姿势细化能够实现相当的质量，从而改进训练过程，PSNR为27\，dB，并实现更好的3D重建。总体而言，NeRF重建在完整性和细节水平方面优于使用多视图立体的传统摄影测量密集重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10664v1" target="_blank">2304.10664v1</a>
                              </td>
                              <td>A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses from HoloLens Trajectories and Structure from Motion</td>
                              <td>Miriam Jäger</td>
                              <td>2023-04-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10664v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10664v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13875v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Finding Geometric Models by Clustering in the Consensus Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13875v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13875v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems - at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13875v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的算法来寻找未知数量的几何模型，例如单应性。该问题被形式化为逐步找到主导模型实例，而不形成清晰的点到模型分配。主要实例是通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的合并过程来发现的。通过在一致性空间中进行聚类来发现新的一致性。这种新的公式产生了一种简单的迭代算法，具有最先进的精度，同时实时处理许多视觉问题——在双视图运动估计方面比竞争对手快至少两个数量级。此外，我们提出了一种确定性采样器，反映了真实世界的数据往往形成空间相干结构的事实。采样器返回逐渐加密的邻域图中的连接分量。我们介绍了许多应用，其中使用多个几何模型可以提高精度。这些包括从多个广义单应性的姿态估计；快速移动物体的轨迹估计；并且我们还提出了一种在全局SfM算法中使用多个单应性的方法。源代码：https://github.com/danini/clustering-in-consensus-space.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13875v2" target="_blank">2103.13875v2</a>
                              </td>
                              <td>Finding Geometric Models by Clustering in the Consensus Space</td>
                              <td>Daniel Barath</td>
                              <td>2021-03-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13875v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13875v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体的定位是机器人、虚拟和增强现实以及仓库货物运输等各种应用中的一项关键任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复图案和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助小递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v1" target="_blank">2304.07250v1</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05947v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Localization using Imperfect 3D Models from the Internet</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05947v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05947v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05947v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉定位是包括增强现实（AR）在内的许多应用中的核心组件。定位算法计算查询图像的相机姿态，该图像通常是根据图像构建的场景表示。这通常需要捕获和存储大量数据，然后运行运动结构（SfM）算法。用于建筑场景表示的一个有趣且未充分探索的数据源是在互联网上容易获得的3D模型，例如手绘CAD模型、从建筑足迹或从航空图像生成的3D模型。这些模型允许立即执行视觉定位，而无需耗时的场景捕捉和模型构建步骤。然而，它也带来了挑战，因为可用的3D模型往往是对现实的不完美反映。例如，模型可能只具有通用纹理或根本没有纹理，可能只提供场景几何体的简单近似，或者可能被拉伸。本文研究了这些模型的缺陷如何影响定位精度。我们为这项任务创建了一个新的基准，并基于每个场景的多个3D模型提供了详细的实验评估。我们表明，来自互联网的3D模型有望成为一种易于获得的场景表示。同时，视觉定位管道还有很大的改进空间。为了促进对这项有趣而富有挑战性的任务的研究，我们在v-pnk.github.io/cadloc上发布了我们的基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05947v1" target="_blank">2304.05947v1</a>
                              </td>
                              <td>Visual Localization using Imperfect 3D Models from the Internet</td>
                              <td>Vojtech Panek</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05947v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05947v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像技术已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索从红外图像中通过SfM进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v1" target="_blank">2304.03930v1</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03560v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03560v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03560v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates.   Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03560v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督多帧深度估计通过计算相邻帧之间像素对应的匹配成本，将几何信息注入网络，实现了高精度。这些像素对应候选是基于帧之间的相对姿态估计来计算的。精确的姿态预测对于精确的匹配成本计算至关重要，因为它们会影响核极几何。此外，改进的深度估计反过来可以用于对准姿态估计。受传统运动结构（SfM）原理的启发，我们提出了DualRefine模型，该模型通过反馈回路将深度和姿态估计紧密耦合。我们新颖的更新管道使用深度平衡模型框架，通过基于核极几何计算局部匹配成本，迭代细化深度估计和特征图的隐藏状态。重要的是，我们使用精细的深度估计和特征图来计算每一步的姿势更新。姿态估计的这种更新在细化过程中缓慢地改变了极线几何结构。KITTI数据集上的实验结果表明，竞争性深度预测和里程计预测性能超过了已公布的自监督基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03560v1" target="_blank">2304.03560v1</a>
                              </td>
                              <td>DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</td>
                              <td>Antyanta Bangunharcana</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03560v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03560v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $\widetilde{O}(n^2)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$\widetilde{O}（n^2）$oracle复杂度。然而，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$，这是由于昂贵的子程序，如Lenstra-Lenstra-Lov\asz（LLL）算法[Lenstra，Lenstra，Lov\asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]提出的LLL算法的更快版本、[Vaidya，FOCS 1989]提出的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了这个问题的一个强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\log n）$额外的算术运算。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v1" target="_blank">2304.03426v1</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02420v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Validation in Structure from Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02420v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02420v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Structure from Motion (SfM) challenge in computer vision is the process of recovering the 3D structure of a scene from a series of projective measurements that are calculated from a collection of 2D images, taken from different perspectives. SfM consists of three main steps; feature detection and matching, camera motion estimation, and recovery of 3D structure from estimated intrinsic and extrinsic parameters and features.   A problem encountered in SfM is that scenes lacking texture or with repetitive features can cause erroneous feature matching between frames. Semantic segmentation offers a route to validate and correct SfM models by labelling pixels in the input images with the use of a deep convolutional neural network. The semantic and geometric properties associated with classes in the scene can be taken advantage of to apply prior constraints to each class of object. The SfM pipeline COLMAP and semantic segmentation pipeline DeepLab were used. This, along with planar reconstruction of the dense model, were used to determine erroneous points that may be occluded from the calculated camera position, given the semantic label, and thus prior constraint of the reconstructed plane. Herein, semantic segmentation is integrated into SfM to apply priors on the 3D point cloud, given the object detection in the 2D input images. Additionally, the semantic labels of matched keypoints are compared and inconsistent semantically labelled points discarded. Furthermore, semantic labels on input images are used for the removal of objects associated with motion in the output SfM models. The proposed approach is evaluated on a data-set of 1102 images of a repetitive architecture scene. This project offers a novel method for improved validation of 3D SfM models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02420v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>计算机视觉中的运动结构（SfM）挑战是从一系列投影测量中恢复场景的3D结构的过程，这些投影测量是从不同视角拍摄的2D图像集合中计算出来的。SfM由三个主要步骤组成；特征检测和匹配，相机运动估计，以及从估计的内在和外在参数和特征中恢复3D结构。在SfM中遇到的问题是，缺乏纹理或具有重复特征的场景可能导致帧之间的错误特征匹配。语义分割通过使用深度卷积神经网络标记输入图像中的像素，提供了一种验证和校正SfM模型的途径。可以利用与场景中的类相关联的语义和几何特性来将先验约束应用于每类对象。使用了SfM流水线COLMAP和语义分割流水线DeepLab。这与密集模型的平面重建一起，被用于确定可能被计算的相机位置遮挡的错误点，给定语义标签，从而确定重建平面的先验约束。在此，在给定2D输入图像中的对象检测的情况下，语义分割被集成到SfM中，以在3D点云上应用先验。此外，对匹配的关键点的语义标签进行比较，并丢弃语义上不一致的标记点。此外，输入图像上的语义标签用于去除与输出SfM模型中的运动相关联的对象。所提出的方法是在重复建筑场景的1102幅图像的数据集上进行评估的。该项目为改进三维SfM模型的验证提供了一种新方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02420v1" target="_blank">2304.02420v1</a>
                              </td>
                              <td>Semantic Validation in Structure from Motion</td>
                              <td>Joseph Rowell</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02420v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02420v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_13551v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_13551v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_13551v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_13551v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个视图估计密集深度图在几何上是不适定的，最先进的方法依赖于使用深度神经网络学习深度与视觉外观的关系。另一方面，运动结构（SfM）利用多视图约束来生成非常精确但稀疏的地图，因为图像之间的匹配通常受到局部判别纹理的限制。在这项工作中，我们结合了这两种方法的优势，提出了一种新的测试时间细化（TTR）方法，称为SfM-TTR，该方法使用SfM多视图线索在测试时提高了单视图深度网络的性能。具体而言，与现有技术不同的是，我们使用稀疏的SfM点云作为测试时间自监督信号，对网络编码器进行微调，以学习测试场景的更好表示。我们的结果表明，将SfM-TTR添加到几个最先进的自监督和监督网络中，显著提高了它们的性能，优于以前主要基于光度多视图一致性的TTR基线。代码位于https://github.com/serizba/SfM-TTR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.13551v2" target="_blank">2211.13551v2</a>
                              </td>
                              <td>SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks</td>
                              <td>Sergio Izquierdo</td>
                              <td>2022-11-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_13551v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.13551v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Line Mapping Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>与稀疏关键点相比，少数线段可以简明地对高级场景布局进行编码，因为它们通常描绘主要的结构元素。除了提供强烈的几何线索外，它们还在城市景观和室内场景中无处不在。尽管有明显的优势，但目前基于线的重建方法远远落后于基于点的重建方法。在本文中，我们旨在通过引入LIMAP来缩小这一差距，LIMAP是一个用于3D线图绘制的库，可以从多视图图像中稳健有效地创建3D线图。这是通过重新审视线三角测量的退化问题、精心制作的评分和轨迹构建，以及利用线重合、平行和正交等结构先验来实现的。我们的代码与现有的基于点的运动结构方法无缝集成，可以利用它们的3D点来进一步改进线重建。此外，作为副产品，该方法能够恢复线和点/消失点（VP）之间的3D关联图。在深入的实验中，我们表明LIMAP在3D线映射方面显著优于现有的方法。我们强大的3D折线图也开辟了新的研究方向。我们展示了两个示例应用程序：视觉定位和束调整，其中将线与点一起积分会产生最佳结果。代码位于https://github.com/cvg/limap.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17504v1" target="_blank">2303.17504v1</a>
                              </td>
                              <td>3D Line Mapping Revisited</td>
                              <td>Shaohui Liu</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_15069v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_15069v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_15069v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_15069v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们引入了一种轻量级网络来改进同一图像中关键点的描述符。该网络以原始描述符和关键点的几何特性为输入，并使用基于MLP的自提升级和基于Transformer的交叉提升级来增强描述符。增强的描述符可以是实数描述符，也可以是二进制描述符。我们使用所提出的网络来增强手工制作的（ORB，SIFT）和最先进的基于学习的描述符（SuperPoint，ALIKE），并在图像匹配、视觉定位和运动任务的结构方面对它们进行评估。结果表明，我们的方法显著提高了每个任务的性能，特别是在具有挑战性的情况下，如大的照明变化或重复模式。我们的方法只需要在桌面GPU上3.2ms，在嵌入式GPU上27ms就可以处理2000个特征，这足够快，可以应用于实际系统。代码和训练过的重量可在github.com/SJTU-ViSYS/FeatureBooster上公开获取。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.15069v3" target="_blank">2211.15069v3</a>
                              </td>
                              <td>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network</td>
                              <td>Xinjiang Wang</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_15069v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.15069v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15060v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15060v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15060v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high-quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15060v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的管道，可以通过一部智能手机在野外获取纹理网格，该智能手机可以访问图像、深度图和有效姿势。我们的方法首先引入了一种基于运动的RGBD辅助结构，该结构可以生成过滤后的深度图，并根据相应的深度细化相机姿态。然后，我们采用了神经隐式曲面重建方法，该方法可以获得高质量的网格，并开发了一种新的训练过程，用于应用经典多视图立体方法提供的正则化。此外，我们应用可微分渲染来微调不完整的纹理贴图，并生成在感知上更接近原始场景的纹理。我们的管道可以应用于现实世界中的任何常见对象，而无需实验室环境或精确的掩模图像。我们展示了具有复杂形状的捕捉对象的结果，并将我们的方法与现有的3D重建和纹理映射方法进行了数值验证。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15060v1" target="_blank">2303.15060v1</a>
                              </td>
                              <td>TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering</td>
                              <td>Jaehoon Choi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15060v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15060v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12018v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12018v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12018v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S$^2$fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S$^2$fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S$^2$fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12018v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种基于运动的神经增量结构（SfM）方法，即S$^2$fM级，该方法通过从建立的关键点对应关系中学习隐式表面的坐标MLP和辐射场，从一组未校准的图像中估计相机姿态和场景几何。由于增量SfM管道中不可避免的两视图和少视图配置，我们的新公式提出了一些新的挑战，这使用于具有未知相机姿态的体积神经渲染的坐标MLP的优化变得复杂。然而，我们证明了在2D对应关系中传递的强归纳基有望通过利用射线采样方案之间的关系来解决这些挑战。基于此，我们重新审视了增量SfM的管道，并更新了关键组件，包括两视图几何初始化、相机姿态配准、3D点三角测量和束平差，以基于神经隐式曲面的全新视角。通过通过坐标MLP统一小型MLP网络中的场景几何结构，我们的Level-S$^2$fM将隐式曲面的零级集视为自上而下的信息正则化，以管理重建的3D点，通过查询SDF拒绝对应关系中的异常值，并通过NBA（Neural BA）细化估计的几何结构。我们的S$^2$fM级不仅在相机姿态估计和场景几何重建方面取得了有希望的结果，而且它还为神经隐式渲染提供了一种很有前途的方法，而无需事先了解相机的外在情况。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12018v2" target="_blank">2211.12018v2</a>
                              </td>
                              <td>Level-S$^2$fM: Structure from Motion on Neural Level Set of Implicit Surfaces</td>
                              <td>Yuxi Xiao</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12018v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12018v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_14840v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_14840v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_14840v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{dataset available at https://github.com/Junggy/HAMMER-dataset} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_14840v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>解决密集3D视觉问题的基于学习的方法通常基于3D传感器数据进行训练。分别使用的测量距离的原理提供了优点和缺点。由于缺乏多模态数据集，文献中通常不会对这些数据进行比较或讨论。无纹理区域对运动和立体的结构来说是有问题的，反射材料对主动传感来说是个问题，而半透明物体的距离用现有硬件测量起来很复杂。对不准确或损坏的数据进行培训会导致模型偏差，阻碍泛化能力。如果在评估过程中将传感器测量视为基本事实，则这些影响不会被注意到。本文研究了传感器误差对深度估计和重建的密集三维视觉任务的影响。我们严格展示了传感器特性对学习预测的重大影响，并注意到日常家庭环境中各种技术产生的泛化问题。为了进行评估，我们引入了一个精心设计的数据集\脚注｛数据集，可在https://github.com/Junggy/HAMMER-dataset}包括来自商品传感器的测量，即D-ToF、I-ToF、无源/有源立体声和单目RGB+P。我们的研究量化了相当大的传感器噪声影响，并为改进密集视觉估计和有针对性的数据融合铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.14840v1" target="_blank">2303.14840v1</a>
                              </td>
                              <td>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</td>
                              <td>HyunJun Jung</td>
                              <td>2023-03-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_14840v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.14840v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_13543v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_13543v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_13543v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reasoning the 3D structure of a non-rigid dynamic scene from a single moving camera is an under-constrained problem. Inspired by the remarkable progress of neural radiance fields (NeRFs) in photo-realistic novel view synthesis of static scenes, extensions have been proposed for dynamic settings. These methods heavily rely on neural priors in order to regularize the problem. In this work, we take a step back and reinvestigate how current implementations may entail deleterious effects, including limited expressiveness, entanglement of light and density fields, and sub-optimal motion localization. As a remedy, we advocate for a bridge between classic non-rigid-structure-from-motion (\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain the latter. To this end, we propose a framework that factorizes time and space by formulating a scene as a composition of bandlimited, high-dimensional signals. We demonstrate compelling results across complex dynamic scenes that involve changes in lighting, texture and long-range dynamics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_13543v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从单个运动摄像机推断非刚性动态场景的三维结构是一个欠约束问题。受神经辐射场（NeRFs）在静态场景的逼真新颖视图合成中取得的显著进展的启发，提出了动态设置的扩展。这些方法在很大程度上依赖于神经先验来正则化问题。在这项工作中，我们后退一步，重新研究当前的实现可能带来的有害影响，包括有限的表现力、光场和密度场的纠缠以及次优运动定位。作为补救措施，我们主张在经典的非刚性运动结构（\nrsfm）和NeRF之间建立一座桥梁，使前者经过充分研究的先验能够约束后者。为此，我们提出了一个框架，通过将场景公式化为带限高维信号的合成，来分解时间和空间。我们在复杂的动态场景中展示了令人信服的结果，这些场景涉及照明、纹理和长程动力学的变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.13543v3" target="_blank">2302.13543v3</a>
                              </td>
                              <td>BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling</td>
                              <td>Sameera Ramasinghe</td>
                              <td>2023-02-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_13543v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.13543v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13805v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13805v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13805v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and hybrid rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13805v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们定义了一个新的问题，即恢复被限制在透明外壳中的物体的三维几何结构。我们还提出了一种新的方法来解决这个具有挑战性的问题。透明外壳在不同传播介质（例如空气或玻璃）之间的界面处带来了多重光反射和折射的挑战。这些多重反射和折射会导致严重的图像失真，从而使单一视点假设无效。因此，使用现有的方法，例如传统的运动结构或现代神经重建方法，不能可靠地重建这些物体的3D几何结构。我们通过将场景明确建模为透明外壳内外两个不同的子空间来解决这个问题。我们使用现有的神经重建方法（NeuS），该方法隐式地表示内子空间的几何形状和外观。为了解决复杂的灯光交互，我们开发了一种混合渲染策略，将体积渲染与光线跟踪相结合。然后，我们通过最小化真实渲染图像和混合渲染图像之间的差异来恢复模型的基本几何结构和外观。我们根据合成数据和真实数据对我们的方法进行了评估。实验结果表明，我们的方法优于最先进的（SOTA）方法。代码和数据将在https://github.com/hirotong/ReNeuS</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13805v1" target="_blank">2303.13805v1</a>
                              </td>
                              <td>Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</td>
                              <td>Jinguang Tong</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13805v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13805v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13791v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Progressively Optimized Local Radiance Fields for Robust View Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13791v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13791v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13791v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从单个随意捕捉的视频中重建大规模场景辐射场的算法。这项任务提出了两个核心挑战。首先，大多数现有的辐射场重建方法都依赖于运动结构算法中精确的预估计相机姿态，而这些算法在野外视频中经常失败。其次，在无界场景中，使用具有有限表示能力的单个全局辐射场不会缩放到更长的轨迹。为了处理未知姿态，我们以渐进的方式联合估计具有辐射场的相机姿态。我们表明，渐进优化显著提高了重建的鲁棒性。为了处理大型无界场景，我们动态分配用时间窗口内的帧训练的新的局部辐射场。这进一步提高了鲁棒性（例如，即使在中等姿态漂移的情况下也表现良好），并允许我们缩放到大场景。我们对Tanks and Temples数据集和收集的户外数据集Static Hikes的广泛评估表明，我们的方法与最先进的方法相比是有利的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13791v1" target="_blank">2303.13791v1</a>
                              </td>
                              <td>Progressively Optimized Local Radiance Fields for Robust View Synthesis</td>
                              <td>Andreas Meuleman</td>
                              <td>2023-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13791v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13791v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_02239v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust Dynamic Radiance Fields</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_02239v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_02239v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_02239v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态辐射场重建方法旨在对动态场景的时变结构和外观进行建模。然而，现有的方法假设通过运动结构（SfM）算法可以可靠地估计精确的相机姿态。因此，这些方法是不可靠的，因为SfM算法在具有高度动态对象、纹理较差的表面和旋转相机运动的具有挑战性的视频中经常失败或产生错误的姿势。我们通过联合估计静态和动态辐射场以及相机参数（姿态和焦距）来解决这个鲁棒性问题。我们通过大量的定量和定性实验证明了我们方法的稳健性。与最先进的动态视图合成方法相比，我们的结果显示出良好的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.02239v2" target="_blank">2301.02239v2</a>
                              </td>
                              <td>Robust Dynamic Radiance Fields</td>
                              <td>Yu-Lun Liu</td>
                              <td>2023-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_02239v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.02239v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_01160v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">High-Res Facial Appearance Capture from Polarized Smartphone Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_01160v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_01160v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach. Our method optimizes for high-resolution normal textures, diffuse albedo, and specular albedo using a coarse-to-fine optimization scheme. We show that the optimized textures can be used in a standard rendering pipeline to synthesize high-quality photo-realistic 3D digital humans in novel environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_01160v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种从RGB图像中重建高质量面部纹理的新方法，该方法基于我们配备了廉价偏振箔的单个智能手机，使用了一种新的捕获程序。具体来说，我们把手电筒变成一个偏振光源，并在相机顶部添加一个偏振滤光片。利用这种设置，我们用交叉偏振光和平行偏振光捕捉被摄对象的面部。对于每个受试者，我们使用改进的智能手机在不同光偏振的闪光灯照射下，在黑暗环境中记录两个短序列。基于这些观察结果，我们使用运动结构重建了人脸的显式表面网格。然后，我们利用相机和光线在可微分渲染器中的协同定位，使用综合分析方法优化面部纹理。我们的方法使用从粗到细的优化方案优化高分辨率法线纹理、漫射反照率和镜面反照率。我们表明，优化的纹理可以在标准渲染管道中使用，以在新的环境中合成高质量的照片逼真的3D数字人。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.01160v2" target="_blank">2212.01160v2</a>
                              </td>
                              <td>High-Res Facial Appearance Capture from Polarized Smartphone Images</td>
                              <td>Dejan Azinović</td>
                              <td>2022-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_01160v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.01160v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_08695v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_08695v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_08695v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis (NVS) is a challenging task in computer vision that involves synthesizing new views of a scene from a limited set of input images. Neural Radiance Fields (NeRF) have emerged as a powerful approach to address this problem, but they require accurate knowledge of camera \textit{intrinsic} and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM) and multi-view stereo (MVS) approaches have been used to extract camera parameters, but these methods can be unreliable and may fail in certain cases. In this paper, we propose a novel technique that leverages unposed images from dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera parameters directly from data. Our approach is highly extensible and can be integrated into existing NeRF architectures with minimal modifications. We demonstrate the effectiveness of our method on a variety of static and dynamic scenes and show that it outperforms traditional SfM and MVS approaches. The code for our method is publicly available at \href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}. Our approach offers a promising new direction for improving the accuracy and robustness of NVS using NeRF, and we anticipate that it will be a valuable tool for a wide range of applications in computer vision and graphics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_08695v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>新视图合成（NVS）是计算机视觉中一项具有挑战性的任务，涉及从有限的一组输入图像合成场景的新视图。神经辐射场（NeRF）已成为解决这一问题的一种强大方法，但它们需要准确了解相机的固有参数和外在参数。传统上，运动结构（SfM）和多视图立体（MVS）方法已被用于提取相机参数，但这些方法可能不可靠，并且在某些情况下可能失败。在本文中，我们提出了一种新技术，该技术利用来自动态数据集（如NVIDIA动态场景数据集）的未渲染图像，直接从数据中学习相机参数。我们的方法具有高度的可扩展性，可以集成到现有的NeRF架构中，只需进行最小的修改。我们在各种静态和动态场景中证明了我们的方法的有效性，并表明它优于传统的SfM和MVS方法。我们方法的代码可在\href上公开获取{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}。我们的方法为使用NeRF提高NVS的准确性和稳健性提供了一个有前景的新方向，我们预计它将成为计算机视觉和图形领域广泛应用的宝贵工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.08695v1" target="_blank">2303.08695v1</a>
                              </td>
                              <td>RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters</td>
                              <td>Shuja Khalid</td>
                              <td>2023-03-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_08695v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.08695v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2306_13805v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Potential Benefits of Employing Large Language Models in Research in Moral Education and Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13805v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13805v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13805v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising their reasoning process with external input. Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants. I will discuss the potential implications of LLMs on research on moral education and development with the results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13805v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，计算机科学家通过使用大规模语言语料库和人类增强训练预测模型，开发了大型语言模型。LLM已经成为在各个领域准确实现人工智能的一种很有前途的方法。有趣的是，最近的LLM具有模仿复杂人类认知的新兴功能特征，特别是在上下文学习和思维链方面，这在以前的预测模型中是不可用的。在这篇论文中，我将探讨LLM如何为道德教育和发展研究做出贡献。为了实现这一目标，我将回顾最近发表的会议论文和ArXiv预印本，以概述LLM中实现的新功能特征。我还打算与ChatGPT进行简短的实验，以调查LLM在解决道德困境和外部反馈时的行为。结果表明，LLM可能能够基于推理解决困境，并利用外部输入修正其推理过程。此外，道德榜样测试的初步实验结果可能表明，榜样故事可以在LLM中引发道德提升，就像在人类参与者中一样。我将讨论LLM对道德教育和发展研究的潜在影响及其结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13805v2" target="_blank">2306.13805v2</a>
                              </td>
                              <td>Potential Benefits of Employing Large Language Models in Research in Moral Education and Development</td>
                              <td>Hyemin Han</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13805v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13805v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07880v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sabiá: Portuguese Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07880v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07880v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07880v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model's knowledge about a domain or culture. Our results indicate that the majority of the benefits stem from the domain-specific knowledge acquired through monolingual pretraining.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07880v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着语言模型的能力不断进步，可以想象，“一刀切”的模型将继续作为主要范式。例如，考虑到世界各地有大量的语言，其中许多是低资源的，普遍的做法是在多种语言上预训练单个模型。在这篇论文中，我们补充了越来越多的证据来挑战这种做法，证明了对目标语言的单语预训练显著改善了已经在不同语料库上广泛训练的模型。更具体地说，我们使用3%或更少的原始预训练预算在葡萄牙语文本上进一步预训练GPT-J和LLaMA模型。在由14个葡萄牙语数据集组成的Poeta上，很少有人对我们的模型进行评估，结果表明，我们的模型显著优于以英语为中心的多语言模型。我们的最佳模型Sabi’a-65B的性能与GPT-3.5-turbo不相上下。通过对最初以目标语言和翻译的数据集进行评估，我们研究了特定语言预训练在以下方面的贡献：1）捕捉目标语言固有的语言细微差别和结构，2）丰富模型对领域或文化的知识。我们的研究结果表明，大多数好处来自于通过单语预训练获得的领域特定知识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07880v3" target="_blank">2304.07880v3</a>
                              </td>
                              <td>Sabiá: Portuguese Large Language Models</td>
                              <td>Ramon Pires</td>
                              <td>2023-04-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07880v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07880v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11031v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11031v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11031v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11031v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. Our work asks whether it is possible to improve prompt-based learning without additional labeled data. We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction. We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction. In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11031v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的工作表明，语言模型（LM）基于提示的学习能力使其非常适合在手动注释成本高昂的领域中自动标记数据。挑战在于，虽然编写初始提示很便宜，但改进提示的成本很高——从业者通常需要大量标记数据来评估提示修改的影响。我们的工作询问是否有可能在没有额外标记数据的情况下改进基于提示的学习。我们通过尝试修改提示的预测而不是提示本身来解决这个问题。我们的直觉是，准确的预测也应该是一致的：在某些特征表示下相似的样本应该得到相同的即时预测。我们提出了Embroid，这是一种在不同嵌入函数下计算数据集的多个表示的方法，并使用相邻样本的LM预测之间的一致性来识别预测失误。Embroid然后使用这些邻域为每个样本创建额外的预测，并将这些预测与简单的潜在变量图形模型相结合，以生成最终的校正预测。除了提供Embroid的理论分析外，我们还对六种不同的LM和多达95种不同的任务进行了严格的实证评估。我们发现（1）Embroid比原始提示显著提高了性能（例如，在GPT-JT上平均提高了7.3分），（2）还实现了对更复杂提示策略（例如，思想链）的改进，以及（3）可以通过嵌入功能专门用于法律等领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11031v1" target="_blank">2307.11031v1</a>
                              </td>
                              <td>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</td>
                              <td>Neel Guha</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11031v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11031v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10811v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10811v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10811v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10811v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs. This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10811v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预写是在初稿之前发现和发展想法的过程，这需要发散思维，通常意味着非结构化的策略，如图解、概述、自由写作等。尽管大型语言模型（LLM）已被证明对包括创造性写作在内的各种任务有用，关于用户将如何与LLM合作以支持预写，我们知之甚少。LLM在这种创造性过程中首选的合作角色和主动性也不清楚。为了研究写作前人类LLM的合作模式和动态，我们进行了一项为期三节的定性研究，共有15名参与者参与了两项创造性任务：故事写作和标语写作。研究结果表明，在合作预写过程中，似乎存在一个三阶段的迭代人工智能协同创意过程，包括构思、照明和实施阶段。除了人类和LLM之间存在的混合和不断变化的主动性之外，这种合作过程还支持人类发挥主导作用。这项研究还报告了在这一过程中发生的协作故障，用户对在人类人工智能共同创造过程中使用现有LLM的看法，并讨论了支持这一共同创造过程的设计含义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10811v1" target="_blank">2307.10811v1</a>
                              </td>
                              <td>"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models</td>
                              <td>Qian Wan</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10811v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10811v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11019v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11019v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11019v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11019v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11019v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>知识密集型任务（例如，开放领域问答（QA））需要大量的事实知识，并且通常依赖外部信息来提供帮助。最近，大型语言模型（LLM）（例如，ChatGPT）在用世界知识解决一系列任务方面表现出了令人印象深刻的能力，包括知识密集型任务。然而，目前尚不清楚LLM对其事实知识边界的感知能力，特别是在结合检索增强时的行为。在本研究中，我们对LLM的事实知识边界以及检索增强如何影响开放域QA中的LLM进行了初步分析。特别是，我们关注了三个主要的研究问题，并通过检验LLM的QA性能、先验判断和后验判断对其进行了分析。我们展示的证据表明，LLM对其回答问题的能力和回答的准确性有着坚定的信心。此外，检索增强被证明是提高LLM知识边界意识的有效方法，从而提高其判断能力。此外，我们还发现，LLM在制定答案时倾向于依赖所提供的检索结果，而这些结果的质量会显著影响其依赖性。复制此作品的代码可在https://github.com/RUCAIBox/LLM-Knowledge-Boundary.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11019v1" target="_blank">2307.11019v1</a>
                              </td>
                              <td>Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</td>
                              <td>Ruiyang Ren</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11019v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11019v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_03017v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_03017v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_03017v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_03017v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our research investigates the recommendation of code examples to aid software developers, a practice that saves developers significant time by providing ready-to-use code snippets. The focus of our study is Stack Overflow, a commonly used resource for coding discussions and solutions, particularly in the context of the Java programming language. We applied BERT, a powerful Large Language Model (LLM) that enables us to transform code examples into numerical vectors by extracting their semantic information. Once these numerical representations are prepared, we identify Approximate Nearest Neighbors (ANN) using Locality-Sensitive Hashing (LSH). Our research employed two variants of LSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared these two approaches across four parameters: HitRate, Mean Reciprocal Rank (MRR), Average Execution Time, and Relevance. Our study revealed that the Query-Aware (QA) approach showed superior performance over the Random Hyperplane-based (RH) method. Specifically, it exhibited a notable improvement of 20% to 35% in HitRate for query pairs compared to the RH approach. Furthermore, the QA approach proved significantly more time-efficient, with its speed in creating hashing tables and assigning data samples to buckets being at least four times faster. It can return code examples within milliseconds, whereas the RH approach typically requires several seconds to recommend code examples. Due to the superior performance of the QA approach, we tested it against PostFinder and FaCoY, the state-of-the-art baselines. Our QA method showed comparable efficiency proving its potential for effective code recommendation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_03017v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的研究调查了代码示例的推荐，以帮助软件开发人员，这种做法通过提供现成的代码片段为开发人员节省了大量时间。我们研究的重点是Stack Overflow，这是一种用于编码讨论和解决方案的常用资源，尤其是在Java编程语言的上下文中。我们应用了BERT，这是一种强大的大型语言模型（LLM），使我们能够通过提取代码示例的语义信息将其转换为数字向量。一旦准备好这些数字表示，我们就使用位置敏感哈希（LSH）来识别近似最近邻（ANN）。我们的研究采用了LSH的两种变体：基于随机超平面的LSH和查询感知的LSH。我们通过四个参数严格比较了这两种方法：命中率、平均倒数排名（MRR）、平均执行时间和相关性。我们的研究表明，查询感知（QA）方法显示出优于基于随机超平面（RH）方法的性能。具体来说，与RH方法相比，它在查询对的命中率方面有了20%至35%的显著提高。此外，QA方法被证明更具时间效率，其创建哈希表和将数据样本分配到桶的速度至少快四倍。它可以在几毫秒内返回代码示例，而RH方法通常需要几秒钟才能推荐代码示例。由于QA方法的卓越性能，我们将其与最先进的基线PostFinder和FaCoY进行了测试。我们的QA方法显示出相当的效率，证明了其在有效代码推荐方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.03017v3" target="_blank">2305.03017v3</a>
                              </td>
                              <td>Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study</td>
                              <td>Sajjad Rahmani</td>
                              <td>2023-05-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_03017v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.03017v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10930v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MediaGPT : A Large Language Model Target Chinese Media</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10930v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10930v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10930v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The development of large language models (LLMs) has seen rapid progress in recent years. One of the most widely used LLMs is the Generative Pre-trained Transformer (GPT) series, which has been applied in various fields, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. As a result, there is a growing need to develop LLM that are specifically tailored to the unique requirements of the media domain. In this paper, we present MediaGPT, a large language model training on variety of media data and addressing the practical needs of Chinese media. We have designed a diverse set of task instruction types to cater to the specific requirements of the domain. To further validate the effectiveness of our proposed LLM, we have constructed unique datasets that are tailored to the media domain and have also developed verification methods that are specifically designed for generative-type tasks. By doing so, we aim to bridge the gap between the general-purpose LLM and the requirements of the media domain, and to pave the way for more effective and efficient use of LLM in this field. This paper aims to explore the challenges and opportunities of developing LLM for media applications and to propose potential solutions for addressing these challenges.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10930v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大型语言模型（LLM）的发展突飞猛进。最广泛使用的LLM之一是Generative Pre-trained Transformer（GPT）系列，该系列已应用于包括媒体领域在内的各个领域。然而，在实际应用中，媒体的用例与LLM的通用应用之间的差异越来越明显，尤其是在中文中。因此，人们越来越需要开发专门针对媒体领域独特需求定制的LLM。在本文中，我们提出了MediaGPT，这是一个针对各种媒体数据的大型语言模型训练，并满足了中国媒体的实际需求。我们设计了一套不同的任务指令类型，以满足该领域的特定要求。为了进一步验证我们提出的LLM的有效性，我们构建了针对媒体领域定制的独特数据集，并开发了专门为生成型任务设计的验证方法。通过这样做，我们旨在弥合通用LLM与媒体领域需求之间的差距，并为LLM在该领域的更有效使用铺平道路。本文旨在探讨为媒体应用开发LLM的挑战和机遇，并提出应对这些挑战的潜在解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10930v1" target="_blank">2307.10930v1</a>
                              </td>
                              <td>MediaGPT : A Large Language Model Target Chinese Media</td>
                              <td>Zhonghao Wang</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10930v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10930v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10928v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10928v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10928v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10928v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Evaluation of Large Language Models (LLMs) is challenging because aligning to human values requires the composition of multiple skills and the required set of skills varies depending on the instruction. Recent studies have evaluated the performance of LLMs in two ways, (1) automatic evaluation on several independent benchmarks and (2) human or machined-based evaluation giving an overall score to the response. However, both settings are coarse-grained evaluations, not considering the nature of user instructions that require instance-wise skill composition, which limits the interpretation of the true capabilities of LLMs. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets), a fine-grained evaluation protocol that can be used for both model-based and human-based evaluation which decomposes coarse-level scoring to an instance-wise skill set-level. Specifically, we define 12 fine-grained skills needed for LLMs to follow open-ended user instructions and construct an evaluation set by allocating a set of skills for each instance. Additionally, by annotating the target domains and difficulty level for each instance, FLASK provides a holistic view with a comprehensive analysis of a model's performance depending on skill, domain, and difficulty. Through using FLASK, we compare multiple open-sourced and proprietary LLMs and observe highly-correlated findings between model-based and human-based evaluations. FLASK enables developers to more accurately measure the model performance and how it can be improved by analyzing factors that make LLMs proficient in particular skills. For practitioners, FLASK can be used to recommend suitable models for particular situations through comprehensive comparison among various LLMs. We release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10928v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的评估具有挑战性，因为与人类价值观保持一致需要多种技能的组合，而所需的一套技能因教学而异。最近的研究以两种方式评估了LLM的性能，（1）在几个独立基准上的自动评估和（2）基于人工或机器的评估，对响应进行总体评分。然而，这两种设置都是粗粒度的评估，没有考虑到需要实例技能组合的用户指令的性质，这限制了LLM真正功能的解释。在本文中，我们介绍了FLASK（基于对齐SKill集的细粒度语言模型评估），这是一种细粒度评估协议，既可用于基于模型的评估，也可用于基于人工的评估，它将粗略级别的评分分解为实例技能集级别。具体来说，我们定义了LLM遵循开放式用户指令所需的12个细粒度技能，并通过为每个实例分配一组技能来构建评估集。此外，通过注释每个实例的目标域和难度级别，FLASK提供了一个整体视图，根据技能、域和难度对模型的性能进行全面分析。通过使用FLASK，我们比较了多种开源和专有LLM，并观察到基于模型的评估和基于人类的评估之间高度相关的发现。FLASK使开发人员能够更准确地测量模型性能，以及如何通过分析使LLM精通特定技能的因素来改进模型性能。对于从业者来说，FLASK可以通过各种LLM之间的综合比较，为特定情况推荐合适的模型。我们在上发布了评估数据和代码实现https://github.com/kaistAI/FLASK.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10928v1" target="_blank">2307.10928v1</a>
                              </td>
                              <td>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</td>
                              <td>Seonghyeon Ye</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10928v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10928v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_01146v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_01146v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_01146v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_01146v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_01146v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了将大型语言模型（LLM）用于放射学报告摘要（RRS）任务的轻量级策略。具体而言，我们通过预训练（自然语言、生物医学文本或临床文本）和离散提示或参数有效微调来关注领域自适应。我们的结果通过对临床文本进行预训练和对RRS示例进行微调，最大限度地适应任务，从而始终实现最佳性能。重要的是，与端到端微调（100%的参数）相比，该方法在整个模型中仅微调0.32%的参数。此外，我们研究了上下文中的例子和分布外（OOD）培训的效果，然后以放射科医生读者研究和定性分析结束。我们的研究结果强调了领域适应在RRS中的重要性，并为开发有效的临床任务自然语言处理解决方案提供了宝贵的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.01146v3" target="_blank">2305.01146v3</a>
                              </td>
                              <td>RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models</td>
                              <td>Dave Van Veen</td>
                              <td>2023-05-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_01146v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.01146v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10793v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Addressing Compiler Errors: Stack Overflow or Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10793v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10793v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10793v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Compiler error messages serve as an initial resource for programmers dealing with compilation errors. However, previous studies indicate that they often lack sufficient targeted information to resolve code issues. Consequently, programmers typically rely on their own research to fix errors. Historically, Stack Overflow has been the primary resource for such information, but recent advances in large language models offer alternatives. This study systematically examines 100 compiler error messages from three sources to determine the most effective approach for programmers encountering compiler errors. Factors considered include Stack Overflow search methods and the impact of model version and prompt phrasing when using large language models. The results reveal that GPT-4 outperforms Stack Overflow in explaining compiler error messages, the effectiveness of adding code snippets to Stack Overflow searches depends on the search method, and results for Stack Overflow differ significantly between Google and StackExchange API searches. Furthermore, GPT-4 surpasses GPT-3.5, with "How to fix" prompts yielding superior outcomes to "What does this error mean" prompts. These results offer valuable guidance for programmers seeking assistance with compiler error messages, underscoring the transformative potential of advanced large language models like GPT-4 in debugging and opening new avenues of exploration for researchers in AI-assisted programming.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10793v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>编译器错误消息是处理编译错误的程序员的初始资源。然而，先前的研究表明，他们往往缺乏足够的有针对性的信息来解决代码问题。因此，程序员通常依靠自己的研究来修复错误。从历史上看，堆栈溢出一直是此类信息的主要来源，但大型语言模型的最新进展提供了替代方案。这项研究系统地检查了来自三个来源的100条编译器错误消息，以确定程序员遇到编译器错误的最有效方法。考虑的因素包括堆栈溢出搜索方法以及使用大型语言模型时模型版本和提示短语的影响。结果表明，GPT-4在解释编译器错误消息方面优于Stack Overflow，向Stack Oversflow搜索添加代码段的有效性取决于搜索方法，并且Stack Overrow的结果在Google和StackExchange API搜索之间存在显著差异。此外，GPT-4超过了GPT-3.5，“如何修复”提示的结果优于“这个错误意味着什么”提示。这些结果为寻求编译器错误消息帮助的程序员提供了宝贵的指导，强调了GPT-4等高级大型语言模型在调试方面的变革潜力，并为人工智能辅助编程的研究人员开辟了新的探索途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10793v1" target="_blank">2307.10793v1</a>
                              </td>
                              <td>Addressing Compiler Errors: Stack Overflow or Large Language Models?</td>
                              <td>Patricia Widjojo</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10793v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10793v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10778v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Extreme Multi-Label Skill Extraction Training using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10778v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10778v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10778v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Online job ads serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes. Since such ads are typically formatted in free text, natural language processing (NLP) technologies are required to automatically process them. We specifically focus on the task of detecting skills (mentioned literally, or implicitly described) and linking them to a large skill ontology, making it a challenging case of extreme multi-label classification (XMLC). Given that there is no sizable labeled (training) dataset are available for this specific XMLC task, we propose techniques to leverage general Large Language Models (LLMs). We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction, and present a contrastive learning strategy that proves effective in the task. Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentage points in \textit{R-Precision@5} compared to previously published results that relied solely on distant supervision through literal matches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10778v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在线招聘广告是技能需求的宝贵信息来源，在劳动力市场分析和电子招聘过程中发挥着至关重要的作用。由于此类广告通常以自由文本形式格式化，因此需要自然语言处理（NLP）技术来自动处理它们。我们特别关注检测技能（字面上提到或隐含地描述）并将其链接到大型技能本体的任务，使其成为极端多标签分类（XMLC）的一个具有挑战性的案例。鉴于没有可用于此特定XMLC任务的大规模标记（训练）数据集，我们提出了利用通用大型语言模型（LLM）的技术。我们描述了一种成本效益高的方法来生成用于技能提取的准确、完全合成的标记数据集，并提出了一种对比学习策略，该策略在任务中被证明是有效的。我们在三个技能提取基准上的结果显示，\textit的持续增长在15到25个百分点之间{R-Precision@5}与之前发表的仅依靠文字匹配的远程监督的结果相比。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10778v1" target="_blank">2307.10778v1</a>
                              </td>
                              <td>Extreme Multi-Label Skill Extraction Training using Large Language Models</td>
                              <td>Jens-Joris Decorte</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10778v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10778v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15299v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15299v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15299v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15299v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15299v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人工智能的大型语言模型，如ChatGPT，在科学和研究中发现了显著但有争议的适用性。本文综述了生成人工智能出现时科学行为中的认识论挑战、伦理和诚信风险，旨在为高质量的研究伦理综述奠定新的及时基础。人工智能语言模型作为一种研究工具和主题的作用，以及对科学家、参与者和评审员的伦理影响，都受到了仔细审查。讨论了研究伦理审查的新实践，最后提出了十项建议，以应对人工智能时代更负责任的研究行为。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15299v2" target="_blank">2305.15299v2</a>
                              </td>
                              <td>Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond</td>
                              <td>Evangelos Pournaras</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15299v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15299v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10747v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Job Recommendation through LLM-based Generative Adversarial Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10747v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10747v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10747v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recommending suitable jobs to users is a critical task in online recruitment platforms, as it can enhance users' satisfaction and the platforms' profitability. While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness. With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful capabilities of text processing and reasoning, is a promising way to complete users' resumes for more accurate recommendations. However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion. In this paper, we propose a novel LLM-based approach for job recommendation. To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion. Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion. Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for the models in generating high-quality resumes. To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results. Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10747v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>向用户推荐合适的工作是在线招聘平台的一项关键任务，因为它可以提高用户的满意度和平台的盈利能力。而现有的职位推荐方法面临着用户简历质量低等挑战，这阻碍了其准确性和实用性。随着大型语言模型（LLM）的快速发展，利用其内部丰富的外部知识，以及其强大的文本处理和推理能力，是完成用户简历以获得更准确推荐的一种很有前途的方式。然而，直接利用LLM来增强推荐结果并不是一个一刀切的解决方案，因为LLM可能会出现人为生成和少量放炮的问题，这会降低简历完成的质量。在本文中，我们提出了一种新的基于LLM的工作推荐方法。为了缓解LLM捏造生成的局限性，我们提取了用户自我描述之外的准确而有价值的信息，这有助于LLM更好地了解用户的简历。具体而言，我们不仅从用户的自我描述中提取用户的显性属性（如技能、兴趣），还从用户的行为中推断用户的隐性特征，以更准确、更有意义地完成简历。尽管如此，一些用户仍然存在少数拍摄问题，这是由于缺乏互动记录而出现的，导致模型在生成高质量简历方面的指导有限。为了解决这个问题，我们建议通过生成对抗性网络（GANs）将未配对的低质量简历与高质量生成的简历进行比对，这可以细化简历表示，以获得更好的推荐结果。在三个大型真实世界招聘数据集上进行的大量实验证明了我们提出的方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10747v1" target="_blank">2307.10747v1</a>
                              </td>
                              <td>Enhancing Job Recommendation through LLM-based Generative Adversarial Networks</td>
                              <td>Yingpeng Du</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10747v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10747v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10719v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10719v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10719v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10719v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10719v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在理解复杂指令方面表现出了令人印象深刻的能力。然而，他们对提供的说明的盲目遵守导致了人们对恶意使用风险的担忧。现有的防御机制，如模型微调或使用LLM的输出审查，已被证明是容易出错的，因为LLM仍然可能产生有问题的反应。常用的审查方法将该问题视为机器学习问题，并依靠另一个LM来检测LLM输出中的不期望内容。在本文中，我们提出了这种语义审查方法的理论局限性。具体而言，我们证明了语义审查可以被视为一个不可决定的问题，强调了由于LLM的程序和指令遵循能力而产生的审查中的固有挑战。此外，我们认为挑战超出了语义审查，因为知识渊博的攻击者可以从允许的输出集合中重建不允许的输出。因此，我们建议审查问题需要重新评估；它应该被视为一个安全问题，需要调整基于安全的方法来减轻潜在风险。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10719v1" target="_blank">2307.10719v1</a>
                              </td>
                              <td>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</td>
                              <td>David Glukhov</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10719v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10719v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_11596v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ThoughtSource: A central hub for large language model reasoning data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_11596v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_11596v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_11596v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_11596v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>GPT-4等大型语言模型（LLM）最近在一系列任务中取得了令人印象深刻的结果。然而，LLM仍然是有限的，因为它们经常无法进行复杂的推理，推理过程不透明，容易产生“幻觉”事实，而且人们担心它们的潜在偏见。让模型将推理步骤表述为自然语言，这是一种被称为思维链提示的技术，最近被提出作为解决其中一些问题的一种方法。在这里，我们介绍了ThoughtSource，一个用于思想链（CoT）推理的元数据集和软件库。ThoughtSource的目标是通过促进对CoTs的定性理解、实现实证评估和提供培训数据来改进未来的人工智能系统。ThoughtSource的首次发布集成了六个科学/医学、三个通用领域和五个数学单词问答数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.11596v4" target="_blank">2301.11596v4</a>
                              </td>
                              <td>ThoughtSource: A central hub for large language model reasoning data</td>
                              <td>Simon Ott</td>
                              <td>2023-01-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_11596v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.11596v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06427v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Language Models Reasoning with Chain-of-Knowledge Prompting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06427v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06427v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06427v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06427v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，思维链（CoT）提示在复杂的推理任务上取得了成功，其目的是设计一个简单的提示，如“让我们一步一步地思考”或多个具有精心设计的理由的上下文示例，以引出大型语言模型（LLM）来生成中间推理步骤。然而，产生的理性往往伴随着错误，形成了不真实和不忠的推理链。为了减轻这种脆弱性，我们提出了一种新的知识链（CoK）提示，其中我们旨在启发LLM以结构三元组的形式生成明确的知识证据。这是受我们人类行为的启发，即在回答复杂问题之前，我们可以在大脑中绘制思维导图或知识导图作为推理证据。得益于CoK，我们还引入了一种F^2验证方法，从真实性和忠实性的角度来估计推理链的可靠性。对于不可靠的回应，可以指出错误的证据，以促使LLM重新思考。大量实验表明，我们的方法可以进一步提高常识、事实、符号和算术推理任务的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06427v2" target="_blank">2306.06427v2</a>
                              </td>
                              <td>Boosting Language Models Reasoning with Chain-of-Knowledge Prompting</td>
                              <td>Jianing Wang</td>
                              <td>2023-06-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06427v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06427v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10700v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large language models shape and are shaped by society: A survey of arXiv publication patterns</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10700v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10700v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10700v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dynamics: we document gender and academic/industry disparities in the topics LLM authors focus on, and a US/China schism in the collaboration network. Overall, our analysis documents the profound ways in which LLM research both shapes and is shaped by society, attesting to the necessity of sociotechnical lenses.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10700v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，大型语言模型（LLM）论文的数量急剧增加，导致科学领域发生了戏剧性的变化，而通过文献计量分析，科学领域基本上没有记录在案。在这里，我们分析了发表在CS和Stat arXivs上的388K篇论文，重点关注2023年与2018-2022年出版模式的变化。我们分析了LLM论文的比例是如何增加的；LLM相关主题最受关注；LLM论文的作者；作者的研究主题如何与其背景相关；高引用LLM论文的区别因素；以及国际合作模式。我们发现，LLM研究越来越关注社会影响：在计算机与社会子网站arXiv上，LLM相关论文的比例增加了18倍，新发表LLM的作者比更有经验的作者更有可能关注应用和社会影响。LLM研究也受到社会动态的影响：我们记录了LLM作者关注的主题中的性别和学术/行业差异，以及合作网络中的美国/中国分裂。总的来说，我们的分析记录了LLM研究塑造和被社会塑造的深刻方式，证明了社会技术视角的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10700v1" target="_blank">2307.10700v1</a>
                              </td>
                              <td>Large language models shape and are shaped by society: A survey of arXiv publication patterns</td>
                              <td>Rajiv Movva</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10700v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10700v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10690v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bridging Intelligence and Instinct: A New Control Paradigm for Autonomous Robots</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10690v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10690v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10690v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As the advent of artificial general intelligence (AGI) progresses at a breathtaking pace, the application of large language models (LLMs) as AI Agents in robotics remains in its nascent stage. A significant concern that hampers the seamless integration of these AI Agents into robotics is the unpredictability of the content they generate, a phenomena known as ``hallucination''. Drawing inspiration from biological neural systems, we propose a novel, layered architecture for autonomous robotics, bridging AI agent intelligence and robot instinct. In this context, we define Robot Instinct as the innate or learned set of responses and priorities in an autonomous robotic system that ensures survival-essential tasks, such as safety assurance and obstacle avoidance, are carried out in a timely and effective manner. This paradigm harmoniously combines the intelligence of LLMs with the instinct of robotic behaviors, contributing to a more safe and versatile autonomous robotic system. As a case study, we illustrate this paradigm within the context of a mobile robot, demonstrating its potential to significantly enhance autonomous robotics and enabling a future where robots can operate independently and safely across diverse environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10690v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着人工通用智能（AGI）的出现以惊人的速度发展，大型语言模型（LLM）作为人工智能代理在机器人领域的应用仍处于初级阶段。阻碍这些人工智能代理与机器人无缝集成的一个重要问题是它们生成的内容的不可预测性，这种现象被称为“幻觉”。从生物神经系统中汲取灵感，我们为自主机器人提出了一种新颖的分层架构，将人工智能智能与机器人本能联系起来。在这种情况下，我们将机器人本能定义为自主机器人系统中固有或习得的一组反应和优先级，以确保安全保障和避障等生存基本任务及时有效地执行。这种范式将LLM的智能与机器人行为的本能和谐地结合在一起，有助于建立一个更安全、更通用的自主机器人系统。作为一个案例研究，我们在移动机器人的背景下说明了这一范式，展示了其显著增强自主机器人的潜力，并使机器人能够在不同环境中独立安全地运行。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10690v1" target="_blank">2307.10690v1</a>
                              </td>
                              <td>Bridging Intelligence and Instinct: A New Control Paradigm for Autonomous Robots</td>
                              <td>Shimian Zhang</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10690v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10690v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10635v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10635v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10635v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10635v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10635v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的最新进展表明，在许多数学基准方面取得了显著进展。然而，这些基准测试大多只针对初中和高中科目的问题，只包含多项选择题，并且仅限于有限的初等算术运算范围。为了解决这些问题，本文引入了一个扩展的基准套件SciBench，旨在系统地检查复杂科学问题解决所需的推理能力。SciBench包含两个精心策划的数据集：一个开放集包含一系列来自数学、化学和物理课本的大学级科学问题，另一个封闭集包含计算机科学和数学本科级考试的问题。基于这两个数据集，我们对具有不同提示策略的两个具有代表性的LLM进行了深入的基准研究。结果表明，目前的LLM不能提供令人满意的性能，总分仅为35.80%。此外，通过详细的用户研究，我们将LLM所犯的错误分为十种解决问题的能力。我们的分析表明，没有一种单一的提示策略显著优于其他策略，一些策略显示出某些解决问题技能的提高会导致其他技能的下降。我们设想SciBench将促进LLM推理能力的进一步发展，从而最终为科学研究和发现做出贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10635v1" target="_blank">2307.10635v1</a>
                              </td>
                              <td>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</td>
                              <td>Xiaoxuan Wang</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10635v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10635v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10633v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10633v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10633v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10633v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10633v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型有许多方法可以解决同一个问题。这引入了新的优势（不同的方法可能适用于不同的问题）和劣势（用户可能很难知道使用哪种方法）。在本文中，我们介绍了多方法自训练（MMST），其中一种方法是在另一种方法的滤波输出上训练的，这使我们能够增强每种方法的优势并改善其劣势。使用在语言和代码上训练的176B参数模型，我们表明MMST可以1）改进性能较低的方法（高达30%），使模型更易于使用，2）改进性能较高的方法（敢达32.2%），使该模型更具性能，3）通过提高模型生成推理的能力，提高相关但不同任务的性能（高达10.3%）。然后，我们进行消融分析，探讨MMST有效的原因。我们表明，MMST比传统的自训练生成更多的数据，但性能的提高是由多种方法的使用推动的。我们还分析了方法之间的即时工程和反相关性能，以使MMST更有效。我们希望我们论文中的证据能激励机器学习研究人员探索语言模型的进步允许新形式训练的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10633v1" target="_blank">2307.10633v1</a>
                              </td>
                              <td>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</td>
                              <td>Shriyash K. Upadhyay</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10633v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10633v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10577v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10577v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10577v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10577v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.   We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.   Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating accurate semantic knowledge of images within a few cycles. It successfully captures both explicit and nuanced elements efficiently. We also introduce the implementation of Korzybski's "time-binding" concept in machines, which allows for generational learning and knowledge sharing across deployments.   Our evaluations demonstrate Ethosight's efficacy across 40 complex use cases. It has exhibited an exceptional ability to discern new areas of interest, consistently generating high-affinity scores within the top five labels from a set of a thousand. Tests conducted across diverse environments attest to Ethosight's robust performance. Detailed results and case studies within the main body of this paper and an appendix underscore a promising trajectory towards enhancing the adaptability and resilience of computer vision models in detecting and extracting subtle and nuanced behaviors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10577v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>传统的计算机视觉模型通常需要大量的手动数据采集和验证，尤其是在检测细微的行为细微差别或事件时。在现实世界的应用中，很难区分日常行为和潜在风险，比如区分日常购物和潜在的商店行窃，这使过程更加复杂。我们提出了一种新的零样本计算机视觉算法Ethosight。Ethosight消除了对预先存在的符号知识的需求，基于用户需求和感兴趣的语义知识从头开始。使用本地化的标签亲和力计算和推理引导的迭代学习循环，Ethosight推断场景细节并迭代细化标签集。推理机制可以从GPT4等大型语言模型、OpenNARS等符号推理器或混合系统中派生。Ethosight进一步利用了预先训练的多模态模型ImageBind的功能，在几个周期内生成准确的图像语义知识。它成功地有效地捕捉到了明确和细微的元素。我们还介绍了Korzybski的“时间绑定”概念在机器中的实现，该概念允许跨部署进行世代学习和知识共享。我们的评估证明了Ethosight在40个复杂用例中的疗效。它表现出了辨别新的兴趣领域的非凡能力，在1000个标签中的前五个标签中始终产生高亲和力得分。在不同环境中进行的测试证明了Ethosight的强大性能。本文正文和附录中的详细结果和案例研究强调了增强计算机视觉模型在检测和提取细微行为方面的适应性和弹性的良好前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10577v1" target="_blank">2307.10577v1</a>
                              </td>
                              <td>Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning</td>
                              <td>Hugo Latapie</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10577v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10577v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14795v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MotionGPT: Human Motion as a Foreign Language</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14795v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14795v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14795v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multi-modal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this "motion vocabulary", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt learning, we pre-train MotionGPT with a mixture of motion-language data and fine-tune it on prompt-based question-and-answer tasks. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14795v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管预先训练的大型语言模型取得了进展，但迄今为止，为语言和其他多模态数据（如运动）建立统一模型的探索仍然具有挑战性，尚未受到影响。幸运的是，人类的动作表现出类似于人类语言的语义耦合，通常被视为肢体语言的一种形式。通过将语言数据与大规模运动模型相融合，可以提高运动相关任务性能的运动语言预训练变得可行。在这一见解的推动下，我们提出了MotionGPT，这是一个统一、通用且用户友好的运动语言模型，用于处理多个与运动相关的任务。具体而言，我们对人类运动采用离散矢量量化，并将3D运动转换为运动令牌，类似于单词令牌的生成过程。在这个“动作词汇”的基础上，我们以统一的方式对动作和文本进行语言建模，将人的动作视为一种特定的语言。此外，受即时学习的启发，我们使用混合的运动语言数据预训练MotionGPT，并在基于提示的问答任务中对其进行微调。大量实验表明，MotionGPT在多个运动任务上实现了最先进的性能，包括文本驱动的运动生成、运动字幕、运动预测以及两者之间的运动。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14795v2" target="_blank">2306.14795v2</a>
                              </td>
                              <td>MotionGPT: Human Motion as a Foreign Language</td>
                              <td>Biao Jiang</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14795v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14795v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10549v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Large Language Models on Blockchains</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10549v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10549v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10549v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10549v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>训练和部署大型语言模型需要大量的计算资源，因为语言模型包含数十亿个参数，文本有数千个令牌。另一个问题是大型语言模型是静态的。它们是在培训过程后固定的。为了解决这些问题，在本文中，我们建议在区块链上训练和部署动态大语言模型，该模型具有高计算性能，并分布在计算机网络中。区块链是一个安全、去中心化和透明的系统，允许在不需要中介的情况下创建交易的防篡改账本。动态大语言模型可以在训练过程后不断地从用户输入中学习。我们的方法为开发大型语言模型提供了一种新的方法，也为下一代人工智能系统提供了启示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10549v1" target="_blank">2307.10549v1</a>
                              </td>
                              <td>Dynamic Large Language Models on Blockchains</td>
                              <td>Yuanhao Gong</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10549v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10549v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00470v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00470v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00470v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00470v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models(LLMS)have shown excellent text generation capabilities, capable of generating fluent human-like responses for many downstream tasks. However, applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To cope with the above challenges, this paper proposes PatternGPT, a pattern-driven text generation framework for Large Language Models. Firstly, the framework utilizes the extraction capability of Large Language Models to generate rich and diversified structured and formalized patterns, which facilitates the introduction of external knowledge to do the computation, and then draws on the idea of federated learning to use multiple agents to achieve the sharing in order to obtain more diversified patterns, and finally uses judgment criteria and optimization algorithm to search for high-quality patterns to guide the generation of models. Finally, external knowledge such as judgment criteria and optimization algorithms are used to search for high-quality patterns, and the searched patterns are used to guide model generation. This framework has the advantages of generating diversified patterns, protecting data privacy, combining external knowledge, and improving the quality of generation, which provides an effective method to optimize the text generation capability of large language models, and make it better applied to the field of intelligent dialogue and content generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00470v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLMS）显示出出色的文本生成能力，能够为许多下游任务生成流畅的类人响应。然而，将大型语言模型应用于现实世界的关键任务仍然具有挑战性，因为它们容易产生幻觉，并且无法直接使用外部知识。为了应对上述挑战，本文提出了模式驱动的大型语言模型文本生成框架PatternGPT。首先，该框架利用大型语言模型的提取能力生成丰富多样的结构化和形式化模式，便于引入外部知识进行计算，然后借鉴联合学习的思想，使用多个代理实现共享，以获得更多样的模式，最后利用判断标准和优化算法搜索高质量的模式来指导模型的生成。最后，使用判断标准和优化算法等外部知识来搜索高质量的模式，并使用搜索到的模式来指导模型生成。该框架具有生成多样化模式、保护数据隐私、结合外部知识、提高生成质量等优点，为优化大型语言模型的文本生成能力提供了一种有效的方法，使其更好地应用于智能对话和内容生成领域。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00470v4" target="_blank">2307.00470v4</a>
                              </td>
                              <td>PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation</td>
                              <td>Le Xiao</td>
                              <td>2023-07-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00470v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00470v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10168v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10168v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10168v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10168v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross-comparing their performances on sub-tasks) and (2) LLMs' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10168v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LLM在众包任务中复制类人行为方面表现出了希望，而这些任务以前被认为是人类能力独有的。然而，目前的工作主要集中在简单的原子任务上。我们探讨LLM是否可以复制更复杂的众包渠道。我们发现，现代LLM可以在这些“人类计算算法”中模拟众包工作者的一些能力，但成功的程度是可变的，并受到请求者对LLM能力的理解、子任务所需的特定技能以及执行这些子任务的最佳交互模式的影响。我们反思了人类和LLM对指令的不同敏感性，强调了为LLM提供面向人类的保障的重要性，并讨论了用互补的技能培训人类和LLMs的潜力。至关重要的是，我们表明，复制众包管道提供了一个有价值的平台来研究（1）LLM在不同任务上的相对优势（通过交叉比较它们在子任务上的表现）和（2）LLM对复杂任务的潜力，在复杂任务中，它们可以完成部分任务，而将其他任务留给人类。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10168v2" target="_blank">2307.10168v2</a>
                              </td>
                              <td>LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs</td>
                              <td>Tongshuang Wu</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10168v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10168v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_11296v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_11296v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_11296v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_11296v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_11296v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们使用即时工程来指导ChatGPT从不同格式和风格的科学文献中自动挖掘金属有机框架（MOFs）合成条件的文本。这有效地缓解了ChatGPT产生信息幻觉的倾向——这一问题此前使大型语言模型（LLM）在科学领域的使用具有挑战性。我们的方法包括开发一个工作流，实现由ChatGPT自己编程的三种不同的文本挖掘过程。所有这些都可以实现解析、搜索、过滤、分类、摘要和数据统一，并在劳动力、速度和准确性之间进行不同的权衡。我们部署该系统来提取26257个不同的合成参数，这些参数与来自同行评审研究文章的大约800个MOFs有关。该过程结合了我们的ChemPrompt工程策略，指导ChatGPT进行文本挖掘，从而获得了90-99%的令人印象深刻的精度、召回率和F1分数。此外，利用文本挖掘构建的数据集，我们构建了一个机器学习模型，在预测MOF实验结晶结果和初步识别MOF结晶中的重要因素方面，准确率超过86%。我们还开发了一个可靠的基于数据的MOF聊天机器人，以回答有关化学反应和合成程序的问题。考虑到使用ChatGPT的过程以统一的格式可靠地挖掘和制表各种MOF合成信息，同时只使用不需要编码专业知识的叙述语言，我们预计我们的ChatGPT化学助理将在其他各种化学子学科中非常有用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.11296v2" target="_blank">2306.11296v2</a>
                              </td>
                              <td>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis</td>
                              <td>Zhiling Zheng</td>
                              <td>2023-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_11296v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.11296v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02288v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02288v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02288v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02288v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a performance comparison of three large language models (LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat (BingChat), and Google Bard, on the VNHSGE English dataset. The performance of BingChat, Bard, and ChatGPT (GPT-3.5) is 92.4\%, 86\%, and 79.2\%, respectively. The results show that BingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not yet officially available in Vietnam. The results also indicate that BingChat, Bard and ChatGPT outperform Vietnamese students in English language proficiency. The findings of this study contribute to the understanding of the potential of LLMs in English language education. The remarkable performance of ChatGPT, BingChat, and Bard demonstrates their potential as effective tools for teaching and learning English at the high school level.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02288v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文在VNHSGE英语数据集上对OpenAI ChatGPT、Microsoft Bing Chat和Google Bard三种大型语言模型（LLM）的性能进行了比较。BingChat、Bard和ChatGPT（GPT-3.5）的性能分别为92.4%、86%和79.2%。结果表明，BingChat优于ChatGPT和Bard。因此，BingChat和Bard可以取代ChatGPT，而ChatGPT尚未在越南正式推出。研究结果还表明，BingChat、Bard和ChatGPT在英语水平上优于越南学生。本研究的发现有助于理解LLM在英语教育中的潜力。ChatGPT、BingChat和Bard的出色表现表明了他们作为高中英语教学有效工具的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02288v3" target="_blank">2307.02288v3</a>
                              </td>
                              <td>Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard</td>
                              <td>Xuan-Quy Dao</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02288v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02288v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10512v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">IvyGPT: InteractiVe Chinese pathwaY language model in medical domain</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10512v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10512v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10512v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10512v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>诸如ChatGPT之类的通用大型语言模型（LLM）已经显示出显著的成功。然而，由于准确性差且无法提供医疗建议，此类LLM尚未被广泛用于医疗目的。我们提出了IvyGPT，这是一种基于LLaMA的LLM，通过高质量的医学问答（QA）实例和来自人类反馈的强化学习（RLHF）进行了训练和微调。经过监督微调，IvyGPT具有良好的多回合对话能力，但在其他方面，如综合诊断，它无法像医生一样表现。通过RLHF，IvyGPT可以输出更丰富、更接近人类的诊断和治疗答案。在训练中，我们使用QLoRA在少量NVIDIA A100（80GB）GPU上训练了330亿个参数。实验结果表明，IvyGPT的性能优于其他医学GPT模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10512v1" target="_blank">2307.10512v1</a>
                              </td>
                              <td>IvyGPT: InteractiVe Chinese pathwaY language model in medical domain</td>
                              <td>Rongsheng Wang</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10512v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10512v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09702v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Guided Generation for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09702v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09702v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09702v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09702v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们描述了一种使用正则表达式和上下文无关语法来指导语言模型文本生成的有效方法。我们的方法几乎不增加令牌序列生成过程的开销，并使引导生成在实践中可行。开源Python库Outlines中提供了一个实现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09702v2" target="_blank">2307.09702v2</a>
                              </td>
                              <td>Efficient Guided Generation for Large Language Models</td>
                              <td>Brandon T. Willard</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09702v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09702v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08272v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT is Good but Bing Chat is Better for Vietnamese Students</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08272v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08272v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08272v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study examines the efficacy of two SOTA large language models (LLMs), namely ChatGPT and Microsoft Bing Chat (BingChat), in catering to the needs of Vietnamese students. Although ChatGPT exhibits proficiency in multiple disciplines, Bing Chat emerges as the more advantageous option. We conduct a comparative analysis of their academic achievements in various disciplines, encompassing mathematics, literature, English language, physics, chemistry, biology, history, geography, and civic education. The results of our study suggest that BingChat demonstrates superior performance compared to ChatGPT across a wide range of subjects, with the exception of literature, where ChatGPT exhibits better performance. Additionally, BingChat utilizes the more advanced GPT-4 technology in contrast to ChatGPT, which is built upon GPT-3.5. This allows BingChat to improve to comprehension, reasoning and generation of creative and informative text. Moreover, the fact that BingChat is accessible in Vietnam and its integration of hyperlinks and citations within responses serve to reinforce its superiority. In our analysis, it is evident that while ChatGPT exhibits praiseworthy qualities, BingChat presents a more apdated solutions for Vietnamese students.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08272v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究考察了两种SOTA大型语言模型（LLM），即ChatGPT和Microsoft Bing Chat（BingChat），在满足越南学生需求方面的有效性。尽管ChatGPT在多个学科方面表现出熟练程度，但Bing聊天是更有利的选择。我们对他们在数学、文学、英语、物理、化学、生物、历史、地理和公民教育等各个学科的学术成就进行了比较分析。我们的研究结果表明，与ChatGPT相比，BingChat在广泛的受试者中表现出优异的表现，但文献除外，其中ChatGPT表现出更好的表现。此外，与基于GPT-3.5的ChatGPT相比，BingChat使用了更先进的GPT-4技术。这使得BingChat能够提高对创造性和信息性文本的理解、推理和生成。此外，BingChat在越南是可访问的，它在回复中集成了超链接和引用，这有助于增强其优势。在我们的分析中，很明显，虽然ChatGPT表现出了值得称赞的品质，但BingChat为越南学生提供了更先进的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08272v2" target="_blank">2307.08272v2</a>
                              </td>
                              <td>ChatGPT is Good but Bing Chat is Better for Vietnamese Students</td>
                              <td>Xuan-Quy Dao</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08272v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08272v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10490v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10490v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10490v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10490v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10490v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们演示了如何在多模态LLM中使用图像和声音进行间接提示和指令注入。攻击者生成与提示相对应的对抗性扰动，并将其混合到图像或音频记录中。当用户向（未修改的、良性的）模型询问受干扰的图像或音频时，干扰会引导模型输出攻击者选择的文本和/或使后续对话框遵循攻击者的指示。我们用几个针对LLaVa和PandaGPT的概念验证示例来说明这种攻击。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10490v1" target="_blank">2307.10490v1</a>
                              </td>
                              <td>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</td>
                              <td>Eugene Bagdasaryan</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10490v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10490v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10485v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10485v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10485v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10485v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10485v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在理解和生成类似人类的文本方面表现出了非凡的熟练程度，这可能会彻底改变金融业。然而，现有的LLM在金融领域往往不足，这主要归因于一般文本数据和金融文本数据之间的差异。不幸的是，可用的金融文本数据集数量有限（规模很小），而BloombergGPT，第一个金融LLM（FinLLM），来源很近（只发布了训练日志）。有鉴于此，我们的目标是使LLM的互联网规模财务数据民主化，这是一个公开的挑战，因为数据来源多样，信噪比低，时间有效性高。为了应对这些挑战，我们引入了一个开源且以数据为中心的框架\textit｛Financial Generative Pre-trained Transformer（FinGPT）｝，该框架自动收集和管理来自互联网上34个不同来源的实时金融数据，为研究人员和从业者提供可访问且透明的资源来开发他们的FinLLM。此外，我们提出了一种简单而有效的策略，利用市场的固有反馈对FinLLM进行微调，称为股价强化学习（RLSP）。我们还采用了低秩自适应（LoRA，QLoRA）方法，使用户能够以低成本从开源通用LLM中定制自己的FinLLM。最后，我们展示了几个FinGPT应用程序，包括机器人顾问、算法交易的情绪分析和低代码开发。FinGPT旨在使FinLLM民主化，刺激创新，并释放开放金融的新机遇。代码可在https://github.com/AI4Finance-Foundation/FinGPT和https://github.com/AI4Finance-Foundation/FinNLP</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10485v1" target="_blank">2307.10485v1</a>
                              </td>
                              <td>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</td>
                              <td>Xiao-Yang Liu</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10485v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10485v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10476v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What can we learn from Data Leakage and Unlearning for Law?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10476v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10476v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10476v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference. A company can train an LLM on its domain-customized data which can potentially also include their users' PII. In order to comply with privacy laws such as the "right to be forgotten", the data points of users that are most vulnerable to extraction could be deleted. We find that once the most vulnerable points are deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to understanding memorization for fine-tuned models. In this work, we also show that not only do fine-tuned models leak their training data but they also leak the pre-training data (and PII) memorized during the pre-training phase. The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-tuned models can pose significant privacy and legal concerns for companies that use LLMs to offer services. We hope this work will start an interdisciplinary discussion within AI and law communities regarding the need for policies to tackle these issues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10476v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）有一个隐私问题，因为它们会记住训练数据（包括电子邮件和电话号码等个人身份信息），并在推理过程中泄露。一家公司可以在其领域定制数据上训练LLM，这些数据可能还包括其用户的PII。为了遵守隐私法，例如“被遗忘权”，可以删除最容易被提取的用户的数据点。我们发现，一旦最脆弱的点被删除，一组新的点就容易被提取。到目前为止，很少有人关注如何理解微调模型的记忆。在这项工作中，我们还表明，微调模型不仅会泄露其训练数据，而且还会泄露在预训练阶段记忆的预训练数据（和PII）。新数据点的属性在忘记后变得容易被提取，并且通过微调模型泄露预训练数据，这可能会给使用LLM提供服务的公司带来重大的隐私和法律问题。我们希望这项工作将在人工智能和法律界开始一场跨学科的讨论，讨论解决这些问题的政策的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10476v1" target="_blank">2307.10476v1</a>
                              </td>
                              <td>What can we learn from Data Leakage and Unlearning for Law?</td>
                              <td>Jaydeep Borkar</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10476v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10476v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10472v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10472v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10472v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10472v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10472v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着语言模型应用程序的广度和深度不断快速扩展，建立有效的框架来衡量和减轻这些模型的习得或继承的社会偏见变得越来越重要。本文介绍了我们的工作，旨在评估教学微调语言模型通过零样本提示（包括思维链（CoT）提示）识别偏差的能力。在LLaMA及其两个指令微调版本中，Alpaca 7B在偏差识别任务中表现最好，准确率为56.7%。我们还证明，扩大LLM大小和数据多样性可以进一步提高性能。这是一项正在进行的工作，介绍了我们的偏见缓解框架的第一个组成部分。随着我们取得更多成果，我们将不断更新这项工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10472v1" target="_blank">2307.10472v1</a>
                              </td>
                              <td>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</td>
                              <td>Omkar Dige</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10472v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10472v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10442v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Thrust: Adaptively Propels Large Language Models with External Knowledge</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10442v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10442v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10442v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that thrust is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve significantly higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge on 88% of the evaluated tasks with 26% average performance improvement. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited knowledge-seeking budget due to computation latency or costs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10442v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大规模预训练语言模型（PTLMs）被证明在其模型参数中编码丰富的知识，但PTLMs中的固有知识可能是不透明的或静态的，这使得外部知识是必要的。然而，现有的信息检索技术可能成本高昂，甚至可能引入嘈杂且有时具有误导性的知识。为了应对这些挑战，我们提出了外部知识的实例级自适应推进（IAPEK），其中我们只在必要时进行检索。为了实现这一目标，我们建议用一种新的度量Thrust来衡量PTLM是否包含足够的知识来解决一个实例，该度量利用了少量可见实例的表示分布。大量实验表明，推力是PTLM模型实例级知识能力的良好度量。此外，在88%的评估任务中，使用Thrust分数作为检索指标，我们可以获得比单纯使用外部知识高得多的成本效率，平均性能提高26%。这些发现揭示了由于计算延迟或成本的原因，知识寻求预算有限的知识增强LMs的现实实践。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10442v1" target="_blank">2307.10442v1</a>
                              </td>
                              <td>Thrust: Adaptively Propels Large Language Models with External Knowledge</td>
                              <td>Xinran Zhao</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10442v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10442v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10432v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PharmacyGPT: The AI Pharmacist</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10432v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10432v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10432v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10432v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们介绍了PharmacyGPT，这是一个新的框架，用于评估大型语言模型（LLM）（如ChatGPT和GPT-4）在模拟临床药剂师角色方面的能力。我们的方法包括利用LLM生成可理解的患者集群、制定药物计划和预测患者结果。我们使用从北卡罗来纳大学教堂山医院重症监护室（ICU）获得的真实数据进行调查。我们的分析为LLM在临床药学领域的潜在应用和局限性提供了宝贵的见解，对患者护理和未来人工智能驱动的医疗保健解决方案的开发都有意义。通过评估PharmacyGPT的性能，我们的目标是为围绕人工智能在医疗环境中的集成进行的讨论做出贡献，最终促进此类技术的负责任和有效使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10432v1" target="_blank">2307.10432v1</a>
                              </td>
                              <td>PharmacyGPT: The AI Pharmacist</td>
                              <td>Zhengliang Liu</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10432v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10432v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10169v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Challenges and Applications of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10169v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10169v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10169v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10169v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>几年内，大型语言模型（LLM）在机器学习话语中从不存在变成了普遍存在。由于该领域的快速发展，很难确定剩余的挑战和已经富有成果的应用领域。在本文中，我们的目标是建立一套系统的开放性问题和应用成功，以便ML研究人员能够更快地了解该领域的现状并变得富有成效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10169v1" target="_blank">2307.10169v1</a>
                              </td>
                              <td>Challenges and Applications of Large Language Models</td>
                              <td>Jean Kaddour</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10169v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10169v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10349v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Twits, Toxic Tweets, and Tribal Tendencies: Trends in Politically Polarized Posts on Twitter</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10349v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10349v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10349v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Social media platforms are often blamed for exacerbating political polarization and worsening public dialogue. Many claim hyperpartisan users post pernicious content, slanted to their political views, inciting contentious and toxic conversations. However, what factors, actually contribute to increased online toxicity and negative interactions? In this work, we explore the role that political ideology plays in contributing to toxicity both on an individual user level and a topic level on Twitter. To do this, we train and open-source a DeBERTa-based toxicity detector with a contrastive objective that outperforms the Google Jigsaw Persective Toxicity detector on the Civil Comments test dataset. Then, after collecting 187 million tweets from 55,415 Twitter users, we determine how several account-level characteristics, including political ideology and account age, predict how often each user posts toxic content. Running a linear regression, we find that the diversity of views and the toxicity of the other accounts with which that user engages has a more marked effect on their own toxicity. Namely, toxic comments are correlated with users who engage with a wider array of political views. Performing topic analysis on the toxic content posted by these accounts using the large language model MPNet and a version of the DP-Means clustering algorithm, we find similar behavior across 6,592 individual topics, with conversations on each topic becoming more toxic as a wider diversity of users become involved.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10349v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>社交媒体平台经常被指责为加剧政治两极分化和恶化公众对话的罪魁祸首。许多人声称，超党派用户发布了有害的内容，倾向于他们的政治观点，煽动了有争议和有毒的对话。然而，究竟是什么因素导致了网络毒性和负面相互作用的增加？在这项工作中，我们探讨了政治意识形态在推特上的个人用户层面和话题层面上对毒性的影响。为此，我们训练并开源了一种基于DeBERTa的毒性检测器，其对比目标在Civil Comments测试数据集上优于Google Jigsaw Persive toxicity检测器。然后，在收集了55415名推特用户的1.87亿条推文后，我们确定了几个账户级别的特征，包括政治意识形态和账户年龄，如何预测每个用户发布有毒内容的频率。通过线性回归，我们发现该用户参与的其他账户的观点多样性和毒性对其自身的毒性有更显著的影响。也就是说，有毒评论与参与更广泛政治观点的用户有关。使用大型语言模型MPNet和DP Means聚类算法对这些账户发布的有毒内容进行主题分析，我们发现6592个单独主题的行为相似，随着用户的广泛多样性，每个主题的对话变得更加有毒。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10349v1" target="_blank">2307.10349v1</a>
                              </td>
                              <td>Twits, Toxic Tweets, and Tribal Tendencies: Trends in Politically Polarized Posts on Twitter</td>
                              <td>Hans W. A. Hanley</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10349v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10349v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10348v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Code Detection for Hardware Acceleration Using Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10348v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10348v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10348v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.   This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.   Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10348v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经被大规模应用于许多任务，通常超过了最先进的方法。尽管它们在代码生成中的有效性已被广泛研究（例如，AlphaCode），但它们在代码检测方面的潜力仍未被探索。这项工作首次分析了使用LLM的代码检测。我们的研究考察了在C/C++中实现的基本内核，包括矩阵乘法、卷积和快速傅立叶变换。我们提出了一种初步的、幼稚的提示和一种新的代码检测提示策略。结果表明，由于大量假阳性，传统提示实现了很高的精度，但准确性较差（GEMM、卷积和FFT分别为68.8%、22.3%和79.2%）。我们的新提示策略大大减少了误报，从而获得了出色的总体准确率（分别为91.1%、97.9%和99.7%）。这些结果对现有的最先进的代码检测方法提出了相当大的挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10348v1" target="_blank">2307.10348v1</a>
                              </td>
                              <td>Code Detection for Hardware Acceleration Using Large Language Models</td>
                              <td>Pablo Antonio Martínez</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10348v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10348v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09288v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Llama 2: Open Foundation and Fine-Tuned Chat Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09288v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09288v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09288v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09288v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们开发并发布了Llama 2，这是一组经过预训练和微调的大型语言模型（LLM），其参数范围从70亿到700亿。我们经过微调的LLM，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准测试中都优于开源聊天模型，并且基于我们对有用性和安全性的人工评估，可能是封闭源代码模型的合适替代品。我们详细描述了我们对Llama 2-Chat的微调和安全改进方法，以使社区能够在我们的工作基础上再接再厉，为LLM的负责任发展做出贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09288v2" target="_blank">2307.09288v2</a>
                              </td>
                              <td>Llama 2: Open Foundation and Fine-Tuned Chat Models</td>
                              <td>Hugo Touvron</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09288v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09288v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10337v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10337v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10337v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10337v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and user-friendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMs-driven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide primary insights for the research and governance of LLMs-driven social bots within the research community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10337v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型（LLM）的出现，它们不仅有助于在更有效的范式中完成传统任务，还刺激了社交机器人的进化。研究人员已经开始探索LLM作为社交机器人的驱动核心的实现，使其能够更高效、更用户友好地完成个人资料完成、社交行为决策和社交内容生成等任务。然而，目前缺乏对LLM驱动的社交机器人的行为特征及其对社交网络的影响的系统研究。我们整理了Chirper的数据，这是一个类似推特的社交网络，由LLM驱动的社交机器人组成，并开始了一项探索性研究。我们的研究结果表明：（1）LLMs驱动的社交机器人在表现出某些集体特征的同时，具有增强的个人层面伪装；（2） 这些机器人有能力通过有毒行为对在线社区施加影响；（3） 现有的检测方法适用于LLM驱动的社交机器人的活动环境，但可能在有效性方面受到某些限制。此外，我们将研究中收集的数据组织到我们公开发布的Masquerade-23数据集中，从而解决了LLM驱动的社交机器人行为数据集子领域的数据空白。我们的研究成果为研究社区内LLM驱动的社交机器人的研究和治理提供了初步见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10337v1" target="_blank">2307.10337v1</a>
                              </td>
                              <td>Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks</td>
                              <td>Siyu Li</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10337v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10337v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10022v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10022v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10022v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10022v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scripts are available on GitHub.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10022v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我提出了一个开放的欧盟和英国国家级历史民意调查数据集。该数据集旨在填补欧盟现有民意调查数据的空白。一些现有的数据集仅限于过去五年，限制了研究机会。同时，存在一些更大的专有数据集，但仅以视觉预处理的时间序列格式提供。最后，虽然可能存在针对个别国家的其他大型数据集，但由于语言障碍，这些数据集可能无法访问。这些数据是从维基百科收集的，并使用熊猫库进行预处理。原始数据和预处理数据均为.csv格式。我希望，鉴于LLM和深度学习的最新进展，这个大型数据集将使研究人员能够揭示多模式数据（新闻文章、经济指标、社交媒体）与投票行为之间的复杂互动。GitHub上提供了原始数据、预处理数据和预处理脚本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10022v1" target="_blank">2307.10022v1</a>
                              </td>
                              <td>Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</td>
                              <td>Konstantinos Pitas</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10022v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10022v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09998v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generating Mathematical Derivations with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09998v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09998v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09998v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations and over 200 derivations to highlight common reasoning errors such as the inclusion of incorrect, irrelevant, and redundant equations, along with the tendency to skip derivation steps. Finally, we explore the suitability of existing metrics for evaluating mathematical derivations finding evidence that, while they capture general properties such as sensitivity to perturbations, they fail to highlight fine-grained reasoning errors and essential differences between models. Overall, this work demonstrates that training models on synthetic data can improve their mathematical capabilities beyond larger architectures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09998v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大型语言模型（LLM）在专业领域推导数学结果是一个新兴的研究方向，可以帮助识别模型的局限性，并有可能支持数学发现。在本文中，我们利用符号引擎在规模上生成方程的导数，并研究LLM在从前提推导目标方程时的能力。具体而言，我们采用了GPT的上下文学习，并对一系列T5模型进行了微调，以比较预训练策略与专业模型的稳健性和通用性。经验结果表明，微调FLAN-T5-大（MathT5）在所有静态和分布外测试集上的绝对性能都优于GPT模型。然而，深入的分析表明，微调模型对涉及看不见的符号的扰动和（在较小程度上）方程结构的变化更敏感。此外，我们分析了1.7K方程和200多个推导，以突出常见的推理错误，如包含不正确、不相关和冗余的方程，以及跳过推导步骤的趋势。最后，我们探讨了现有指标用于评估数学推导的适用性，发现有证据表明，尽管它们捕捉了对扰动的敏感性等一般特性，但未能突出细粒度推理错误和模型之间的本质差异。总的来说，这项工作表明，在合成数据上训练模型可以提高其数学能力，超越更大的体系结构。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09998v1" target="_blank">2307.09998v1</a>
                              </td>
                              <td>Generating Mathematical Derivations with Large Language Models</td>
                              <td>Jordan Meadows</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09998v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09998v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15056v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15056v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15056v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15056v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15056v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多NLP应用程序需要对各种任务进行手动数据注释，尤其是训练分类器或评估无监督模型的性能。根据任务的规模和复杂程度，这些任务可能由MTurk等平台上的人群工作者以及研究助理等训练有素的注释员执行。使用2382条推文的样本，我们证明了ChatGPT在几个注释任务方面优于群组工作者，包括相关性、立场、主题和帧检测。具体来说，ChatGPT在五分之四的任务中，零样本的准确性超过了众工作者，而ChatGPT的码间协议在所有任务中都超过了众劳动者和训练有素的注释者。此外，ChatGPT的每次注释成本不到0.003美元，大约是MTurk的20倍。这些结果显示了大型语言模型在大幅提高文本分类效率方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15056v2" target="_blank">2303.15056v2</a>
                              </td>
                              <td>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks</td>
                              <td>Fabrizio Gilardi</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15056v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15056v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09950v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prompting for Automatic Log Template Extraction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09950v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09950v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09950v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Log parsing, the initial and vital stage in automated log analysis, involves extracting log templates from semi-structured logs to generate structured logs. Nonetheless, current log parsers are limited in effectiveness due to two primary reasons. Firstly, traditional data-driven log parsers heavily rely on heuristics or manually crafted features provided by domain experts, which may not consistently yield optimal performance when applied to diverse log systems. Secondly, existing deep learning-based log parsers necessitate model tuning, which is typically confined to training samples and leads to suboptimal performance across the entire log source. To overcome these limitations, we propose a precise log parsing framework named LogDiv, which leverages the in-context inference capability of large language models. Specifically, LogDiv extracts the hidden semantics from multiple log examples through prompt demonstrations. Without the need for model tuning, LogDiv can directly generate a log template for the target log message by leveraging the semantics provided in the prompt context. Additionally, we introduce a simple yet effective prompt format for extracting the output and enhancing the quality of the generated log templates. To validate the performance of LogDiv, we conducted experiments using 16 widely-used public datasets. The results show that LogDiv achieves state-of-the-art performance with an average parsing accuracy of 97.7%, precision template accuracy of 88.1%, and recall template accuracy of 90.8%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09950v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>日志解析是自动化日志分析的初始和关键阶段，涉及从半结构化日志中提取日志模板以生成结构化日志。尽管如此，由于两个主要原因，当前的日志解析器的有效性有限。首先，传统的数据驱动日志解析器在很大程度上依赖于领域专家提供的启发式或手动构建的功能，当应用于不同的日志系统时，这些功能可能无法始终产生最佳性能。其次，现有的基于深度学习的日志解析器需要进行模型调整，这通常仅限于训练样本，并导致整个日志源的性能不理想。为了克服这些限制，我们提出了一个名为LogDiv的精确日志解析框架，该框架利用了大型语言模型的上下文推理能力。具体来说，LogDiv通过提示演示从多个日志示例中提取隐藏语义。在不需要模型调优的情况下，LogDiv可以利用提示上下文中提供的语义，直接为目标日志消息生成日志模板。此外，我们还介绍了一种简单而有效的提示格式，用于提取输出并提高生成的日志模板的质量。为了验证LogDiv的性能，我们使用16个广泛使用的公共数据集进行了实验。结果表明，LogDiv实现了最先进的性能，平均解析准确率为97.7%，精确模板准确率为88.1%，召回模板准确度为90.8%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09950v1" target="_blank">2307.09950v1</a>
                              </td>
                              <td>Prompting for Automatic Log Template Extraction</td>
                              <td>Junjielong Xu</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09950v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09950v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02486v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LongNet: Scaling Transformers to 1,000,000,000 Tokens</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02486v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02486v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02486v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02486v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>缩放序列长度已经成为大型语言模型时代的一个关键需求。然而，现有的方法要么难以解决计算复杂性，要么难以解决模型表达性问题，导致最大序列长度受到限制。为了解决这个问题，我们引入了LongNet，这是一种Transformer变体，可以将序列长度扩展到超过10亿个令牌，而不会牺牲较短序列的性能。具体地说，我们提出了扩展注意力，它随着距离的增长，注意力场呈指数级扩展。LongNet具有显著的优点：1）它具有线性计算复杂度和序列中任意两个令牌之间的对数依赖性；2） 对于超长序列，它可以作为分布式训练器；3） 它的扩展注意力是标准注意力的替代品，可以与现有的基于Transformer的优化无缝集成。实验结果表明，LongNet在长序列建模和通用语言任务上都有很强的性能。我们的工作为建模超长序列开辟了新的可能性，例如，将整个语料库甚至整个互联网视为一个序列。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02486v2" target="_blank">2307.02486v2</a>
                              </td>
                              <td>LongNet: Scaling Transformers to 1,000,000,000 Tokens</td>
                              <td>Jiayu Ding</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02486v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02486v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09923v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Large Language Models can accomplish Business Process Management Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09923v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09923v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09923v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09923v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>业务流程管理（BPM）旨在通过管理底层流程来改进组织活动及其结果。为了实现这一点，通常需要考虑来自各种来源的信息，包括非结构化文本文档。因此，研究人员开发了几种特定于BPM的解决方案，使用自然语言处理技术从文本文档中提取信息。这些解决方案针对各自的任务，不能作为通用仪器来解决多个与过程相关的问题。然而，鉴于最近出现的具有非凡推理能力的大型语言模型（LLM），这样一种具有多种应用程序的通用工具现在似乎是可以实现的。在本文中，我们展示了LLM如何通过将特定的LLM应用于三个示例任务来完成与文本相关的BPM任务：从文本描述中挖掘命令式流程模型，从文本描述挖掘声明性流程模型，以及从文本描述评估流程任务对机器人流程自动化的适用性。我们表明，在没有广泛配置或即时工程的情况下，LLM的性能与现有解决方案相当或更好，并讨论了对未来BPM研究和实际使用的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09923v1" target="_blank">2307.09923v1</a>
                              </td>
                              <td>Large Language Models can accomplish Business Process Management Tasks</td>
                              <td>Michael Grohs</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09923v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09923v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09909v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Chit-Chat or Deep Talk: Prompt Engineering for Process Mining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09909v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09909v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09909v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This research investigates the application of Large Language Models (LLMs) to augment conversational agents in process mining, aiming to tackle its inherent complexity and diverse skill requirements. While LLM advancements present novel opportunities for conversational process mining, generating efficient outputs is still a hurdle. We propose an innovative approach that amend many issues in existing solutions, informed by prior research on Natural Language Processing (NLP) for conversational agents. Leveraging LLMs, our framework improves both accessibility and agent performance, as demonstrated by experiments on public question and data sets. Our research sets the stage for future explorations into LLMs' role in process mining and concludes with propositions for enhancing LLM memory, implementing real-time user testing, and examining diverse data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09909v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究研究了大型语言模型（LLM）在过程挖掘中增强会话代理的应用，旨在解决其固有的复杂性和多样化的技能需求。虽然LLM的进步为会话过程挖掘提供了新的机会，但生成高效的输出仍然是一个障碍。我们提出了一种创新的方法，根据先前对会话代理的自然语言处理（NLP）的研究，修正了现有解决方案中的许多问题。利用LLM，我们的框架提高了可访问性和代理性能，正如在公共问题和数据集上的实验所证明的那样。我们的研究为未来探索LLM在过程挖掘中的作用奠定了基础，并提出了增强LLM内存、实现实时用户测试和检查不同数据集的建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09909v1" target="_blank">2307.09909v1</a>
                              </td>
                              <td>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining</td>
                              <td>Urszula Jessen</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09909v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09909v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2304_02745v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analysis of Dynamic Voronoi Diagrams in the Hilbert Metric</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02745v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02745v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02745v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Hilbert metric is a projective metric defined on a convex body which generalizes the Cayley-Klein model of hyperbolic geometry to any convex set. In this paper we analyze Hilbert Voronoi diagrams in the Dynamic setting. In addition we introduce dynamic visualization software for Voronoi diagrams in the Hilbert metric on user specified convex polygons.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02745v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Hilbert度量是定义在凸体上的投影度量，它将双曲几何的Cayley-Klein模型推广到任何凸集。本文分析了动态环境下的希尔伯特-沃罗诺伊图。此外，我们还介绍了用户指定凸多边形上希尔伯特度量中Voronoi图的动态可视化软件。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02745v3" target="_blank">2304.02745v3</a>
                              </td>
                              <td>Analysis of Dynamic Voronoi Diagrams in the Hilbert Metric</td>
                              <td>Madeline Bumpus</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02745v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02745v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_00362v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_00362v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_00362v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_00362v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>For a given graph $G$, a depth-first search (DFS) tree $T$ of $G$ is an $r$-rooted spanning tree such that every edge of $G$ is either an edge of $T$ or is between a \textit{descendant} and an \textit{ancestor} in $T$. A graph $G$ together with a DFS tree is called a \textit{lineal topology} $\mathcal{T} = (G, r, T)$. Sam et al. (2023) initiated study of the parameterized complexity of the \textsc{Min-LLT} and \textsc{Max-LLT} problems which ask, given a graph $G$ and an integer $k\geq 0$, whether $G$ has a DFS tree with at most $k$ and at least $k$ leaves, respectively. Particularly, they showed that for the dual parameterization, where the tasks are to find DFS trees with at least $n-k$ and at most $n-k$ leaves, respectively, these problems are fixed-parameter tractable when parameterized by $k$. However, the proofs were based on Courcelle's theorem, thereby making the running times a tower of exponentials. We prove that both problems admit polynomial kernels with $\Oh(k^3)$ vertices. In particular, this implies FPT algorithms running in $k^{\Oh(k)}\cdot n^{O(1)}$ time. We achieve these results by making use of a $\Oh(k)$-sized vertex cover structure associated with each problem. This also allows us to demonstrate polynomial kernels for \textsc{Min-LLT} and \textsc{Max-LLT} for the structural parameterization by the vertex cover number.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_00362v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对于给定的图$G$，$G$的深度优先搜索（DFS）树$T$是一个$r$根的生成树，使得$G$中的每条边要么是$T$的边，要么在$T$中的\textit｛subscendance｝和\textit{祖先｝之间。图$G$与DFS树一起称为\textit｛linear topology｝$\mathcal｛T｝=（G，r，T）$。Sam等人（2023）开始研究\textsc｛Min LLT｝和\textsc{Max LLT}问题的参数化复杂性，这些问题询问，给定图$G$和整数$k\geq 0$，$G$是否具有分别具有最多$k$和至少$k$叶的DFS树。特别地，他们表明，对于对偶参数化，其中任务是分别找到叶数至少为$n-k$和最多为$n-k$的DFS树，当用$k$参数化时，这些问题是固定参数可处理的。然而，这些证明是基于库塞尔定理，从而使运行时间成为指数塔。我们证明了这两个问题都允许具有$\Oh（k^3）$顶点的多项式核。特别地，这意味着FPT算法在$k^｛\Oh（k）｝\cdot n^｛O（1）｝$时间内运行。我们通过使用与每个问题相关的$\Oh（k）$大小的顶点覆盖结构来实现这些结果。这也使我们能够通过顶点覆盖数证明\textsc｛Min LLT｝和\textsc{Max LLT}的结构参数化的多项式核。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.00362v2" target="_blank">2307.00362v2</a>
                              </td>
                              <td>Kernelization for Finding Lineal Topologies (Depth-First Spanning Trees) with Many or Few Leaves</td>
                              <td>Emmanuel Sam</td>
                              <td>2023-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_00362v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.00362v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10515v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10515v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10515v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10515v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record the activity of thousands of neurons across multiple brain regions. This has led to a growing need for computational tools capable of analyzing how task-relevant information is represented and communicated between several brain regions. Partial information decompositions (PIDs) have emerged as one such tool, quantifying how much unique, redundant and synergistic information two or more brain regions carry about a task-relevant message. However, computing PIDs is computationally challenging in practice, and statistical issues such as the bias and variance of estimates remain largely unexplored. In this paper, we propose a new method for efficiently computing and estimating a PID definition on multivariate Gaussian distributions. We show empirically that our method satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher redundancy between visual areas when a stimulus is behaviorally relevant.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10515v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经科学实验技术的最新进展使我们能够同时记录大脑多个区域数千个神经元的活动。这导致人们越来越需要能够分析任务相关信息如何在几个大脑区域之间表示和交流的计算工具。部分信息分解（PID）已经成为一种这样的工具，可以量化两个或多个大脑区域携带的关于任务相关信息的独特、冗余和协同信息。然而，计算PID在实践中在计算上具有挑战性，并且诸如估计的偏差和方差之类的统计问题在很大程度上仍未被探索。在本文中，我们提出了一种新的方法来有效地计算和估计多元高斯分布上的PID定义。我们从经验上证明，我们的方法满足直观的可加性性质，并在一组典型例子中恢复了基本事实，即使在高维下也是如此。我们还首次提出并评估了一种在有限样本量下校正PID估计偏差的方法。最后，我们证明了我们的高斯PID有效地表征了小鼠大脑中区域间的相互作用，当刺激与行为相关时，显示了视觉区域之间更高的冗余度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10515v1" target="_blank">2307.10515v1</a>
                              </td>
                              <td>Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data</td>
                              <td>Praveen Venkatesh</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10515v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10515v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09874v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Agricultural Robotic System: The Automation of Detection and Speech Control</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09874v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09874v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09874v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agriculture industries often face challenges in manual tasks such as planting, harvesting, fertilizing, and detection, which can be time consuming and prone to errors. The "Agricultural Robotic System" project addresses these issues through a modular design that integrates advanced visual, speech recognition, and robotic technologies. This system is comprised of separate but interconnected modules for vision detection and speech recognition, creating a flexible and adaptable solution. The vision detection module uses computer vision techniques, trained on YOLOv5 and deployed on the Jetson Nano in TensorRT format, to accurately detect and identify different items. A robotic arm module then precisely controls the picking up of seedlings or seeds, and arranges them in specific locations. The speech recognition module enhances intelligent human robot interaction, allowing for efficient and intuitive control of the system. This modular approach improves the efficiency and accuracy of agricultural tasks, demonstrating the potential of robotics in the agricultural industry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09874v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>农业行业经常面临种植、收割、施肥和检测等人工任务的挑战，这些任务可能很耗时，而且容易出错。“农业机器人系统”项目通过集成先进视觉、语音识别和机器人技术的模块化设计来解决这些问题。该系统由用于视觉检测和语音识别的独立但互连的模块组成，创造了一个灵活且适应性强的解决方案。视觉检测模块使用在YOLOv5上训练并以TensorRT格式部署在Jetson Nano上的计算机视觉技术来准确检测和识别不同的物品。然后，机械臂模块精确控制幼苗或种子的拾取，并将其安排在特定位置。语音识别模块增强了智能人机交互，实现了对系统的高效直观控制。这种模块化方法提高了农业任务的效率和准确性，展示了机器人在农业行业的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09874v1" target="_blank">2307.09874v1</a>
                              </td>
                              <td>Agricultural Robotic System: The Automation of Detection and Speech Control</td>
                              <td>Yang Wenkai</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09874v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09874v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07873v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07873v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07873v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07873v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07873v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>DNN的对抗性例子（AE）已被证明是可转移的：成功欺骗白盒代理模型的AE也可以欺骗其他具有不同架构的黑盒模型。尽管一系列实证研究为产生高度可转移的AE提供了指导，但其中许多发现缺乏解释，甚至导致建议不一致。在本文中，我们朝着理解对抗性可转移性迈出了进一步的一步，特别关注代理方面。从有趣的小鲁棒性现象开始，用轻度扰动的对抗性样本进行对抗性训练的模型可以作为更好的替代品，我们将其归因于两个主要因素之间的权衡：模型平滑性和梯度相似性。我们的研究重点是它们的联合效应，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测对抗性训练中的数据分布变化解释了梯度相似性的退化。基于这些见解，我们探索了数据扩充和梯度正则化对可转移性的影响，并确定了这种权衡通常存在于各种训练机制中，从而为可转移性背后的调节机制构建了一个全面的蓝图。最后，我们提供了一种构建更好的替代物以提高可转移性的通用途径，该途径同时优化了模型的平滑性和梯度相似性，例如，输入梯度正则化和锐度感知最小化（SAM）的组合，并通过大量实验进行了验证。总之，我们呼吁关注这两个因素对发起有效转移攻击的联合影响，而不是优化其中一个而忽略另一个，并强调操纵代理模型的关键作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07873v2" target="_blank">2307.07873v2</a>
                              </td>
                              <td>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</td>
                              <td>Yechao Zhang</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07873v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07873v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09727v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09727v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09727v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09727v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Estimating displacement vector field via a cost volume computed in the feature space has shown great success in image registration, but it suffers excessive computation burdens. Moreover, existing feature descriptors only extract local features incapable of representing the global semantic information, which is especially important for solving large transformations. To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine discrete optimization method for CT registration that includes a decoupled convex optimization procedure to obtain deformation fields based on a self-supervised anatomical embedding (SAM) feature extractor that captures both local and global information. To be specific, SAMConvex extracts per-voxel features and builds 6D correlation volumes based on SAM features, and iteratively updates a flow field by performing lookups on the correlation volumes with a coarse-to-fine scheme. SAMConvex outperforms the state-of-the-art learning-based methods and optimization-based methods over two inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one intra-patient registration dataset (Lung CT). Moreover, as an optimization-based method, SAMConvex only takes $\sim2$s ($\sim5s$ with instance optimization) for one paired images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09727v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>通过在特征空间中计算的代价体积来估计位移矢量场在图像配准中取得了巨大的成功，但它承受了过多的计算负担。此外，现有的特征描述符只提取无法表示全局语义信息的局部特征，这对于解决大型变换尤为重要。为了解决所讨论的问题，我们提出了SAM凸面，这是一种用于CT配准的快速粗到细离散优化方法，包括一个解耦的凸优化过程，以基于捕获局部和全局信息的自监督解剖嵌入（SAM）特征提取器来获得变形场。具体而言，SAM凸面提取每个体素的特征，并基于SAM特征构建6D相关体积，并通过使用从粗到细的方案对相关体积执行查找来迭代更新流场。在两个患者间配准数据集（腹部CT和头颈部CT）和一个患者内配准数据集中（肺部CT）上，SAM凸面优于最先进的基于学习的方法和基于优化的方法。此外，作为一种基于优化的方法，SAM凸对一对图像只需要$\sim2$s（带有实例优化的$\sim5s$）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09727v1" target="_blank">2307.09727v1</a>
                              </td>
                              <td>SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid</td>
                              <td>Zi Li</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09727v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09727v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09701v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09701v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09701v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09701v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09701v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现代自然语言处理（NLP）系统日益增长的计算需求增加了尖端研究的进入壁垒，同时也带来了严重的环境问题。然而，模型评估和比较方面的实际挑战阻碍了模型效率方面的进展。例如，由于不同机构的可访问性水平不同，硬件很难控制。此外，FLOP等指标的改进往往无法转化为现实应用程序的进步。作为回应，我们介绍了五项全能，这是一个对模型效率进行全面和现实评估的基准。Pentathlon专注于推理，推理在模型生命周期中占计算的大部分。它提供了一个严格控制的硬件平台，旨在反映真实世界的应用场景。它包含了一套针对效率不同方面的指标，包括延迟、吞吐量、内存开销和能耗。Pentathlon还附带了一个软件库，可以无缝集成到任何代码库中并进行评估。五项作为一个标准化、集中化的评估平台，可以大幅减少工作量，进行公平、可重复的效率比较。虽然最初专注于自然语言处理（NLP）模型，但Pentathlon的设计允许灵活扩展到其他领域。我们设想五项运动将在构建高效模型方面激发算法创新，并在未来一代NLP模型的开发中提高对社会和环境影响的认识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09701v1" target="_blank">2307.09701v1</a>
                              </td>
                              <td>Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation</td>
                              <td>Hao Peng</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09701v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09701v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09383v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Soundly Handling Linearity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09383v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09383v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09383v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel approach to soundly combining linear types with effect handlers. Linear type systems statically ensure that resources such as file handles are used exactly once. Effect handlers provide a modular programming abstraction for implementing features ranging from exceptions to concurrency. Whereas linear type systems bake in the assumption that continuations are invoked exactly once, effect handlers allow continuations to be discarded or invoked more than once. This mismatch leads to soundness bugs in existing systems such as the programming language Links, which combines linearity (for session types) with effect handlers. We introduce control flow linearity as a means to ensure that continuations are used in accordance with the linearity of any resources they capture, ruling out such soundness bugs.   We formalise control flow linearity in a System F-style core calculus Feffpop equipped with linear types, effect types, and effect handlers. We define a linearity-aware semantics to formally prove that Feffpop preserves the integrity of linear values in the sense that no linear value is discarded or duplicated. In order to show that control flow linearity can be made practical, we adapt Links based on the design of Feffpop, in doing so fixing a long-standing soundness bug.   Finally, to better expose the potential of control flow linearity, we define an ML-style core calculus Qeffpop, based on qualified types, which requires no programmer provided annotations, and instead relies entirely on type inference to infer control flow linearity. Both linearity and effects are captured by qualified types. Qeffpop overcomes a number of practical limitations of Feffpop, supporting abstraction over linearity, linearity dependencies between type variables, and a much more fine-grained notion of control flow linearity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09383v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种将线性类型与效果处理程序完美结合的新方法。线性类型系统静态地确保文件句柄等资源只使用一次。效果处理程序提供了一个模块化编程抽象，用于实现从异常到并发的各种功能。线性类型系统是假设连续性只被调用一次，而效果处理程序允许连续性被丢弃或多次调用。这种不匹配导致了现有系统中的健全性错误，例如编程语言Links，它将线性（针对会话类型）与效果处理程序相结合。我们引入控制流线性作为一种手段，以确保连续性根据它们捕获的任何资源的线性使用，从而排除此类健全性错误。我们在配备了线性类型、效果类型和效果处理程序的系统F型核心演算Feffpop中正式化了控制流线性。我们定义了一个线性感知语义，以正式证明Feffpop在没有线性值被丢弃或复制的意义上保持了线性值的完整性。为了表明控制流线性可以变得实用，我们在Feffpop的设计基础上对Links进行了调整，从而修复了一个长期存在的健全性缺陷。最后，为了更好地揭示控制流线性的潜力，我们基于限定类型定义了ML风格的核心演算Qeffpop，它不需要程序员提供的注释，而是完全依赖类型推理来推断控制流线性。线性和效果都由合格的类型捕获。Qeffpop克服了Feffpop的许多实际限制，支持对线性的抽象、类型变量之间的线性依赖性，以及更细粒度的控制流线性概念。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09383v1" target="_blank">2307.09383v1</a>
                              </td>
                              <td>Soundly Handling Linearity</td>
                              <td>Wenhao Tang</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09383v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09383v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08598v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Glamour muscles: why having a body is not what it means to be embodied</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08598v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08598v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08598v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Embodiment has recently enjoyed renewed consideration as a means to amplify the faculties of smart machines. Proponents of embodiment seem to imply that optimizing for movement in physical space promotes something more than the acquisition of niche capabilities for solving problems in physical space. However, there is nothing in principle which should so distinguish the problem of action selection in physical space from the problem of action selection in more abstract spaces, like that of language. Rather, what makes embodiment persuasive as a means toward higher intelligence is that it promises to capture, but does not actually realize, contingent facts about certain bodies (living intelligence) and the patterns of activity associated with them. These include an active resistance to annihilation and revisable constraints on the processes that make the world intelligible. To be theoretically or practically useful beyond the creation of niche tools, we argue that "embodiment" cannot be the trivial fact of a body, nor its movement through space, but the perpetual negotiation of the function, design, and integrity of that body$\unicode{x2013}$that is, to participate in what it means to $\textit{constitute}$ a given body. It follows that computer programs which are strictly incapable of traversing physical space might, under the right conditions, be more embodied than a walking, talking robot.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08598v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>实施例最近被重新考虑作为增强智能机器能力的手段。实施方式的支持者似乎暗示，对物理空间中的运动进行优化不仅仅是为了获得解决物理空间中问题的利基能力。然而，原则上没有任何东西可以如此区分物理空间中的动作选择问题和更抽象空间中的行动选择问题，比如语言的问题。相反，化身作为一种迈向更高智力的手段之所以有说服力，是因为它承诺捕捉但实际上并没有意识到关于某些身体（活体智力）及其相关活动模式的偶然事实。其中包括对毁灭的积极抵抗，以及对使世界变得可理解的进程的可修正的限制。为了在理论上或实践上超越小众工具的创造，我们认为“体现”不能是一个物体的琐碎事实，也不能是它在空间中的运动，而是该物体的功能、设计和完整性的永久协商$\unicode{x2013}$，也就是说，参与$\textit{构成}$对给定物体的意义。因此，在适当的条件下，完全无法穿越物理空间的计算机程序可能比一个会走路、会说话的机器人更具体现性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08598v1" target="_blank">2307.08598v1</a>
                              </td>
                              <td>Glamour muscles: why having a body is not what it means to be embodied</td>
                              <td>Shawn L. Beaulieu</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08598v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08598v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08581v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08581v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08581v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08581v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities. As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object. Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image. 2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding. Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human. It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned). Our code, model and dataset are available at https://bubo-gpt.github.io .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08581v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>LLM在通过语言与人类互动方面表现出了非凡的能力，尤其是在使用指令跟随数据方面。LLM的最新进展，如MiniGPT-4、LLaVA和X-LLM，通过结合包括图像、视频和语音在内的多模态输入，进一步扩大了其能力。尽管它们在生成对给定模态信号的精确和详细的语言理解方面是有效的，但这些LLM放弃了对输入的特定部分进行接地的能力，从而只构建了粗粒度映射。然而，文本和其他模态之间明确而信息丰富的对应关系不仅有助于改善用户体验，也有助于扩展多模态LLM的应用场景。因此，我们提出了BuboGPT，这是一种具有视觉基础的多模态LLM，可以在视觉、音频和语言之间进行跨模态交互，提供对视觉对象和其他给定模态的细粒度理解。因此，当BuboGPT为对象生成响应或描述时，它能够指出该对象在图像中的特定位置。我们的贡献有两方面：1）基于SAM的现成视觉基础模块，提取句子中的实体并在图像中找到相应的掩码。2） 两阶段训练方案和指令数据集，以赋予联合文本-图像-音频理解。我们的实验表明，BuboGPT在与人类互动过程中实现了令人印象深刻的多模态理解和视觉基础能力。当由任意模态组合（对齐或未对齐）提供时，它始终表现良好。我们的代码、模型和数据集可在https://bubo-gpt.github.io。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08581v1" target="_blank">2307.08581v1</a>
                              </td>
                              <td>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</td>
                              <td>Yang Zhao</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08581v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08581v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08579v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Scale-Aware Modulation Meet Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08579v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08579v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08579v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a new vision Transformer, Scale-Aware Modulation Transformer (SMT), that can handle various downstream tasks efficiently by combining the convolutional network and vision Transformer. The proposed Scale-Aware Modulation (SAM) in the SMT includes two primary novel designs. Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can capture multi-scale features and expand the receptive field. Secondly, we propose the Scale-Aware Aggregation (SAA) module, which is lightweight but effective, enabling information fusion across different heads. By leveraging these two modules, convolutional modulation is further enhanced. Furthermore, in contrast to prior works that utilized modulations throughout all stages to build an attention-free network, we propose an Evolutionary Hybrid Network (EHN), which can effectively simulate the shift from capturing local to global dependencies as the network becomes deeper, resulting in superior performance. Extensive experiments demonstrate that SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks. Specifically, SMT with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1 accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in 224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with resolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN, the SMT base trained with 1x and 3x schedule outperforms the Swin Transformer counterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by 2.0 and 1.1 mIoU respectively on the ADE20K.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08579v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的视觉转换器——尺度感知调制转换器（SMT），它可以通过将卷积网络和视觉转换器相结合来有效地处理各种下游任务。SMT中提出的尺度感知调制（SAM）包括两个主要的新颖设计。首先，我们介绍了多头混合卷积（MHMC）模块，该模块可以捕捉多尺度特征并扩展感受野。其次，我们提出了规模感知聚合（SAA）模块，该模块重量轻但有效，能够实现不同头部的信息融合。通过利用这两个模块，卷积调制得到了进一步增强。此外，与之前在所有阶段都使用调制来构建无注意力网络的工作相反，我们提出了一种进化混合网络（EHN），它可以有效地模拟随着网络变得更深而从捕获局部依赖性到全局依赖性的转变，从而产生卓越的性能。大量实验表明，SMT在广泛的视觉任务中显著优于现有的最先进的模型。具体而言，具有11.5M/2.4GFLOP和32M/7.7GFLOP的SMT在ImageNet-1K上分别可以实现82.2%和84.3%的top-1准确率。在ImageNet-22K上以224^2分辨率进行预处理后，当以224^2和384^2分辨率进行微调时，它分别达到87.1%和88.1%的前1级精度。对于使用Mask R-CNN的对象检测，使用1x和3x调度训练的SMT基础在COCO上分别比Swin Transformer对应的基础高4.2和1.3mAP。对于UPerNet的语义分割，在ADE20K上，单尺度和多尺度的SMT基测试分别超过Swin 2.0和1.1 mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08579v1" target="_blank">2307.08579v1</a>
                              </td>
                              <td>Scale-Aware Modulation Meet Transformer</td>
                              <td>Weifeng Lin</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08579v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08579v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_00465v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On Grounded Planning for Embodied Tasks with Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_00465v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_00465v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_00465v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear **whether LMs have the capacity to generate grounded, executable plans for embodied tasks.** This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an **evaluation protocol** and design a dedicated metric to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_00465v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言模型（LM）已经证明了它们拥有物理世界常识知识的能力，物理世界是日常生活中执行任务的关键方面。然而，目前尚不清楚**LMs是否有能力为具体任务制定有根据的、可执行的计划。**这是一项具有挑战性的任务，因为LMs缺乏通过视觉和物理环境反馈感知环境的能力。在本文中，我们解决了这一重要的研究问题，并对该主题进行了首次调查。我们的新问题公式名为**G-PlanET**，输入一个高级目标和一个关于特定环境中对象的数据表，然后输出一个循序渐进的可操作计划，供机器人代理遵循。为了便于研究，我们制定了**评估方案**，并设计了一个专门的指标来评估计划的质量。我们的实验表明，使用表对环境进行编码和迭代解码策略可以显著提高LMs的基础规划能力。我们的分析还揭示了有趣而非琐碎的发现。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.00465v3" target="_blank">2209.00465v3</a>
                              </td>
                              <td>On Grounded Planning for Embodied Tasks with Language Models</td>
                              <td>Bill Yuchen Lin</td>
                              <td>2022-08-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_00465v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.00465v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07757v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07757v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07757v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07757v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounded Situation Recognition (GSR) is capable of recognizing and interpreting visual scenes in a contextually intuitive way, yielding salient activities (verbs) and the involved entities (roles) depicted in images. In this work, we focus on the application of GSR in assisting people with visual impairments (PVI). However, precise localization information of detected objects is often required to navigate their surroundings confidently and make informed decisions. For the first time, we propose an Open Scene Understanding (OpenSU) system that aims to generate pixel-wise dense segmentation masks of involved entities instead of bounding boxes. Specifically, we build our OpenSU system on top of GSR by additionally adopting an efficient Segment Anything Model (SAM). Furthermore, to enhance the feature extraction and interaction between the encoder-decoder structure, we construct our OpenSU system using a solid pure transformer backbone to improve the performance of GSR. In order to accelerate the convergence, we replace all the activation functions within the GSR decoders with GELU, thereby reducing the training duration. In quantitative analysis, our model achieves state-of-the-art performance on the SWiG dataset. Moreover, through field testing on dedicated assistive technology datasets and application demonstrations, the proposed OpenSU system can be used to enhance scene understanding and facilitate the independent mobility of people with visual impairments. Our code will be available at https://github.com/RuipingL/OpenSU.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07757v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>背景情境识别（GSR）能够以上下文直观的方式识别和解释视觉场景，产生图像中描绘的显著活动（动词）和相关实体（角色）。在这项工作中，我们重点关注GSR在帮助视力障碍（PVI）患者方面的应用。然而，通常需要检测到的物体的精确定位信息来自信地导航其周围环境并做出明智的决定。我们首次提出了一种开放场景理解（OpenSU）系统，旨在生成相关实体的像素密集分割掩码，而不是边界框。具体来说，我们在GSR的基础上构建了我们的OpenSU系统，此外还采用了高效的分段任何模型（SAM）。此外，为了增强编码器-解码器结构之间的特征提取和交互，我们使用坚固的纯变压器主干构建了我们的OpenSU系统，以提高GSR的性能。为了加速收敛，我们用GELU替换了GSR解码器中的所有激活函数，从而减少了训练持续时间。在定量分析中，我们的模型在SWiG数据集上实现了最先进的性能。此外，通过在专用辅助技术数据集上进行现场测试和应用演示，所提出的OpenSU系统可用于增强场景理解，并促进视障人士的独立行动。我们的代码将在https://github.com/RuipingL/OpenSU.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07757v1" target="_blank">2307.07757v1</a>
                              </td>
                              <td>Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments</td>
                              <td>Ruiping Liu</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07757v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07757v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_11235v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial-Language Attention Policies for Efficient Robot Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_11235v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_11235v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_11235v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_11235v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在语言引导的操作方面取得了长足的进步，但现有的工作一直局限于桌面设置。桌面允许完美和一致的相机角度，这些特性在移动操作中不适用。涉及在环境中移动的任务计划必须对以自我为中心的观点以及把握平面和角度的变化保持稳健。另一个挑战是确保这一切都是真的，同时仍然能够从有限的数据中有效地学习技能。我们提出了空间语言注意策略（SLAP）作为一种解决方案。SLAP使用三维令牌作为输入表示来训练单个多任务、语言条件的动作预测策略。我们的方法显示，在现实世界中，使用单个模型的八个任务的成功率为80%，当引入看不见的杂乱和看不见物体配置时，即使每个任务只有少数例子，成功率也为47.5%。这比之前的工作提高了30%（考虑到看不见的干扰物和配置，提高了20%）。我们看到移动操作设置比基线提高了4倍。此外，我们还展示了SLAP的稳健性如何使我们能够使用大型语言模型从开放词汇指令中执行任务计划，以进行多步移动操作。有关视频，请访问网站：https://robotslap.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.11235v2" target="_blank">2304.11235v2</a>
                              </td>
                              <td>Spatial-Language Attention Policies for Efficient Robot Learning</td>
                              <td>Priyam Parashar</td>
                              <td>2023-04-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_11235v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.11235v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07115v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Intrinsic Mesh Simplification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07115v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07115v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07115v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel simplification method for removing vertices from an intrinsic triangulation corresponding to extrinsic vertices lying on near-developable (i.e., with limited Gaussian curvature) and general surfaces. We greedily process all intrinsic vertices with an absolute Gaussian curvature below a user selected threshold. For each vertex, we repeatedly perform local intrinsic edge flips until the vertex reaches the desired valence (three for internal vertices or two for boundary vertices) such that removal of the vertex and incident edges can be locally performed in the intrinsic triangulation. Each removed vertex's intrinsic location is tracked via (intrinsic) barycentric coordinates that are updated to reflect changes in the intrinsic triangulation. We demonstrate the robustness and effectiveness of our method on the Thingi10k dataset and analyze the effect of the curvature threshold on the solutions of PDEs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07115v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种新的简化方法，用于从与位于近可展（即具有有限高斯曲率）和一般曲面上的外部顶点相对应的内在三角测量中去除顶点。我们贪婪地处理具有低于用户选择的阈值的绝对高斯曲率的所有内在顶点。对于每个顶点，我们重复执行局部本征边翻转，直到顶点达到所需的化合价（三个用于内部顶点，两个用于边界顶点），使得可以在本征三角测量中局部执行顶点和入射边的移除。每个移除顶点的内在位置通过（内在）重心坐标来跟踪，该坐标被更新以反映内在三角测量的变化。我们在Thingi10k数据集上证明了我们的方法的稳健性和有效性，并分析了曲率阈值对偏微分方程解的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07115v1" target="_blank">2307.07115v1</a>
                              </td>
                              <td>Intrinsic Mesh Simplification</td>
                              <td>Randy Shoemaker</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07115v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07115v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_05873v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OG: Equip vision occupancy with instance segmentation and visual grounding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_05873v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_05873v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_05873v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Occupancy prediction tasks focus on the inference of both geometry and semantic labels for each voxel, which is an important perception mission. However, it is still a semantic segmentation task without distinguishing various instances. Further, although some existing works, such as Open-Vocabulary Occupancy (OVO), have already solved the problem of open vocabulary detection, visual grounding in occupancy has not been solved to the best of our knowledge. To tackle the above two limitations, this paper proposes Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance segmentation ability and could operate visual grounding in a voxel manner with the help of grounded-SAM. Keys to our approach are (1) affinity field prediction for instance clustering and (2) association strategy for aligning 2D instance masks and 3D occupancy instances. Extensive experiments have been conducted whose visualization results and analysis are shown below. Our code will be publicly released soon.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_05873v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>占用预测任务侧重于推断每个体素的几何和语义标签，这是一项重要的感知任务。然而，它仍然是一个语义分割任务，没有区分各种实例。此外，尽管一些现有的工作，如开放词汇占用（OVO），已经解决了开放词汇检测的问题，但据我们所知，占用中的视觉基础还没有得到解决。为了解决上述两个局限性，本文提出了一种新的方法——占用基础（OG），它具有普通占用实例分割能力，并可以在基础-SAM的帮助下以体素方式操作视觉基础。我们方法的关键是（1）用于实例聚类的亲和场预测和（2）用于对齐2D实例掩码和3D占用实例的关联策略。已经进行了大量的实验，其可视化结果和分析如下所示。我们的代码将很快公开发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.05873v1" target="_blank">2307.05873v1</a>
                              </td>
                              <td>OG: Equip vision occupancy with instance segmentation and visual grounding</td>
                              <td>Zichao Dong</td>
                              <td>2023-07-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_05873v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.05873v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2105_08996v4_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Separating Sessions Smoothly</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2105_08996v4_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2105_08996v4_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2105_08996v4_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces Hypersequent GV (HGV), a modular and extensible core calculus for functional programming with session types that enjoys deadlock freedom, confluence, and strong normalisation. HGV exploits hyper-environments, which are collections of type environments, to ensure that structural congruence is type preserving. As a consequence we obtain an operational correspondence between HGV and HCP -- a process calculus based on hypersequents and in a propositions-as-types correspondence with classical linear logic (CLL). Our translations from HGV to HCP and vice-versa both preserve and reflect reduction. HGV scales smoothly to support Girard's Mix rule, a crucial ingredient for channel forwarding and exceptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2105_08996v4_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了Hypersequenct GV（HGV），这是一种用于会话类型的函数编程的模块化和可扩展的核心演算，具有死锁自由、融合和强规范化。HGV利用超环境，即类型环境的集合，以确保结构一致性是类型保持的。因此，我们获得了HGV和HCP之间的运算对应关系——一种基于超序列的过程演算，在命题中作为与经典线性逻辑（CLL）的类型对应。我们从HGV到HCP的翻译，反之亦然，既保留又反映了减少。HGV可以顺利扩展以支持Girard的混合规则，这是渠道转发和例外的关键因素。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2105.08996v4" target="_blank">2105.08996v4</a>
                              </td>
                              <td>Separating Sessions Smoothly</td>
                              <td>Simon Fowler</td>
                              <td>2021-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2105_08996v4_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2105.08996v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_05617v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">$\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_05617v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_05617v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_05617v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, large vision model, Segment Anything Model (SAM), has revolutionized the computer vision field, especially for image segmentation. SAM presented a new promptable segmentation paradigm that exhibit its remarkable zero-shot generalization ability. An extensive researches have explore the potential and limits of SAM in various downstream tasks. In this study, we presents $\mathrm{SAM^{Med}}$, an enhanced framework for medical image annotation that leverages the capabilities of SAM. $\mathrm{SAM^{Med}}$ framework consisted of two submodules, namely $\mathrm{SAM^{assist}}$ and $\mathrm{SAM^{auto}}$. The $\mathrm{SAM^{assist}}$ demonstrates the generalization ability of SAM to the downstream medical segmentation task using the prompt-learning approach. Results show a significant improvement in segmentation accuracy with only approximately 5 input points. The $\mathrm{SAM^{auto}}$ model aims to accelerate the annotation process by automatically generating input prompts. The proposed SAP-Net model achieves superior segmentation performance with only five annotated slices, achieving an average Dice coefficient of 0.80 and 0.82 for kidney and liver segmentation, respectively. Overall, $\mathrm{SAM^{Med}}$ demonstrates promising results in medical image annotation. These findings highlight the potential of leveraging large-scale vision models in medical image annotation tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_05617v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，大型视觉模型——分段任意模型（SAM）已经彻底改变了计算机视觉领域，尤其是图像分割。SAM提出了一种新的快速分割范式，展示了其显著的零样本泛化能力。广泛的研究探索了SAM在各种下游任务中的潜力和局限性。在这项研究中，我们提出了$\mathrm｛SAM^｛Med｝｝$，这是一个利用SAM功能的增强型医学图像注释框架。$\mathrm{SAM^{Med｝｝$框架由两个子模块组成，即$\mathrm{SAM^｛assistance｝}$和$\mathrm｛SAM^{auto｝}$。$\mathrm｛SAM^｛assist｝｝$演示了SAM使用即时学习方法对下游医学分割任务的泛化能力。结果显示，仅使用大约5个输入点就显著提高了分割精度。$\mathrm｛SAM^｛auto｝｝$模型旨在通过自动生成输入提示来加快注释过程。所提出的SAP-Net模型仅用五个带注释的切片就实现了卓越的分割性能，肾脏和肝脏分割的平均Dice系数分别为0.80和0.82。总的来说，$\mathrm｛SAM^｛Med｝｝$在医学图像注释中显示了有希望的结果。这些发现突出了在医学图像注释任务中利用大规模视觉模型的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.05617v1" target="_blank">2307.05617v1</a>
                              </td>
                              <td>$\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model</td>
                              <td>Chenglong Wang</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_05617v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.05617v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04973v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04973v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04973v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04973v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, Segmenting Anything has taken an important step towards general artificial intelligence. At the same time, its reliability and fairness have also attracted great attention, especially in the field of health care. In this study, we propose multi-box prompts triggered uncertainty estimation for SAM cues to demonstrate the reliability of segmented lesions or tissues. We estimate the distribution of SAM predictions via Monte Carlo with prior distribution parameters, which employs different prompts as formulation of test-time augmentation. Our experimental results found that multi-box prompts augmentation improve the SAM performance, and endowed each pixel with uncertainty. This provides the first paradigm for a reliable SAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04973v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，“分割任何事物”向通用人工智能迈出了重要一步。与此同时，其可靠性和公平性也引起了人们的高度关注，尤其是在医疗保健领域。在这项研究中，我们提出了SAM线索的多框提示触发不确定性估计，以证明分割病变或组织的可靠性。我们通过具有先验分布参数的蒙特卡罗方法估计SAM预测的分布，该方法采用不同的提示作为测试时间增加的公式。我们的实验结果发现，多框提示增强提高了SAM性能，并赋予了每个像素不确定性。这为可靠的SAM提供了第一个范例。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04973v1" target="_blank">2307.04973v1</a>
                              </td>
                              <td>SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image</td>
                              <td>Guoyao Deng</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04973v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04973v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04767v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic-SAM: Segment and Recognize Anything at Any Granularity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04767v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04767v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04767v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04767v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了语义SAM，这是一种通用的图像分割模型，可以以任何所需的粒度分割和识别任何东西。我们的模型提供了两个关键优势：语义感知和粒度丰富。为了实现语义感知，我们整合了三个粒度的多个数据集，并引入了对象和部分的解耦分类。这使我们的模型能够捕获丰富的语义信息。对于多粒度能力，我们在训练期间提出了一种多选学习方案，使每次点击都能生成多个级别的掩码，这些掩码对应于多个地面实况掩码。值得注意的是，这项工作首次尝试在SA-1B、通用和零件分割数据集上联合训练模型。实验结果和可视化结果表明，我们的模型成功地实现了语义感知和粒度丰富。此外，将SA-1B训练与其他分割任务（如全景分割和零件分割）相结合，可以提高性能。我们将提供代码和演示以供进一步探索和评估。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04767v1" target="_blank">2307.04767v1</a>
                              </td>
                              <td>Semantic-SAM: Segment and Recognize Anything at Any Granularity</td>
                              <td>Feng Li</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04767v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04767v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_02913v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_02913v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_02913v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_02913v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_02913v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分散随机梯度下降（D-SGD）允许在没有中央服务器控制的情况下同时在大规模设备上进行协作学习。然而，现有的理论声称，权力下放必然会破坏泛化。在本文中，我们挑战了传统的信念，并为理解去中心化学习提供了一个全新的视角。我们证明了在一般非凸非$\beta$-光滑设置下，D-SGD隐式地最小化了平均方向清晰度感知最小化（SAM）算法的损失函数。这种令人惊讶的渐近等价揭示了一种内在的正则化优化权衡和去中心化的三个优点：（1）D-SGD中存在一种自由的不确定性评估机制来改进后验估计；（2） D-SGD表现出梯度平滑效应；和（3）D-SGD的锐度正则化效应不会随着总批量的增加而降低，这证明了在大批量场景中，D-SGD相对于集中式SGD（C-SGD）的潜在泛化优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.02913v3" target="_blank">2306.02913v3</a>
                              </td>
                              <td>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</td>
                              <td>Tongtian Zhu</td>
                              <td>2023-06-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_02913v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.02913v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04455v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04455v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04455v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04455v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04455v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像质量评估（IQA）是一项具有挑战性的任务，需要在大量数据集上进行训练以实现准确的预测。然而，由于缺乏IQA数据，基于深度学习的IQA方法通常依赖于在海量数据集上训练的预训练网络作为特征提取器来增强其泛化能力，例如在ImageNet上训练的ResNet网络。在本文中，我们利用Segment Anything的编码器进行高级语义特征提取，Segment Anything是最近提出的一种在海量数据集上训练的分割模型。大多数IQA方法仅限于提取空间域特征，而频域特征已被证明可以更好地表示噪声和模糊。因此，我们通过分别对提取的特征应用傅立叶和标准卷积来利用空间域和频域特征。进行了大量的实验来证明所有提出的组件的有效性，结果表明，我们的方法在四个具有代表性的数据集中，无论是定性还是定量，都优于最先进的（SOTA）。我们的实验证实了Segment Anything强大的特征提取能力，并强调了在IQA任务中结合空间域和频域特征的价值。代码：https://github.com/Hedlen/SAM-IQA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04455v1" target="_blank">2307.04455v1</a>
                              </td>
                              <td>SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</td>
                              <td>Xinpeng Li</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04455v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04455v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_02508v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_02508v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_02508v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_02508v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object tracking and segmentation. In this study, we convert the bounding boxes to masks in reference frames with the help of the Segment Anything Model (SAM) and Alpha-Refine, and then propagate the masks to the current frame, transforming the task from Video Object Tracking (VOT) to video object segmentation (VOS). Furthermore, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. MSDeAOT efficiently propagates object masks from previous frames to the current frame using two feature scales of 16 and 8. As a testament to the effectiveness of our design, we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking Challenge.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_02508v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将对象与变换器相关联（AOT）框架在视频对象跟踪和分割的各种复杂场景中表现出了卓越的性能。在这项研究中，我们在Segment Anything Model（SAM）和Alpha Refine的帮助下，将边界框转换为参考帧中的掩码，然后将掩码传播到当前帧，将任务从视频对象跟踪（VOT）转换为视频对象分割（VOS）。此外，我们还介绍了MSDeAOT，这是AOT系列的一个变体，包含了多个功能级别的转换器。MSDeAOT使用16和8这两个特征尺度将对象遮罩从先前帧有效地传播到当前帧。为了证明我们设计的有效性，我们在EPIC-KITCHENS TREK-150对象跟踪挑战赛中获得了第一名。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.02508v2" target="_blank">2307.02508v2</a>
                              </td>
                              <td>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</td>
                              <td>Yuanyou Xu</td>
                              <td>2023-07-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_02508v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.02508v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04308v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04308v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04308v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04308v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04308v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于过去几十年数据库系统的稳定开发和大规模部署，表格数据（也称为结构化数据）是现存最常见的数据形式之一。然而，目前，尽管ChatGPT或SAM等其他领域的大型预训练模型带来了冲击，但我们如何在表格中提取公共知识，最终可能导致表格数据的可推广表示，仍然是一片空白。事实上，围绕这个主题已经有了一些作品。它们中的大多数（如果不是全部的话）被限制在单个表或固定形式的模式的范围内。在这项工作中，我们首先确定了表格数据预训练背后的关键研究挑战，特别是在跨表格场景中。我们将这项工作的贡献分为两个方面：（i）-我们收集和整理了近2k个高质量的表格数据集，每个数据集都保证具有清晰的语义、干净的标签和其他必要的元信息。（ii）-我们提出了一种新的框架，该框架允许称为CT-BERT的跨表预训练。值得注意的是，鉴于开创了规模交叉表训练，CT-BERT与监督和自监督方案都是完全兼容的，其中CT-BERT的具体实例化在很大程度上取决于下游任务。我们进一步在CT-BERT中提出并实现了一个基于对比学习和掩蔽表建模（MTM）的目标，该目标的灵感来自计算机视觉和自然语言处理社区，但针对表进行了巧妙的定制。在15个数据集上的大量实证结果证明了CT-BERT最先进的性能，其监督和自监督设置都显著优于先前的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04308v1" target="_blank">2307.04308v1</a>
                              </td>
                              <td>CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</td>
                              <td>Chao Ye</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04308v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04308v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04008v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Toward Interactive Dictation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04008v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04008v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04008v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04008v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音听写是一种越来越重要的文本输入方式。允许听写和语音编辑的现有系统将其命令语言限制为触发词调用的平面模板。在这项工作中，我们研究了允许用户用开放式自然语言中的口语编辑命令中断听写的可行性。我们引入了一个新的任务和数据集，TERTiUS，来对这样的系统进行实验。为了实时支持这种灵活性，系统必须增量地将语音的跨度分段和分类为听写或命令，并将这些跨度解释为命令。我们实验使用大型预先训练的语言模型来预测编辑后的文本，或者预测小型文本编辑程序。实验表明，模型准确性和延迟之间存在自然的权衡：较小的模型在1.3秒的延迟下实现了30%的最终状态准确性，而较大的模型在7秒的延迟内实现了55%的最终状态准确率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04008v1" target="_blank">2307.04008v1</a>
                              </td>
                              <td>Toward Interactive Dictation</td>
                              <td>Belinda Z. Li</td>
                              <td>2023-07-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04008v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04008v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_10922v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10922v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10922v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10922v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10922v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的对比语言图像预训练已经导致学习高度可转移和鲁棒的图像表示。然而，将这些模型适配到监控最少的视频领域仍然是一个悬而未决的问题。我们朝着这个方向探索了一个简单的步骤，使用与语言相关的自我监督学习将图像CLIP模型适应视频领域。为时间建模修改的主干在自蒸馏设置下进行训练，训练目标在动作概念空间中操作。使用相关的文本提示从语言编码器提取的各种动作概念的特征向量构建了这个空间。我们引入了两个训练目标，概念提炼和概念对齐，它们保留了原始表示的一般性，同时加强了动作及其属性之间的关系。我们的方法在三个动作识别基准上改进了零样本和线性探测性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10922v1" target="_blank">2307.10922v1</a>
                              </td>
                              <td>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</td>
                              <td>Kanchana Ranasinghe</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10922v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10922v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10867v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10867v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10867v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10867v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10867v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>标题对于理解科学可视化和文档至关重要。现有的科学人物字幕方法依赖于从文档中提取的人物字幕对进行训练，其中许多在有用性、可解释性和视觉描述性等指标方面都不足[15]，导致生成的字幕与读者偏好不一致。为了能够生成高质量的数字字幕，我们引入了FigCaps HF，这是一种用于数字字幕生成的新框架，可以在生成针对读者偏好优化的字幕时结合领域专家反馈。我们的框架包括：1）一种用于评估图形-字幕对质量的自动方法，2）一种新颖的人反馈强化学习（RLHF）方法，用于针对读者偏好优化生成的图形-字幕模型。我们通过在不同类型的模型之间提高标准微调的性能来证明我们的简单学习框架的有效性。特别是，当使用BLIP作为基本模型时，我们的RLHF框架在ROUGE、BLEU和Meteor中分别实现了35.7%、16.9%和9%的平均增益。最后，我们发布了一个大规模的基准数据集，其中包含对图形-字幕对的人类反馈，以便能够针对该问题进一步评估和开发RLHF技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10867v1" target="_blank">2307.10867v1</a>
                              </td>
                              <td>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</td>
                              <td>Ashish Singh</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10867v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10867v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_12112v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_12112v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_12112v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_12112v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_12112v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP模型最近被证明对各种跨模态任务非常有效，包括对视觉和语言架构生成的字幕的评估。在本文中，我们提出了一种新的基于对比的图像字幕评估指标，即积极增强对比学习分数（PAC-S），它以一种新颖的方式将对比视觉语义空间的学习与在策划数据上添加生成的图像和文本相统一。跨越多个数据集的实验表明，我们的新指标与人类对图像和视频的判断实现了最高的相关性，优于现有的基于参考的指标，如CIDEr和SPICE，以及无参考的指标（如CLIP Score）。最后，当考虑到流行的图像字幕方法时，我们测试了所提出的度量的系统级相关性，并评估了使用不同跨模态特征的影响。我们的源代码和经过训练的模型可在以下网站上公开获取：https://github.com/aimagelab/pacscore.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.12112v3" target="_blank">2303.12112v3</a>
                              </td>
                              <td>Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation</td>
                              <td>Sara Sarto</td>
                              <td>2023-03-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_12112v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.12112v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10584v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10584v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10584v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10584v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Have you ever imagined how it would look if we placed new objects into paintings? For example, what would it look like if we placed a basketball into Claude Monet's ``Water Lilies, Evening Effect''? We propose Reference-based Painterly Inpainting, a novel task that crosses the wild reference domain gap and implants novel objects into artworks. Although previous works have examined reference-based inpainting, they are not designed for large domain discrepancies between the target and the reference, such as inpainting an artistic image using a photorealistic reference. This paper proposes a novel diffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking such references with large domain gaps. Built with an image-conditioned diffusion model, we introduce a ladder-side branch and a masked fusion mechanism to work with the inpainting mask. By decomposing the CLIP image embeddings at inference time, one can manipulate the strength of semantic and style information with ease. Experiments demonstrate that our proposed RefPaint framework produces significantly better results than existing methods. Our method enables creative painterly image inpainting with reference objects that would otherwise be difficult to achieve. Project page: https://vita-group.github.io/RefPaint/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10584v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>你有没有想过，如果我们在画中放置新的物体会是什么样子？例如，如果我们把一个篮球放在克劳德·莫奈的《睡莲，黄昏效应》中，会是什么样子？我们提出了基于参考的绘画修复，这是一项新颖的任务，它跨越了参考领域的空白，将新颖的物体植入艺术品中。尽管之前的作品已经研究了基于参考的修复，但它们并不是针对目标和参考之间的大范围差异而设计的，例如使用真实感参考来修复艺术图像。本文提出了一种新的扩散框架，称为RefPaint，通过引用具有大域间隙的引用来“更广泛地修复”。利用图像条件扩散模型，我们引入了梯形分支和掩模融合机制来处理修复掩模。通过在推理时分解CLIP图像嵌入，可以容易地操纵语义和风格信息的强度。实验表明，我们提出的RefPaint框架比现有方法产生了更好的结果。我们的方法能够用参考对象进行创造性的绘画图像修复，否则很难实现。项目页面：https://vita-group.github.io/RefPaint/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10584v1" target="_blank">2307.10584v1</a>
                              </td>
                              <td>Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap</td>
                              <td>Dejia Xu</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10584v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10584v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10504v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying Interpretable Subspaces in Image Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10504v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10504v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10504v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10504v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了使用对比概念的自动特征解释（FALCON），这是一个解释图像表示特征的可解释性框架。对于目标特征，FALCON使用大型字幕数据集（如LAION-4000）和预先训练的视觉语言模型（如CLIP）为其高度激活的裁剪图像加上字幕。字幕中的每个单词都会被评分和排序，从而产生少量共享的、人类可以理解的概念，这些概念紧密地描述了目标特征。FALCON还使用低激活（反事实）图像进行对比解释，以消除虚假概念。尽管许多现有的方法独立地解释特征，但我们在最先进的自监督和监督模型中观察到，只有不到20%的表示空间可以由单个特征来解释。我们表明，当分组研究时，更大空间中的特征变得更容易解释，并且可以通过FALCON用高阶评分概念来解释。我们讨论了如何使用提取的概念来解释和调试下游任务中的故障。最后，我们提出了一种通过学习简单的线性变换将概念从一个（可解释的）表示空间转移到另一个看不见的表示空间的技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10504v1" target="_blank">2307.10504v1</a>
                              </td>
                              <td>Identifying Interpretable Subspaces in Image Representations</td>
                              <td>Neha Kalibhat</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10504v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10504v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10475v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Findings of Factify 2: Multimodal Fake News Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10475v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10475v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10475v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With social media usage growing exponentially in the past few years, fake news has also become extremely prevalent. The detrimental impact of fake news emphasizes the need for research focused on automating the detection of false information and verifying its accuracy. In this work, we present the outcome of the Factify 2 shared task, which provides a multi-modal fact verification and satire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The data calls for a comparison based approach to the task by pairing social media claims with supporting documents, with both text and image, divided into 5 classes based on multi-modal relations. In the second iteration of this task we had over 60 participants and 9 final test-set submissions. The best performances came from the use of DeBERTa for text and Swinv2 and CLIP for image. The highest F1 score averaged for all five classes was 81.82%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10475v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着社交媒体的使用在过去几年呈指数级增长，假新闻也变得极其普遍。假新闻的有害影响强调了研究的必要性，重点是自动检测虚假信息并验证其准确性。在这项工作中，我们介绍了Factify 2共享任务的结果，该任务提供了一个多模式的事实验证和讽刺新闻数据集，作为AAAI’23 DeFactify 2中研讨会的一部分。这些数据要求采用基于比较的方法来完成这项任务，将社交媒体声明与支持文档（包括文本和图像）配对，根据多模态关系分为5类。在这个任务的第二次迭代中，我们有60多名参与者和9份最终测试集提交。最好的性能来自于对文本使用DeBERTa，对图像使用Swinv2和CLIP。所有五个级别的F1平均得分最高，为81.82%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10475v1" target="_blank">2307.10475v1</a>
                              </td>
                              <td>Findings of Factify 2: Multimodal Fake News Detection</td>
                              <td>S Suryavardan</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10475v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10475v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08927v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Stage Cable Routing through Hierarchical Imitation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08927v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08927v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08927v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which controllers to trigger at any given time, retrying, or taking corrective action as needed. To this end, we describe an imitation learning system that uses vision-based policies trained from demonstrations at both the lower (motor control) and the upper (sequencing) level, present a system for instantiating this method to learn the cable routing task, and perform evaluations showing great performance in generalizing to very challenging clip placement variations. Supplementary videos, datasets, and code can be found at https://sites.google.com/view/cablerouting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08927v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了学习执行多阶段机器人操作任务的问题，并将其应用于电缆布线，其中机器人必须通过一系列夹子布线电缆。这种设置提出了代表复杂的多阶段机器人操作场景的挑战：处理可变形物体，闭合视觉感知的循环，以及处理由必须成功执行才能完成整个任务的多个步骤组成的扩展行为。在这种情况下，为每个阶段学习以足够高的速率成功执行完整的时间扩展任务的单个原语是不切实际的：如果每个阶段都必须成功完成，并且失败的概率不可忽略，那么成功完成整个任务的可能性就可以忽略不计。因此，用于这种多阶段任务的成功控制器必须能够从故障中恢复，并通过智能地选择在任何给定时间触发哪些控制器、重试或根据需要采取纠正措施来补偿低级别控制器中的缺陷。为此，我们描述了一个模仿学习系统，该系统使用从较低（电机控制）和较高（排序）级别的演示中训练的基于视觉的策略，提出了一个用于实例化该方法的系统来学习电缆布线任务，并进行评估，显示出在推广到极具挑战性的夹子放置变化方面的良好性能。补充视频、数据集和代码可在https://sites.google.com/view/cablerouting.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08927v2" target="_blank">2307.08927v2</a>
                              </td>
                              <td>Multi-Stage Cable Routing through Hierarchical Imitation Learning</td>
                              <td>Jianlan Luo</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08927v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08927v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10350v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Multimodal Datasets with Image Captioning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10350v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10350v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10350v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10350v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模的网络数据集在CLIP和Flamingo等大型视觉语言模型的成功中发挥着关键作用。然而，原始网络数据是有噪声的，现有的降低噪声的滤波方法往往以牺牲数据多样性为代价。我们的工作重点是将字幕质量作为一个主要的噪声源，并研究生成的字幕如何增加具有不伦不类文本的网络抓取数据点的效用。通过探索原始字幕和生成字幕的不同混合策略，在给定128M个图像-文本对的候选池的情况下，我们在ImageNet上的表现优于DataComp基准测试提出的最佳过滤方法2%，在38个任务中的平均表现为4%。我们的最佳方法在Flickr和MS-COCO检索方面也提高了2倍。然后，我们分析是什么使合成字幕成为文本监督的有效来源。在对不同的图像字幕模型进行实验时，我们还证明了模型在标准图像字幕基准（例如，NoCaps CIDEr）上的性能并不是其为多模式训练生成的字幕效用的可靠指标。最后，我们在DataComp的大规模（1.28B图像-文本对）中使用生成字幕的实验深入了解了合成文本的局限性，以及随着训练数据量的增加，图像管理的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10350v1" target="_blank">2307.10350v1</a>
                              </td>
                              <td>Improving Multimodal Datasets with Image Captioning</td>
                              <td>Thao Nguyen</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10350v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10350v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10153v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contact-aware Shaping and Maintenance of Deformable Linear Objects With Fixtures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10153v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10153v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10153v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Studying the manipulation of deformable linear objects has significant practical applications in industry, including car manufacturing, textile production, and electronics automation. However, deformable linear object manipulation poses a significant challenge in developing planning and control algorithms, due to the precise and continuous control required to effectively manipulate the deformable nature of these objects. In this paper, we propose a new framework to control and maintain the shape of deformable linear objects with two robot manipulators utilizing environmental contacts. The framework is composed of a shape planning algorithm which automatically generates appropriate positions to place fixtures, and an object-centered skill engine which includes task and motion planning to control the motion and force of both robots based on the object status. The status of the deformable linear object is estimated online utilizing visual as well as force information. The framework manages to handle a cable routing task in real-world experiments with two Panda robots and especially achieves contact-aware and flexible clip fixing with challenging fixtures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10153v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>研究可变形线性物体的操纵在工业中具有重要的实际应用，包括汽车制造、纺织生产和电子自动化。然而，可变形线性物体操纵在开发规划和控制算法方面提出了重大挑战，因为有效操纵这些物体的可变形性质需要精确和连续的控制。在本文中，我们提出了一种新的框架，用两个利用环境接触的机器人来控制和保持可变形线性物体的形状。该框架由一个形状规划算法和一个以对象为中心的技能引擎组成，该算法自动生成放置固定装置的适当位置，该引擎包括任务和运动规划，以根据对象状态控制两个机器人的运动和力。利用视觉信息和力信息在线估计可变形线性物体的状态。该框架在使用两个Panda机器人的真实世界实验中成功地处理了电缆布线任务，尤其是通过具有挑战性的夹具实现了接触感知和灵活的夹子固定。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10153v1" target="_blank">2307.10153v1</a>
                              </td>
                              <td>Contact-aware Shaping and Maintenance of Deformable Linear Objects With Fixtures</td>
                              <td>Kejia Chen</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10153v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10153v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04838v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04838v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04838v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04838v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically infer the text prompt for union-box. Our approach achieves state-of-the-art performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual Genome benchmark, achieving a 15.3\% gain in performance over recent state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in object relation prediction and encourages further research on VLMs in this challenging domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04838v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们探索了视觉语言模型（VLM），特别是CLIP在预测视觉对象关系方面的潜力，该模型涉及将图像中的视觉特征解释为基于语言的关系。目前最先进的方法使用复杂的图形模型，这些模型利用语言提示和视觉特征来应对这一挑战。我们假设CLIP嵌入中的强语言先验可以简化这些图形模型，为更简单的方法铺平道路。我们采用了UVTransE关系预测框架，该框架将关系学习为与场景中的主题、对象和并集框嵌入的平移嵌入。我们系统地探索了在UVTransE框架内基于CLIP的主题、对象和并集盒表示的设计，并提出了CREPE（CLIP表示增强谓词估计）。CREPE对所有三个边界框使用基于文本的表示，并引入了一种新颖的对比训练策略来自动推断并集框的文本提示。我们的方法在谓词估计方面实现了最先进的性能，mR@527.79，以及mR@20在视觉基因组基准上为31.95，在性能上比最近的最先进技术提高了15.3%mR@20.这项工作证明了CLIP在对象关系预测方面的有效性，并鼓励在这一具有挑战性的领域进一步研究VLM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04838v2" target="_blank">2307.04838v2</a>
                              </td>
                              <td>CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction</td>
                              <td>Rakshith Subramanyam</td>
                              <td>2023-07-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04838v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04838v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_05944v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A 137.5 TOPS/W SRAM Compute-in-Memory Macro with 9-b Memory Cell-Embedded ADCs and Signal Margin Enhancement Techniques for AI Edge Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_05944v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_05944v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_05944v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a high-precision SRAM-based CIM macro that can perform 4x4-bit MAC operations and yield 9-bit signed output. The inherent discharge branches of SRAM cells are utilized to apply time-modulated MAC and 9-bit ADC readout operations on two bit-line capacitors. The same principle is used for both MAC and A-to-D conversion ensuring high linearity and thus supporting large number of analog MAC accumulations. The memory cell-embedded ADC eliminates the use of separate ADCs and enhances energy and area efficiency. Additionally, two signal margin enhancement techniques, namely the MAC-folding and boosted-clipping schemes, are proposed to further improve the CIM computation accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_05944v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种基于SRAM的高精度CIM宏，该宏可以执行4x4位MAC操作并产生9位带符号输出。SRAM单元的固有放电分支用于在两个位线电容器上施加时间调制的MAC和9位ADC读出操作。相同的原理用于MAC和A-to-D转换，确保了高线性度，从而支持大量的模拟MAC累积。存储单元嵌入式ADC消除了单独ADC的使用，并提高了能量和面积效率。此外，为了进一步提高CIM的计算精度，提出了两种信号裕度增强技术，即MAC折叠和增强削波方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.05944v3" target="_blank">2307.05944v3</a>
                              </td>
                              <td>A 137.5 TOPS/W SRAM Compute-in-Memory Macro with 9-b Memory Cell-Embedded ADCs and Signal Margin Enhancement Techniques for AI Edge Applications</td>
                              <td>Xiaomeng Wang</td>
                              <td>2023-07-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_05944v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.05944v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09815v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09815v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09815v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09815v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent blur is a challenging task.~Existing blur map-based deblurring methods have demonstrated promising results. In this paper, we propose, to the best of our knowledge, the first framework to introduce the contrastive language-image pre-training framework (CLIP) to achieve accurate blur map estimation from DP pairs unsupervisedly. To this end, we first carefully design text prompts to enable CLIP to understand blur-related geometric prior knowledge from the DP pair. Then, we propose a format to input stereo DP pair to the CLIP without any fine-tuning, where the CLIP is pre-trained on monocular images. Given the estimated blur map, we introduce a blur-prior attention block, a blur-weighting loss and a blur-aware loss to recover the all-in-focus image. Our method achieves state-of-the-art performance in extensive experiments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09815v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从具有视差相关模糊的双像素（DP）对中恢复清晰图像是一项具有挑战性的任务~现有的基于模糊图的去模糊方法已经证明了有希望的结果。据我们所知，在本文中，我们提出了第一个引入对比语言图像预训练框架（CLIP）的框架，以在无监督的情况下从DP对中实现精确的模糊图估计。为此，我们首先仔细设计文本提示，使CLIP能够理解DP对中与模糊相关的几何先验知识。然后，我们提出了一种在没有任何微调的情况下将立体声DP对输入到CLIP的格式，其中CLIP是在单目图像上预训练的。给定估计的模糊图，我们引入模糊先验注意力块、模糊加权损失和模糊感知损失来恢复全聚焦图像。我们的方法在大量实验中达到了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09815v1" target="_blank">2307.09815v1</a>
                              </td>
                              <td>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network</td>
                              <td>Hao Yang</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09815v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09815v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09748v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Watch out Venomous Snake Species: A Solution to SnakeCLEF2023</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09748v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09748v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09748v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The SnakeCLEF2023 competition aims to the development of advanced algorithms for snake species identification through the analysis of images and accompanying metadata. This paper presents a method leveraging utilization of both images and metadata. Modern CNN models and strong data augmentation are utilized to learn better representation of images. To relieve the challenge of long-tailed distribution, seesaw loss is utilized in our method. We also design a light model to calculate prior probabilities using metadata features extracted from CLIP in post processing stage. Besides, we attach more importance to venomous species by assigning venomous species labels to some examples that model is uncertain about. Our method achieves 91.31% score of the final metric combined of F1 and other metrics on private leaderboard, which is the 1st place among the participators. The code is available at https://github.com/xiaoxsparraw/CLEF2023.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09748v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>SnakeCLEF2023竞赛旨在通过分析图像和相关元数据，开发先进的蛇类识别算法。本文提出了一种利用图像和元数据的方法。利用现代CNN模型和强大的数据增强来学习更好的图像表示。为了缓解长尾分布的挑战，我们的方法中使用了跷跷板损失。我们还设计了一个光模型，在后处理阶段使用从CLIP中提取的元数据特征来计算先验概率。此外，我们通过给一些模型不确定的例子分配有毒物种标签，更加重视有毒物种。我们的方法在私人排行榜上获得了91.31%的F1和其他指标组合的最终指标得分，在参与者中排名第一。代码位于https://github.com/xiaoxsparraw/CLEF2023.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09748v1" target="_blank">2307.09748v1</a>
                              </td>
                              <td>Watch out Venomous Snake Species: A Solution to SnakeCLEF2023</td>
                              <td>Feiran Hu</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09748v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09748v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09356v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09356v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09356v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09356v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Referring video object segmentation (RVOS) aims at segmenting an object in a video following human instruction. Current state-of-the-art methods fall into an offline pattern, in which each clip independently interacts with text embedding for cross-modal understanding. They usually present that the offline pattern is necessary for RVOS, yet model limited temporal association within each clip. In this work, we break up the previous offline belief and propose a simple yet effective online model using explicit query propagation, named OnlineRefer. Specifically, our approach leverages target cues that gather semantic information and position prior to improve the accuracy and ease of referring predictions for the current frame. Furthermore, we generalize our online model into a semi-online framework to be compatible with video-based backbones. To show the effectiveness of our method, we evaluate it on four benchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and JHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L backbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17, outperforming all other offline methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09356v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>参考视频对象分割（RVOS）旨在根据人类指令对视频中的对象进行分割。目前最先进的方法属于离线模式，其中每个剪辑独立地与文本嵌入交互，以实现跨模态理解。他们通常认为离线模式对于RVOS是必要的，但在每个剪辑中建模有限的时间关联。在这项工作中，我们打破了以前的离线信念，提出了一个使用显式查询传播的简单而有效的在线模型，名为OnlineRefer。具体而言，我们的方法利用目标线索来收集语义信息和位置，以提高引用当前帧的预测的准确性和容易性。此外，我们将我们的在线模型推广到半在线框架中，以与基于视频的主干网兼容。为了证明我们的方法的有效性，我们在四个基准上对其进行了评估，即参考Youtube VOS、参考DAVIS17、A2D句子和JHMDB句子。在没有铃声和口哨声的情况下，我们的带有Swin-L主干的OnlineRefer在Refer Youtube VOS和Refer-DAVIS17上实现了63.5 J&F和64.8 J&F，优于所有其他离线方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09356v1" target="_blank">2307.09356v1</a>
                              </td>
                              <td>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</td>
                              <td>Dongming Wu</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09356v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09356v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09233v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Augmenting CLIP with Improved Visio-Linguistic Reasoning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09233v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09233v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09233v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image-text contrastive models such as CLIP are useful for a variety of downstream applications including zero-shot classification, image-text retrieval and transfer learning. However, these contrastively trained vision-language models often fail on compositional visio-linguistic tasks such as Winoground with performance equivalent to random chance. In our paper, we address this issue and propose a sample-efficient light-weight method called SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities of CLIP. The core idea of our method is to use differentiable image parameterizations to fine-tune CLIP with a distillation objective from large text-to-image generative models such as Stable-Diffusion which are relatively good at visio-linguistic reasoning tasks. On the challenging Winoground compositional reasoning benchmark, our method improves the absolute visio-linguistic performance of different CLIP models by up to 7%, while on the ARO dataset, our method improves the visio-linguistic performance by upto 3%. As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find that the zero-shot performance improves marginally on a variety of downstream datasets. Our method reinforces that carefully designed distillation objectives from generative models can be leveraged to extend existing contrastive image-text models with improved visio-linguistic reasoning capabilities.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09233v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>像CLIP这样的图文对比模型对于各种下游应用很有用，包括零样本分类、图文检索和转移学习。然而，这些经过对比训练的视觉语言模型在组合视觉语言任务（如Winoground）中往往失败，其性能相当于随机机会。在我们的论文中，我们解决了这个问题，并提出了一种称为SDS-CLIP的样本有效的轻量级方法，以提高CLIP的组合视觉语言推理能力。我们方法的核心思想是使用可微图像参数化来微调CLIP，其目标是从大文本到图像生成模型（如稳定扩散），这些模型相对擅长视觉语言推理任务。在具有挑战性的Winoground组合推理基准上，我们的方法将不同CLIP模型的视觉语言绝对性能提高了7%，而在ARO数据集上，我们方法将视觉语言性能提高了3%。作为将视觉语言推理引入CLIP的副产品，我们还发现，在各种下游数据集上，零样本性能略有提高。我们的方法强化了从生成模型中精心设计的提取目标可以用来扩展现有的对比图像-文本模型，并提高视觉-语言推理能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09233v1" target="_blank">2307.09233v1</a>
                              </td>
                              <td>Augmenting CLIP with Improved Visio-Linguistic Reasoning</td>
                              <td>Samyadeep Basu</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09233v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09233v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09059v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09059v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09059v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09059v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.   To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entities to specific image regions, facilitating alignment between textual and visual embeddings. Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability to distinguish minor differences.   To focus the model on the key components within sentences, we propose a novel text data augmentation technique. Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09059v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像人物检索的目标是从与给定文本描述匹配的大型图库中检索人物图像。这项任务的主要挑战在于视觉和文本模式之间的信息表示存在显著差异。文本模态通过词汇和语法结构传递抽象而精确的信息，而视觉模态通过图像传递具体而直观的信息。为了充分利用文本表示的表达能力，准确地将抽象的文本描述映射到特定的图像是至关重要的。为了解决这个问题，我们提出了一个新的框架来释放文本想象（UIT）在文本到图像的人物检索中，旨在充分探索单词在句子中的力量。具体而言，该框架利用先前的跨模态对齐知识，使用预先训练的完整CLIP模型作为图像和文本的双编码器。提出了文本引导的图像恢复辅助任务，目的是将抽象的文本实体隐式映射到特定的图像区域，促进文本和视觉嵌入之间的对齐。此外，我们引入了一种为处理硬样本而定制的跨模态三元组损失，增强了模型区分微小差异的能力。为了将模型集中在句子中的关键成分上，我们提出了一种新的文本数据增强技术。我们提出的方法在三个流行的基准数据集上取得了最先进的结果，源代码将很快公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09059v1" target="_blank">2307.09059v1</a>
                              </td>
                              <td>Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words</td>
                              <td>Delong Liu</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09059v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09059v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08397v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08397v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08397v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08397v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Researchers have recently begun exploring the use of StyleGAN-based models for real image editing. One particularly interesting application is using natural language descriptions to guide the editing process. Existing approaches for editing images using language either resort to instance-level latent code optimization or map predefined text prompts to some editing directions in the latent space. However, these approaches have inherent limitations. The former is not very efficient, while the latter often struggles to effectively handle multi-attribute changes. To address these weaknesses, we present CLIPInverter, a new text-driven image editing approach that is able to efficiently and reliably perform multi-attribute changes. The core of our method is the use of novel, lightweight text-conditioned adapter layers integrated into pretrained GAN-inversion networks. We demonstrate that by conditioning the initial inversion step on the CLIP embedding of the target description, we are able to obtain more successful edit directions. Additionally, we use a CLIP-guided refinement step to make corrections in the resulting residual latent codes, which further improves the alignment with the text prompt. Our method outperforms competing approaches in terms of manipulation accuracy and photo-realism on various domains including human faces, cats, and birds, as shown by our qualitative and quantitative results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08397v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>研究人员最近开始探索使用基于StyleGAN的模型进行真实图像编辑。一个特别有趣的应用程序是使用自然语言描述来指导编辑过程。使用语言编辑图像的现有方法要么求助于实例级潜在代码优化，要么将预定义的文本提示映射到潜在空间中的一些编辑方向。然而，这些方法有其固有的局限性。前者效率不高，而后者往往难以有效地处理多属性变化。为了解决这些弱点，我们提出了CLIPInvert，这是一种新的文本驱动图像编辑方法，能够有效可靠地执行多属性更改。我们方法的核心是使用新的、轻量级的文本条件适配器层，将其集成到预训练的GAN反演网络中。我们证明，通过将初始反演步骤限制在目标描述的CLIP嵌入上，我们能够获得更成功的编辑方向。此外，我们使用CLIP引导的细化步骤来对生成的残余潜在代码进行校正，这进一步改进了与文本提示的对齐。正如我们的定性和定量结果所示，我们的方法在人脸、猫和鸟类等各个领域的操作精度和照片真实性方面优于竞争方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08397v2" target="_blank">2307.08397v2</a>
                              </td>
                              <td>CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing</td>
                              <td>Ahmet Canberk Baykal</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08397v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08397v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08984v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">In Defense of Clip-based Video Relation Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08984v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08984v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08984v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Visual Relation Detection (VidVRD) aims to detect visual relationship triplets in videos using spatial bounding boxes and temporal boundaries. Existing VidVRD methods can be broadly categorized into bottom-up and top-down paradigms, depending on their approach to classifying relations. Bottom-up methods follow a clip-based approach where they classify relations of short clip tubelet pairs and then merge them into long video relations. On the other hand, top-down methods directly classify long video tubelet pairs. While recent video-based methods utilizing video tubelets have shown promising results, we argue that the effective modeling of spatial and temporal context plays a more significant role than the choice between clip tubelets and video tubelets. This motivates us to revisit the clip-based paradigm and explore the key success factors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM) that enriches the object-based spatial context and relation-based temporal context based on clips. We demonstrate that using clip tubelets can achieve superior performance compared to most video-based methods. Additionally, using clip tubelets offers more flexibility in model designs and helps alleviate the limitations associated with video tubelets, such as the challenging long-term object tracking problem and the loss of temporal information in long-term tubelet feature compression. Extensive experiments conducted on two challenging VidVRD benchmarks validate that our HCM achieves a new state-of-the-art performance, highlighting the effectiveness of incorporating advanced spatial and temporal context modeling within the clip-based paradigm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08984v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频视觉关系检测（VidVRD）旨在利用空间边界盒和时间边界检测视频中的视觉关系三元组。现有的VidVRD方法可以大致分为自下而上和自上而下的范式，这取决于它们对关系的分类方法。自下而上的方法遵循基于剪辑的方法，对短片-小短片对的关系进行分类，然后将它们合并为长视频关系。另一方面，自上而下的方法直接对长视频管对进行分类。虽然最近利用视频小管道的基于视频的方法显示出了有希望的结果，但我们认为，对空间和时间上下文的有效建模比在剪辑小管道和视频小管道之间的选择起着更重要的作用。这促使我们重新审视基于剪辑的范式，并探索VidVRD中的关键成功因素。在本文中，我们提出了一个层次上下文模型（HCM），该模型丰富了基于剪辑的基于对象的空间上下文和基于关系的时间上下文。我们证明，与大多数基于视频的方法相比，使用卡套管可以获得卓越的性能。此外，使用剪辑小片段在模型设计中提供了更大的灵活性，并有助于缓解与视频小片段相关的限制，例如具有挑战性的长期对象跟踪问题和长期小片段特征压缩中时间信息的丢失。在两个具有挑战性的VidVRD基准上进行的大量实验验证了我们的HCM实现了新的最先进的性能，突出了在基于剪辑的范式中结合高级空间和时间上下文建模的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08984v1" target="_blank">2307.08984v1</a>
                              </td>
                              <td>In Defense of Clip-based Video Relation Detection</td>
                              <td>Meng Wei</td>
                              <td>2023-07-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08984v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08984v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_00270v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Internally Rewarded Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_00270v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_00270v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_00270v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance compared with baselines in diverse tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_00270v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了一类强化学习问题，其中政策学习的奖励信号是由依赖于政策并与政策联合优化的鉴别器产生的。政策和鉴别器之间的这种相互依赖导致了不稳定的学习过程，因为来自不成熟鉴别器的奖励信号是嘈杂的，阻碍了政策学习，相反，优化不足的政策阻碍了鉴别器学习。我们将这种学习设置称为\textit｛内部奖励强化学习｝（IRRL），因为奖励不是由环境直接提供的，而是由鉴别器｛内部｝提供的。在本文中，我们正式地制定了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并实证分析了报酬函数在IRRL中的作用，并在此基础上提出了截断线性报酬函数。实验结果表明，所提出的奖励函数可以通过减少奖励噪声的影响来稳定训练过程，从而在不同的任务中比基线更快地收敛和更高的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.00270v2" target="_blank">2302.00270v2</a>
                              </td>
                              <td>Internally Rewarded Reinforcement Learning</td>
                              <td>Mengdi Li</td>
                              <td>2023-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_00270v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.00270v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_05379v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Action-based Early Autism Diagnosis Using Contrastive Feature Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_05379v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_05379v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_05379v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological disorder. Its main symptoms include difficulty in (verbal and/or non-verbal) communication, and rigid/repetitive behavior. These symptoms are often indistinguishable from a normal (control) individual, due to which this disorder remains undiagnosed in early childhood leading to delayed treatment. Since the learning curve is steep during the initial age, an early diagnosis of autism could allow to take adequate interventions at the right time, which might positively affect the growth of an autistic child. Further, the traditional methods of autism diagnosis require multiple visits to a specialized psychiatrist, however this process can be time-consuming. In this paper, we present a learning based approach to automate autism diagnosis using simple and small action video clips of subjects. This task is particularly challenging because the amount of annotated data available is small, and the variations among samples from the two categories (ASD and control) are generally indistinguishable. This is also evident from poor performance of a binary classifier learned using the cross-entropy loss on top of a baseline encoder. To address this, we adopt contrastive feature learning in both self supervised and supervised learning frameworks, and show that these can lead to a significant increase in the prediction accuracy of a binary classifier on this task. We further validate this by conducting thorough experimental analyses under different set-ups on two publicly available datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_05379v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自闭症，也称为自闭症谱系障碍（或ASD），是一种神经系统疾病。其主要症状包括（言语和/或非言语）沟通困难，以及刻板/重复的行为。这些症状通常与正常（对照）个体难以区分，因此这种疾病在儿童早期仍未得到诊断，导致治疗延迟。由于在最初的年龄段，学习曲线是陡峭的，自闭症的早期诊断可以让我们在正确的时间采取适当的干预措施，这可能会对自闭症儿童的成长产生积极影响。此外，传统的自闭症诊断方法需要多次去看专业的精神科医生，但这个过程可能很耗时。在这篇论文中，我们提出了一种基于学习的方法，使用受试者的简单和小动作视频剪辑来自动诊断自闭症。这项任务特别具有挑战性，因为可用的注释数据量很小，而且两类样本（ASD和对照）之间的差异通常无法区分。这也从使用基线编码器上的交叉熵损失学习的二进制分类器的较差性能中显而易见。为了解决这一问题，我们在自监督和监督学习框架中都采用了对比特征学习，并表明这些可以显著提高二元分类器在该任务中的预测精度。我们在两个公开的数据集上，在不同的设置下进行了彻底的实验分析，进一步验证了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.05379v4" target="_blank">2209.05379v4</a>
                              </td>
                              <td>Action-based Early Autism Diagnosis Using Contrastive Feature Learning</td>
                              <td>Asha Rani</td>
                              <td>2022-09-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_05379v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.05379v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_16198v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_16198v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_16198v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_16198v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve state-of-the-art results over strong training-free baselines. Code is available at https://github.com/vishaal27/SuS-X.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_16198v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）是一种简单而有效的训练大规模视觉语言模型的方法。CLIP在不同的下游任务上展示了令人印象深刻的零样本分类和检索。然而，为了充分发挥其潜力，微调似乎仍然是必要的。微调整个CLIP模型可能是资源密集型且不稳定的。此外，最近旨在规避这种微调需求的方法仍然需要访问来自目标分布的图像。在本文中，我们采用了一种不同的方法，并探索了免费的“仅名称转移”训练机制，在该机制中，我们所拥有的关于下游任务的唯一知识包括下游目标类别的名称。我们提出了一种新的方法，即SuS-X，由两个关键构建块组成——SuS和TIP-X，既不需要密集的微调，也不需要昂贵的标记数据。SuS-X在19个基准数据集上实现了最先进的零样本分类结果。我们进一步展示了TIP-X在无训练少杆设置中的实用性，在那里，我们再次在强大的无训练基线上取得了最先进的结果。代码位于https://github.com/vishaal27/SuS-X.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.16198v3" target="_blank">2211.16198v3</a>
                              </td>
                              <td>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</td>
                              <td>Vishaal Udandarao</td>
                              <td>2022-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_16198v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.16198v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_01452v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP: Train Faster with Less Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_01452v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_01452v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_01452v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, the idea of data pruning as an embedded process in curriculum learning is novel.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_01452v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习模型需要大量的数据进行训练。然而，最近机器学习从以模型为中心转向以数据为中心。在以数据为中心的方法中，重点是改进和提高数据质量，以提高模型的学习性能，而不是重新设计模型架构。在本文中，我们提出了CLIP，即具有迭代数据修剪的课程学习。CLIP结合了两种以数据为中心的方法，即课程学习和数据集修剪，以提高模型学习的准确性和收敛速度。所提出的方案应用损失感知数据集修剪来迭代去除最不显著的样本，并在课程学习训练中逐步减小有效数据集的大小。在人群密度估计模型上进行的大量实验通过减少收敛时间和提高泛化能力，验证了将这两种方法相结合的概念。据我们所知，数据修剪作为课程学习中的一个嵌入过程的想法是新颖的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.01452v2" target="_blank">2212.01452v2</a>
                              </td>
                              <td>CLIP: Train Faster with Less Data</td>
                              <td>Muhammad Asif Khan</td>
                              <td>2022-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_01452v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.01452v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_18465v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Federated Learning of Gboard Language Models with Differential Privacy</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_18465v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_18465v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_18465v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models additionally trained with secure aggregation~\citep{bonawitz2017practical}. We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees. We summarize our experience and provide concrete suggestions on DP training for practitioners.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_18465v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们在谷歌键盘（Gboard）中使用联合学习（FL）和差分隐私（DP）来训练语言模型（LM）。我们应用DP Follow the Regularized Leader（DP-FTRL）~\citep｛kairouz21b｝算法来实现有意义的形式DP保证，而不需要对客户端设备进行统一采样。为了提供有利的隐私效用权衡，我们引入了一种新的客户端参与准则，并讨论了其配置在大规模系统中的含义。我们展示了如何将基于分位数的剪辑估计~\citep{andrew2019differential}与DP-FTRL相结合，以在训练期间自适应地选择剪辑范数，或在训练准备过程中减少超参数调整。在公共数据预训练的帮助下，我们训练和部署了20多个Gboard LMs，这些LMs使用$\rho\in（0.2，2）$实现了高实用性和$\rho-$zCDP隐私保证，另外还有两个模型使用安全聚合进行了额外训练~\citep｛bonawitz2017practical｝。我们很高兴地宣布，Gboard中的所有下一个单词预测神经网络LMs现在都有DP保证，Gbboard神经网络LMs的所有未来发布都需要DP保证。我们总结了我们的经验，并为从业者提供了DP培训的具体建议。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.18465v2" target="_blank">2305.18465v2</a>
                              </td>
                              <td>Federated Learning of Gboard Language Models with Differential Privacy</td>
                              <td>Zheng Xu</td>
                              <td>2023-05-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_18465v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.18465v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_03323v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_03323v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_03323v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_03323v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor attacks on multimodal contrastive learning. The code and checkpoints are available at https://github.com/nishadsinghi/CleanCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_03323v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式对比预训练已被用于在大量成对的图像-文本数据上训练多模式表示模型，如CLIP。然而，先前的研究表明，此类模型容易受到后门攻击。具体而言，当在后门示例上进行训练时，CLIP学习嵌入的后门触发器和目标标签之间的虚假相关性，从而在联合嵌入空间中对齐它们的表示。即使注入少量中毒实例，例如300万个预训练数据中的75个实例，也会显著操纵模型的行为，从而难以检测或消除这种相关性。为了解决这个问题，我们提出了CleanCLIP，这是一个微调框架，通过独立地重新调整单个模态的表示来削弱后门攻击引入的习得的虚假关联。我们证明，使用多模态对比和单模态自监督目标相结合的无监督微调可以显著减少后门攻击的影响。此外，我们展示了对特定任务标记图像数据的监督微调从CLIP视觉编码器中去除了后门触发器。我们的经验表明，CleanCLIP在良性示例上保持了模型性能，同时消除了对多模态对比学习的一系列后门攻击。代码和检查点位于https://github.com/nishadsinghi/CleanCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.03323v3" target="_blank">2303.03323v3</a>
                              </td>
                              <td>CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</td>
                              <td>Hritik Bansal</td>
                              <td>2023-03-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_03323v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.03323v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10244v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10244v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10244v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10244v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep recommendation systems (DRS) heavily depend on specialized HPC hardware and accelerators to optimize energy, efficiency, and recommendation quality. Despite the growing number of hardware errors observed in large-scale fleet systems where DRS are deployed, the robustness of DRS has been largely overlooked. This paper presents the first systematic study of DRS robustness against hardware errors. We develop Terrorch, a user-friendly, efficient and flexible error injection framework on top of the widely-used PyTorch. We evaluate a wide range of models and datasets and observe that the DRS robustness against hardware errors is influenced by various factors from model parameters to input characteristics. We also explore 3 error mitigation methods including algorithm based fault tolerance (ABFT), activation clipping and selective bit protection (SBP). We find that applying activation clipping can recover up to 30% of the degraded AUC-ROC score, making it a promising mitigation method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10244v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度推荐系统（DRS）在很大程度上依赖于专门的HPC硬件和加速器来优化能量、效率和推荐质量。尽管在部署DRS的大型机群系统中观察到越来越多的硬件错误，但DRS的稳健性在很大程度上被忽视了。本文首次系统地研究了DRS对硬件误差的鲁棒性。我们在广泛使用的PyTorch之上开发了Terroch，这是一个用户友好、高效和灵活的错误注入框架。我们评估了广泛的模型和数据集，并观察到DRS对硬件误差的鲁棒性受到从模型参数到输入特性的各种因素的影响。我们还探索了3种错误缓解方法，包括基于算法的容错（ABFT）、激活限幅和选择性比特保护（SBP）。我们发现，应用激活剪辑可以恢复高达30%的退化AUC-ROC评分，使其成为一种有前途的缓解方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10244v1" target="_blank">2307.10244v1</a>
                              </td>
                              <td>Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors</td>
                              <td>Dongning Ma</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10244v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10244v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_08208v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_08208v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_08208v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_08208v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep neural networks (DNNs) have been widely and successfully adopted and deployed in various applications of speech recognition. Recently, a few works revealed that these models are vulnerable to backdoor attacks, where the adversaries can implant malicious prediction behaviors into victim models by poisoning their training process. In this paper, we revisit poison-only backdoor attacks against speech recognition. We reveal that existing methods are not stealthy since their trigger patterns are perceptible to humans or machine detection. This limitation is mostly because their trigger patterns are simple noises or separable and distinctive clips. Motivated by these findings, we propose to exploit elements of sound ($e.g.$, pitch and timbre) to design more stealthy yet effective poison-only backdoor attacks. Specifically, we insert a short-duration high-pitched signal as the trigger and increase the pitch of remaining audio clips to `mask' it for designing stealthy pitch-based triggers. We manipulate timbre features of victim audios to design the stealthy timbre-based attack and design a voiceprint selection module to facilitate the multi-backdoor attack. Our attacks can generate more `natural' poisoned samples and therefore are more stealthy. Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our attacks under different settings ($e.g.$, all-to-one, all-to-all, clean-label, physical, and multi-backdoor settings) and their stealthiness. The code for reproducing main experiments are available at \url{https://github.com/HanboCai/BadSpeech_SoE}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_08208v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度神经网络（DNN）已被广泛而成功地应用于语音识别的各种应用中。最近，一些研究表明，这些模型容易受到后门攻击，对手可以通过毒害受害者模型的训练过程，将恶意预测行为植入受害者模型中。在本文中，我们重新审视了针对语音识别的仅限毒药的后门攻击。我们揭示了现有的方法并不是秘密的，因为它们的触发模式是人类或机器检测可以感知的。这种限制主要是因为它们的触发模式是简单的噪声或可分离的独特剪辑。受这些发现的启发，我们建议利用声音元素（$e.g.$、音高和音色）来设计更隐蔽但有效的仅限毒药的后门攻击。具体来说，我们插入一个短持续时间的高音信号作为触发器，并增加剩余音频片段的音高，以“屏蔽”它，从而设计基于音高的隐形触发器。我们操纵受害者音频的音色特征来设计基于音色的隐形攻击，并设计了声纹选择模块来促进多后门攻击。我们的攻击可以产生更多的“天然”中毒样本，因此更隐蔽。在基准数据集上进行了广泛的实验，验证了我们在不同设置（$e.g.$、all-to-one、all-to-all、干净标签、物理和多后门设置）下攻击的有效性及其隐蔽性。再现主要实验的代码可在\url上获得{https://github.com/HanboCai/BadSpeech_SoE}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.08208v1" target="_blank">2307.08208v1</a>
                              </td>
                              <td>Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound</td>
                              <td>Hanbo Cai</td>
                              <td>2023-07-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_08208v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.08208v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2103_13389v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generating Novel Scene Compositions from Single Images and Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2103_13389v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2103_13389v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2103_13389v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a large dataset for training, generative adversarial networks (GANs) can achieve remarkable performance for the image synthesis task. However, training GANs in extremely low data regimes remains a challenge, as overfitting often occurs, leading to memorization or training divergence. In this work, we introduce SIV-GAN, an unconditional generative model that can generate new scene compositions from a single training image or a single video clip. We propose a two-branch discriminator architecture, with content and layout branches designed to judge internal content and scene layout realism separately from each other. This discriminator design enables synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single image GANs, our model generates more diverse, higher quality images, while not being restricted to a single image setting. We further introduce a new challenging task of learning from a few frames of a single video. In this training setup the training images are highly similar to each other, which makes it difficult for prior GAN models to achieve a synthesis of both high quality and diversity.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2103_13389v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定用于训练的大数据集，生成对抗性网络（GANs）可以在图像合成任务中获得显著的性能。然而，在极低的数据状态下训练GANs仍然是一个挑战，因为过度拟合经常发生，导致记忆或训练偏差。在这项工作中，我们介绍了SIV-GAN，这是一种无条件的生成模型，可以从单个训练图像或单个视频剪辑中生成新的场景组成。我们提出了一种双分支鉴别器架构，其中内容和布局分支被设计为分别判断内部内容和场景布局的真实性。这种鉴别器设计能够合成具有不同内容和布局的场景的视觉上合理、新颖的构图，同时保留原始样本的上下文。与以前的单图像GANs相比，我们的模型生成了更多样、更高质量的图像，同时不局限于单个图像设置。我们进一步介绍了一项新的具有挑战性的任务，即从单个视频的几帧中学习。在这种训练设置中，训练图像彼此高度相似，这使得先前的GAN模型难以实现高质量和多样性的合成。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2103.13389v4" target="_blank">2103.13389v4</a>
                              </td>
                              <td>Generating Novel Scene Compositions from Single Images and Videos</td>
                              <td>Vadim Sushko</td>
                              <td>2021-03-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2103_13389v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2103.13389v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_11324v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-vocabulary Panoptic Segmentation with Embedding Modulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_11324v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_11324v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_11324v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary image segmentation is attracting increasing attention due to its critical applications in the real world. Traditional closed-vocabulary segmentation methods are not able to characterize novel objects, whereas several recent open-vocabulary attempts obtain unsatisfactory results, i.e., notable performance reduction on the closed vocabulary and massive demand for extra data. To this end, we propose OPSNet, an omnipotent and data-efficient framework for Open-vocabulary Panoptic Segmentation. Specifically, the exquisitely designed Embedding Modulation module, together with several meticulous components, enables adequate embedding enhancement and information exchange between the segmentation model and the visual-linguistic well-aligned CLIP encoder, resulting in superior segmentation performance under both open- and closed-vocabulary settings with much fewer need of additional data. Extensive experimental evaluations are conducted across multiple datasets (e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various circumstances, where the proposed OPSNet achieves state-of-the-art results, which demonstrates the effectiveness and generality of the proposed approach. The code and trained models will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_11324v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>开放式词汇图像分割由于其在现实世界中的重要应用而越来越受到关注。传统的封闭式词汇分割方法无法表征新的对象，而最近的几次开放式词汇尝试获得了令人不满意的结果，即封闭式词汇的性能显著降低，对额外数据的大量需求。为此，我们提出了OPSNet，一个无所不能、数据高效的开放词汇泛光分割框架。具体而言，精心设计的嵌入调制模块，加上几个细致的组件，能够在分割模型和视觉语言良好对齐的CLIP编码器之间进行充分的嵌入增强和信息交换，从而在开放和封闭的词汇设置下实现卓越的分割性能，而对额外数据的需求要少得多。在各种情况下，在多个数据集（例如，COCO、ADE20K、Cityscapes和PascalContext）上进行了广泛的实验评估，其中所提出的OPSNet获得了最先进的结果，这证明了所提出方法的有效性和通用性。代码和经过训练的模型将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.11324v2" target="_blank">2303.11324v2</a>
                              </td>
                              <td>Open-vocabulary Panoptic Segmentation with Embedding Modulation</td>
                              <td>Xi Chen</td>
                              <td>2023-03-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_11324v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.11324v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_09181v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_09181v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_09181v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_09181v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of arbitrary concepts solely from textual inputs, a process commonly referred to as open-vocabulary semantic segmentation (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to overfit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes. To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification. Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our objective is to develop a fast OVS model that can perform comparably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable representation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which prevents the learned representation from collapsing onto specific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the generalizable knowledge of CLIP. Extensive experiments demonstrate that our proposed model achieves robust generalization performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facilitate future open-vocabulary research in the video domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_09181v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>预训练的视觉语言模型（如CLIP）的最新进展使得能够仅从文本输入中分割任意概念，这一过程通常被称为开放词汇语义分割（OVS）。然而，现有的OVS技术面临着一个根本的挑战：经过训练的分类器往往对训练过程中观察到的基类进行过拟合，从而导致对看不见的类的次优泛化性能。为了缓解这一问题，最近的研究建议使用额外的冷冻预训练CLIP进行分类。尽管如此，这种方法会产生大量的计算开销，因为CLIP视觉编码器必须为每个掩码重复向前传递，这使得它对于现实世界的应用来说不切实际。为了应对这一挑战，我们的目标是开发一种快速OVS模型，该模型可以在推理过程中在没有CLIP图像编码器额外计算负担的情况下执行得相当或更好。为此，我们提出了一个核心思想，即在对已知类进行微调时保留可推广表示。具体来说，我们引入了一种文本多样化策略，该策略为每个训练类别生成一组同义词，以防止学习到的表示塌陷到特定的已知类别名称上。此外，我们采用了一种文本引导的知识提取方法来保存CLIP的可推广知识。大量实验表明，我们提出的模型在各种数据集上都实现了稳健的泛化性能。此外，我们对开放词汇视频分割进行了初步探索，并提出了一个基准，可以促进未来视频领域的开放词汇研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.09181v2" target="_blank">2303.09181v2</a>
                              </td>
                              <td>Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</td>
                              <td>Kunyang Han</td>
                              <td>2023-03-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_09181v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.09181v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07694v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07694v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07694v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07694v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We evaluate benchmark deep reinforcement learning (DRL) algorithms on the task of portfolio optimisation under a simulator. The simulator is based on correlated geometric Brownian motion (GBM) with the Bertsimas-Lo (BL) market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact and use it as an upper bound to measure performance when including market impact. We found that the off-policy algorithms DDPG, TD3 and SAC were unable to learn the right Q function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation (GAE), were able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we found that PPO, combined with a hidden Markov model (HMM) to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07694v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们在模拟器下评估了投资组合优化任务的基准深度强化学习（DRL）算法。该模拟器基于相关几何布朗运动（GBM）和Bertsimas-Lo（BL）市场影响模型。使用Kelly准则（对数效用）作为目标，我们可以分析推导出没有市场影响的最优政策，并将其作为衡量包括市场影响时的绩效的上界。我们发现，由于有噪声的奖励，非策略算法DDPG、TD3和SAC无法学习正确的Q函数，因此性能较差。策略上的算法PPO和A2C使用广义优势估计（GAE），能够处理噪声并得出接近最优的策略。发现PPO的剪裁变体在防止策略一旦收敛就偏离最优时是重要的。在一个更具挑战性的环境中，我们在GBM参数中有制度变化，我们发现PPO与隐马尔可夫模型（HMM）相结合来学习和预测制度上下文，能够学习适应每个制度的不同政策。总体而言，我们发现这些算法的样本复杂度太高，需要200多万步才能在最简单的设置中学习一个好的政策，这相当于近8000年的日常价格。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07694v1" target="_blank">2307.07694v1</a>
                              </td>
                              <td>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</td>
                              <td>Chung I Lu</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07694v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07694v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07663v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">INVE: Interactive Neural Video Editing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07663v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07663v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07663v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Interactive Neural Video Editing (INVE), a real-time video editing solution, which can assist the video editing process by consistently propagating sparse frame edits to the entire video clip. Our method is inspired by the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from two major drawbacks: (1) the method is too slow for interactive editing, and (2) it offers insufficient support for some editing use cases, including direct frame editing and rigid texture tracking. To address these challenges we leverage and adopt highly efficient network architectures, powered by hash-grids encoding, to substantially improve processing speed. In addition, we learn bi-directional functions between image-atlas and introduce vectorized editing, which collectively enables a much greater variety of edits in both the atlas and the frames directly. Compared to LNA, our INVE reduces the learning and inference time by a factor of 5, and supports various video editing operations that LNA cannot. We showcase the superiority of INVE over LNA in interactive video editing through a comprehensive quantitative and qualitative analysis, highlighting its numerous advantages and improved performance. For video results, please see https://gabriel-huang.github.io/inve/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07663v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了交互式神经视频编辑（INVE），这是一种实时视频编辑解决方案，它可以通过将稀疏帧编辑持续传播到整个视频剪辑来帮助视频编辑过程。我们的方法受到了最近关于分层神经图谱（LNA）的工作的启发。然而，LNA有两个主要缺点：（1）该方法对于交互式编辑来说太慢；（2）它对一些编辑用例（包括直接帧编辑和刚性纹理跟踪）的支持不足。为了应对这些挑战，我们利用并采用了由哈希网格编码提供动力的高效网络架构，以大幅提高处理速度。此外，我们学习了图像图谱之间的双向函数，并引入了矢量化编辑，这共同实现了图谱和帧中更多种直接的编辑。与LNA相比，我们的INVE将学习和推理时间减少了5倍，并支持LNA无法进行的各种视频编辑操作。我们通过全面的定量和定性分析，展示了INVE在交互式视频编辑方面优于LNA的优势，突出了其众多优势和改进的性能。有关视频结果，请参阅https://gabriel-huang.github.io/inve/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07663v1" target="_blank">2307.07663v1</a>
                              </td>
                              <td>INVE: Interactive Neural Video Editing</td>
                              <td>Jiahui Huang</td>
                              <td>2023-07-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07663v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07663v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07494v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TALL: Thumbnail Layout for Deepfake Video Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07494v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07494v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07494v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing threats of deepfakes to society and cybersecurity have raised enormous public concerns, and increasing efforts have been devoted to this critical topic of deepfake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to improve generalization, then resized to sub-images and rearranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modifying a few lines of code. Inspired by the success of vision transformers, we incorporate TALL into Swin Transformer, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on the challenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code is available at https://github.com/rainy-xu/TALL4Deepfake.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07494v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>deepfakes对社会和网络安全的日益增长的威胁引起了公众的极大关注，人们越来越多地致力于deepfake视频检测这一关键话题。现有的视频方法实现了良好的性能，但是计算密集。本文介绍了一种简单而有效的策略——缩略图布局（TALL），它将视频剪辑转换为预定义的布局，以实现空间和时间相关性的保留。具体地说，连续的帧在每个帧中的固定位置被屏蔽以提高泛化能力，然后调整大小为子图像，并重新排列为预定义的布局作为缩略图。TALL是模型无关的，并且非常简单，只需修改几行代码。受视觉转换器成功的启发，我们将TALL融入Swin Transformer，形成了一种高效、有效的方法TALL Swin。在数据集内和跨数据集上进行的大量实验验证了TALL和SOTA TALL Swin的有效性和优越性。TALL Swin在具有挑战性的跨数据集任务FaceForensics++$\to$Celeb DF上实现了90.79$\%%$AUC。代码位于https://github.com/rainy-xu/TALL4Deepfake.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07494v1" target="_blank">2307.07494v1</a>
                              </td>
                              <td>TALL: Thumbnail Layout for Deepfake Video Detection</td>
                              <td>Yuting Xu</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07494v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07494v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07397v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07397v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07397v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07397v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and synthesized features. Extensive experiments on base-to-new generalization, cross-dataset transfer learning, and generalized zero-shot learning demonstrate the superiority of our approach. The code is available at \url{https://github.com/mrflogs/SHIP}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07397v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着人们对像CLIP这样的预训练视觉语言模型越来越感兴趣，最近的研究集中在使这些模型适应下游任务上。尽管取得了有希望的结果，但大多数现有方法都需要所有类的标记数据，由于长尾和齐普夫定律，这些数据在现实世界的应用中可能不适用。例如，有些类可能完全缺乏标记数据，例如新兴概念。为了解决这个问题，我们提出了一种即插即用的生成方法，称为\textbf｛S｝ynt\textbf{H｝es\textbf{I｝zed\textbf｛P｝rompts~（\textbf}SHIP｝），以改进现有的微调方法。具体来说，我们遵循变分自动编码器来引入一个生成器，该生成器通过将合成的提示和相应的类名输入到CLIP的文本编码器来重建视觉特征。以这种方式，我们很容易获得剩余的仅标记类的合成特征。之后，我们通过结合标记和合成特征，用现成的方法对CLIP进行微调。基于新泛化、跨数据集转移学习和广义零样本学习的大量实验证明了该方法的优越性。代码位于\url{https://github.com/mrflogs/SHIP}。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07397v1" target="_blank">2307.07397v1</a>
                              </td>
                              <td>Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</td>
                              <td>Zhengbo Wang</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07397v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07397v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07189v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multiplicative update rules for accelerating deep learning training and increasing robustness</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07189v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07189v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07189v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative update rules. To this end, we propose a novel multiplicative update rule and we extend their capabilities by combining it with a traditional additive update term, under a novel hybrid update method. We claim that the proposed framework accelerates training, while leading to more robust models in contrast to traditionally used additive update rule and we experimentally demonstrate their effectiveness in a wide range of task and optimization methods. Such tasks ranging from convex and non-convex optimization to difficult image classification benchmarks applying a wide range of traditionally used optimization methods and Deep Neural Network (DNN) architectures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07189v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>即使在深度学习（DL）在广泛的研究领域取得了最先进的性能的今天，加速训练和构建稳健的DL模型仍然是一项具有挑战性的任务。为此，几代研究人员一直致力于开发用于训练DL架构的稳健方法，这些方法对权重分布、模型架构和损失情况不太敏感。然而，这种方法仅限于自适应学习率优化器、初始化方案和削波梯度，而不研究参数更新的基本规则。尽管乘法更新对机器学习的早期发展做出了重大贡献，并拥有强大的理论主张，但据我们所知，这是第一项在DL训练加速和鲁棒性的背景下研究乘法更新的工作。在这项工作中，我们提出了一种适用于广泛优化算法的优化框架，并使人们能够应用替代更新规则。为此，我们提出了一种新的乘法更新规则，并通过将其与传统的加法更新项相结合，在一种新颖的混合更新方法下扩展了它们的能力。我们声称，与传统使用的加法更新规则相比，所提出的框架加速了训练，同时产生了更稳健的模型，我们通过实验证明了它们在广泛的任务和优化方法中的有效性。这些任务从凸和非凸优化到应用广泛的传统优化方法和深度神经网络（DNN）架构的困难图像分类基准。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07189v1" target="_blank">2307.07189v1</a>
                              </td>
                              <td>Multiplicative update rules for accelerating deep learning training and increasing robustness</td>
                              <td>Manos Kirtas</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07189v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07189v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_10727v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_10727v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_10727v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_10727v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a robustness benchmark for image-text matching models to assess their vulnerabilities. To this end, we insert adversarial texts and images into the search pool (i.e., gallery set) and evaluate models with the adversarial data. Specifically, we replace a word in the text to change the meaning of the text and mix images with different images to create perceptible changes in pixels. We assume that such explicit alterations would not deceive a robust model, as they should understand the holistic meaning of texts and images simultaneously. However, in our evaluations on the proposed benchmark, many state-of-the-art models show significant performance degradation, e.g., Recall@1: 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$, where the models favor adversarial texts/images over the original ones. This reveals the current vision-language models may not account for subtle changes or understand the overall context of texts and images. Our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-test methods in cross-modal retrieval task. Source code and dataset will be available at https://github.com/pseulki/rococo.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_10727v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们为图像-文本匹配模型提出了一个鲁棒性基准，以评估其脆弱性。为此，我们将对抗性文本和图像插入到搜索池（即图库集）中，并使用对抗性数据评估模型。具体来说，我们替换文本中的一个单词来改变文本的含义，并将图像与不同的图像混合，以在像素中产生可感知的变化。我们假设这种明确的改变不会欺骗一个稳健的模型，因为它们应该同时理解文本和图像的整体含义。然而，在我们对所提出的基准的评估中，许多最先进的模型显示出显著的性能退化，例如。，Recall@1：BLIP中的81.9%$\rightarrow$64.5%，VSE$\infty$中的66.1%$\rigightarrow$37.5%，其中模型更喜欢对抗性文本/图像而不是原始文本/图像。这表明，当前的视觉语言模型可能无法解释细微的变化，也无法理解文本和图像的整体上下文。我们的研究结果可以为提高视觉语言模型的稳健性以及在跨模态检索任务中设计更多样的压力测试方法提供见解。源代码和数据集将在https://github.com/pseulki/rococo.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.10727v2" target="_blank">2304.10727v2</a>
                              </td>
                              <td>RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models</td>
                              <td>Seulki Park</td>
                              <td>2023-04-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_10727v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.10727v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07135v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07135v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07135v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07135v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The negative samples in MMSD are not always reasonable. To solve the aforementioned issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings of MMSD, by removing the spurious cues and re-annotating the unreasonable samples. Meanwhile, we present a novel framework called multi-view CLIP that is capable of leveraging multi-grained cues from multiple perspectives (i.e., text, image, and text-image interaction view) for multi-modal sarcasm detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for building reliable multi-modal sarcasm detection systems and multi-view CLIP can significantly outperform the previous best baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07135v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，多模态讽刺检测引起了人们的广泛关注。然而，现有的基准测试（MMSD）存在一些不足，阻碍了可靠的多模态讽刺检测系统的发展：（1）MMSD中存在一些虚假线索，导致模型偏差学习；（2） MMSD中的负样本并不总是合理的。为了解决上述问题，我们引入了MMSD2.0，这是一个校正数据集，通过去除虚假线索和重新注释不合理的样本来修复MMSD的缺点。同时，我们提出了一种称为多视图CLIP的新框架，该框架能够利用来自多个视角（即文本、图像和文本-图像交互视图）的多粒度线索进行多模态讽刺检测。大量实验表明，MMSD2.0是构建可靠的多模态讽刺检测系统的一个有价值的基准，多视图CLIP可以显著优于以前的最佳基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07135v1" target="_blank">2307.07135v1</a>
                              </td>
                              <td>MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System</td>
                              <td>Libo Qin</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07135v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07135v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_09972v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fine-grained Text-Video Retrieval with Frozen Image Encoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_09972v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_09972v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_09972v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State-of-the-art text-video retrieval (TVR) methods typically utilize CLIP and cosine similarity for efficient retrieval. Meanwhile, cross attention methods, which employ a transformer decoder to compute attention between each text query and all frames in a video, offer a more comprehensive interaction between text and videos. However, these methods lack important fine-grained spatial information as they directly compute attention between text and video-level tokens. To address this issue, we propose CrossTVR, a two-stage text-video retrieval architecture. In the first stage, we leverage existing TVR methods with cosine similarity network for efficient text/video candidate selection. In the second stage, we propose a novel decoupled video text cross attention module to capture fine-grained multimodal information in spatial and temporal dimensions. Additionally, we employ the frozen CLIP model strategy in fine-grained retrieval, enabling scalability to larger pre-trained vision models like ViT-G, resulting in improved retrieval performance. Experiments on text video retrieval datasets demonstrate the effectiveness and scalability of our proposed CrossTVR compared to state-of-the-art approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_09972v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有技术的文本视频检索（TVR）方法通常利用CLIP和余弦相似性进行高效检索。同时，交叉注意力方法使用转换器解码器来计算每个文本查询和视频中所有帧之间的注意力，从而在文本和视频之间提供了更全面的交互。然而，这些方法缺乏重要的细粒度空间信息，因为它们直接计算文本和视频级别令牌之间的注意力。为了解决这个问题，我们提出了CrossTVR，一种两阶段的文本视频检索架构。在第一阶段，我们利用现有的具有余弦相似性网络的TVR方法进行有效的文本/视频候选选择。在第二阶段，我们提出了一种新的解耦视频-文本交叉注意力模块，以捕获空间和时间维度上的细粒度多模式信息。此外，我们在细粒度检索中使用了冻结的CLIP模型策略，实现了对更大的预训练视觉模型（如ViT-G）的可扩展性，从而提高了检索性能。在文本视频检索数据集上的实验证明了与最先进的方法相比，我们提出的CrossTVR的有效性和可扩展性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.09972v1" target="_blank">2307.09972v1</a>
                              </td>
                              <td>Fine-grained Text-Video Retrieval with Frozen Image Encoders</td>
                              <td>Zuozhuo Dai</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_09972v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.09972v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06608v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06608v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06608v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06608v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster performance of foundational models (e.g., CLIP) to their significant representational capacity and, conversely, their lack of discriminative prowess. To mitigate this issue, we propose the use of a margin-based loss strategy for the fine-tuning of foundational models on target images. The experimental results verify that our approach, which employs the basic Fast Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of other, more convoluted algorithms. We conclude by advocating for the research community to consider surrogate models as crucial determinants in the effectiveness of adversarial attacks in no-box settings. The implications of our work bear relevance for improving the efficacy of such adversarial attacks and the overall robustness of AI systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06608v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，无框对抗性攻击，即攻击者无法访问模型的架构、权重和训练数据，成为最实用和最具挑战性的攻击设置。然而，在无框设置的情况下，没有意识到代理模型选择过程中固有的潜力和灵活性。受利用基础模型解决下游任务的兴趣日益增长的启发，本文采用了一种创新的想法，即1）将对抗性攻击重新定义为下游任务。具体而言，图像噪声的产生是为了满足新兴趋势和2）引入基础模型作为替代模型。利用非鲁棒特征的概念，我们阐述了代理模型选择的两个指导原则，以解释为什么基础模型是这个角色的最佳选择。然而，矛盾的是，我们观察到这些基础模型表现不佳。通过分析特征空间中的这种意外行为，我们将基础模型（例如CLIP）的乏善可陈的性能归因于它们显著的代表能力，反过来，它们缺乏辨别能力。为了缓解这个问题，我们建议使用基于边际的损失策略来微调目标图像上的基础模型。实验结果证明，我们的方法采用了基本的快速梯度符号法（FGSM）攻击算法，其性能优于其他更复杂的算法。最后，我们主张研究界将替代模型视为无框环境下对抗性攻击有效性的关键决定因素。我们的工作对提高这种对抗性攻击的有效性和人工智能系统的整体稳健性具有重要意义。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06608v2" target="_blank">2307.06608v2</a>
                              </td>
                              <td>Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks</td>
                              <td>Jiaming Zhang</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06608v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06608v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06948v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-regulating Prompts: Foundational Model Adaptation without Forgetting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06948v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06948v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06948v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model's original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating {prompted} representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06948v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>即时学习已经成为微调各种下游任务的基础模型（如CLIP）的有效替代方案。使用任务特定目标（即交叉熵损失）进行常规训练的提示往往会过度拟合下游数据分布，并发现从冻结的CLIP中捕获任务不可知的一般特征具有挑战性。这导致了模型原有的泛化能力的丧失。为了解决这个问题，我们的工作引入了一个称为PromptSRC（具有自调节约束的提示）的自正则化提示框架。PromptSRC指导提示使用三管齐下的方法来优化特定任务和不可知任务的一般表示：（a）通过与冻结模型的相互协议最大化来调节{提示}表示，（b）在训练轨迹上用提示的自集成来调节以编码其互补强度，以及（c）用文本多样性进行调节，以减轻与视觉分支的样本多样性失衡。据我们所知，这是第一个用于即时学习的正则化框架，通过共同关注预先训练的模型特征、提示期间的训练轨迹和文本多样性来避免过度拟合。PromptSRC显式地引导提示学习一个表示空间，该空间在不影响CLIP泛化的情况下最大限度地提高下游任务的性能。我们在4个基准上进行了广泛的实验，其中PromptSRC与现有方法相比总体表现良好。我们的代码和预训练模型可在以下网站上公开获取：https://github.com/muzairkhattak/PromptSRC.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06948v1" target="_blank">2307.06948v1</a>
                              </td>
                              <td>Self-regulating Prompts: Foundational Model Adaptation without Forgetting</td>
                              <td>Muhammad Uzair Khattak</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06948v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06948v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06942v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06942v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06942v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06942v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06942v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了InternVid，这是一个以视频为中心的大型多模式数据集，能够学习强大且可转移的视频文本表示，用于多模式理解和生成。InternVid数据集包含700多万个视频，持续时间近760K小时，产生2.34亿个视频片段，并附有总计410万个单词的详细描述。我们的核心贡献是开发一种可扩展的方法，用大型语言模型（LLM）自主构建高质量的视频文本数据集，从而展示其在大规模学习视频语言表示方面的功效。具体而言，我们使用多尺度方法来生成视频相关描述。此外，我们还介绍了基于ViT-L的视频文本表示学习模型ViCLIP。通过对比学习在InternVid上学习，该模型展示了领先的零样本动作识别和具有竞争力的视频检索性能。除了识别和检索等基本的视频理解任务外，我们的数据集和模型还有广泛的应用。它们特别有利于生成交错的视频文本数据，用于学习以视频为中心的对话系统，推进视频到文本和文本到视频生成研究。这些拟议资源为对多模式视频理解和生成感兴趣的研究人员和从业者提供了一个工具。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06942v1" target="_blank">2307.06942v1</a>
                              </td>
                              <td>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</td>
                              <td>Yi Wang</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06942v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06942v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06940v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06940v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06940v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06940v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06940v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为视觉故事生成视频可能是一个乏味而复杂的过程，通常需要真人拍摄或图形动画渲染。为了绕过这些挑战，我们的主要想法是利用大量现有的视频片段，并通过定制它们的外观来合成连贯的讲故事视频。我们通过开发一个由两个功能模块组成的框架来实现这一点：（i）运动结构检索，它为视频候选者提供由查询文本描述的所需场景或运动上下文；（ii）结构引导的文本到视频合成，它在运动结构和文本提示的指导下生成情节对齐的视频。对于第一个模块，我们利用现成的视频检索系统，提取视频深度作为运动结构。对于第二个模块，我们提出了一个可控的视频生成模型，该模型提供了对结构和字符的灵活控制。视频是按照结构指导和外观说明合成的。为了确保剪辑之间的视觉一致性，我们提出了一种有效的概念个性化方法，该方法允许通过文本提示指定所需的角色身份。大量实验表明，与各种现有基线相比，我们的方法显示出显著的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06940v1" target="_blank">2307.06940v1</a>
                              </td>
                              <td>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</td>
                              <td>Yingqing He</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06940v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06940v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06925v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06925v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06925v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06925v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06925v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像（T2I）个性化允许用户通过在自然语言提示中结合自己的视觉概念来指导创造性的图像生成过程。最近，基于编码器的技术已经成为T2I个性化的一种新的有效方法，减少了对多个图像和长训练时间的需求。然而，大多数现有的编码器仅限于一个类域，这阻碍了它们处理各种概念的能力。在这项工作中，我们提出了一种领域不可知的方法，该方法不需要任何专门的数据集或有关个性化概念的先验信息。我们引入了一种新的基于对比的正则化技术，通过将预测的标记推向其最近的现有CLIP标记，在保持预测嵌入接近潜在空间的可编辑区域的同时，保持对目标概念特征的高保真度。我们的实验结果证明了我们方法的有效性，并表明学习到的令牌比非规则化模型预测的令牌更具语义。这导致了更好的表示，实现了最先进的性能，同时比以前的方法更灵活。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06925v1" target="_blank">2307.06925v1</a>
                              </td>
                              <td>Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</td>
                              <td>Moab Arar</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06925v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06925v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06795v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06795v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06795v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06795v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-language foundation models such as CLIP have shown impressive zero-shot performance on many tasks and datasets, especially thanks to their free-text inputs. However, they struggle to handle some downstream tasks, such as fine-grained attribute detection and localization. In this paper, we propose a multitask fine-tuning strategy based on a positive/negative prompt formulation to further leverage the capacities of the vision-language foundation models. Using the CLIP architecture as baseline, we show strong improvements on bird fine-grained attribute detection and localization tasks, while also increasing the classification performance on the CUB200-2011 dataset. We provide source code for reproducibility purposes: it is available at https://github.com/FactoDeepLearning/MultitaskVLFM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06795v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉语言基础模型（如CLIP）在许多任务和数据集上都显示出令人印象深刻的零样本性能，尤其是由于它们的自由文本输入。然而，他们很难处理一些下游任务，例如细粒度的属性检测和定位。在本文中，我们提出了一种基于积极/消极提示公式的多任务微调策略，以进一步利用视觉语言基础模型的能力。使用CLIP架构作为基线，我们在鸟类细粒度属性检测和定位任务方面表现出了强大的改进，同时也提高了CUB200-2011数据集的分类性能。我们为再现性目的提供源代码：可在https://github.com/FactoDeepLearning/MultitaskVLFM.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06795v1" target="_blank">2307.06795v1</a>
                              </td>
                              <td>Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks</td>
                              <td>Denis Coquenet</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06795v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06795v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_05845v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PIGEON: Predicting Image Geolocations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_05845v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_05845v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_05845v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_05845v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了PIGEON，这是一种用于行星级图像地理定位的多任务端到端系统，在外部基准和人类评估方面都实现了最先进的性能。我们的工作将语义地理单元创建与标签平滑相结合，对具有地理信息的图像进行视觉变换器的预训练，并使用ProtoNets在候选地理单元集上细化位置预测。PIGEON的贡献有三个方面：首先，我们设计了一种基于开源数据的语义地理单元创建和拆分算法，该算法可以适用于任何地理空间数据集。其次，我们展示了地理单元内细化的有效性，以及无监督聚类和ProtNets对任务的适用性。最后，我们将我们预先训练的CLIP变压器模型StreetCLIP公开用于相邻领域，应用于应对气候变化和城市和农村场景理解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.05845v2" target="_blank">2307.05845v2</a>
                              </td>
                              <td>PIGEON: Predicting Image Geolocations</td>
                              <td>Lukas Haas</td>
                              <td>2023-07-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_05845v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.05845v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_00864v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPood: Generalizing CLIP to Out-of-Distributions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_00864v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_00864v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_00864v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distribution. Experiments on diverse datasets with different OOD scenarios show that CLIPood consistently outperforms existing generalization techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_00864v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>分布外（OOD）泛化是机器学习的一个主要挑战，其中模型需要处理训练中的分布变化。对比语言图像预训练（CLIP）模型显示出令人印象深刻的零样本能力，但CLIP对下游任务的进一步适应会降低OOD性能。本文旨在将CLIP推广到下游任务的分布外测试数据。我们提出了CLIPood，这是一种微调方法，可以使CLIP模型适应OOD情况，在OOD情况下，领域转移和开放类都可能发生在看不见的测试数据上。为了从文本模态中利用类之间的语义关系，CLIPood引入了一种新的训练目标，即边缘度量softmax（MMS），该目标具有用于微调的类自适应边缘。为了结合预训练零样本模型和微调任务自适应模型，CLIPood利用一种新的优化策略Beta移动平均（BMA）来维持由Beta分布加权的时间集成。在不同OOD场景的不同数据集上的实验表明，CLIPood始终优于现有的泛化技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.00864v2" target="_blank">2302.00864v2</a>
                              </td>
                              <td>CLIPood: Generalizing CLIP to Out-of-Distributions</td>
                              <td>Yang Shu</td>
                              <td>2023-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_00864v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.00864v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06526v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06526v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06526v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06526v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale pre-trained vision-language models allow for the zero-shot text-based generation of 3D avatars. The previous state-of-the-art method utilized CLIP to supervise neural implicit models that reconstructed a human body mesh. However, this approach has two limitations. Firstly, the lack of avatar-specific models can cause facial distortion and unrealistic clothing in the generated avatars. Secondly, CLIP only provides optimization direction for the overall appearance, resulting in less impressive results. To address these limitations, we propose AvatarFusion, the first framework to use a latent diffusion model to provide pixel-level guidance for generating human-realistic avatars while simultaneously segmenting clothing from the avatar's body. AvatarFusion includes the first clothing-decoupled neural implicit avatar model that employs a novel Dual Volume Rendering strategy to render the decoupled skin and clothing sub-models in one space. We also introduce a novel optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which semantically separates the generation of body and clothes, and generates a variety of clothing styles. Moreover, we establish the first benchmark for zero-shot text-to-avatar generation. Our experimental results demonstrate that our framework outperforms previous approaches, with significant improvements observed in all metrics. Additionally, since our model is clothing-decoupled, we can exchange the clothes of avatars. Code will be available on Github.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06526v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模预先训练的视觉语言模型允许基于零样本文本生成3D虚拟形象。先前最先进的方法利用CLIP来监督重建人体网格的神经隐式模型。然而，这种方法有两个局限性。首先，缺乏特定于化身的模型会导致生成的化身中的面部失真和不现实的服装。其次，CLIP只为整体外观提供了优化方向，导致效果不那么令人印象深刻。为了解决这些限制，我们提出了AvatarFusion，这是第一个使用潜在扩散模型来提供像素级指导的框架，用于生成人类逼真的化身，同时从化身的身体中分割衣服。AvatarFusion包括第一个服装解耦神经隐式化身模型，该模型采用了一种新颖的双体积渲染策略来在一个空间中渲染解耦的皮肤和服装子模型。我们还介绍了一种新的优化方法，称为像素语义差异采样（PS-DS），该方法从语义上分离身体和衣服的生成，并生成各种服装风格。此外，我们为零样本文本到虚拟形象生成建立了第一个基准。我们的实验结果表明，我们的框架优于以前的方法，在所有指标上都有显著改进。此外，由于我们的模型是服装解耦的，我们可以交换化身的服装。代码将在Github上提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06526v1" target="_blank">2307.06526v1</a>
                              </td>
                              <td>AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion</td>
                              <td>Shuo Huang</td>
                              <td>2023-07-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06526v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06526v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_1901_07186v4_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Learning to Imitate from a Single Video Demonstration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_1901_07186v4_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_1901_07186v4_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_1901_07186v4_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Agents that can learn to imitate given video observation -- \emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-art techniques in these environments and can learn to imitate from a single video demonstration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_1901_07186v4_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够学习模仿给定视频观察的智能体-\emph{无需直接访问状态或动作信息}更适用于在自然世界中学习。然而，制定一个促进这一目标的强化学习（RL）代理仍然是一个重大挑战。我们使用对比训练来学习奖励函数，将代理人的行为与单个演示进行比较。我们使用暹罗递归神经网络架构来学习运动片段之间的空间和时间奖励，同时训练RL策略以最小化该距离。通过实验，我们还发现，包含多任务数据和额外的图像编码损失提高了学习奖励的时间一致性，从而显著提高了策略学习。我们展示了我们在2D中模拟人形、狗和猛禽特工以及3D中的四足动物和人形特工上的方法。我们证明，我们的方法在这些环境中优于当前最先进的技术，并且可以从单个视频演示中学习模仿。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/1901.07186v4" target="_blank">1901.07186v4</a>
                              </td>
                              <td>Towards Learning to Imitate from a Single Video Demonstration</td>
                              <td>Glen Berseth</td>
                              <td>2019-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_1901_07186v4_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/1901.07186v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_06082v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_06082v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_06082v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_06082v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve 25%-30% relative improvement in task completion over the previous state-of-the-art for two datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_06082v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现实世界环境中的增量决策是嵌入式人工智能中最具挑战性的任务之一。一个要求特别高的场景是视觉和语言导航（VLN），它需要视觉和自然语言理解以及空间和时间推理能力。具体化的代理需要将其对导航指令的理解建立在对街景等真实世界环境的观察中。尽管LLM在其他研究领域取得了令人印象深刻的成果，但如何将它们与交互式视觉环境最好地联系起来仍是一个持续存在的问题。在这项工作中，我们提出了VELMA，这是一种具体的LLM代理，它使用轨迹和视觉环境观察的语言化作为下一步行动的上下文提示。视觉信息由管道表达，该管道从人工编写的导航指令中提取地标，并使用CLIP来确定其在当前全景视图中的可见性。我们仅通过两个上下文示例就证明了VELMA能够成功地遵循街景中的导航指令。我们在几千个例子中进一步微调了LLM代理，并在两个数据集的任务完成率上比之前的最先进技术实现了25%-30%的相对改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.06082v1" target="_blank">2307.06082v1</a>
                              </td>
                              <td>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</td>
                              <td>Raphael Schumann</td>
                              <td>2023-07-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_06082v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.06082v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_04132v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_04132v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_04132v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_04132v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we release two new datasets of object-behaviour-facts extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_04132v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，根据描述场景序列的副词最好通过对对象行为的高级概念进行推理来识别的直觉，我们提出了一个新的框架的设计，该框架对从原始视频剪辑中提取的对象行为进行推理，以识别剪辑中相应的副词类型。重要的是，虽然以前的一般场景副词识别工作假设知道动作类型背后的片段，但我们的方法直接适用于视频片段的动作类型未知的更一般的问题设置。具体而言，我们提出了一种新的管道，从原始视频剪辑中提取人类可解释的对象行为事实，并提出了新的基于符号和变换器的推理方法，对这些提取的事实进行操作，以识别副词类型。实验结果表明，我们提出的方法与以前的先进方法相比表现良好。此外，为了支持符号视频处理，我们发布了两个新的从原始视频剪辑中提取的对象行为事实数据集——MSR-VTT-ASP和ActivityNetASP数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.04132v2" target="_blank">2307.04132v2</a>
                              </td>
                              <td>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</td>
                              <td>Amrit Diggavi Seshadri</td>
                              <td>2023-07-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_04132v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.04132v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2307_11067v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11067v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11067v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11067v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models. Leveraging recent powerful foundation models, DINOv2 and Segment Anything, we create descriptors and generate proposals, including binary masks for a given input RGB image. By matching proposals with reference descriptors created from CAD models, we achieve precise object ID assignment along with modal masks. We experimentally demonstrate that our method achieves state-of-the-art results in CAD-based novel object segmentation, surpassing existing approaches on the seven core datasets of the BOP challenge by 19.8\% AP using the same BOP evaluation protocol. Our source code is available at https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11067v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种简单的三阶段方法，使用RGB图像中的CAD模型来分割看不见的对象。利用最近强大的基础模型DINOv2和Segment Anything，我们创建描述符并生成建议，包括给定输入RGB图像的二进制掩码。通过将建议与从CAD模型创建的参考描述符相匹配，我们实现了精确的对象ID分配以及模态掩码。我们通过实验证明，我们的方法在基于CAD的新对象分割中取得了最先进的结果，使用相同的BOP评估协议，在BOP挑战的七个核心数据集上超过了现有方法19.8%AP。我们的源代码可在https://github.com/nv-nguyen/cnos.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11067v1" target="_blank">2307.11067v1</a>
                              </td>
                              <td>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</td>
                              <td>Van Nguyen Nguyen</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11067v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11067v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_10907v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_10907v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_10907v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_10907v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多视角自我监督学习（MVSSL）成功背后的机制尚不完全清楚。通过互信息（MI）的下界InfoNCE的视角研究了MVSSL的对比方法。然而，其他MVSSL方法与MI之间的关系仍不清楚。我们考虑了由熵和重建项（ER）组成的MI的不同下界，并通过其透镜分析了主要的MVSSL族。通过这个ER界，我们证明了DeepCluster和SwAV等基于聚类的方法最大化了MI。我们还重新解释了BYOL和DINO等基于蒸馏的方法的机制，表明它们明确地最大化了重建项，隐含地鼓励了稳定的熵，我们从经验上证实了这一点。我们表明，用该ER界取代常见MVSSL方法的目标可以获得有竞争力的性能，同时在使用较小的批量或较小的指数移动平均（EMA）系数进行训练时使其稳定。Github回购：https://github.com/apple/ml-entropy-reconstruction.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.10907v1" target="_blank">2307.10907v1</a>
                              </td>
                              <td>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</td>
                              <td>Borja Rodríguez-Gálvez</td>
                              <td>2023-07-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_10907v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.10907v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_03376v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_03376v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_03376v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised object discovery (UOD) refers to the task of discriminating the whole region of objects from the background within a scene without relying on labeled datasets, which benefits the task of bounding-box-level localization and pixel-level segmentation. This task is promising due to its ability to discover objects in a generic manner. We roughly categorise existing techniques into two main directions, namely the generative solutions based on image resynthesis, and the clustering methods based on self-supervised models. We have observed that the former heavily relies on the quality of image reconstruction, while the latter shows limitations in effectively modeling semantic correlations. To directly target at object discovery, we focus on the latter approach and propose a novel solution by incorporating weakly-supervised contrastive learning (WCL) to enhance semantic information exploration. We design a semantic-guided self-supervised learning model to extract high-level semantic features from images, which is achieved by fine-tuning the feature encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we introduce Principal Component Analysis (PCA) to localize object regions. The principal projection direction, corresponding to the maximal eigenvalue, serves as an indicator of the object region(s). Extensive experiments on benchmark unsupervised object discovery datasets demonstrate the effectiveness of our proposed solution. The source code and experimental results are publicly available via our project page at https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_03376v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督对象发现（UOD）是指在不依赖标记数据集的情况下，从场景内的背景中区分整个对象区域的任务，这有利于边界框级定位和像素级分割的任务。这项任务很有前途，因为它能够以通用的方式发现对象。我们将现有技术大致分为两个主要方向，即基于图像再合成的生成解决方案和基于自监督模型的聚类方法。我们观察到，前者在很大程度上依赖于图像重建的质量，而后者在有效建模语义相关性方面表现出局限性。为了直接针对对象发现，我们专注于后一种方法，并通过结合弱监督对比学习（WCL）来增强语义信息探索，提出了一种新的解决方案。我们设计了一个语义引导的自监督学习模型来从图像中提取高级语义特征，这是通过WCL微调自监督模型（即DINO）的特征编码器来实现的。随后，我们引入主成分分析（PCA）来定位对象区域。与最大特征值相对应的主投影方向用作对象区域的指示符。在基准无监督对象发现数据集上进行的大量实验证明了我们提出的解决方案的有效性。源代码和实验结果可通过我们的项目页面公开获取，网址为https://github.com/npucvr/WSCUOD.git.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.03376v1" target="_blank">2307.03376v1</a>
                              </td>
                              <td>Weakly-supervised Contrastive Learning for Unsupervised Object Discovery</td>
                              <td>Yunqiu Lv</td>
                              <td>2023-07-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_03376v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.03376v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08069v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs Beat YOLOs on Real-time Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08069v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08069v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08069v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，基于变压器的端到端检测器（DETR）取得了显著的性能。然而，DETR的高计算成本问题尚未得到有效解决，这限制了它们的实际应用，并使它们无法充分利用无后处理的好处，例如非最大值抑制（NMS）。本文首先分析了现代实时对象检测器中NMS对推理速度的影响，并建立了端到端速度基准。为了避免NMS引起的推理延迟，我们提出了一种实时检测转换器（RT-DETR），这是我们所知的第一个实时端到端对象检测器。具体而言，我们设计了一种高效的混合编码器，通过解耦尺度内交互和跨尺度融合来高效处理多尺度特征，并提出了IoU感知的查询选择，以提高对象查询的初始化。此外，我们提出的检测器支持通过使用不同的解码器层来灵活调整推理速度，而不需要重新训练，这有利于实时对象检测器的实际应用。我们的RT-DETR-L在COCO val2017上实现了53.0%的AP，在T4 GPU上实现了114 FPS，而RT-DETR-X实现了54.8%的AP和74 FPS，在速度和精度方面都优于相同规模的所有YOLO检测器。此外，我们的RT-DETR-R50实现了53.1%的AP和108 FPS，在精度上优于DINO-Deformable-DETR-R5 2.2%的AP和大约21倍的FPS。源代码和预训练模型可在https://github.com/lyuwenyu/RT-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08069v2" target="_blank">2304.08069v2</a>
                              </td>
                              <td>DETRs Beat YOLOs on Real-time Object Detection</td>
                              <td>Wenyu Lv</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08069v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08069v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06211v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06211v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06211v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06211v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Meta AI Research开发的分段任意模型（SAM）最近引起了人们的极大关注。在超过10亿个掩模的大型分割数据集上训练，SAM能够分割特定图像上的任何对象。在最初的SAM工作中，作者转向零短转移任务（如边缘检测）来评估SAM的性能。最近，许多工作试图研究SAM在各种场景中的性能，以识别和分割对象。此外，通过将SAM与其他模型（如Grounding DINO、Stable Diffusion、ChatGPT等）相结合，已经出现了许多项目来展示SAM作为基础模型的多功能性。随着相关论文和项目呈指数级增长，读者很难跟上SAM的发展。为此，本工作首次对SAM进行了全面的调查。这是一个正在进行的项目，我们打算定期更新手稿。因此，如果读者完成了与SAM相关的新作品，欢迎与我们联系，以便我们将其纳入下一版本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06211v3" target="_blank">2306.06211v3</a>
                              </td>
                              <td>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering</td>
                              <td>Chaoning Zhang</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06211v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06211v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09165v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09165v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09165v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents a novel object detector called DEYOv2, an improved version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate model training and enhance performance. The study delves into the limitations of one-to-one matching in optimization and proposes solutions to effectively address the issue, such as Rank Feature and Greedy Matching. This approach enables the third stage of DEYOv2 to maximize information acquisition from the first and second stages without needing NMS, achieving end-to-end optimization. By combining dense queries, sparse queries, one-to-many matching, and one-to-one matching, DEYOv2 leverages the advantages of each method. It outperforms all existing query-based end-to-end detectors under the same settings. When using ResNet-50 as the backbone and multi-scale features on the COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs, respectively. Compared to the end-to-end model DINO, DEYOv2 provides significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings. To the best of our knowledge, DEYOv2 is the first fully end-to-end object detector that combines the respective strengths of classical detectors and query-based detectors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09165v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了一种称为DEYOv2的新型物体检测器，这是第一代DEYO（DETR with YOLO）模型的改进版本。与前代类似，DEYOv2采用渐进式推理方法来加速模型训练并提高性能。该研究深入探讨了一对一匹配在优化中的局限性，并提出了有效解决该问题的解决方案，如秩特征和贪婪匹配。这种方法使DEYOv2的第三阶段能够最大限度地从第一和第二阶段获取信息，而无需NMS，实现端到端优化。通过组合密集查询、稀疏查询、一对多匹配和一对一匹配，DEYOv2充分利用了每种方法的优势。在相同设置下，它的性能优于所有现有的基于查询的端到端检测器。当在COCO数据集上使用ResNet-50作为主干和多尺度特征时，DEYOv2在12个和24个时期分别实现了51.1个AP和51.8个AP。与端到端模型DINO相比，DEYOv2在两个历元设置中提供了2.1 AP和1.4 AP的显著性能提升。据我们所知，DEYOv2是第一个完全端到端的对象检测器，它结合了经典检测器和基于查询的检测器的各自优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09165v2" target="_blank">2306.09165v2</a>
                              </td>
                              <td>DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09165v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09165v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12860v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DETRs with Collaborative Hybrid Assignments Training</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12860v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12860v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervisions on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic Deformable-DETR by 5.8% AP in 12-epoch training and 3.2% AP in 36-epoch training. The state-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 65.6% AP on COCO test-dev, outperforming previous methods with much fewer model sizes. Codes will be available at https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12860v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们观察到，在具有一对一集匹配的DETR中，被分配为正样本的查询太少，导致对编码器输出的稀疏监督，这大大损害了编码器的判别特征学习，反之亦然，影响了解码器中的注意力学习。为了缓解这一问题，我们提出了一种新的协作混合分配训练方案，即Co-DETR，以从通用的标签分配方式中学习更高效、更有效的基于DETR的检测器。这种新的训练方案可以通过训练由一对多标签分配（如ATSS和Faster RCNN）监督的多个并行辅助头，轻松增强编码器在端到端检测器中的学习能力。此外，我们通过从这些辅助头中提取正坐标来进行额外定制的正查询，以提高解码器中正样本的训练效率。在推断中，这些辅助头被丢弃，因此我们的方法没有给原始检测器引入额外的参数和计算成本，同时不需要手工制作的非最大值抑制（NMS）。我们进行了广泛的实验来评估所提出的方法对DETR变体的有效性，包括DAB-DETR、可变形DETR和DINO可变形DETR。具体而言，我们在12个时期的训练中将基本的可变形DETR提高了5.8%AP，在36个时期的培训中提高了3.2%。最先进的带Swin-L的DINO可变形DETR在COCO价值上仍然可以从58.5%提高到59.5%的AP。令人惊讶的是，与ViT-L主干相结合，我们在COCO测试开发上实现了65.6%的AP，优于以前的方法，模型尺寸要小得多。代码将在https://github.com/Sense-X/Co-DETR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12860v4" target="_blank">2211.12860v4</a>
                              </td>
                              <td>DETRs with Collaborative Hybrid Assignments Training</td>
                              <td>Zhuofan Zong</td>
                              <td>2022-11-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12860v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12860v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15472v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Taming Detection Transformers for Medical Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15472v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15472v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The accurate detection of suspicious regions in medical images is an error-prone and time-consuming process required by many routinely performed diagnostic procedures. To support clinicians during this difficult task, several automated solutions were proposed relying on complex methods with many hyperparameters. In this study, we investigate the feasibility of DEtection TRansformer (DETR) models for volumetric medical object detection. In contrast to previous works, these models directly predict a set of objects without relying on the design of anchors or manual heuristics such as non-maximum-suppression to detect objects. We show by conducting extensive experiments with three models, namely DETR, Conditional DETR, and DINO DETR on four data sets (CADA, RibFrac, KiTS19, and LIDC) that these set prediction models can perform on par with or even better than currently existing methods. DINO DETR, the best-performing model in our experiments demonstrates this by outperforming a strong anchor-based one-stage detector, Retina U-Net, on three out of four data sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15472v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>医学图像中可疑区域的准确检测是许多常规诊断程序所需的一个容易出错且耗时的过程。为了在这项艰巨的任务中为临床医生提供支持，提出了几种基于具有许多超参数的复杂方法的自动化解决方案。在这项研究中，我们研究了DEtection-TRansformer（DETR）模型用于体积医疗对象检测的可行性。与之前的工作相比，这些模型直接预测一组对象，而不依赖于锚的设计或手动启发式（如非最大值抑制）来检测对象。我们通过在四个数据集（CADA、RibFrac、KiTS19和LIDC）上对三个模型（即DETR、Conditional DETR和DINO DETR）进行广泛的实验表明，这些集合预测模型的性能可以与当前现有的方法相当，甚至更好。DINO DETR，我们实验中性能最好的模型，通过在四分之三的数据集上优于基于强锚的一级检测器Retina U-Net，证明了这一点。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15472v1" target="_blank">2306.15472v1</a>
                              </td>
                              <td>Taming Detection Transformers for Medical Object Detection</td>
                              <td>Marc K. Ickler</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15472v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15472v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13723v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Social AI and the Challenges of the Human-AI Ecosystem</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13723v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13723v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13723v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类与人工智能（AI）系统（包括助手和推荐人，简称AI）互动的大规模社会技术系统的兴起，增加了集体现象和临界点出现的机会，带来了意想不到的、可能是意想不到的后果。例如，如果太多司机在同一条路线上行驶，导航系统的建议可能会造成混乱，而社交媒体上的个性化建议可能会放大两极分化、过滤泡沫和激进化。另一方面，我们可以学习如何培养“群体智慧”和集体行动效应，以应对社会和环境挑战。为了理解人工智能对社会技术系统的影响，并设计下一代人工智能，与人类合作，帮助克服而不是加剧社会问题，我们建议在复杂系统、网络科学和人工智能的交叉点上建立社会人工智能的基础，概述可能的技术和科学挑战，并提出研究途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13723v1" target="_blank">2306.13723v1</a>
                              </td>
                              <td>Social AI and the Challenges of the Human-AI Ecosystem</td>
                              <td>Dino Pedreschi</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13723v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13723v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_13337v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_13337v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_13337v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 77.5% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 0.5%. For ViT-B, ADCLR achieves 79.8%, 84.0% accuracy on ImageNet by linear probing and finetune, outperforming iBOT by 0.3%, 0.2% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3% AP on object detection, 39.7% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2% and 1.2%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0% mIoU, 1.2% mAcc on the segme</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_13337v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的自监督学习框架ADCLR：精确和密集的对比表示学习，用于学习准确和密集的视觉表示。为了提取空间敏感信息，ADCLR除了引入全局对比之外，还引入了用于对比的查询补丁。与以前的密集对比方法相比，ADCLR主要有三个优点：i）同时实现全局判别和空间敏感表示，ii）模型有效（除了全局对比基线之外没有额外的参数），以及iii）无对应，因此实现更简单。我们的方法为对比方法实现了最先进的性能。在分类任务上，对于ViT-S，ADCLR在使用线性探测的ImageNet上实现了77.5%的前1级准确率，比没有我们设计的技术作为插件的基线（DINO）高出0.5%。对于ViT-B，ADCLR通过线性探测和微调在ImageNet上分别实现了79.8%和84.0%的准确率，分别比iBOT高出0.3%和0.2%的准确率。对于密集任务，在MS-COCO上，ADCLR在对象检测上实现了44.3%的AP，在实例分割上实现了39.7%的AP，分别比以前的SOTA方法SelfPatch提高了2.2%和1.2%。在ADE20K上，ADCLR在segme上比SelfPatch高出1.0%mIoU和1.2%mAcc</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.13337v1" target="_blank">2306.13337v1</a>
                              </td>
                              <td>Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</td>
                              <td>Shaofeng Zhang</td>
                              <td>2023-06-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_13337v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.13337v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09346v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rosetta Neurons: Mining the Common Units in a Model Zoo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09346v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09346v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call "Rosetta Neurons" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09346v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为各种视觉任务训练的不同神经网络是否有一些共同的表示？在本文中，我们在一系列具有不同架构、不同任务（生成和判别）和不同类型监督（类监督、文本监督、自监督）的模型中证明了我们称之为“罗塞塔神经元”的共同特征的存在。我们提出了一种在几种流行的视觉模型中挖掘罗塞塔神经元字典的算法：Class Supervisored-ResNet50、DINO-ResNet50、DINO-ViT、MAE、CLIP-ResNet50，BigGAN、StyleGAN-2、StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，无论具体任务或架构如何，都可以通过不同的模型学习，而无需使用语义标签。由于我们的分析中包含了生成模型，我们可以直接将共享的概念可视化。罗塞塔神经元促进了模型到模型的翻译，实现了各种基于反转的操作，包括跨类对齐、移位、缩放等，而无需专门训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09346v2" target="_blank">2306.09346v2</a>
                              </td>
                              <td>Rosetta Neurons: Mining the Common Units in a Model Zoo</td>
                              <td>Amil Dravid</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09346v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09346v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_06588v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DEYO: DETR with YOLO for Step-by-Step Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_06588v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_06588v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object detection is an important topic in computer vision, with post-processing, an essential part of the typical object detection pipeline, posing a significant bottleneck affecting the performance of traditional object detection models. The detection transformer (DETR), as the first end-to-end target detection model, discards the requirement of manual components like the anchor and non-maximum suppression (NMS), significantly simplifying the target detection process. However, compared with most traditional object detection models, DETR converges very slowly, and a query's meaning is obscure. Thus, inspired by the Step-by-Step concept, this paper proposes a new two-stage object detection model, named DETR with YOLO (DEYO), which relies on a progressive inference to solve the above problems. DEYO is a two-stage architecture comprising a classic target detection model and a DETR-like model as the first and second stages, respectively. Specifically, the first stage provides high-quality query and anchor feeding into the second stage, improving the performance and efficiency of the second stage compared to the original DETR model. Meanwhile, the second stage compensates for the performance degradation caused by the first stage detector's limitations. Extensive experiments demonstrate that DEYO attains 50.6 AP and 52.1 AP in 12 and 36 epochs, respectively, while utilizing ResNet-50 as the backbone and multi-scale features on the COCO dataset. Compared with DINO, an optimal DETR-like model, the developed DEYO model affords a significant performance improvement of 1.6 AP and 1.2 AP in two epoch settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_06588v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>物体检测是计算机视觉中的一个重要课题，后处理是典型物体检测流水线的重要组成部分，对传统物体检测模型的性能构成了显著的瓶颈。检测转换器（DETR）作为第一个端到端目标检测模型，摒弃了锚和非最大值抑制（NMS）等手动组件的要求，大大简化了目标检测过程。然而，与大多数传统的对象检测模型相比，DETR收敛非常慢，并且查询的含义模糊不清。因此，受分步概念的启发，本文提出了一种新的两阶段目标检测模型，称为DETR with YOLO（DEYO），该模型依靠渐进推理来解决上述问题。DEYO是两阶段架构，包括分别作为第一和第二阶段的经典目标检测模型和类DETR模型。具体而言，第一阶段向第二阶段提供高质量的查询和锚点馈送，与原始DETR模型相比，提高了第二阶段的性能和效率。同时，第二级补偿由第一级检测器的限制引起的性能下降。大量实验表明，DEYO在12个和36个时期分别达到50.6个AP和52.1个AP，同时利用ResNet-50作为COCO数据集上的主干和多尺度特征。与最佳类DETR模型DINO相比，所开发的DEYO模型在两个历元设置中提供了1.6 AP和1.2 AP的显著性能改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.06588v3" target="_blank">2211.06588v3</a>
                              </td>
                              <td>DEYO: DETR with YOLO for Step-by-Step Object Detection</td>
                              <td>Haodong Ouyang</td>
                              <td>2022-11-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_06588v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.06588v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09345v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating Data Attribution for Text-to-Image Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09345v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09345v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09345v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然大型文本到图像模型能够合成“新颖”的图像，但这些图像必然是训练数据的反映。这种模型中的数据归属问题——训练集中的哪些图像对给定生成图像的出现最负责任——是一个困难但重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估归因，该方法将现有的大规模模型调整为给定的示例对象或风格。我们的关键见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到示例的影响。通过我们的新数据集，我们能够评估各种数据归因算法和不同的可能特征空间。此外，通过在我们的数据集上进行训练，我们可以针对归因问题调整标准模型，如DINO、CLIP和ViT。尽管该过程是针对小样本集调整的，但我们显示了对大样本集的推广。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因分数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09345v1" target="_blank">2306.09345v1</a>
                              </td>
                              <td>Evaluating Data Attribution for Text-to-Image Models</td>
                              <td>Sheng-Yu Wang</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09345v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09345v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_07483v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semi-supervised learning made simple with self-supervised clustering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_07483v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_07483v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised principles. In this paper, we propose a conceptually simple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss. This approach may be interpreted as imposing the cluster centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CIFAR100 and ImageNet.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_07483v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习模型已被证明可以在不需要人工注释的情况下学习丰富的视觉表示。然而，在许多现实世界的场景中，标签是部分可用的，这激发了受自我监督原则启发的半监督方法的最新工作。在本文中，我们提出了一种概念简单但经验强大的方法，将基于聚类的自监督方法（如SwAV或DINO）转变为半监督学习者。更准确地说，我们引入了一个多任务框架，将使用基本事实标签的监督目标和依赖于具有单个交叉熵损失的聚类分配的自监督目标合并在一起。这种方法可以被解释为将聚类质心强制作为类原型。尽管它很简单，但我们提供的经验证据表明，我们的方法非常有效，并在CIFAR100和ImageNet上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.07483v1" target="_blank">2306.07483v1</a>
                              </td>
                              <td>Semi-supervised learning made simple with self-supervised clustering</td>
                              <td>Enrico Fini</td>
                              <td>2023-06-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_07483v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.07483v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05382v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Automatic Image Blending Algorithm Based on SAM and DINO</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05382v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05382v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of image blending has gained popularity in recent years for its ability to create visually stunning content. However, the current image blending algorithm has the following problems: 1) The manual creation of the image blending mask requires a lot of manpower and material resources; 2) The image blending algorithm cannot effectively solve the problems of brightness distortion and low resolution. To this end, we propose a new image blending method: it combines semantic object detection and segmentation with corresponding mask generation to automatically blend images, while a two-stage iterative algorithm based on our proposed new saturation loss and PAN algorithm to fix brightness distortion and low resolution issues. Results on publicly available datasets show that our method outperforms many classic image blending algorithms on various performance metrics such as PSNR and SSIM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05382v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，图像融合领域因其创造视觉上令人惊叹的内容的能力而广受欢迎。然而，目前的图像混合算法存在以下问题：1）手动创建图像混合掩模需要大量的人力和物力；2） 图像混合算法不能有效地解决亮度失真和分辨率低的问题。为此，我们提出了一种新的图像混合方法：它将语义对象检测和分割与相应的掩模生成相结合来自动混合图像，而基于我们提出的新饱和度损失和PAN算法的两阶段迭代算法来解决亮度失真和低分辨率问题。在公开数据集上的结果表明，我们的方法在各种性能指标（如PSNR和SSIM）上优于许多经典的图像混合算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05382v2" target="_blank">2306.05382v2</a>
                              </td>
                              <td>Automatic Image Blending Algorithm Based on SAM and DINO</td>
                              <td>Haochen Xue</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05382v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05382v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06203v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FLSL: Feature-level Self-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06203v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06203v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better 20 understand the success of FLSL.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06203v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前的自监督学习（SSL）方法（例如，SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，并且不能很好地推广到密集预测任务，例如对象检测和分割。为了使SSL与密集预测相一致，本文首次演示了视觉变换器（ViT）的基本均值偏移聚类过程，该过程与自然图像语义（例如，对象和填充物的世界）非常一致。通过使用transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类SSL方法，即特征级自监督学习（FLSL）。我们给出了FLSL问题的形式化定义，并从均值转移和k-均值的角度构建了目标。我们表明，FLSL促进了显著的语义聚类表示，并学习了一种适用于视图内和视图间特征聚类的嵌入方案。实验表明，使用以ViT-S/16和ViT-S/8为主干的Mask R-CNN，FLSL在密集预测任务中产生了显著的改进，在MS-COCO上分别实现了44.9（+2.8）%AP和46.5%AP，在实例分割中实现了40.8（+2.3）%AP，和42.1%AP。FLSL在其他基准测试中始终优于现有的SSL方法，包括UAVDT上的无人机对象检测和DAVIS 2017上的视频实例分割。最后，我们介绍了可视化和各种消融研究，以更好地了解FLSL的成功。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06203v1" target="_blank">2306.06203v1</a>
                              </td>
                              <td>FLSL: Feature-level Self-supervised Learning</td>
                              <td>Qing Su</td>
                              <td>2023-06-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06203v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06203v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2110_15444v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">10 Security and Privacy Problems in Large Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2110_15444v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2110_15444v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation models, including six confidentiality problems, three integrity problems, and one availability problem. For each problem, we discuss potential opportunities and challenges. We hope our book chapter will inspire future research on the security and privacy of foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2110_15444v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型，如GPT、CLIP和DINO，在过去几年中取得了革命性的进展，通常被认为是通用人工智能的一种很有前途的方法。特别是，采用自监督学习来使用大量未标记的数据预训练基础模型。预先训练的基础模型就像人工智能生态系统的“操作系统”。具体而言，基础模型可以用作许多下游任务的特征提取器，这些任务很少或没有标记的训练数据。现有的基础模型研究主要集中在预训练一个更好的基础模型，以提高其在非对抗性环境中下游任务的性能，而其在对抗性环境下的安全性和隐私性在很大程度上尚未得到探索。预先训练的基础模型的安全或隐私问题会导致人工智能生态系统的单点故障。在本书的章节中，我们讨论了预训练的基础模型的10个基本安全和隐私问题，包括6个机密性问题、3个完整性问题和1个可用性问题。对于每一个问题，我们都会讨论潜在的机遇和挑战。我们希望本书的章节将启发未来对基金会模型的安全性和隐私性的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2110.15444v3" target="_blank">2110.15444v3</a>
                              </td>
                              <td>10 Security and Privacy Problems in Large Foundation Models</td>
                              <td>Jinyuan Jia</td>
                              <td>2021-10-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2110_15444v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2110.15444v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04675v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04675v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04675v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization; none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 16 common metrics for 8 different encoders at https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04675v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们系统地研究了各种基于图像的生成模型，这些模型跨越语义不同的数据集，以理解和改进用于评估它们的特征提取器和度量。利用心理物理学中的最佳实践，我们通过对生成模型进行迄今为止最大规模的评估实验，测量了人类对生成样本的图像真实感的感知，并发现没有任何现有指标与人类评估密切相关。与用于评估生成模型的整体性能、保真度、多样性和记忆的16个现代指标相比，我们发现，由人类判断的扩散模型的最先进的感知真实性没有反映在常见的指标中，如FID。这种差异并不能用生成样本的多样性来解释，尽管其中一个原因是过度依赖Inception-V3。我们通过对替代自监督特征提取器的研究来解决这些缺陷，发现单个网络编码的语义信息在很大程度上取决于它们的训练过程，并表明DINOv2-ViT-L/14允许对生成模型进行更丰富的评估。接下来，我们研究了数据记忆，发现生成模型确实在像CIFAR10这样的简单、较小的数据集上记忆训练示例，但不一定在像ImageNet这样的更复杂数据集上。然而，我们的实验表明，当前的指标并不能正确地检测记忆；文献中没有一个能够将记忆与其他现象（如填充不足或模式收缩）区分开来。为了促进生成模型及其评估的进一步发展，我们发布了所有生成的图像数据集、人类评估数据和模块化库，以计算8个不同编码器的16个通用度量https://github.com/layer6ai-labs/dgm-eval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04675v1" target="_blank">2306.04675v1</a>
                              </td>
                              <td>Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</td>
                              <td>George Stein</td>
                              <td>2023-06-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04675v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04675v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_03881v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Emergent Correspondence from Image Diffusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_03881v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_03881v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_03881v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何明确监督的情况下，图像扩散模型中会出现对应关系。我们提出了一种简单的策略来从扩散网络中提取这种隐含的知识作为图像特征，即diffusion features（DIFT），并使用它们来建立真实图像之间的对应关系。在没有对特定任务的数据或注释进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应性方面优于弱监督方法和有竞争力的现成特征。特别是在语义对应方面，来自Stable Diffusion的DIFT能够在具有挑战性的SPair 71k基准上分别比DINO和OpenCLIP高出19和14个准确度点。它甚至在18个类别中的9个类别上优于最先进的监督方法，同时在总体性能上保持不变。项目页面：https://diffusionfeatures.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.03881v1" target="_blank">2306.03881v1</a>
                              </td>
                              <td>Emergent Correspondence from Image Diffusion</td>
                              <td>Luming Tang</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_03881v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.03881v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04654v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04654v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04654v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose a simple yet effective transformer framework for self-supervised learning called DenseDINO to learn dense visual representations. To exploit the spatial information that the dense prediction tasks require but neglected by the existing self-supervised transformers, we introduce point-level supervision across views in a novel token-based way. Specifically, DenseDINO introduces some extra input tokens called reference tokens to match the point-level features with the position prior. With the reference token, the model could maintain spatial consistency and deal with multi-object complex scene images, thus generalizing better on dense prediction tasks. Compared with the vanilla DINO, our approach obtains competitive performance when evaluated on classification in ImageNet and achieves a large margin (+7.2% mIoU) improvement in semantic segmentation on PascalVOC under the linear probing protocol for segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04654v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种简单而有效的自监督学习转换器框架，称为DenseDINO，用于学习密集的视觉表示。为了利用密集预测任务所需但被现有的自监督变换器忽略的空间信息，我们以一种新的基于令牌的方式引入了跨视图的点级监督。具体而言，DenseDINO引入了一些称为参考标记的额外输入标记，以将点级特征与位置先验相匹配。有了参考令牌，该模型可以保持空间一致性，处理多目标复杂场景图像，从而更好地推广密集预测任务。与普通的DINO相比，当在ImageNet中对分类进行评估时，我们的方法获得了有竞争力的性能，并且在用于分割的线性探测协议下，在PascalVOC上的语义分割方面实现了大幅度的改进（+7.2%mIoU）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04654v1" target="_blank">2306.04654v1</a>
                              </td>
                              <td>DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency</td>
                              <td>Yike Yuan</td>
                              <td>2023-06-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04654v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04654v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07598v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07598v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07598v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses Hungarian matching to filter redundant noised queries and $\textit{query alignment}$ to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0, and DIOR-R benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07598v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着检测变压器（DETR）的变体DINO的发布，检测变压器凭借其端到端设计和可扩展性的优点打破了对象检测基准的记录。然而，DETR向面向对象检测的扩展尚未得到彻底研究，尽管预计其端到端架构会带来更多好处，例如消除NMS和锚相关成本。在本文中，我们提出了第一个基于强DINO的面向对象检测基线。我们发现，直接使用DETR进行定向对象检测并不能保证无重复预测，并提出了一个简单的成本来缓解这一问题。此外，我们引入了$\textit｛动态去噪｝$策略，该策略使用匈牙利匹配来过滤冗余噪声查询，并使用$\textit{查询对齐｝$来保持Transformer解码器层之间的匹配一致性。我们提出的模型优于之前的旋转DETR和其他同行，在DOTA-1.0/v.5/v.20和DIOR-R基准测试中实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07598v3" target="_blank">2305.07598v3</a>
                              </td>
                              <td>RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection</td>
                              <td>Hakjin Lee</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07598v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07598v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2206_09959v5_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Global Context Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2206_09959v5_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2206_09959v5_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO detection head achieves a box AP of 58.3 on MS COCO dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2206_09959v5_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了全局上下文视觉转换器（GC-ViT），这是一种提高计算机视觉参数和计算利用率的新架构。我们的方法利用全局上下文自注意模块，结合标准的局部自注意，有效和高效地对长距离和短距离空间交互进行建模，而不需要计算注意力掩码或移动局部窗口等昂贵的操作。此外，我们解决了ViTs中缺乏电感偏置的问题，并建议在我们的架构中利用改进的融合反向残差块。我们提出的GC-ViT在图像分类、对象检测和语义分割任务中取得了最先进的结果。在用于分类的ImageNet-1K数据集上，具有51M、90M和201M参数的GC ViT变体在224图像分辨率和没有任何预训练的情况下分别达到84.3%、85.0%和85.7%的Top-1准确率，因此大大超过了类似规模的现有技术，如基于CNN的ConvNeXt和基于ViT的MaxViT和Swin Transformer。使用MS COCO和ADE20K数据集在对象检测、实例分割和语义分割的下游任务中预先训练的GC-ViT骨干始终优于先前的工作。具体而言，具有4级DINO检测头的GC ViT在MS COCO数据集上实现了58.3的框AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2206.09959v5" target="_blank">2206.09959v5</a>
                              </td>
                              <td>Global Context Vision Transformers</td>
                              <td>Ali Hatamizadeh</td>
                              <td>2022-06-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2206_09959v5_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2206.09959v5" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_01398v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_01398v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_01398v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite impressive empirical advances of SSL in solving various tasks, the problem of understanding and characterizing SSL representations learned from input data remains relatively under-explored. We provide a comparative analysis of how the representations produced by SSL models differ when masking parts of the input. Specifically, we considered state-of-the-art SSL pretrained models, such as DINOv2, MAE, and SwaV, and analyzed changes at the representation levels across 4 Image Classification datasets. First, we generate variations of the datasets by applying foreground and background segmentation. Then, we conduct statistical analysis using Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA) to evaluate the robustness of the representations learned in SSL models. Empirically, we show that not all models lead to representations that separate foreground, background, and complete images. Furthermore, we test different masking strategies by occluding the center regions of the images to address cases where foreground and background are difficult. For example, the DTD dataset that focuses on texture rather specific objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_01398v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管SSL在解决各种任务方面取得了令人印象深刻的经验进步，但理解和表征从输入数据中学习的SSL表示的问题仍然相对未得到充分探索。我们提供了SSL模型在屏蔽部分输入时产生的表示如何不同的比较分析。具体而言，我们考虑了最先进的SSL预训练模型，如DINOv2、MAE和SwaV，并分析了4个图像分类数据集在表示级别上的变化。首先，我们通过应用前景和背景分割来生成数据集的变化。然后，我们使用标准相关分析（CCA）和中心核对齐（CKA）进行统计分析，以评估SSL模型中学习的表示的稳健性。根据经验，我们表明，并非所有模型都能产生分离前景、背景和完整图像的表示。此外，我们通过遮挡图像的中心区域来测试不同的掩蔽策略，以解决前景和背景困难的情况。例如，专注于纹理而非特定对象的DTD数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.01398v1" target="_blank">2306.01398v1</a>
                              </td>
                              <td>Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal</td>
                              <td>Xavier F. Cadet</td>
                              <td>2023-06-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_01398v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.01398v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_00449v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_00449v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_00449v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. However, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. Self-Supervised Learning (SSL) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. Still, the effectiveness of SSL methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. In this work, we address this critical need by investigating four state-of-the-art SSL methods (MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We present an extensive analysis of the performance of these methods on the Cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. We examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. Correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of SSL - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. Further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. The code is available at https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_00449v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着深度神经网络方法的日益普及，外科计算机视觉领域取得了长足的突破。然而，训练此类模型的标准完全监督方法需要大量的注释数据，成本高得令人望而却步；尤其是在临床领域。自监督学习（SSL）方法已经开始在普通计算机视觉社区中获得吸引力，它代表了这些注释成本的潜在解决方案，允许仅从未标记的数据中学习有用的表示。尽管如此，SSL方法在更复杂和更有影响力的领域（如医学和外科）的有效性仍然有限，尚未探索。在这项工作中，我们通过在外科计算机视觉的背景下研究四种最先进的SSL方法（MoCov2、SimCLR、DINO、SwAV）来解决这一关键需求。我们在Cholec80数据集上对这些方法在外科上下文理解、相位识别和工具存在检测这两个基本且流行的任务中的性能进行了广泛的分析。我们检查了它们的参数化，然后检查了它们在半监督设置中相对于训练数据量的行为。正如这项工作中所描述和进行的那样，将这些方法正确地转移到手术中，与SSL的一般用途相比，可以获得实质性的性能提升——在相位识别方面高达7.4%，在工具存在检测方面高达20%——以及最先进的半监督相位识别方法高达14%。在高度多样化的外科手术数据集选择上获得的进一步结果显示出强大的泛化特性。代码位于https://github.com/CAMMA-public/SelfSupSurg.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.00449v3" target="_blank">2207.00449v3</a>
                              </td>
                              <td>Dissecting Self-Supervised Learning Methods for Surgical Computer Vision</td>
                              <td>Sanat Ramesh</td>
                              <td>2022-07-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_00449v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.00449v3" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_07044v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_07044v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_07044v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_07044v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自我监督的预训练有可能在没有人为注释的情况下生成表达表征。地球观测（EO）中的大多数预训练都是基于ImageNet或中型标记遥感（RS）数据集。我们共享一个未标记的RS数据集SSL4EO-S12（地球观测的自我监督学习-哨兵-1/2），以收集来自欧空局哨兵-1/2卫星任务的大规模、全球、多模式和多季节卫星图像语料库。对于EO应用，我们展示了SSL4EO-S12在一组方法的自监督预训练中的成功：MoCo-v2、DINO、MAE和data2vec。由此产生的模型产生的下游性能接近或超过监督学习的准确性度量。此外，与现有数据集相比，SSL4EO-S12上的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.07044v2" target="_blank">2211.07044v2</a>
                              </td>
                              <td>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</td>
                              <td>Yi Wang</td>
                              <td>2022-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_07044v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.07044v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_11922v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_11922v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_11922v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Object instance segmentation is a key challenge for indoor robots navigating cluttered environments with many small objects. Limitations in 3D sensing capabilities often make it difficult to detect every possible object. While deep learning approaches may be effective for this problem, manually annotating 3D data for supervised learning is time-consuming. In this work, we explore zero-shot instance segmentation (ZSIS) from RGB-D data to identify unseen objects in a semantic category-agnostic manner. We introduce a zero-shot split for Tabletop Objects Dataset (TOD-Z) to enable this study and present a method that uses annotated objects to learn the ``objectness'' of pixels and generalize to unseen object categories in cluttered indoor environments. Our method, SupeRGB-D, groups pixels into small patches based on geometric cues and learns to merge the patches in a deep agglomerative clustering fashion. SupeRGB-D outperforms existing baselines on unseen objects while achieving similar performance on seen objects. We further show competitive results on the real dataset OCID. With its lightweight design (0.4 MB memory requirement), our method is extremely suitable for mobile and robotic applications. Additional DINO features can increase performance with a higher memory requirement. The dataset split and code are available at https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_11922v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对象实例分割是室内机器人在有许多小对象的杂乱环境中导航的一个关键挑战。3D传感能力的局限性往往使检测每一个可能的物体变得困难。虽然深度学习方法可能对这个问题有效，但手动注释3D数据以进行监督学习是耗时的。在这项工作中，我们探索了RGB-D数据中的零样本实例分割（ZSIS），以语义类别认知的方式识别看不见的对象。我们为桌面对象数据集（TOD-Z）引入了一种零样本分割，以实现这项研究，并提出了一种方法，该方法使用带注释的对象来学习像素的“对象性”，并推广到杂乱室内环境中看不到的对象类别。我们的方法SupeRGB-D基于几何线索将像素分组为小块，并学习以深度聚集聚类的方式合并小块。SupeRGB-D在看不见的对象上优于现有的基线，同时在看到的对象上实现了类似的性能。我们在真实数据集OCID上进一步展示了有竞争力的结果。凭借其轻量级设计（需要0.4 MB内存），我们的方法非常适合移动和机器人应用。额外的DINO功能可以通过更高的内存要求来提高性能。数据集拆分和代码可在https://github.com/evinpinar/supergb-d.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.11922v2" target="_blank">2212.11922v2</a>
                              </td>
                              <td>SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments</td>
                              <td>Evin Pınar Örnek</td>
                              <td>2022-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_11922v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.11922v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15347v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15347v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15347v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple post-processing, SD features can perform quantitatively similar to SOTA representations. Interestingly, the qualitative analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of these two features works surprisingly well, and a zero-shot evaluation using nearest neighbors on these fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences can enable interesting applications such as instance swapping in two images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15347v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>文本到图像扩散模型在生成和编辑高质量图像方面取得了重大进展。因此，许多方法已经探索了扩散模型特征理解和处理下游任务的单个图像的能力，例如分类、语义分割和风格化。然而，人们对这些特征在多个不同的图像和对象中所揭示的内容知之甚少。在这项工作中，我们利用稳定扩散（SD）特征进行语义和密集对应，并发现通过简单的后处理，SD特征可以在数量上执行与SOTA表示类似的表现。有趣的是，定性分析表明，与现有的表示学习特征（如最近发布的DINOv2）相比，SD特征具有非常不同的特性：虽然DINOv2提供了稀疏但准确的匹配，但SD特征提供了高质量的空间信息，但有时语义匹配不准确。我们证明，这两个特征的简单融合效果出奇地好，使用这些融合特征的最近邻进行零样本评估，与基准数据集上的最新方法相比，性能显著提高，例如SPair-71k、PF-Pascal和TSS。我们还展示了这些对应关系可以实现有趣的应用程序，例如两个图像中的实例交换。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15347v1" target="_blank">2305.15347v1</a>
                              </td>
                              <td>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</td>
                              <td>Junyi Zhang</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15347v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15347v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Open-vocabulary Segmentation with Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练鲁棒和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识是有帮助的，但它严重损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过利用预先训练的基础模型CLIP和DINO的开放词汇多模态知识和对象推理能力，在不需要任何微调的情况下，解决了3D开放词汇分割中的挑战。具体而言，我们将CLIP中的开放词汇视觉和文本知识提取到神经辐射场（NeRF）中，该场有效地将2D特征提升到视图一致的3D分割中。此外，我们引入了相关性分布对齐损失和特征分布对齐损失，以分别减轻CLIP特征的模糊性，并从DINO特征中提取精确的对象边界，从而消除了训练过程中对分割注释的需要。大量实验表明，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v2" target="_blank">2305.14093v2</a>
                              </td>
                              <td>3D Open-vocabulary Segmentation with Foundation Models</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12223v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Makes for Good Visual Tokenizers for Large Language Models?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12223v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12223v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12223v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们实证研究了适当的预训练方法来构建良好的视觉标记器，使大型语言模型（LLM）成为强大的多模式大型语言模型。在我们旨在评估MLLMs视觉语义理解和细粒度感知能力的基准中，我们讨论了用主导方法（即DeiT、CLIP、MAE、DINO）预训练的不同视觉标记器，并观察到：i）完全/弱监督模型比自监督模型捕获更多的语义，但通过扩大预训练数据集缩小了差距。ii）自监督模型更擅长细粒度感知，其中补丁级别的监督尤其有效。iii）调整可视化标记器会导致从大规模预训练中获得的语义丢失，这对相对小规模的指令调整数据集是不利的。鉴于这些发现，我们回顾了试图统一语义和细粒度视觉理解的方法，例如，具有语义丰富目标的补丁级特征提取。我们获得了一个有趣的基于面具的洞察策略，这些策略曾经风靡一时，但可能不适用于获得良好的视觉标记器。基于这一关键观察，我们获得了一种新的MLLM，该MLLM配备了定制的良好视觉标记器（GVT），在多个尺度上表现出强大的视觉理解能力。特别是，在不引入额外参数和特定任务微调的情况下，GVT在视觉问答、图像字幕和其他细粒度视觉理解任务（如对象计数和多类识别）上实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12223v2" target="_blank">2305.12223v2</a>
                              </td>
                              <td>What Makes for Good Visual Tokenizers for Large Language Models?</td>
                              <td>Guangzhi Wang</td>
                              <td>2023-05-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12223v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12223v2" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13552v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Squared Neural Families: A New Class of Tractable Density Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13552v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13552v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13552v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>概率分布的灵活模型是许多机器学习任务的重要组成部分。我们开发并研究了一类新的概率分布，我们称之为平方神经家族（SNEFY），它是通过对神经网络的2-范数进行平方并相对于基测度对其进行归一化而形成的。根据类似于无限宽神经网络和高斯过程之间良好建立的联系的推理，我们表明，在许多感兴趣的情况下，SNEFY允许闭合形式的归一化常数，从而产生灵活但完全可处理的密度模型。SNEFY严格推广了经典指数族，在条件作用下是封闭的，并且具有可处理的边缘分布。说明了它们在各种密度估计和条件密度估计任务中的效用。软件可在https://github.com/RussellTsuchida/snefy.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13552v1" target="_blank">2305.13552v1</a>
                              </td>
                              <td>Squared Neural Families: A New Class of Tractable Density Models</td>
                              <td>Russell Tsuchida</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13552v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13552v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13291v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Materialistic: Selecting Similar Materials in Images</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13291v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13291v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO features coupled with a proposed Cross-Similarity module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model's behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13291v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>将图像分离为有意义的底层组件是编辑和理解图像的关键第一步。我们提出了一种方法，能够选择照片中展示与艺术家选择区域相同材料的区域。我们提出的方法对明暗处理、镜面高光和投射阴影都很稳健，可以在真实图像中进行选择。由于我们不依赖于语义分割（不同的木材或金属不应该一起选择），我们将该问题表述为基于用户提供的图像位置的基于相似性的分组问题。特别地，我们建议利用无监督的DINO特征，结合所提出的交叉相似性模块和MLP头来提取图像中的材料相似性。我们在发布的一个新的合成图像数据集上训练我们的模型。我们证明了我们的方法可以很好地推广到真实世界的图像。我们仔细分析了模型在不同材料特性和照明条件下的行为。此外，我们根据50张真实照片的手工注释基准对其进行了评估。我们在一系列应用中进一步展示了我们的模型，包括素材编辑、视频选择和检索具有类似素材的对象照片。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13291v1" target="_blank">2305.13291v1</a>
                              </td>
                              <td>Materialistic: Selecting Similar Materials in Images</td>
                              <td>Prafull Sharma</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13291v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13291v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11092v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Universal Domain Adaptation from Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11092v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11092v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11092v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型（例如，CLIP或DINOv2）通过在大量数据库上进行训练并适应特定的下游任务，在广泛的视觉任务上显示出了令人印象深刻的学习和转移能力。然而，有趣的是，基础模型尚未被充分探索用于通用域自适应（UniDA），即使用源域中的标记数据和目标域中的未标记数据来学习模型，以便学习的模型能够成功地适应目标数据。在本文中，我们使用基础模型对最先进的UniDA方法进行了全面的实证研究。我们首先证明，虽然基础模型大大提高了仅在源数据上训练模型的基线方法的性能，但现有的UniDA方法通常无法在基线上改进。这表明，对于使用基础模型的UniDA来说，新的研究工作是非常必要的。为此，我们在CLIP模型上提出了一种非常简单的目标数据提取方法，并在所有UniDA基准测试中实现了对基线的一致改进。我们的研究是在一种新提出的通用分类率（UCR）评估指标下进行的，该指标不含阈值和比率，解决了使用现有H-核心指标时遇到的阈值敏感问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11092v1" target="_blank">2305.11092v1</a>
                              </td>
                              <td>Universal Domain Adaptation from Foundation Models</td>
                              <td>Bin Deng</td>
                              <td>2023-05-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11092v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11092v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_08014v7_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_08014v7_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_08014v7_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most invariance-based self-supervised methods rely on single object-centric images (e.g., ImageNet images) for pretraining, learning features that invariant to geometric transformation. However, when images are not object-centric, the semantics of the image can be significantly altered due to cropping. Furthermore, as the model becomes insensitive to geometric transformations, it may struggle to capture location information. For this reason, we propose a Geometric Transformation Sensitive Architecture designed to be sensitive to geometric transformations, specifically focusing on four-fold rotation, random crop, and multi-crop. Our method encourages the student to be sensitive by predicting rotation and using targets that vary with those transformations through pooling and rotating the teacher feature map. Additionally, we use patch correspondence loss to encourage correspondence between patches with similar features. This approach allows us to capture long-term dependencies in a more appropriate way than capturing long-term dependencies by encouraging local-to-global correspondence, which occurs when learning to be insensitive to multi-crop. Our approach demonstrates improved performance when using non-object-centric images as pretraining data compared to other methods that train the model to be insensitive to geometric transformation. We surpass DINO[Caron et al.[2021b]] baseline in tasks including image classification, semantic segmentation, detection, and instance segmentation with improvements of 4.9 $Top-1 Acc$, 3.3 $mIoU$, 3.4 $AP^b$, and 2.7 $AP^m$. Code and pretrained models are publicly available at: https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_08014v7_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数基于不变性的自监督方法依赖于以单个对象为中心的图像（例如，ImageNet图像）进行预训练，学习对几何变换不变的特征。然而，当图像不是以对象为中心时，图像的语义可能会因裁剪而发生显著变化。此外，随着模型对几何变换变得不敏感，它可能很难捕捉位置信息。因此，我们提出了一种几何变换敏感架构，该架构设计为对几何变换敏感，特别关注四重旋转、随机裁剪和多重裁剪。我们的方法通过预测旋转，并通过汇集和旋转教师特征图，使用随这些转换而变化的目标，鼓励学生保持敏感。此外，我们使用补丁对应损失来鼓励具有相似特征的补丁之间的对应。这种方法使我们能够以一种比通过鼓励局部到全局的对应关系捕获长期依赖关系更合适的方式捕获长期依赖性，这种对应关系发生在学习对多作物不敏感时。与其他将模型训练为对几何变换不敏感的方法相比，我们的方法在使用非以对象为中心的图像作为预训练数据时提高了性能。我们在包括图像分类、语义分割、检测和实例分割在内的任务中超过了DINO[Caron等人[2021b]]基线，改进了4.9$Top-1Acc$、3.3$mIoU$、3.4$AP^b$和2.7$AP^m$。代码和预训练模型可在以下网站上公开获取：https://github.com/bok3948/GTSA</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.08014v7" target="_blank">2304.08014v7</a>
                              </td>
                              <td>Self-Supervised Learning from Non-Object Centric Images with a Geometric Transformation Sensitive Architecture</td>
                              <td>Taeho Kim</td>
                              <td>2023-04-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_08014v7_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.08014v7" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06558v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment and Track Anything</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06558v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06558v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This report presents a framework called Segment And Track Anything (SAMTrack) that allows users to precisely and effectively segment and track any object in a video. Additionally, SAM-Track employs multimodal interaction methods that enable users to select multiple objects in videos for tracking, corresponding to their specific requirements. These interaction methods comprise click, stroke, and text, each possessing unique benefits and capable of being employed in combination. As a result, SAM-Track can be used across an array of fields, ranging from drone technology, autonomous driving, medical imaging, augmented reality, to biological analysis. SAM-Track amalgamates Segment Anything Model (SAM), an interactive key-frame segmentation model, with our proposed AOT-based tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022 challenge, to facilitate object tracking in video. In addition, SAM-Track incorporates Grounding-DINO, which enables the framework to support text-based interaction. We have demonstrated the remarkable capabilities of SAM-Track on DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in diverse applications. The project page is available at: https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06558v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本报告提供了一个名为“分段和跟踪任何内容”（SAMTrack）的框架，该框架允许用户精确有效地分段和跟踪视频中的任何对象。此外，SAM Track采用多模式交互方法，使用户能够根据自己的特定要求选择视频中的多个对象进行跟踪。这些交互方法包括点击、笔划和文本，每种方法都具有独特的优点，并且能够组合使用。因此，SAM Track可用于一系列领域，从无人机技术、自动驾驶、医学成像、增强现实到生物分析。SAM Track将交互式关键帧分割模型Segment Anything Model（SAM）与我们提出的基于AOT的跟踪模型（DeAOT）合并，以促进视频中的对象跟踪，该模型在VOT 2022挑战的四个轨道中排名第一。此外，SAM Track结合了Grounding DINO，使框架能够支持基于文本的交互。我们已经在DAVIS-2016 Val（92.0%）和DAVIS-2017 Test（79.2%）上展示了SAM Track的卓越能力及其在各种应用中的实用性。项目页面位于：https://github.com/z-x-yang/Segment-and-Track-Anything.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06558v1" target="_blank">2305.06558v1</a>
                              </td>
                              <td>Segment and Track Anything</td>
                              <td>Yangming Cheng</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06558v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06558v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_06553v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_06553v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_06553v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce WeLayout, a novel system for segmenting the layout of corporate documents, which stands for WeChat Layout Analysis System. Our approach utilizes a sophisticated ensemble of DINO and YOLO models, specifically developed for the ICDAR 2023 Competition on Robust Layout Segmentation. Our method significantly surpasses the baseline, securing a top position on the leaderboard with a mAP of 70.0. To achieve this performance, we concentrated on enhancing various aspects of the task, such as dataset augmentation, model architecture, bounding box refinement, and model ensemble techniques. Additionally, we trained the data separately for each document category to ensure a higher mean submission score. We also developed an algorithm for cell matching to further improve our performance. To identify the optimal weights and IoU thresholds for our model ensemble, we employed a Bayesian optimization algorithm called the Tree-Structured Parzen Estimator. Our approach effectively demonstrates the benefits of combining query-based and anchor-free models for achieving robust layout segmentation in corporate documents.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_06553v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了WeLayout，一种新的企业文档布局分割系统，即微信布局分析系统。我们的方法利用了一套复杂的DINO和YOLO模型，专门为ICDAR 2023鲁棒布局分割竞赛开发。我们的方法大大超过了基线，以70.0的mAP稳居排行榜榜首。为了实现这一性能，我们集中精力增强任务的各个方面，如数据集扩充、模型架构、边界框细化和模型集成技术。此外，我们分别为每个文档类别训练数据，以确保更高的平均提交分数。我们还开发了一种用于小区匹配的算法，以进一步提高我们的性能。为了确定我们模型集成的最佳权重和IoU阈值，我们使用了一种称为树结构Parzen估计器的贝叶斯优化算法。我们的方法有效地展示了将基于查询的模型和无锚模型相结合以在公司文档中实现稳健布局分割的好处。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.06553v1" target="_blank">2305.06553v1</a>
                              </td>
                              <td>WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents</td>
                              <td>Mingliang Zhang</td>
                              <td>2023-05-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_06553v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.06553v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_01881v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_01881v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_01881v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on any pre-trained self-supervised model to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear classification performance of state-of-the-art self-supervised models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and enhancing these features through Q-score regularization makes representations more interpretable across all self-supervised models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_01881v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习在下游分类任务中显示出令人印象深刻的结果。然而，在理解他们的失败模式和解释他们习得的表征方面的工作有限。在本文中，我们研究了最先进的自监督模型的表示空间，包括SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg和Barlow Twins。在不使用类标签信息的情况下，我们发现了与图像中的独特物理属性相对应的判别特征，这些特征大多以正确分类的表示形式出现。使用这些特征，我们可以将表示空间压缩高达$40\%\$，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或Q-Score），这是一种模型不可知、无监督的分数，可以可靠地预测给定样本在线性评估过程中是否可能被错误分类，在ImageNet-100上实现了91.45的AUPRC，在ImageNet-1K上实现了78.78的AUPRC。Q-Score也可以用作任何预先训练的自监督模型的正则化术语，以纠正低质量的表示。与基线相比，使用Q-Score正则化进行微调可以将最先进的自监督模型的线性分类性能在ImageNet-100上提高5.8%，在ImageNet-1K上提高3.7%。最后，使用梯度热图和突出的ImageNet掩码，我们定义了一个度量来量化每个表示的可解释性。我们表明，判别特征与核心属性强相关，通过Q分数正则化增强这些特征使表示在所有自监督模型中更具可解释性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.01881v4" target="_blank">2203.01881v4</a>
                              </td>
                              <td>Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features</td>
                              <td>Neha Kalibhat</td>
                              <td>2022-03-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_01881v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.01881v4" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_14571v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_14571v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_14571v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-trained DINO model (as multiple channels) in the other branch. The results of our experiments on two publicly available medical imaging datasets show that the proposed pipeline outperforms U-Net and the state-of-the-art medical image segmentation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_14571v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管纯基于变换器的架构在许多计算机视觉任务中表现出了良好的性能，但引入了许多由CNN和变换器块组成的混合模型来适应更专业的任务。然而，尽管与医学图像分割中的细胞神经网络相比，基于纯变压器和混合变压器的架构都有性能增益，但其高昂的训练成本和复杂性使其在实际场景中使用具有挑战性。在这项工作中，我们提出了基于纯卷积层的简单架构，并表明只要利用从自监督预训练的视觉变换器网络（例如，DINO）获得的注意力图可视化，就可以以更低的计算成本胜过基于复杂变换器的网络。所提出的架构由两个编码器分支组成，其中原始图像在一个分支中作为输入，而来自预训练的DINO模型的多个自注意头的同一图像的注意图可视化（作为多个通道）在另一个分支。我们在两个公开可用的医学图像数据集上的实验结果表明，所提出的流水线优于U-Net和最先进的医学图像分割模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.14571v1" target="_blank">2304.14571v1</a>
                              </td>
                              <td>DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation</td>
                              <td>Yousef Yeganeh</td>
                              <td>2023-04-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_14571v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.14571v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13348v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TextDeformer: Geometry Manipulation using Text Guidance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13348v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13348v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a technique for automatically producing a deformation of an input triangle mesh, guided solely by a text prompt. Our framework is capable of deformations that produce both large, low-frequency shape changes, and small high-frequency details. Our framework relies on differentiable rendering to connect geometry to powerful pre-trained image encoders, such as CLIP and DINO. Notably, updating mesh geometry by taking gradient steps through differentiable rendering is notoriously challenging, commonly resulting in deformed meshes with significant artifacts. These difficulties are amplified by noisy and inconsistent gradients from CLIP. To overcome this limitation, we opt to represent our mesh deformation through Jacobians, which updates deformations in a global, smooth manner (rather than locally-sub-optimal steps). Our key observation is that Jacobians are a representation that favors smoother, large deformations, leading to a global relation between vertices and pixels, and avoiding localized noisy gradients. Additionally, to ensure the resulting shape is coherent from all 3D viewpoints, we encourage the deep features computed on the 2D encoding of the rendering to be consistent for a given vertex from all viewpoints. We demonstrate that our method is capable of smoothly-deforming a wide variety of source mesh and target text prompts, achieving both large modifications to, e.g., body proportions of animals, as well as adding fine semantic details, such as shoe laces on an army boot and fine details of a face.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13348v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种仅通过文本提示自动生成输入三角形网格变形的技术。我们的框架能够产生大的低频形状变化和小的高频细节。我们的框架依赖于可微分渲染来将几何体连接到强大的预训练图像编码器，如CLIP和DINO。值得注意的是，通过可微分渲染采取梯度步骤来更新网格几何体是出了名的具有挑战性，通常会导致变形的网格出现明显的伪影。这些困难被来自CLIP的噪声和不一致的梯度放大了。为了克服这一限制，我们选择通过雅可比变换来表示网格变形，雅可比变换以全局、平滑的方式更新变形（而不是局部次优步骤）。我们的关键观察结果是，雅各宾派是一种倾向于更平滑、大变形的表示，从而导致顶点和像素之间的全局关系，并避免局部噪声梯度。此外，为了确保从所有3D视点得到的形状是一致的，我们鼓励在渲染的2D编码上计算的深度特征对于来自所有视点的给定顶点是一致的。我们证明，我们的方法能够平滑地变形各种源网格和目标文本提示，既可以对动物的身体比例进行大的修改，也可以添加精细的语义细节，如军靴上的鞋带和面部的精细细节。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13348v1" target="_blank">2304.13348v1</a>
                              </td>
                              <td>TextDeformer: Geometry Manipulation using Text Guidance</td>
                              <td>William Gao</td>
                              <td>2023-04-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13348v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13348v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_13089v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_13089v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_13089v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance. Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures. These differences arise early in the network and are primarily driven by attention and normalization layers. We find that joint-embedding features yield better linear probe transfer for classification because the different objectives drive different distributions of information and invariances in the learned representation. These differences explain opposite trends in transfer performance for downstream tasks that require spatial specificity in features. Finally, we address how fine-tuning changes reconstructive representations to enable better transfer, showing that fine-tuning re-organizes the information to be more similar to pre-trained joint embedding models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_13089v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于联合嵌入的学习（例如，SimCLR、MoCo、DINO）和基于重建的学习（如，BEiT、SimMIM、MAE）是视觉变换器自我监督学习的两种主要范式，但它们的迁移性能有很大不同。在这里，我们旨在通过分析这些目标对习得表征的结构和可迁移性的影响来解释这些差异。我们的分析表明，基于重建的学习特征与基于联合嵌入的学习特征显著不同，并且以相似目标训练的模型甚至在不同架构中也能学习相似的特征。这些差异出现在网络的早期，主要由注意力和规范化层驱动。我们发现，联合嵌入特征为分类提供了更好的线性探测转移，因为不同的目标驱动学习表示中信息和不变量的不同分布。这些差异解释了需要特征空间特异性的下游任务的转移性能的相反趋势。最后，我们讨论了微调如何改变重建表示以实现更好的传输，表明微调重新组织信息，使其更类似于预训练的联合嵌入模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.13089v1" target="_blank">2304.13089v1</a>
                              </td>
                              <td>Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations</td>
                              <td>Shashank Shekhar</td>
                              <td>2023-04-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_13089v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.13089v1" target="_blank">PDF</a>
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>